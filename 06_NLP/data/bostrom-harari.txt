SuperintelligenceSUPERINTELLIGENCE
Paths, Dangers, Strategies
NICK BOSTROM
Director, Future of Humanity Institute
Professor, Faculty of Philosophy & Oxford Martin School
University of OxfordGreat Clarendon Street, Oxford, OX2 6DP,
United Kingdom
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide. Oxford is a registered trade mark of
Oxford University Press in the UK and in certain other countries
© Nick Bostrom 2014
The moral rights of the author have been asserted
First Edition published in 2014
Impression: 1
All rights reserved. No part of this publication may be reproduced, stored in a
retrieval system, or transmitted, in any form or by any means, without the prior
permission in writing of Oxford University Press, or as expressly permitted by law,
by licence or under terms agreed with the appropriate reprographics rights
organization. Enquiries concerning reproduction outside the scope of the above
should be sent to the Rights Department, Oxford University Press, at the address
above
You must not circulate this work in any other form
and you must impose this same condition on any acquirer
British Library Cataloguing in Publication Data
Data available
Library of Congress Control Number: 2013955152
ISBN 978–0–19–967811–2
Printed in Italy by
L.E.G.O. S.p.A.—Lavis TN
Links to third party websites are provided by Oxford in good faith and for
information only. Oxford disclaims any responsibility for the materials contained inany third party website referenced in this work.The Unfinished Fable of the Sparrows
It was the nest-building season, but after days of long hard work, the sparrows sat in
the evening glow, relaxing and chirping away.
“We are all so small and weak. Imagine how easy life would be if we had an owl
who could help us build our nests!”
“Yes!” said another. “And we could use it to look after our elderly and our
young.”
“It could give us advice and keep an eye out for the neighborhood cat,” added a
third.
Then Pastus, the elder-bird, spoke: “Let us send out scouts in all directions and try
to find an abandoned owlet somewhere, or maybe an egg. A crow chick might also
do, or a baby weasel. This could be the best thing that ever happened to us, at least
since the opening of the Pavilion of Unlimited Grain in yonder backyard.”
The flock was exhilarated, and sparrows everywhere started chirping at the top of
their lungs.
Only Scronkfinkle, a one-eyed sparrow with a fretful temperament, was
unconvinced of the wisdom of the endeavor. Quoth he: “This will surely be our
undoing. Should we not give some thought to the art of owl-domestication and owl-
taming first, before we bring such a creature into our midst?”
Replied Pastus: “Taming an owl sounds like an exceedingly difficult thing to do.
It will be difficult enough to find an owl egg. So let us start there. After we have
succeeded in raising an owl, then we can think about taking on this other challenge.”
“There is a flaw in that plan!” squeaked Scronkfinkle; but his protests were in
vain as the flock had already lifted off to start implementing the directives set out by
Pastus.
Just two or three sparrows remained behind. Together they began to try to work
out how owls might be tamed or domesticated. They soon realized that Pastus had
been right: this was an exceedingly difficult challenge, especially in the absence of
an actual owl to practice on. Nevertheless they pressed on as best they could,
constantly fearing that the flock might return with an owl egg before a solution to
the control problem had been found.
It is not known how the story ends, but the author dedicates this book to
Scronkfinkle and his followers.PREFACE
Inside your cranium is the thing that does the reading. This thing, the human brain,
has some capabilities that the brains of other animals lack. It is to these distinctive
capabilities that we owe our dominant position on the planet. Other animals have
stronger muscles and sharper claws, but we have cleverer brains. Our modest
advantage in general intelligence has led us to develop language, technology, and
complex social organization. The advantage has compounded over time, as each
generation has built on the achievements of its predecessors.
If some day we build machine brains that surpass human brains in general
intelligence, then this new superintelligence could become very powerful. And, as
the fate of the gorillas now depends more on us humans than on the gorillas
themselves, so the fate of our species would depend on the actions of the machine
superintelligence.
We do have one advantage: we get to build the stuff. In principle, we could build a
kind of superintelligence that would protect human values. We would certainly have
strong reason to do so. In practice, the control problem—the problem of how to
control what the superintelligence would do—looks quite difficult. It also looks like
we will only get one chance. Once unfriendly superintelligence exists, it would
prevent us from replacing it or changing its preferences. Our fate would be sealed.
In this book, I try to understand the challenge presented by the prospect of
superintelligence, and how we might best respond. This is quite possibly the most
important and most daunting challenge humanity has ever faced. And—whether we
succeed or fail—it is probably the last challenge we will ever face.
It is no part of the argument in this book that we are on the threshold of a big
breakthrough in artificial intelligence, or that we can predict with any precision
when such a development might occur. It seems somewhat likely that it will happen
sometime in this century, but we don’t know for sure. The first couple of chapters do
discuss possible pathways and say something about the question of timing. The bulk
of the book, however, is about what happens after. We study the kinetics of an
intelligence explosion, the forms and powers of superintelligence, and the strategic
choices available to a superintelligent agent that attains a decisive advantage. We
then shift our focus to the control problem and ask what we could do to shape the
initial conditions so as to achieve a survivable and beneficial outcome. Toward the
end of the book, we zoom out and contemplate the larger picture that emerges from
our investigations. Some suggestions are offered on what ought to be done now to
increase our chances of avoiding an existential catastrophe later.
This has not been an easy book to write. I hope the path that has been cleared willenable other investigators to reach the new frontier more swiftly and conveniently,
so that they can arrive there fresh and ready to join the work to further expand the
reach of our comprehension. (And if the way that has been made is a little bumpy
and bendy, I hope that reviewers, in judging the result, will not underestimate the
hostility of the terrain ex ante!)
This has not been an easy book to write: I have tried to make it an easy book to
read, but I don’t think I have quite succeeded. When writing, I had in mind as the
target audience an earlier time-slice of myself, and I tried to produce a kind of book
that I would have enjoyed reading. This could prove a narrow demographic.
Nevertheless, I think that the content should be accessible to many people, if they
put some thought into it and resist the temptation to instantaneously misunderstand
each new idea by assimilating it with the most similar-sounding cliché available in
their cultural larders. Non-technical readers should not be discouraged by the
occasional bit of mathematics or specialized vocabulary, for it is always possible to
glean the main point from the surrounding explanations. (Conversely, for those
readers who want more of the nitty-gritty, there is quite a lot to be found among the
endnotes. 1 )
Many of the points made in this book are probably wrong. 2 It is also likely that
there are considerations of critical importance that I fail to take into account,
thereby invalidating some or all of my conclusions. I have gone to some length to
indicate nuances and degrees of uncertainty throughout the text—encumbering it
with an unsightly smudge of “possibly,” “might,” “may,” “could well,” “it seems,”
“probably,” “very likely,” “almost certainly.” Each qualifier has been placed where
it is carefully and deliberately. Yet these topical applications of epistemic modesty
are not enough; they must be supplemented here by a systemic admission of
uncertainty and fallibility. This is not false modesty: for while I believe that my
book is likely to be seriously wrong and misleading, I think that the alternative
views that have been presented in the literature are substantially worse—including
the default view, or “null hypothesis,” according to which we can for the time being
safely or reasonably ignore the prospect of superintelligence.ACKNOWLEDGMENTS
The membrane that has surrounded the writing process has been fairly permeable.
Many concepts and ideas generated while working on the book have been allowed to
seep out and have become part of a wider conversation; and, of course, numerous
insights originating from the outside while the book was underway have been
incorporated into the text. I have tried to be somewhat diligent with the citation
apparatus, but the influences are too many to fully document.
For extensive discussions that have helped clarify my thinking I am grateful to a
large set of people, including Ross Andersen, Stuart Armstrong, Owen Cotton-
Barratt, Nick Beckstead, David Chalmers, Paul Christiano, Milan Ćirković, Daniel
Dennett, David Deutsch, Daniel Dewey, Eric Drexler, Peter Eckersley, Amnon Eden,
Owain Evans, Benja Fallenstein, Alex Flint, Carl Frey, Ian Goldin, Katja Grace, J.
Storrs Hall, Robin Hanson, Demis Hassabis, James Hughes, Marcus Hutter, Garry
Kasparov, Marcin Kulczycki, Shane Legg, Moshe Looks, Willam MacAskill, Eric
Mandelbaum, James Martin, Lillian Martin, Roko Mijic, Vincent Mueller, Elon
Musk, Seán Ó hÉigeartaigh, Toby Ord, Dennis Pamlin, Derek Parfit, David Pearce,
Huw Price, Martin Rees, Bill Roscoe, Stuart Russell, Anna Salamon, Lou Salkind,
Anders Sandberg, Julian Savulescu, Jürgen Schmidhuber, Nicholas Shackel, Murray
Shanahan, Noel Sharkey, Carl Shulman, Peter Singer, Dan Stoicescu, Jaan Tallinn,
Alexander Tamas, Max Tegmark, Roman Yampolskiy, and Eliezer Yudkowsky.
For especially detailed comments, I am grateful to Milan Ćirković, Daniel Dewey,
Owain Evans, Nick Hay, Keith Mansfield, Luke Muehlhauser, Toby Ord, Jess
Riedel, Anders Sandberg, Murray Shanahan, and Carl Shulman. For advice or
research help with different parts I want to thank Stuart Armstrong, Daniel Dewey,
Eric Drexler, Alexandre Erler, Rebecca Roache, and Anders Sandberg.
For help with preparing the manuscript, I am thankful to Caleb Bell, Malo
Bourgon, Robin Brandt, Lance Bush, Cathy Douglass, Alexandre Erler, Kristian
Rönn, Susan Rogers, Andrew Snyder-Beattie, Cecilia Tilli, and Alex Vermeer. I
want particularly to thank my editor Keith Mansfield for his plentiful
encouragement throughout the project.
My apologies to everybody else who ought to have been remembered here.
Finally, a most fond thank you to funders, friends, and family: without your
backing, this work would not have been done.CONTENTS
Lists of Figures, Tables, and Boxes
1. Past developments and present capabilities
Growth modes and big history
Great expectations
Seasons of hope and despair
State of the art
Opinions about the future of machine intelligence
2. Paths to superintelligence
Artificial intelligence
Whole brain emulation
Biological cognition
Brain–computer interfaces
Networks and organizations
Summary
3. Forms of superintelligence
Speed superintelligence
Collective superintelligence
Quality superintelligence
Direct and indirect reach
Sources of advantage for digital intelligence
4. The kinetics of an intelligence explosion
Timing and speed of the takeoff
Recalcitrance
Non-machine intelligence paths
Emulation and AI paths
Optimization power and explosivity
5. Decisive strategic advantage
Will the frontrunner get a decisive strategic advantage?
How large will the successful project be?
Monitoring
International collaborationFrom decisive strategic advantage to singleton
6. Cognitive superpowers
Functionalities and superpowers
An AI takeover scenario
Power over nature and agents
7. The superintelligent will
The relation between intelligence and motivation
Instrumental convergence
Self-preservation
Goal-content integrity
Cognitive enhancement
Technological perfection
Resource acquisition
8. Is the default outcome doom?
Existential catastrophe as the default outcome of an intelligence explosion?
The treacherous turn
Malignant failure modes
Perverse instantiation
Infrastructure profusion
Mind crime
9. The control problem
Two agency problems
Capability control methods
Boxing methods
Incentive methods
Stunting
Tripwires
Motivation selection methods
Direct specification
Domesticity
Indirect normativity
Augmentation
Synopsis
10. Oracles, genies, sovereigns, tools
OraclesGenies and sovereigns
Tool-AIs
Comparison
11. Multipolar scenarios
Of horses and men
Wages and unemployment
Capital and welfare
The Malthusian principle in a historical perspective
Population growth and investment
Life in an algorithmic economy
Voluntary slavery, casual death
Would maximally efficient work be fun?
Unconscious outsourcers?
Evolution is not necessarily up
Post-transition formation of a singleton?
A second transition
Superorganisms and scale economies
Unification by treaty
12. Acquiring values
The value-loading problem
Evolutionary selection
Reinforcement learning
Associative value accretion
Motivational scaffolding
Value learning
Emulation modulation
Institution design
Synopsis
13. Choosing the criteria for choosing
The need for indirect normativity
Coherent extrapolated volition
Some explications
Rationales for CEV
Further remarks
Morality modelsDo What I Mean
Component list
Goal content
Decision theory
Epistemology
Ratification
Getting close enough
14. The strategic picture
Science and technology strategy
Differential technological development
Preferred order of arrival
Rates of change and cognitive enhancement
Technology couplings
Second-guessing
Pathways and enablers
Effects of hardware progress
Should whole brain emulation research be promoted?
The person-affecting perspective favors speed
Collaboration
The race dynamic and its perils
On the benefits of collaboration
Working together
15. Crunch time
Philosophy with a deadline
What is to be done?
Seeking the strategic light
Building good capacity
Particular measures
Will the best in human nature please stand up
Notes
Bibliography
IndexLISTS OF FIGURES, TABLES, AND BOXES
List of Figures
1. Long-term history of world GDP.
2. Overall long-term impact of HLMI.
3. Supercomputer performance.
4. Reconstructing 3D neuroanatomy from electron microscope images.
5. Whole brain emulation roadmap.
6. Composite faces as a metaphor for spell-checked genomes.
7. Shape of the takeoff.
8. A less anthropomorphic scale?
9. One simple model of an intelligence explosion.
10. Phases in an AI takeover scenario.
11. Schematic illustration of some possible trajectories for a hypothetical wise
singleton.
12. Results of anthropomorphizing alien motivation.
13. Artificial intelligence or whole brain emulation first?
14. Risk levels in AI technology races.
List of Tables
1. Game-playing AI
2. When will human-level machine intelligence be attained?
3. How long from human level to superintelligence?
4. Capabilities needed for whole brain emulation
5. Maximum IQ gains from selecting among a set of embryos
6. Possible impacts from genetic selection in different scenarios
7. Some strategically significant technology races
8. Superpowers: some strategically relevant tasks and corresponding skill sets
9. Different kinds of tripwires
10. Control methods
11. Features of different system castes
12. Summary of value-loading techniques13. Component list
List of Boxes
1. An optimal Bayesian agent
2. The 2010 Flash Crash
3. What would it take to recapitulate evolution?
4. On the kinetics of an intelligence explosion
5. Technology races: some historical examples
6. The mail-ordered DNA scenario
7. How big is the cosmic endowment?
8. Anthropic capture
9. Strange solutions from blind search
10. Formalizing value learning
11. An AI that wants to be friendly
12. Two recent (half-baked) ideas
13. A risk-race to the bottomCHAPTER 1
Past developments and present capabilities
We begin by looking back. History, at the largest scale, seems to exhibit a
sequence of distinct growth modes, each much more rapid than its predecessor.
This pattern has been taken to suggest that another (even faster) growth mode
might be possible. However, we do not place much weight on this observation—
this is not a book about “technological acceleration” or “exponential growth” or
the miscellaneous notions sometimes gathered under the rubric of “the
singularity.” Next, we review the history of artificial intelligence. We then
survey the field’s current capabilities. Finally, we glance at some recent expert
opinion surveys, and contemplate our ignorance about the timeline of future
advances.
Growth modes and big history
A mere few million years ago our ancestors were still swinging from the branches in
the African canopy. On a geological or even evolutionary timescale, the rise of
Homo sapiens from our last common ancestor with the great apes happened swiftly.
We developed upright posture, opposable thumbs, and—crucially—some relatively
minor changes in brain size and neurological organization that led to a great leap in
cognitive ability. As a consequence, humans can think abstractly, communicate
complex thoughts, and culturally accumulate information over the generations far
better than any other species on the planet.
These capabilities let humans develop increasingly efficient productive
technologies, making it possible for our ancestors to migrate far away from the
rainforest and the savanna. Especially after the adoption of agriculture, population
densities rose along with the total size of the human population. More people meant
more ideas; greater densities meant that ideas could spread more readily and that
some individuals could devote themselves to developing specialized skills. These
developments increased the rate of growth of economic productivity and
technological capacity. Later developments, related to the Industrial Revolution,
brought about a second, comparable step change in the rate of growth.
Such changes in the rate of growth have important consequences. A few hundred
thousand years ago, in early human (or hominid) prehistory, growth was so slow that
it took on the order of one million years for human productive capacity to increasesufficiently to sustain an additional one million individuals living at subsistence
level. By 5000 BC , following the Agricultural Revolution, the rate of growth had
increased to the point where the same amount of growth took just two centuries.
Today, following the Industrial Revolution, the world economy grows on average by
that amount every ninety minutes. 1
Even the present rate of growth will produce impressive results if maintained for a
moderately long time. If the world economy continues to grow at the same pace as it
has over the past fifty years, then the world will be some 4.8 times richer by 2050
and about 34 times richer by 2100 than it is today. 2
Yet the prospect of continuing on a steady exponential growth path pales in
comparison to what would happen if the world were to experience another step
change in the rate of growth comparable in magnitude to those associated with the
Agricultural Revolution and the Industrial Revolution. The economist Robin Hanson
estimates, based on historical economic and population data, a characteristic world
economy doubling time for Pleistocene hunter–gatherer society of 224,000 years; for
farming society, 909 years; and for industrial society, 6.3 years. 3 (In Hanson’s
model, the present epoch is a mixture of the farming and the industrial growth
modes—the world economy as a whole is not yet growing at the 6.3-year doubling
rate.) If another such transition to a different growth mode were to occur, and it were
of similar magnitude to the previous two, it would result in a new growth regime in
which the world economy would double in size about every two weeks.
Such a growth rate seems fantastic by current lights. Observers in earlier epochs
might have found it equally preposterous to suppose that the world economy would
one day be doubling several times within a single lifespan. Yet that is the
extraordinary condition we now take to be ordinary.
The idea of a coming technological singularity has by now been widely
popularized, starting with Vernor Vinge’s seminal essay and continuing with the
writings of Ray Kurzweil and others. 4 The term “singularity,” however, has been
used confusedly in many disparate senses and has accreted an unholy (yet almost
millenarian) aura of techno-utopian connotations. 5 Since most of these meanings
and connotations are irrelevant to our argument, we can gain clarity by dispensing
with the “singularity” word in favor of more precise terminology.
The singularity-related idea that interests us here is the possibility of an
intelligence explosion, particularly the prospect of machine superintelligence. There
may be those who are persuaded by growth diagrams like the ones in Figure 1 that
another drastic change in growth mode is in the cards, comparable to the
Agricultural or Industrial Revolution. These folk may then reflect that it is hard to
conceive of a scenario in which the world economy’s doubling time shortens to mere
weeks that does not involve the creation of minds that are much faster and more
efficient than the familiar biological kind. However, the case for taking seriously theprospect of a machine intelligence revolution need not rely on curve-fitting
exercises or extrapolations from past economic growth. As we shall see, there are
stronger reasons for taking heed.
Figure 1 Long-term history of world GDP. Plotted on a linear scale, the history of
the world economy looks like a flat line hugging the x-axis, until it suddenly spikes
vertically upward. (a) Even when we zoom in on the most recent 10,000 years, the
pattern remains essentially one of a single 90° angle. (b) Only within the past 100
years or so does the curve lift perceptibly above the zero-level. (The different lines
in the plot correspond to different data sets, which yield slightly different
estimates. 6 )
Great expectationsMachines matching humans in general intelligence—that is, possessing common
sense and an effective ability to learn, reason, and plan to meet complex
information-processing challenges across a wide range of natural and abstract
domains—have been expected since the invention of computers in the 1940s. At that
time, the advent of such machines was often placed some twenty years into the
future. 7 Since then, the expected arrival date has been receding at a rate of one year
per year; so that today, futurists who concern themselves with the possibility of
artificial general intelligence still often believe that intelligent machines are a
couple of decades away. 8
Two decades is a sweet spot for prognosticators of radical change: near enough to
be attention-grabbing and relevant, yet far enough to make it possible to suppose
that a string of breakthroughs, currently only vaguely imaginable, might by then
have occurred. Contrast this with shorter timescales: most technologies that will
have a big impact on the world in five or ten years from now are already in limited
use, while technologies that will reshape the world in less than fifteen years
probably exist as laboratory prototypes. Twenty years may also be close to the
typical duration remaining of a forecaster’s career, bounding the reputational risk of
a bold prediction.
From the fact that some individuals have overpredicted artificial intelligence in
the past, however, it does not follow that AI is impossible or will never be
developed. 9 The main reason why progress has been slower than expected is that the
technical difficulties of constructing intelligent machines have proved greater than
the pioneers foresaw. But this leaves open just how great those difficulties are and
how far we now are from overcoming them. Sometimes a problem that initially
looks hopelessly complicated turns out to have a surprisingly simple solution
(though the reverse is probably more common).
In the next chapter, we will look at different paths that may lead to human-level
machine intelligence. But let us note at the outset that however many stops there are
between here and human-level machine intelligence, the latter is not the final
destination. The next stop, just a short distance farther along the tracks, is
superhuman-level machine intelligence. The train might not pause or even
decelerate at Humanville Station. It is likely to swoosh right by.
The mathematician I. J. Good, who had served as chief statistician in Alan
Turing’s code-breaking team in World War II, might have been the first to enunciate
the essential aspects of this scenario. In an oft-quoted passage from 1965, he wrote:
Let an ultraintelligent machine be defined as a machine that can far surpass all
the intellectual activities of any man however clever. Since the design of
machines is one of these intellectual activities, an ultraintelligent machine
could design even better machines; there would then unquestionably be an“intelligence explosion,” and the intelligence of man would be left far behind.
Thus the first ultraintelligent machine is the last invention that man need ever
make, provided that the machine is docile enough to tell us how to keep it under
control. 10
It may seem obvious now that major existential risks would be associated with such
an intelligence explosion, and that the prospect should therefore be examined with
the utmost seriousness even if it were known (which it is not) to have but a
moderately small probability of coming to pass. The pioneers of artificial
intelligence, however, notwithstanding their belief in the imminence of human-level
AI, mostly did not contemplate the possibility of greater-than-human AI. It is as
though their speculation muscle had so exhausted itself in conceiving the radical
possibility of machines reaching human intelligence that it could not grasp the
corollary—that machines would subsequently become superintelligent.
The AI pioneers for the most part did not countenance the possibility that their
enterprise might involve risk. 11 They gave no lip service—let alone serious thought
—to any safety concern or ethical qualm related to the creation of artificial minds
and potential computer overlords: a lacuna that astonishes even against the
background of the era’s not-so-impressive standards of critical technology
assessment. 12 We must hope that by the time the enterprise eventually does become
feasible, we will have gained not only the technological proficiency to set off an
intelligence explosion but also the higher level of mastery that may be necessary to
make the detonation survivable.
But before we turn to what lies ahead, it will be useful to take a quick glance at
the history of machine intelligence to date.
Seasons of hope and despair
In the summer of 1956 at Dartmouth College, ten scientists sharing an interest in
neural nets, automata theory, and the study of intelligence convened for a six-week
workshop. This Dartmouth Summer Project is often regarded as the cockcrow of
artificial intelligence as a field of research. Many of the participants would later be
recognized as founding figures. The optimistic outlook among the delegates is
reflected in the proposal submitted to the Rockefeller Foundation, which provided
funding for the event:
We propose that a 2 month, 10 man study of artificial intelligence be carried
out.... The study is to proceed on the basis of the conjecture that every aspect oflearning or any other feature of intelligence can in principle be so precisely
described that a machine can be made to simulate it. An attempt will be made
to find how to make machines that use language, form abstractions and
concepts, solve kinds of problems now reserved for humans, and improve
themselves. We think that a significant advance can be made in one or more of
these problems if a carefully selected group of scientists work on it together for
a summer.
In the six decades since this brash beginning, the field of artificial intelligence has
been through periods of hype and high expectations alternating with periods of
setback and disappointment.
The first period of excitement, which began with the Dartmouth meeting, was
later described by John McCarthy (the event’s main organizer) as the “Look, Ma, no
hands!” era. During these early days, researchers built systems designed to refute
claims of the form “No machine could ever do X!” Such skeptical claims were
common at the time. To counter them, the AI researchers created small systems that
achieved X in a “microworld” (a well-defined, limited domain that enabled a pared-
down version of the performance to be demonstrated), thus providing a proof of
concept and showing that X could, in principle, be done by machine. One such early
system, the Logic Theorist, was able to prove most of the theorems in the second
chapter of Whitehead and Russell’s Principia Mathematica, and even came up with
one proof that was much more elegant than the original, thereby debunking the
notion that machines could “only think numerically” and showing that machines
were also able to do deduction and to invent logical proofs. 13 A follow-up program,
the General Problem Solver, could in principle solve a wide range of formally
specified problems. 14 Programs that could solve calculus problems typical of first-
year college courses, visual analogy problems of the type that appear in some IQ
tests, and simple verbal algebra problems were also written. 15 The Shakey robot (so
named because of its tendency to tremble during operation) demonstrated how
logical reasoning could be integrated with perception and used to plan and control
physical activity. 16 The ELIZA program showed how a computer could impersonate
a Rogerian psychotherapist. 17 In the mid-seventies, the program SHRDLU showed
how a simulated robotic arm in a simulated world of geometric blocks could follow
instructions and answer questions in English that were typed in by a user. 18 In later
decades, systems would be created that demonstrated that machines could compose
music in the style of various classical composers, outperform junior doctors in
certain clinical diagnostic tasks, drive cars autonomously, and make patentable
inventions. 19 There has even been an AI that cracked original jokes. 20 (Not that its
level of humor was high—“What do you get when you cross an optic with a mentalobject? An eye-dea”—but children reportedly found its puns consistently
entertaining.)
The methods that produced successes in the early demonstration systems often
proved difficult to extend to a wider variety of problems or to harder problem
instances. One reason for this is the “combinatorial explosion” of possibilities that
must be explored by methods that rely on something like exhaustive search. Such
methods work well for simple instances of a problem, but fail when things get a bit
more complicated. For instance, to prove a theorem that has a 5-line long proof in a
deduction system with one inference rule and 5 axioms, one could simply enumerate
the 3,125 possible combinations and check each one to see if it delivers the intended
conclusion. Exhaustive search would also work for 6- and 7-line proofs. But as the
task becomes more difficult, the method of exhaustive search soon runs into trouble.
Proving a theorem with a 50-line proof does not take ten times longer than proving a
theorem that has a 5-line proof: rather, if one uses exhaustive search, it requires
combing through 5 50 ≈ 8.9 × 10 34 possible sequences—which is computationally
infeasible even with the fastest supercomputers.
To overcome the combinatorial explosion, one needs algorithms that exploit
structure in the target domain and take advantage of prior knowledge by using
heuristic search, planning, and flexible abstract representations—capabilities that
were poorly developed in the early AI systems. The performance of these early
systems also suffered because of poor methods for handling uncertainty, reliance on
brittle and ungrounded symbolic representations, data scarcity, and severe hardware
limitations on memory capacity and processor speed. By the mid-1970s, there was a
growing awareness of these problems. The realization that many AI projects could
never make good on their initial promises led to the onset of the first “AI winter”: a
period of retrenchment, during which funding decreased and skepticism increased,
and AI fell out of fashion.
A new springtime arrived in the early 1980s, when Japan launched its Fifth-
Generation Computer Systems Project, a well-funded public–private partnership that
aimed to leapfrog the state of the art by developing a massively parallel computing
architecture that would serve as a platform for artificial intelligence. This occurred
at peak fascination with the Japanese “post-war economic miracle,” a period when
Western government and business leaders anxiously sought to divine the formula
behind Japan’s economic success in hope of replicating the magic at home. When
Japan decided to invest big in AI, several other countries followed suit.
The ensuing years saw a great proliferation of expert systems. Designed as support
tools for decision makers, expert systems were rule-based programs that made
simple inferences from a knowledge base of facts, which had been elicited from
human domain experts and painstakingly hand-coded in a formal language.
Hundreds of these expert systems were built. However, the smaller systems providedlittle benefit, and the larger ones proved expensive to develop, validate, and keep
updated, and were generally cumbersome to use. It was impractical to acquire a
standalone computer just for the sake of running one program. By the late 1980s,
this growth season, too, had run its course.
The Fifth-Generation Project failed to meet its objectives, as did its counterparts
in the United States and Europe. A second AI winter descended. At this point, a
critic could justifiably bemoan “the history of artificial intelligence research to date,
consisting always of very limited success in particular areas, followed immediately
by failure to reach the broader goals at which these initial successes seem at first to
hint.” 21 Private investors began to shun any venture carrying the brand of “artificial
intelligence.” Even among academics and their funders, “AI” became an unwanted
epithet. 22
Technical work continued apace, however, and by the 1990s, the second AI winter
gradually thawed. Optimism was rekindled by the introduction of new techniques,
which seemed to offer alternatives to the traditional logicist paradigm (often
referred to as “Good Old-Fashioned Artificial Intelligence,” or “GOFAI” for short),
which had focused on high-level symbol manipulation and which had reached its
apogee in the expert systems of the 1980s. The newly popular techniques, which
included neural networks and genetic algorithms, promised to overcome some of the
shortcomings of the GOFAI approach, in particular the “brittleness” that
characterized classical AI programs (which typically produced complete nonsense if
the programmers made even a single slightly erroneous assumption). The new
techniques boasted a more organic performance. For example, neural networks
exhibited the property of “graceful degradation”: a small amount of damage to a
neural network typically resulted in a small degradation of its performance, rather
than a total crash. Even more importantly, neural networks could learn from
experience, finding natural ways of generalizing from examples and finding hidden
statistical patterns in their input. 23 This made the nets good at pattern recognition
and classification problems. For example, by training a neural network on a data set
of sonar signals, it could be taught to distinguish the acoustic profiles of submarines,
mines, and sea life with better accuracy than human experts—and this could be done
without anybody first having to figure out in advance exactly how the categories
were to be defined or how different features were to be weighted.
While simple neural network models had been known since the late 1950s, the
field enjoyed a renaissance after the introduction of the backpropagation algorithm,
which made it possible to train multi-layered neural networks. 24 Such multilayered
networks, which have one or more intermediary (“hidden”) layers of neurons
between the input and output layers, can learn a much wider range of functions than
their simpler predecessors. 25 Combined with the increasingly powerful computers
that were becoming available, these algorithmic improvements enabled engineers tobuild neural networks that were good enough to be practically useful in many
applications.
The brain-like qualities of neural networks contrasted favorably with the rigidly
logic-chopping but brittle performance of traditional rule-based GOFAI systems—
enough so to inspire a new “-ism,” connectionism, which emphasized the importance
of massively parallel sub-symbolic processing. More than 150,000 academic papers
have since been published on artificial neural networks, and they continue to be an
important approach in machine learning.
Evolution-based methods, such as genetic algorithms and genetic programming,
constitute another approach whose emergence helped end the second AI winter. It
made perhaps a smaller academic impact than neural nets but was widely
popularized. In evolutionary models, a population of candidate solutions (which can
be data structures or programs) is maintained, and new candidate solutions are
generated randomly by mutating or recombining variants in the existing population.
Periodically, the population is pruned by applying a selection criterion (a fitness
function) that allows only the better candidates to survive into the next generation.
Iterated over thousands of generations, the average quality of the solutions in the
candidate pool gradually increases. When it works, this kind of algorithm can
produce efficient solutions to a very wide range of problems—solutions that may be
strikingly novel and unintuitive, often looking more like natural structures than
anything that a human engineer would design. And in principle, this can happen
without much need for human input beyond the initial specification of the fitness
function, which is often very simple. In practice, however, getting evolutionary
methods to work well requires skill and ingenuity, particularly in devising a good
representational format. Without an efficient way to encode candidate solutions (a
genetic language that matches latent structure in the target domain), evolutionary
search tends to meander endlessly in a vast search space or get stuck at a local
optimum. Even if a good representational format is found, evolution is
computationally demanding and is often defeated by the combinatorial explosion.
Neural networks and genetic algorithms are examples of methods that stimulated
excitement in the 1990s by appearing to offer alternatives to the stagnating GOFAI
paradigm. But the intention here is not to sing the praises of these two methods or to
elevate them above the many other techniques in machine learning. In fact, one of
the major theoretical developments of the past twenty years has been a clearer
realization of how superficially disparate techniques can be understood as special
cases within a common mathematical framework. For example, many types of
artificial neural network can be viewed as classifiers that perform a particular kind
of statistical calculation (maximum likelihood estimation). 26 This perspective
allows neural nets to be compared with a larger class of algorithms for learning
classifiers from examples—“decision trees,” “logistic regression models,” “supportvector machines,” “naive Bayes,” “k-nearest-neighbors regression,” among others. 27
In a similar manner, genetic algorithms can be viewed as performing stochastic hill-
climbing, which is again a subset of a wider class of algorithms for optimization.
Each of these algorithms for building classifiers or for searching a solution space
has its own profile of strengths and weaknesses which can be studied
mathematically. Algorithms differ in their processor time and memory space
requirements, which inductive biases they presuppose, the ease with which
externally produced content can be incorporated, and how transparent their inner
workings are to a human analyst.
Behind the razzle-dazzle of machine learning and creative problem-solving thus
lies a set of mathematically well-specified tradeoffs. The ideal is that of the perfect
Bayesian agent, one that makes probabilistically optimal use of available
information. This ideal is unattainable because it is too computationally demanding
to be implemented in any physical computer (see Box 1). Accordingly, one can view
artificial intelligence as a quest to find shortcuts: ways of tractably approximating
the Bayesian ideal by sacrificing some optimality or generality while preserving
enough to get high performance in the actual domains of interest.
A reflection of this picture can be seen in the work done over the past couple of
decades on probabilistic graphical models, such as Bayesian networks. Bayesian
networks provide a concise way of representing probabilistic and conditional
independence relations that hold in some particular domain. (Exploiting such
independence relations is essential for overcoming the combinatorial explosion,
which is as much of a problem for probabilistic inference as it is for logical
deduction.) They also provide important insight into the concept of causality. 28
One advantage of relating learning problems from specific domains to the general
problem of Bayesian inference is that new algorithms that make Bayesian inference
more efficient will then yield immediate improvements across many different areas.
Advances in Monte Carlo approximation techniques, for example, are directly
applied in computer vision, robotics, and computational genetics. Another advantage
is that it lets researchers from different disciplines more easily pool their findings.
Graphical models and Bayesian statistics have become a shared focus of research in
many fields, including machine learning, statistical physics, bioinformatics,
combinatorial optimization, and communication theory. 35 A fair amount of the
recent progress in machine learning has resulted from incorporating formal results
originally derived in other academic fields. (Machine learning applications have also
benefitted enormously from faster computers and greater availability of large data
sets.)
Box 1 An optimal Bayesian agentAn ideal Bayesian agent starts out with a “prior probability distribution,” a function
that assigns probabilities to each “possible world” (i.e. to each maximally specific
way the world could turn out to be). 29 This prior incorporates an inductive bias such
that simpler possible worlds are assigned higher probabilities. (One way to formally
define the simplicity of a possible world is in terms of its “Kolmogorov
complexity,” a measure based on the length of the shortest computer program that
generates a complete description of the world. 30 ) The prior also incorporates any
background knowledge that the programmers wish to give to the agent.
As the agent receives new information from its sensors, it updates its probability
distribution by conditionalizing the distribution on the new information according to
Bayes’ theorem. 31 Conditionalization is the mathematical operation that sets the
new probability of those worlds that are inconsistent with the information received
to zero and renormalizes the probability distribution over the remaining possible
worlds. The result is a “posterior probability distribution” (which the agent may use
as its new prior in the next time step). As the agent makes observations, its
probability mass thus gets concentrated on the shrinking set of possible worlds that
remain consistent with the evidence; and among these possible worlds, simpler ones
always have more probability.
Metaphorically, we can think of a probability as sand on a large sheet of paper.
The paper is partitioned into areas of various sizes, each area corresponding to one
possible world, with larger areas corresponding to simpler possible worlds. Imagine
also a layer of sand of even thickness spread across the entire sheet: this is our prior
probability distribution. Whenever an observation is made that rules out some
possible worlds, we remove the sand from the corresponding areas of the paper and
redistribute it evenly over the areas that remain in play. Thus, the total amount of
sand on the sheet never changes, it just gets concentrated into fewer areas as
observational evidence accumulates. This is a picture of learning in its purest form.
(To calculate the probability of a hypothesis, we simply measure the amount of sand
in all the areas that correspond to the possible worlds in which the hypothesis is
true.)
So far, we have defined a learning rule. To get an agent, we also need a decision
rule. To this end, we endow the agent with a “utility function” which assigns a
number to each possible world. The number represents the desirability of that world
according to the agent’s basic preferences. Now, at each time step, the agent selects
the action with the highest expected utility. 32 (To find the action with the highest
expected utility, the agent could list all possible actions. It could then compute the
conditional probability distribution given the action—the probability distribution
that would result from conditionalizing its current probability distribution on the
observation that the action had just been taken. Finally, it could calculate theexpected value of the action as the sum of the value of each possible world
multiplied by the conditional probability of that world given the action. 33 )
The learning rule and the decision rule together define an “optimality notion” for
an agent. (Essentially the same optimality notion has been broadly used in artificial
intelligence, epistemology, philosophy of science, economics, and statistics. 34 ) In
reality, it is impossible to build such an agent because it is computationally
intractable to perform the requisite calculations. Any attempt to do so succumbs to a
combinatorial explosion just like the one described in our discussion of GOFAI. To
see why this is so, consider one tiny subset of all possible worlds: those that consist
of a single computer monitor floating in an endless vacuum. The monitor has 1, 000
× 1, 000 pixels, each of which is perpetually either on or off. Even this subset of
possible worlds is enormously large: the 2 (1,000 × 1,000) possible monitor states
outnumber all the computations expected ever to take place in the observable
universe. Thus, we could not even enumerate all the possible worlds in this tiny
subset of all possible worlds, let alone perform more elaborate computations on each
of them individually.
Optimality notions can be of theoretical interest even if they are physically
unrealizable. They give us a standard by which to judge heuristic approximations,
and sometimes we can reason about what an optimal agent would do in some special
case. We will encounter some alternative optimality notions for artificial agents in
Chapter 12.
State of the art
Artificial intelligence already outperforms human intelligence in many domains.
Table 1 surveys the state of game-playing computers, showing that AIs now beat
human champions in a wide range of games. 36
These achievements might not seem impressive today. But this is because our
standards for what is impressive keep adapting to the advances being made. Expert
chess playing, for example, was once thought to epitomize human intellection. In the
view of several experts in the late fifties: “If one could devise a successful chess
machine, one would seem to have penetrated to the core of human intellectual
endeavor.” 55 This no longer seems so. One sympathizes with John McCarthy, who
lamented: “As soon as it works, no one calls it AI anymore.” 56
Table 1 Game-playing AIArthur Samuel’s checkers program, originally written
in 1952 and later improved (the 1955 version
incorporating machine learning), becomes the first
program to learn to play a game better than its
creator. 37 In 1994, the program CHINOOK beats the
Checkers
Superhuman reigning human champion, marking the first time a
program wins an official world championship in a
game of skill. In 2002, Jonathan Schaeffer and his team
“solve” checkers, i.e. produce a program that always
makes the best possible move (combining alpha-beta
search with a database of 39 trillion endgame
positions). Perfect play by both sides leads to a draw. 38
1979: The backgammon program BKG by Hans
Berliner defeats the world champion—the first
Backgammon Superhuman computer program to defeat (in an exhibition match) a
world champion in any game—though Berliner later
attributes the win to luck with the dice rolls. 39
1992: The backgammon program TD-Gammon by
Gerry Tesauro reaches championship-level ability,
using temporal difference learning (a form of
reinforcement learning) and repeated plays against
itself to improve. 40
In the years since, backgammon programs have far
surpassed the best human players. 41
In both 1981 and 1982, Douglas Lenat’s program
Superhuman
Eurisko wins the US championship in Traveller TCS (a
in
futuristic naval war game), prompting rule changes to
Traveller
collaboration
block its unorthodox strategies. 43 Eurisko had
TCS
with
heuristics for designing its fleet, and it also had
human 42
heuristics for modifying its heuristics.
Othello
Chess
1997: The program Logistello wins every game in a
Superhuman six-game match against world champion Takeshi
Murakami. 44
1997: Deep Blue beats the world chess champion,
Garry Kasparov. Kasparov claims to have seen
Superhuman glimpses of true intelligence and creativity in some of
the computer’s moves. 45 Since then, chess engines
have continued to improve. 46Crosswords
Scrabble
Bridge
Jeopardy!
Poker
FreeCell
Go
Expert level 1999: The crossword-solving program Proverb
outperforms the average crossword-solver. 47
2012: The program Dr. Fill, created by Matt Ginsberg,
scores in the top quartile among the otherwise human
contestants in the American Crossword Puzzle
Tournament. (Dr. Fill’s performance is uneven. It
completes perfectly the puzzle rated most difficult by
humans, yet is stumped by a couple of nonstandard
puzzles that involved spelling backwards or writing
answers diagonally.) 48
As of 2002, Scrabble-playing software surpasses the
Superhuman
best human players. 49
Equal to the By 2005, contract bridge playing software reaches
best
parity with the best human bridge players. 50
2010: IBM’s Watson defeats the two all-time-greatest
human Jeopardy! champions, Ken Jennings and Brad
Rutter. 51 Jeopardy! is a televised game show with trivia
Superhuman questions about history, literature, sports, geography,
pop culture, science, and other topics. Questions are
presented in the form of clues, and often involve
wordplay.
Computer poker players remain slightly below the best
humans for full-ring Texas hold ‘em but perform at a
Varied
superhuman level in some poker variants. 52
Heuristics evolved using genetic algorithms produce a
solver for the solitaire game FreeCell (which in its
Superhuman
generalized form is NP-complete) that is able to beat
high-ranking human players. 53
As of 2012, the Zen series of go-playing programs has
reached rank 6 dan in fast games (the level of a very
strong amateur player), using Monte Carlo tree search
Very strong
and machine learning techniques. 54 Go-playing
amateur
programs have been improving at a rate of about 1
level
dan/year in recent years. If this rate of improvement
continues, they might beat the human world champion
in about a decade.
There is an important sense, however, in which chess-playing AI turned out to be
a lesser triumph than many imagined it would be. It was once supposed, perhaps notunreasonably, that in order for a computer to play chess at grandmaster level, it
would have to be endowed with a high degree of general intelligence. 57 One might
have thought, for example, that great chess playing requires being able to learn
abstract concepts, think cleverly about strategy, compose flexible plans, make a
wide range of ingenious logical deductions, and maybe even model one’s opponent’s
thinking. Not so. It turned out to be possible to build a perfectly fine chess engine
around a special-purpose algorithm. 58 When implemented on the fast processors that
became available towards the end of the twentieth century, it produces very strong
play. But an AI built like that is narrow. It plays chess; it can do no other. 59
In other domains, solutions have turned out to be more complicated than initially
expected, and progress slower. The computer scientist Donald Knuth was struck that
“AI has by now succeeded in doing essentially everything that requires ‘thinking’
but has failed to do most of what people and animals do ‘without thinking’—that,
somehow, is much harder!” 60 Analyzing visual scenes, recognizing objects, or
controlling a robot’s behavior as it interacts with a natural environment has proved
challenging. Nevertheless, a fair amount of progress has been made and continues to
be made, aided by steady improvements in hardware.
Common sense and natural language understanding have also turned out to be
difficult. It is now often thought that achieving a fully human-level performance on
these tasks is an “AI-complete” problem, meaning that the difficulty of solving these
problems is essentially equivalent to the difficulty of building generally human-
level intelligent machines. 61 In other words, if somebody were to succeed in creating
an AI that could understand natural language as well as a human adult, they would in
all likelihood also either already have succeeded in creating an AI that could do
everything else that human intelligence can do, or they would be but a very short
step from such a general capability. 62
Chess-playing expertise turned out to be achievable by means of a surprisingly
simple algorithm. It is tempting to speculate that other capabilities—such as general
reasoning ability, or some key ability involved in programming—might likewise be
achievable through some surprisingly simple algorithm. The fact that the best
performance at one time is attained through a complicated mechanism does not
mean that no simple mechanism could do the job as well or better. It might simply
be that nobody has yet found the simpler alternative. The Ptolemaic system (with the
Earth in the center, orbited by the Sun, the Moon, planets, and stars) represented the
state of the art in astronomy for over a thousand years, and its predictive accuracy
was improved over the centuries by progressively complicating the model: adding
epicycles upon epicycles to the postulated celestial motions. Then the entire system
was overthrown by the heliocentric theory of Copernicus, which was simpler and—
though only after further elaboration by Kepler—more predictively accurate. 63
Artificial intelligence methods are now used in more areas than it would makesense to review here, but mentioning a sampling of them will give an idea of the
breadth of applications. Aside from the game AIs listed in Table 1, there are hearing
aids with algorithms that filter out ambient noise; route-finders that display maps
and offer navigation advice to drivers; recommender systems that suggest books and
music albums based on a user’s previous purchases and ratings; and medical
decision support systems that help doctors diagnose breast cancer, recommend
treatment plans, and aid in the interpretation of electrocardiograms. There are
robotic pets and cleaning robots, lawn-mowing robots, rescue robots, surgical robots,
and over a million industrial robots. 64 The world population of robots exceeds 10
million. 65
Modern speech recognition, based on statistical techniques such as hidden Markov
models, has become sufficiently accurate for practical use (some fragments of this
book were drafted with the help of a speech recognition program). Personal digital
assistants, such as Apple’s Siri, respond to spoken commands and can answer simple
questions and execute commands. Optical character recognition of handwritten and
typewritten text is routinely used in applications such as mail sorting and
digitization of old documents. 66
Machine translation remains imperfect but is good enough for many applications.
Early systems used the GOFAI approach of hand-coded grammars that had to be
developed by skilled linguists from the ground up for each language. Newer systems
use statistical machine learning techniques that automatically build statistical
models from observed usage patterns. The machine infers the parameters for these
models by analyzing bilingual corpora. This approach dispenses with linguists: the
programmers building these systems need not even speak the languages they are
working with. 67
Face recognition has improved sufficiently in recent years that it is now used at
automated border crossings in Europe and Australia. The US Department of State
operates a face recognition system with over 75 million photographs for visa
processing. Surveillance systems employ increasingly sophisticated AI and data-
mining technologies to analyze voice, video, or text, large quantities of which are
trawled from the world’s electronic communications media and stored in giant data
centers.
Theorem-proving and equation-solving are by now so well established that they
are hardly regarded as AI anymore. Equation solvers are included in scientific
computing programs such as Mathematica. Formal verification methods, including
automated theorem provers, are routinely used by chip manufacturers to verify the
behavior of circuit designs prior to production.
The US military and intelligence establishments have been leading the way to the
large-scale deployment of bomb-disposing robots, surveillance and attack drones,
and other unmanned vehicles. These still depend mainly on remote control by humanoperators, but work is underway to extend their autonomous capabilities.
Intelligent scheduling is a major area of success. The DART tool for automated
logistics planning and scheduling was used in Operation Desert Storm in 1991 to
such effect that DARPA (the Defense Advanced Research Projects Agency in the
United States) claims that this single application more than paid back their thirty-
year investment in AI. 68 Airline reservation systems use sophisticated scheduling
and pricing systems. Businesses make wide use of AI techniques in inventory
control systems. They also use automatic telephone reservation systems and
helplines connected to speech recognition software to usher their hapless customers
through labyrinths of interlocking menu options.
AI technologies underlie many Internet services. Software polices the world’s
email traffic, and despite continual adaptation by spammers to circumvent the
countermeasures being brought against them, Bayesian spam filters have largely
managed to hold the spam tide at bay. Software using AI components is responsible
for automatically approving or declining credit card transactions, and continuously
monitors account activity for signs of fraudulent use. Information retrieval systems
also make extensive use of machine learning. The Google search engine is, arguably,
the greatest AI system that has yet been built.
Now, it must be stressed that the demarcation between artificial intelligence and
software in general is not sharp. Some of the applications listed above might be
viewed more as generic software applications rather than AI in particular—though
this brings us back to McCarthy’s dictum that when something works it is no longer
called AI. A more relevant distinction for our purposes is that between systems that
have a narrow range of cognitive capability (whether they be called “AI” or not) and
systems that have more generally applicable problem-solving capacities. Essentially
all the systems currently in use are of the former type: narrow. However, many of
them contain components that might also play a role in future artificial general
intelligence or be of service in its development—components such as classifiers,
search algorithms, planners, solvers, and representational frameworks.
One high-stakes and extremely competitive environment in which AI systems
operate today is the global financial market. Automated stock-trading systems are
widely used by major investing houses. While some of these are simply ways of
automating the execution of particular buy or sell orders issued by a human fund
manager, others pursue complicated trading strategies that adapt to changing market
conditions. Analytic systems use an assortment of data-mining techniques and time
series analysis to scan for patterns and trends in securities markets or to correlate
historical price movements with external variables such as keywords in news tickers.
Financial news providers sell newsfeeds that are specially formatted for use by such
AI programs. Other systems specialize in finding arbitrage opportunities within or
between markets, or in high-frequency trading that seeks to profit from minute pricemovements that occur over the course of milliseconds (a timescale at which
communication latencies even for speed-of-light signals in optical fiber cable
become significant, making it advantageous to locate computers near the exchange).
Algorithmic high-frequency traders account for more than half of equity shares
traded on US markets. 69 Algorithmic trading has been implicated in the 2010 Flash
Crash (see Box 2).
Box 2 The 2010 Flash Crash
By the afternoon of May, 6, 2010, US equity markets were already down 4% on
worries about the European debt crisis. At 2:32 p.m., a large seller (a mutual fund
complex) initiated a sell algorithm to dispose of a large number of the E-Mini S&P
500 futures contracts to be sold off at a sell rate linked to a measure of minute-to-
minute liquidity on the exchange. These contracts were bought by algorithmic high-
frequency traders, which were programmed to quickly eliminate their temporary
long positions by selling the contracts on to other traders. With demand from
fundamental buyers slacking, the algorithmic traders started to sell the E-Minis
primarily to other algorithmic traders, which in turn passed them on to other
algorithmic traders, creating a “hot potato” effect driving up trading volume—this
being interpreted by the sell algorithm as an indicator of high liquidity, prompting it
to increase the rate at which it was putting E-Mini contracts on the market, pushing
the downward spiral. At some point, the high-frequency traders started withdrawing
from the market, drying up liquidity while prices continued to fall. At 2:45 p.m.,
trading on the E-Mini was halted by an automatic circuit breaker, the exchange’s
stop logic functionality. When trading was restarted, a mere five seconds later,
prices stabilized and soon began to recover most of the losses. But for a while, at the
trough of the crisis, a trillion dollars had been wiped off the market, and spillover
effects had led to a substantial number of trades in individual securities being
executed at “absurd” prices, such as one cent or 100,000 dollars. After the market
closed for the day, representatives of the exchanges met with regulators and decided
to break all trades that had been executed at prices 60% or more away from their
pre-crisis levels (deeming such transactions “clearly erroneous” and thus subject to
post facto cancellation under existing trade rules). 70
The retelling here of this episode is a digression because the computer programs
involved in the Flash Crash were not particularly intelligent or sophisticated, and the
kind of threat they created is fundamentally different from the concerns we shall
raise later in this book in relation to the prospect of machine superintelligence.
Nevertheless, these events illustrate several useful lessons. One is the reminder thatinteractions between individually simple components (such as the sell algorithm and
the high-frequency algorithmic trading programs) can produce complicated and
unexpected effects. Systemic risk can build up in a system as new elements are
introduced, risks that are not obvious until after something goes wrong (and
sometimes not even then). 71
Another lesson is that smart professionals might give an instruction to a program
based on a sensible-seeming and normally sound assumption (e.g. that trading
volume is a good measure of market liquidity), and that this can produce
catastrophic results when the program continues to act on the instruction with iron-
clad logical consistency even in the unanticipated situation where the assumption
turns out to be invalid. The algorithm just does what it does; and unless it is a very
special kind of algorithm, it does not care that we clasp our heads and gasp in
dumbstruck horror at the absurd inappropriateness of its actions. This is a theme that
we will encounter again.
A third observation in relation to the Flash Crash is that while automation
contributed to the incident, it also contributed to its resolution. The pre-
preprogrammed stop order logic, which suspended trading when prices moved too
far out of whack, was set to execute automatically because it had been correctly
anticipated that the triggering events could happen on a timescale too swift for
humans to respond. The need for pre-installed and automatically executing safety
functionality—as opposed to reliance on runtime human supervision—again
foreshadows a theme that will be important in our discussion of machine
superintelligence. 72
Opinions about the future of machine intelligence
Progress on two major fronts—towards a more solid statistical and information-
theoretic foundation for machine learning on the one hand, and towards the practical
and commercial success of various problem-specific or domain-specific applications
on the other—has restored to AI research some of its lost prestige. There may,
however, be a residual cultural effect on the AI community of its earlier history that
makes many mainstream researchers reluctant to align themselves with over-grand
ambition. Thus Nils Nilsson, one of the old-timers in the field, complains that his
present-day colleagues lack the boldness of spirit that propelled the pioneers of his
own generation:
Concern for “respectability” has had, I think, a stultifying effect on some AIresearchers. I hear them saying things like, “AI used to be criticized for its
flossiness. Now that we have made solid progress, let us not risk losing our
respectability.” One result of this conservatism has been increased
concentration on “weak AI”—the variety devoted to providing aids to human
thought—and away from “strong AI”—the variety that attempts to mechanize
human-level intelligence. 73
Nilsson’s sentiment has been echoed by several others of the founders, including
Marvin Minsky, John McCarthy, and Patrick Winston. 74
The last few years have seen a resurgence of interest in AI, which might yet spill
over into renewed efforts towards artificial general intelligence (what Nilsson calls
“strong AI”). In addition to faster hardware, a contemporary project would benefit
from the great strides that have been made in the many subfields of AI, in software
engineering more generally, and in neighboring fields such as computational
neuroscience. One indication of pent-up demand for quality information and
education is shown in the response to the free online offering of an introductory
course in artificial intelligence at Stanford University in the fall of 2011, organized
by Sebastian Thrun and Peter Norvig. Some 160,000 students from around the world
signed up to take it (and 23,000 completed it). 75
Expert opinions about the future of AI vary wildly. There is disagreement about
timescales as well as about what forms AI might eventually take. Predictions about
the future development of artificial intelligence, one recent study noted, “are as
confident as they are diverse.” 76
Although the contemporary distribution of belief has not been very carefully
measured, we can get a rough impression from various smaller surveys and informal
observations. In particular, a series of recent surveys have polled members of
several relevant expert communities on the question of when they expect “human-
level machine intelligence” (HLMI) to be developed, defined as “one that can carry
out most human professions at least as well as a typical human.” 77 Results are
shown in Table 2. The combined sample gave the following (median) estimate: 10%
probability of HLMI by 2022, 50% probability by 2040, and 90% probability by
2075. (Respondents were asked to premiss their estimates on the assumption that
“human scientific activity continues without major negative disruption.”)
These numbers should be taken with some grains of salt: sample sizes are quite
small and not necessarily representative of the general expert population. They are,
however, in concordance with results from other surveys. 78
The survey results are also in line with some recently published interviews with
about two-dozen researchers in AI-related fields. For example, Nils Nilsson has
spent a long and productive career working on problems in search, planning,knowledge representation, and robotics; he has authored textbooks in artificial
intelligence; and he recently completed the most comprehensive history of the field
written to date. 79 When asked about arrival dates for HLMI, he offered the following
opinion: 80
10% chance: 2030
50% chance: 2050
90% chance: 2100
Table 2 When will human-level machine intelligence be attained? 81
Judging from the published interview transcripts, Professor Nilsson’s probability
distribution appears to be quite representative of many experts in the area—though
again it must be emphasized that there is a wide spread of opinion: there are
practitioners who are substantially more boosterish, confidently expecting HLMI in
the 2020–40 range, and others who are confident either that it will never happen or
that it is indefinitely far off. 82 In addition, some interviewees feel that the notion of
a “human level” of artificial intelligence is ill-defined or misleading, or are for other
reasons reluctant to go on record with a quantitative prediction.
My own view is that the median numbers reported in the expert survey do not
have enough probability mass on later arrival dates. A 10% probability of HLMI not
having been developed by 2075 or even 2100 (after conditionalizing on “human
scientific activity continuing without major negative disruption”) seems too low.
Historically, AI researchers have not had a strong record of being able to predict
the rate of advances in their own field or the shape that such advances would take.
On the one hand, some tasks, like chess playing, turned out to be achievable by
means of surprisingly simple programs; and naysayers who claimed that machines
would “never” be able to do this or that have repeatedly been proven wrong. On the
other hand, the more typical errors among practitioners have been to underestimatethe difficulties of getting a system to perform robustly on real-world tasks, and to
overestimate the advantages of their own particular pet project or technique.
The survey also asked two other questions of relevance to our inquiry. One
inquired of respondents about how much longer they thought it would take to reach
superintelligence assuming human-level machine is first achieved. The results are in
Table 3.
Another question inquired what they thought would be the overall long-term
impact for humanity of achieving human-level machine intelligence. The answers
are summarized in Figure 2.
My own views again differ somewhat from the opinions expressed in the survey. I
assign a higher probability to superintelligence being created relatively soon after
human-level machine intelligence. I also have a more polarized outlook on the
consequences, thinking an extremely good or an extremely bad outcome to be
somewhat more likely than a more balanced outcome. The reasons for this will
become clear later in the book.
Table 3 How long from human level to superintelligence?
Within 2 years after HLMI Within 30 years after HLMI
TOP100 5%
50%
Combined 10%
75%
Figure 2 Overall long-term impact of HLMI. 83Small sample sizes, selection biases, and—above all—the inherent unreliability of
the subjective opinions elicited mean that one should not read too much into these
expert surveys and interviews. They do not let us draw any strong conclusion. But
they do hint at a weak conclusion. They suggest that (at least in lieu of better data or
analysis) it may be reasonable to believe that human-level machine intelligence has
a fairly sizeable chance of being developed by mid-century, and that it has a non-
trivial chance of being developed considerably sooner or much later; that it might
perhaps fairly soon thereafter result in superintelligence; and that a wide range of
outcomes may have a significant chance of occurring, including extremely good
outcomes and outcomes that are as bad as human extinction. 84 At the very least, they
suggest that the topic is worth a closer look.CHAPTER 2
Paths to superintelligence
Machines are currently far inferior to humans in general intelligence. Yet one
day (we have suggested) they will be superintelligent. How do we get from here
to there? This chapter explores several conceivable technological paths. We look
at artificial intelligence, whole brain emulation, biological cognition, and
human–machine interfaces, as well as networks and organizations. We evaluate
their different degrees of plausibility as pathways to superintelligence. The
existence of multiple paths increases the probability that the destination can be
reached via at least one of them.
We can tentatively define a superintelligence as any intellect that greatly exceeds
the cognitive performance of humans in virtually all domains of interest. 1 We will
have more to say about the concept of superintelligence in the next chapter, where
we will subject it to a kind of spectral analysis to distinguish some different possible
forms of superintelligence. But for now, the rough characterization just given will
suffice. Note that the definition is noncommittal about how the superintelligence is
implemented. It is also noncommittal regarding qualia: whether a superintelligence
would have subjective conscious experience might matter greatly for some questions
(in particular for some moral questions), but our primary focus here is on the causal
antecedents and consequences of superintelligence, not on the metaphysics of mind. 2
The chess program Deep Fritz is not a superintelligence on this definition, since
Fritz is only smart within the narrow domain of chess. Certain kinds of domain-
specific superintelligence could, however, be important. When referring to
superintelligent performance limited to a particular domain, we will note the
restriction explicitly. For instance, an “engineering superintelligence” would be an
intellect that vastly outperforms the best current human minds in the domain of
engineering. Unless otherwise noted, we use the term to refer to systems that have a
superhuman level of general intelligence.
But how might we create superintelligence? Let us examine some possible paths.
Artificial intelligence
Readers of this chapter must not expect a blueprint for programming an artificial
general intelligence. No such blueprint exists yet, of course. And had I been inpossession of such a blueprint, I most certainly would not have published it in a
book. (If the reasons for this are not immediately obvious, the arguments in
subsequent chapters will make them clear.)
We can, however, discern some general features of the kind of system that would
be required. It now seems clear that a capacity to learn would be an integral feature
of the core design of a system intended to attain general intelligence, not something
to be tacked on later as an extension or an afterthought. The same holds for the
ability to deal effectively with uncertainty and probabilistic information. Some
faculty for extracting useful concepts from sensory data and internal states, and for
leveraging acquired concepts into flexible combinatorial representations for use in
logical and intuitive reasoning, also likely belong among the core design features in
a modern AI intended to attain general intelligence.
The early Good Old-Fashioned Artificial Intelligence systems did not, for the
most part, focus on learning, uncertainty, or concept formation, perhaps because
techniques for dealing with these dimensions were poorly developed at the time.
This is not to say that the underlying ideas are all that novel. The idea of using
learning as a means of bootstrapping a simpler system to human-level intelligence
can be traced back at least to Alan Turing’s notion of a “child machine,” which he
wrote about in 1950:
Instead of trying to produce a programme to simulate the adult mind, why not
rather try to produce one which simulates the child’s? If this were then
subjected to an appropriate course of education one would obtain the adult
brain. 3
Turing envisaged an iterative process to develop such a child machine:
We cannot expect to find a good child machine at the first attempt. One must
experiment with teaching one such machine and see how well it learns. One can
then try another and see if it is better or worse. There is an obvious connection
between this process and evolution.... One may hope, however, that this process
will be more expeditious than evolution. The survival of the fittest is a slow
method for measuring advantages. The experimenter, by the exercise of
intelligence, should be able to speed it up. Equally important is the fact that he
is not restricted to random mutations. If he can trace a cause for some weakness
he can probably think of the kind of mutation which will improve it. 4
We know that blind evolutionary processes can produce human-level general
intelligence, since they have already done so at least once. Evolutionary processeswith foresight—that is, genetic programs designed and guided by an intelligent
human programmer—should be able to achieve a similar outcome with far greater
efficiency. This observation has been used by some philosophers and scientists,
including David Chalmers and Hans Moravec, to argue that human-level AI is not
only theoretically possible but feasible within this century. 5 The idea is that we can
estimate the relative capabilities of evolution and human engineering to produce
intelligence, and find that human engineering is already vastly superior to evolution
in some areas and is likely to become superior in the remaining areas before too
long. The fact that evolution produced intelligence therefore indicates that human
engineering will soon be able to do the same. Thus, Moravec wrote (already back in
1976):
The existence of several examples of intelligence designed under these
constraints should give us great confidence that we can achieve the same in
short order. The situation is analogous to the history of heavier than air flight,
where birds, bats and insects clearly demonstrated the possibility before our
culture mastered it. 6
One needs to be cautious, though, in what inferences one draws from this line of
reasoning. It is true that evolution produced heavier-than-air flight, and that human
engineers subsequently succeeded in doing likewise (albeit by means of a very
different mechanism). Other examples could also be adduced, such as sonar,
magnetic navigation, chemical weapons, photoreceptors, and all kinds of mechanic
and kinetic performance characteristics. However, one could equally point to areas
where human engineers have thus far failed to match evolution: in morphogenesis,
self-repair, and the immune defense, for example, human efforts lag far behind what
nature has accomplished. Moravec’s argument, therefore, cannot give us “great
confidence” that we can achieve human-level artificial intelligence “in short order.”
At best, the evolution of intelligent life places an upper bound on the intrinsic
difficulty of designing intelligence. But this upper bound could be quite far above
current human engineering capabilities.
Another way of deploying an evolutionary argument for the feasibility of AI is via
the idea that we could, by running genetic algorithms on sufficiently fast computers,
achieve results comparable to those of biological evolution. This version of the
evolutionary argument thus proposes a specific method whereby intelligence could
be produced.
But is it true that we will soon have computing power sufficient to recapitulate the
relevant evolutionary processes that produced human intelligence? The answer
depends both on how much computing technology will advance over the next
decades and on how much computing power would be required to run geneticalgorithms with the same optimization power as the evolutionary process of natural
selection that lies in our past. Although, in the end, the conclusion we get from
pursuing this line of reasoning is disappointingly indeterminate, it is instructive to
attempt a rough estimate (see Box 3). If nothing else, the exercise draws attention to
some interesting unknowns.
The upshot is that the computational resources required to simply replicate the
relevant evolutionary processes on Earth that produced human-level intelligence are
severely out of reach—and will remain so even if Moore’s law were to continue for
a century (cf. Figure 3). It is plausible, however, that compared with brute-force
replication of natural evolutionary processes, vast efficiency gains are achievable by
designing the search process to aim for intelligence, using various obvious
improvements over natural selection. Yet it is very hard to bound the magnitude of
those attainable efficiency gains. We cannot even say whether they amount to five or
to twenty-five orders of magnitude. Absent further elaboration, therefore,
evolutionary arguments are not able to meaningfully constrain our expectations of
either the difficulty of building human-level machine intelligence or the timescales
for such developments.
Box 3 What would it take to recapitulate evolution?
Not every feat accomplished by evolution in the course of the development of
human intelligence is relevant to a human engineer trying to artificially evolve
machine intelligence. Only a small portion of evolutionary selection on Earth has
been selection for intelligence. More specifically, the problems that human
engineers cannot trivially bypass may have been the target of a very small portion of
total evolutionary selection. For example, since we can run our computers on
electrical power, we do not have to reinvent the molecules of the cellular energy
economy in order to create intelligent machines—yet such molecular evolution of
metabolic pathways might have used up a large part of the total amount of selection
power that was available to evolution over the course of Earth’s history. 7
One might argue that the key insights for AI are embodied in the structure of
nervous systems, which came into existence less than a billion years ago. 8 If we take
that view, then the number of relevant “experiments” available to evolution is
drastically curtailed. There are some 4–6×10 30 prokaryotes in the world today, but
only 10 19 insects, and fewer than 10 10 humans (while pre-agricultural populations
were orders of magnitude smaller). 9 These numbers are only moderately
intimidating.
Evolutionary algorithms, however, require not only variations to select among butalso a fitness function to evaluate variants, and this is typically the most
computationally expensive component. A fitness function for the evolution of
artificial intelligence plausibly requires simulation of neural development, learning,
and cognition to evaluate fitness. We might thus do better not to look at the raw
number of organisms with complex nervous systems, but instead to attend to the
number of neurons in biological organisms that we might need to simulate to mimic
evolution’s fitness function. We can make a crude estimate of that latter quantity by
considering insects, which dominate terrestrial animal biomass (with ants alone
estimated to contribute some 15–20%). 10 Insect brain size varies substantially, with
large and social insects sporting larger brains: a honeybee brain has just under 10 6
neurons, a fruit fly brain has 10 5 neurons, and ants are in between with 250,000
neurons. 11 The majority of smaller insects may have brains of only a few thousand
neurons. Erring on the side of conservatively high, if we assigned all 10 19 insects
fruit-fly numbers of neurons, the total would be 10 24 insect neurons in the world.
This could be augmented with an additional order of magnitude to account for
aquatic copepods, birds, reptiles, mammals, etc., to reach 10 25 . (By contrast, in pre-
agricultural times there were fewer than 10 7 humans, with under 10 11 neurons each:
thus fewer than 10 18 human neurons in total, though humans have a higher number
of synapses per neuron.)
The computational cost of simulating one neuron depends on the level of detail
that one includes in the simulation. Extremely simple neuron models use about
1,000 floating-point operations per second (FLOPS) to simulate one neuron (in real-
time). The electrophysiologically realistic Hodgkin–Huxley model uses 1,200,000
FLOPS. A more detailed multi-compartmental model would add another three to
four orders of magnitude, while higher-level models that abstract systems of neurons
could subtract two to three orders of magnitude from the simple models. 12 If we
were to simulate 10 25 neurons over a billion years of evolution (longer than the
existence of nervous systems as we know them), and we allow our computers to run
for one year, these figures would give us a requirement in the range of 10 31 –10 44
FLOPS. For comparison, China’s Tianhe-2, the world’s most powerful
supercomputer as of September 2013, provides only 3.39×10 16 FLOPS. In recent
decades, it has taken approximately 6.7 years for commodity computers to increase
in power by one order of magnitude. Even a century of continued Moore’s law would
not be enough to close this gap. Running more specialized hardware, or allowing
longer run-times, could contribute only a few more orders of magnitude.
This figure is conservative in another respect. Evolution achieved human
intelligence without aiming at this outcome. In other words, the fitness functions for
natural organisms do not select only for intelligence and its precursors. 13 Even
environments in which organisms with superior information processing skills reapvarious rewards may not select for intelligence, because improvements to
intelligence can (and often do) impose significant costs, such as higher energy
consumption or slower maturation times, and those costs may outweigh whatever
benefits are gained from smarter behavior. Excessively deadly environments also
reduce the value of intelligence: the shorter one’s expected lifespan, the less time
there will be for increased learning ability to pay off. Reduced selective pressure for
intelligence slows the spread of intelligence-enhancing innovations, and thus the
opportunity for selection to favor subsequent innovations that depend on them.
Furthermore, evolution may wind up stuck in local optima that humans would notice
and bypass by altering tradeoffs between exploitation and exploration or by
providing a smooth progression of increasingly difficult intelligence tests. 14 And as
mentioned earlier, evolution scatters much of its selection power on traits that are
unrelated to intelligence (such as Red Queen’s races of competitive co-evolution
between immune systems and parasites). Evolution continues to waste resources
producing mutations that have proved consistently lethal, and it fails to take
advantage of statistical similarities in the effects of different mutations. These are
all inefficiencies in natural selection (when viewed as a means of evolving
intelligence) that it would be relatively easy for a human engineer to avoid while
using evolutionary algorithms to develop intelligent software.
It is plausible that eliminating inefficiencies like those just described would trim
many orders of magnitude off the 10 31 –10 44 FLOPS range calculated earlier.
Unfortunately, it is difficult to know how many orders of magnitude. It is difficult
even to make a rough estimate—for aught we know, the efficiency savings could be
five orders of magnitude, or ten, or twenty-five. 15
Figure 3 Supercomputer performance. In a narrow sense, “Moore’s law” refers to
the observation that the number of transistors on integrated circuits have for severaldecades doubled approximately every two years. However, the term is often used to
refer to the more general observation that many performance metrics in computing
technology have followed a similarly fast exponential trend. Here we plot peak
speed of the world’s fastest supercomputer as a function of time (on a logarithmic
vertical scale). In recent years, growth in the serial speed of processors has
stagnated, but increased use of parallelization has enabled the total number of
computations performed to remain on the trend line. 16
There is a further complication with these kinds of evolutionary considerations,
one that makes it hard to derive from them even a very loose upper bound on the
difficulty of evolving intelligence. We must avoid the error of inferring, from the
fact that intelligent life evolved on Earth, that the evolutionary processes involved
had a reasonably high prior probability of producing intelligence. Such an inference
is unsound because it fails to take account of the observation selection effect that
guarantees that all observers will find themselves having originated on a planet
where intelligent life arose, no matter how likely or unlikely it was for any given
such planet to produce intelligence. Suppose, for example, that in addition to the
systematic effects of natural selection it required an enormous amount of lucky
coincidence to produce intelligent life—enough so that intelligent life evolves on
only one planet out of every 10 30 planets on which simple replicators arise. In that
case, when we run our genetic algorithms to try to replicate what natural evolution
did, we might find that we must run some 10 30 simulations before we find one where
all the elements come together in just the right way. This seems fully consistent with
our observation that life did evolve here on Earth. Only by careful and somewhat
intricate reasoning—by analyzing instances of convergent evolution of intelligence-
related traits and engaging with the subtleties of observation selection theory—can
we partially circumvent this epistemological barrier. Unless one takes the trouble to
do so, one is not in a position to rule out the possibility that the alleged “upper
bound” on the computational requirements for recapitulating the evolution of
intelligence derived in Box 3 might be too low by thirty orders of magnitude (or
some other such large number). 17
Another way of arguing for the feasibility of artificial intelligence is by pointing
to the human brain and suggesting that we could use it as a template for a machine
intelligence. One can distinguish different versions of this approach based on how
closely they propose to imitate biological brain functions. At one extreme—that of
very close imitation—we have the idea of whole brain emulation, which we will
discuss in the next subsection. At the other extreme are approaches that take their
inspiration from the functioning of the brain but do not attempt low-level imitation.
Advances in neuroscience and cognitive psychology—which will be aided by
improvements in instrumentation—should eventually uncover the general principlesof brain function. This knowledge could then guide AI efforts. We have already
encountered neural networks as an example of a brain-inspired AI technique.
Hierarchical perceptual organization is another idea that has been transferred from
brain science to machine learning. The study of reinforcement learning has been
motivated (at least in part) by its role in psychological theories of animal cognition,
and reinforcement learning techniques (e.g. the “TD-algorithm”) inspired by these
theories are now widely used in AI. 18 More cases like these will surely accumulate
in the future. Since there is a limited number—perhaps a very small number—of
distinct fundamental mechanisms that operate in the brain, continuing incremental
progress in brain science should eventually discover them all. Before this happens,
though, it is possible that a hybrid approach, combining some brain-inspired
techniques with some purely artificial methods, would cross the finishing line. In
that case, the resultant system need not be recognizably brain-like even though some
brain-derived insights were used in its development.
The availability of the brain as template provides strong support for the claim that
machine intelligence is ultimately feasible. This, however, does not enable us to
predict when it will be achieved because it is hard to predict the future rate of
discoveries in brain science. What we can say is that the further into the future we
look, the greater the likelihood that the secrets of the brain’s functionality will have
been decoded sufficiently to enable the creation of machine intelligence in this
manner.
Different people working toward machine intelligence hold different views about
how promising neuromorphic approaches are compared with approaches that aim for
completely synthetic designs. The existence of birds demonstrated that heavier-than-
air flight was physically possible and prompted efforts to build flying machines. Yet
the first functioning airplanes did not flap their wings. The jury is out on whether
machine intelligence will be like flight, which humans achieved through an artificial
mechanism, or like combustion, which we initially mastered by copying naturally
occurring fires.
Turing’s idea of designing a program that acquires most of its content by learning,
rather than having it pre-programmed at the outset, can apply equally to
neuromorphic and synthetic approaches to machine intelligence.
A variation on Turing’s conception of a child machine is the idea of a “seed AI.” 19
Whereas a child machine, as Turing seems to have envisaged it, would have a
relatively fixed architecture that simply develops its inherent potentialities by
accumulating content, a seed AI would be a more sophisticated artificial intelligence
capable of improving its own architecture. In the early stages of a seed AI, such
improvements might occur mainly through trial and error, information acquisition,
or assistance from the programmers. At its later stages, however, a seed AI should
be able to understand its own workings sufficiently to engineer new algorithms andcomputational structures to bootstrap its cognitive performance. This needed
understanding could result from the seed AI reaching a sufficient level of general
intelligence across many domains, or from crossing some threshold in a particularly
relevant domain such as computer science or mathematics.
This brings us to another important concept, that of “recursive self-improvement.”
A successful seed AI would be able to iteratively enhance itself: an early version of
the AI could design an improved version of itself, and the improved version—being
smarter than the original—might be able to design an even smarter version of itself,
and so forth. 20 Under some conditions, such a process of recursive self-improvement
might continue long enough to result in an intelligence explosion—an event in
which, in a short period of time, a system’s level of intelligence increases from a
relatively modest endowment of cognitive capabilities (perhaps sub-human in most
respects, but with a domain-specific talent for coding and AI research) to radical
superintelligence. We will return to this important possibility in Chapter 4, where
the dynamics of such an event will be analyzed more closely. Note that this model
suggests the possibility of surprises: attempts to build artificial general intelligence
might fail pretty much completely until the last missing critical component is put in
place, at which point a seed AI might become capable of sustained recursive self-
improvement.
Before we end this subsection, there is one more thing that we should emphasize,
which is that an artificial intelligence need not much resemble a human mind. AIs
could be—indeed, it is likely that most will be—extremely alien. We should expect
that they will have very different cognitive architectures than biological
intelligences, and in their early stages of development they will have very different
profiles of cognitive strengths and weaknesses (though, as we shall later argue, they
could eventually overcome any initial weakness). Furthermore, the goal systems of
AIs could diverge radically from those of human beings. There is no reason to
expect a generic AI to be motivated by love or hate or pride or other such common
human sentiments: these complex adaptations would require deliberate expensive
effort to recreate in AIs. This is at once a big problem and a big opportunity. We will
return to the issue of AI motivation in later chapters, but it is so central to the
argument in this book that it is worth bearing in mind throughout.
Whole brain emulation
In whole brain emulation (also known as “uploading”), intelligent software would be
produced by scanning and closely modeling the computational structure of a
biological brain. This approach thus represents a limiting case of drawing inspiration
from nature: barefaced plagiarism. Achieving whole brain emulation requires theaccomplishment of the following steps.
First, a sufficiently detailed scan of a particular human brain is created. This
might involve stabilizing the brain post-mortem through vitrification (a process that
turns tissue into a kind of glass). A machine could then dissect the tissue into thin
slices, which could be fed into another machine for scanning, perhaps by an array of
electron microscopes. Various stains might be applied at this stage to bring out
different structural and chemical properties. Many scanning machines could work in
parallel to process multiple brain slices simultaneously.
Second, the raw data from the scanners is fed to a computer for automated image
processing to reconstruct the three-dimensional neuronal network that implemented
cognition in the original brain. In practice, this step might proceed concurrently with
the first step to reduce the amount of high-resolution image data stored in buffers.
The resulting map is then combined with a library of neurocomputational models of
different types of neurons or of different neuronal elements (such as particular kinds
of synaptic connectors). Figure 4 shows some results of scanning and image
processing produced with present-day technology.
In the third stage, the neurocomputational structure resulting from the previous
step is implemented on a sufficiently powerful computer. If completely successful,
the result would be a digital reproduction of the original intellect, with memory and
personality intact. The emulated human mind now exists as software on a computer.
The mind can either inhabit a virtual reality or interface with the external world by
means of robotic appendages.
The whole brain emulation path does not require that we figure out how human
cognition works or how to program an artificial intelligence. It requires only that we
understand the low-level functional characteristics of the basic computational
elements of the brain. No fundamental conceptual or theoretical breakthrough is
needed for whole brain emulation to succeed.
Whole brain emulation does, however, require some rather advanced enabling
technologies. There are three key prerequisites: (1) scanning: high-throughput
microscopy with sufficient resolution and detection of relevant properties; (2)
translation: automated image analysis to turn raw scanning data into an interpreted
three-dimensional model of relevant neurocomputational elements; and (3)
simulation: hardware powerful enough to implement the resultant computational
structure (see Table 4). (In comparison with these more challenging steps, the
construction of a basic virtual reality or a robotic embodiment with an audiovisual
input channel and some simple output channel is relatively easy. Simple yet
minimally adequate I/O seems feasible already with present technology. 23 )Figure 4 Reconstructing 3D neuroanatomy from electron microscope images. Upper
left: A typical electron micrograph showing cross-sections of neuronal matter—
dendrites and axons. Upper right: Volume image of rabbit retinal neural tissue
acquired by serial block-face scanning electron microscopy. 21 Individual 2D images
have been stacked into a cube (with a side of approximately 11 μm). Bottom:
Reconstruction of a subset of the neuronal projections filling a volume of neuropil,
generated by an automated segmentation algorithm. 22
There is good reason to think that the requisite enabling technologies are
attainable, though not in the near future. Reasonable computational models of many
types of neuron and neuronal processes already exist. Image recognition software
has been developed that can trace axons and dendrites through a stack of two-
dimensional images (though reliability needs to be improved). And there are
imaging tools that provide the necessary resolution—with a scanning tunneling
microscope it is possible to “see” individual atoms, which is a far higher resolution
than needed. However, although present knowledge and capabilities suggest that
there is no in-principle barrier to the development of the requisite enabling
technologies, it is clear that a very great deal of incremental technical progresswould be needed to bring human whole brain emulation within reach. 24 For example,
microscopy technology would need not just sufficient resolution but also sufficient
throughput. Using an atomic-resolution scanning tunneling microscope to image the
needed surface area would be far too slow to be practicable. It would be more
plausible to use a lower-resolution electron microscope, but this would require new
methods for preparing and staining cortical tissue to make visible relevant details
such as synaptic fine structure. A great expansion of neurocomputational libraries
and major improvements in automated image processing and scan interpretation
would also be needed.
Table 4 Capabilities needed for whole brain emulationIn general, whole brain emulation relies less on theoretical insight and more on
technological capability than artificial intelligence. Just how much technology is
required for whole brain emulation depends on the level of abstraction at which the
brain is emulated. In this regard there is a tradeoff between insight and technology.
In general, the worse our scanning equipment and the feebler our computers, the less
we could rely on simulating low-level chemical and electrophysiological brain
processes, and the more theoretical understanding would be needed of the
computational architecture that we are seeking to emulate in order to create more
abstract representations of the relevant functionalities. 25 Conversely, with
sufficiently advanced scanning technology and abundant computing power, it might
be possible to brute-force an emulation even with a fairly limited understanding of
the brain. In the unrealistic limiting case, we could imagine emulating a brain at the
level of its elementary particles using the quantum mechanical Schrödinger
equation. Then one could rely entirely on existing knowledge of physics and not at
all on any biological model. This extreme case, however, would place utterly
impracticable demands on computational power and data acquisition. A far moreplausible level of emulation would be one that incorporates individual neurons and
their connectivity matrix, along with some of the structure of their dendritic trees
and maybe some state variables of individual synapses. Neurotransmitter molecules
would not be simulated individually, but their fluctuating concentrations would be
modeled in a coarse-grained manner.
To assess the feasibility of whole brain emulation, one must understand the
criterion for success. The aim is not to create a brain simulation so detailed and
accurate that one could use it to predict exactly what would have happened in the
original brain if it had been subjected to a particular sequence of stimuli. Instead, the
aim is to capture enough of the computationally functional properties of the brain to
enable the resultant emulation to perform intellectual work. For this purpose, much
of the messy biological detail of a real brain is irrelevant.
A more elaborate analysis would distinguish between different levels of emulation
success based on the extent to which the information-processing functionality of the
emulated brain has been preserved. For example, one could distinguish among (1) a
high-fidelity emulation that has the full set of knowledge, skills, capacities, and
values of the emulated brain; (2) a distorted emulation whose dispositions are
significantly non-human in some ways but which is mostly able to do the same
intellectual labor as the emulated brain; and (3) a generic emulation (which might
also be distorted) that is somewhat like an infant, lacking the skills or memories that
had been acquired by the emulated adult brain but with the capacity to learn most of
what a normal human can learn. 26
While it appears ultimately feasible to produce a high-fidelity emulation, it seems
quite likely that the first whole brain emulation that we would achieve if we went
down this path would be of a lower grade. Before we would get things to work
perfectly, we would probably get things to work imperfectly. It is also possible that a
push toward emulation technology would lead to the creation of some kind of
neuromorphic AI that would adapt some neurocomputational principles discovered
during emulation efforts and hybridize them with synthetic methods, and that this
would happen before the completion of a fully functional whole brain emulation.
The possibility of such a spillover into neuromorphic AI, as we shall see in a later
chapter, complicates the strategic assessment of the desirability of seeking to
expedite emulation technology.
How far are we currently from achieving a human whole brain emulation? One
recent assessment presented a technical roadmap and concluded that the prerequisite
capabilities might be available around mid-century, though with a large uncertainty
interval. 27 Figure 5 depicts the major milestones in this roadmap. The apparent
simplicity of the map may be deceptive, however, and we should be careful not to
understate how much work remains to be done. No brain has yet been emulated.
Consider the humble model organism Caenorhabditis elegans, which is a transparentroundworm, about 1 mm in length, with 302 neurons. The complete connectivity
matrix of these neurons has been known since the mid-1980s, when it was
laboriously mapped out by means of slicing, electron microscopy, and hand-labeling
of specimens. 29 But knowing merely which neurons are connected with which is not
enough. To create a brain emulation one would also need to know which synapses
are excitatory and which are inhibitory; the strength of the connections; and various
dynamical properties of axons, synapses, and dendritic trees. This information is not
yet available even for the small nervous system of C. elegans (although it may now
be within range of a targeted moderately sized research project). 30 Success at
emulating a tiny brain, such as that of C. elegans, would give us a better view of
what it would take to emulate larger brains.
Figure 5 Whole brain emulation roadmap. Schematic of inputs, activities, and
milestones. 28
At some point in the technology development process, once techniques are
available for automatically emulating small quantities of brain tissue, the problem
reduces to one of scaling. Notice “the ladder” at the right side of Figure 5. This
ascending series of boxes represents a final sequence of advances which can
commence after preliminary hurdles have been cleared. The stages in this sequence
correspond to whole brain emulations of successively more neurologicallysophisticated model organisms—for example, C. elegans → honeybee → mouse →
rhesus monkey → human. Because the gaps between these rungs—at least after the
first step—are mostly quantitative in nature and due mainly (though not entirely) to
the differences in size of the brains to be emulated, they should be tractable through
a relatively straightforward scale-up of scanning and simulation capacity. 31
Once we start ascending this final ladder, the eventual attainment of human whole
brain emulation becomes more clearly foreseeable. 32 We can thus expect to get
some advance warning before arrival at human-level machine intelligence along the
whole brain emulation path, at least if the last among the requisite enabling
technologies to reach sufficient maturity is either high-throughput scanning or the
computational power needed for real-time simulation. If, however, the last enabling
technology to fall into place is neurocomputational modeling, then the transition
from unimpressive prototypes to a working human emulation could be more abrupt.
One could imagine a scenario in which, despite abundant scanning data and fast
computers, it is proving difficult to get our neuronal models to work right. When
finally the last glitch is ironed out, what was previously a completely dysfunctional
system—analogous perhaps to an unconscious brain undergoing a grand mal seizure
—might snap into a coherent wakeful state. In this case, the key advance would not
be heralded by a series of functioning animal emulations of increasing magnitude
(provoking newspaper headlines of correspondingly escalating font size). Even for
those paying attention it might be difficult to tell in advance of success just how
many flaws remained in the neurocomputational models at any point and how long it
would take to fix them, even up to the eve of the critical breakthrough. (Once a
human whole brain emulation has been achieved, further potentially explosive
developments would take place; but we postpone discussion of this until Chapter 4.)
Surprise scenarios are thus imaginable for whole brain emulation even if all the
relevant research were conducted in the open. Nevertheless, compared with the AI
path to machine intelligence, whole brain emulation is more likely to be preceded by
clear omens since it relies more on concrete observable technologies and is not
wholly based on theoretical insight. We can also say, with greater confidence than
for the AI path, that the emulation path will not succeed in the near future (within
the next fifteen years, say) because we know that several challenging precursor
technologies have not yet been developed. By contrast, it seems likely that
somebody could in principle sit down and code a seed AI on an ordinary present-day
personal computer; and it is conceivable—though unlikely—that somebody
somewhere will get the right insight for how to do this in the near future.
Biological cognitionA third path to greater-than-current-human intelligence is to enhance the functioning
of biological brains. In principle, this could be achieved without technology, through
selective breeding. Any attempt to initiate a classical large-scale eugenics program,
however, would confront major political and moral hurdles. Moreover, unless the
selection were extremely strong, many generations would be required to produce
substantial results. Long before such an initiative would bear fruit, advances in
biotechnology will allow much more direct control of human genetics and
neurobiology, rendering otiose any human breeding program. We will therefore
focus on methods that hold the potential to deliver results faster, on the timescale of
a few generations or less.
Our individual cognitive capacities can be strengthened in various ways, including
by such traditional methods as education and training. Neurological development
can be promoted by low-tech interventions such as optimizing maternal and infant
nutrition, removing lead and other neurotoxic pollutants from the environment,
eradicating parasites, ensuring adequate sleep and exercise, and preventing diseases
that affect the brain. 33 Improvements in cognition can certainly be obtained through
each of these means, though the magnitudes of the gains are likely to be modest,
especially in populations that are already reasonably well-nourished and -schooled.
We will certainly not achieve superintelligence by any of these means, but they
might help on the margin, particularly by lifting up the deprived and expanding the
catchment of global talent. (Lifelong depression of intelligence due to iodine
deficiency remains widespread in many impoverished inland areas of the world—an
outrage given that the condition can be prevented by fortifying table salt at a cost of
a few cents per person and year. 34 )
Biomedical enhancements could give bigger boosts. Drugs already exist that are
alleged to improve memory, concentration, and mental energy in at least some
subjects. 35 (Work on this book was fueled by coffee and nicotine chewing gum.)
While the efficacy of the present generation of smart drugs is variable, marginal,
and generally dubious, future nootropics might offer clearer benefits and fewer side
effects. 36 However, it seems implausible, on both neurological and evolutionary
grounds, that one could by introducing some chemical into the brain of a healthy
person spark a dramatic rise in intelligence. 37 The cognitive functioning of a human
brain depends on a delicate orchestration of many factors, especially during the
critical stages of embryo development—and it is much more likely that this self-
organizing structure, to be enhanced, needs to be carefully balanced, tuned, and
cultivated rather than simply flooded with some extraneous potion.
Manipulation of genetics will provide a more powerful set of tools than
psychopharmacology. Consider again the idea of genetic selection: instead of trying
to implement a eugenics program by controlling mating patterns, one could use
selection at the level of embryos or gametes. 38 Pre-implantation genetic diagnosishas already been used during in vitro fertilization procedures to screen embryos
produced for monogenic disorders such as Huntington’s disease and for
predisposition to some late-onset diseases such as breast cancer. It has also been
used for sex selection and for matching human leukocyte antigen type with that of a
sick sibling, who can then benefit from a cord-blood stem cell donation when the
new baby is born. 39 The range of traits that can be selected for or against will expand
greatly over the next decade or two. A strong driver of progress in behavioral
genetics is the rapidly falling cost of genotyping and gene sequencing. Genome-wide
complex trait analysis, using studies with vast numbers of subjects, is just now
starting to become feasible and will greatly increase our knowledge of the genetic
architectures of human cognitive and behavioral traits. 40 Any trait with a non-
negligible heritability—including cognitive capacity—could then become
susceptible to selection. 41 Embryo selection does not require a deep understanding
of the causal pathways by which genes, in complicated interplay with environments,
produce phenotypes: it requires only (lots of) data on the genetic correlates of the
traits of interest.
It is possible to calculate some rough estimates of the magnitude of the gains
obtainable in different selection scenarios. 42 Table 5 shows expected increases in
intelligence resulting from various amounts of selection, assuming complete
information about the common additive genetic variants underlying the narrow-
sense heritability of intelligence. (With partial information, the effectiveness of
selection would be reduced, though not quite to the extent one might naively
expect. 44 ) Unsurprisingly, selecting between larger numbers of embryos produces
larger gains, but there are steeply diminishing returns: selection between 100
embryos does not produce a gain anywhere near fifty times as large as that which
one would get from selection between 2 embryos. 45
Table 5 Maximum IQ gains from selecting among a set of embryos 43
Selection
1 in 2
1 in 10
1 in 100
1 in 1000
5 generations of 1 in 10
10 generations of 1 in 10
Cumulative limits (additive variants optimized for
cognition)
IQ points gained
4.2
11.5
18.8
24.3
< 65 (b/c diminishing returns)
< 130 (b/c diminishing returns)
100 + (< 300 (b/c diminishing
returns))Interestingly, the diminishment of returns is greatly abated when the selection is
spread over multiple generations. Thus, repeatedly selecting the top 1 in 10 over ten
generations (where each new generation consists of the offspring of those selected in
the previous generation) will produce a much greater increase in the trait value than
a one-off selection of 1 in 100. The problem with sequential selection, of course, is
that it takes longer. If each generational step takes twenty or thirty years, then even
just five successive generations would push us well into the twenty-second century.
Long before then, more direct and powerful modes of genetic engineering (not to
mention machine intelligence) will most likely be available.
There is, however, a complementary technology, one which, once it has been
developed for use in humans, would greatly potentiate the enhancement power of
pre-implantation genetic screening: namely, the derivation of viable sperm and eggs
from embryonic stem cells. 46 The techniques for this have already been used to
produce fertile offspring in mice and gamete-like cells in humans. Substantial
scientific challenges remain, however, in translating the animal results to humans
and in avoiding epigenetic abnormalities in the derived stem cell lines. According to
one expert, these challenges might put human application “10 or even 50 years in the
future.” 47
With stem cell-derived gametes, the amount of selection power available to a
couple could be greatly increased. In current practice, an in vitro fertilization
procedure typically involves the creation of fewer than ten embryos. With stem cell-
derived gametes, a few donated cells might be turned into a virtually unlimited
number of gametes that could be combined to produce embryos, which could then be
genotyped or sequenced, and the most promising one chosen for implantation.
Depending on the cost of preparing and screening each individual embryo, this
technology could yield a severalfold increase in the selective power available to
couples using in vitro fertilization.
More importantly still, stem cell-derived gametes would allow multiple
generations of selection to be compressed into less than a human maturation period,
by enabling iterated embryo selection. This is a procedure that would consist of the
following steps: 48
1 Genotype and select a number of embryos that are higher in desired genetic
characteristics.
2 Extract stem cells from those embryos and convert them to sperm and ova,
maturing within six months or less. 49
3 Cross the new sperm and ova to produce embryos.
4 Repeat until large genetic changes have been accumulated.In this manner, it would be possible to accomplish ten or more generations of
selection in just a few years. (The procedure would be time-consuming and
expensive; however, in principle, it would need to be done only once rather than
repeated for each birth. The cell lines established at the end of the procedure could
be used to generate very large numbers of enhanced embryos.)
A s Table 5 indicates, the average level of intelligence among individuals
conceived in this manner could be very high, possibly equal to or somewhat above
that of the most intelligent individual in the historical human population. A world
that had a large population of such individuals might (if it had the culture, education,
communications infrastructure, etc., to match) constitute a collective
superintelligence.
The impact of this technology will be dampened and delayed by several factors.
There is the unavoidable maturational lag while the finally selected embryos grow
into adult human beings: at least twenty years before an enhanced child reaches full
productivity, longer still before such children come to constitute a substantial
segment of the labor force. Furthermore, even after the technology has been
perfected, adoption rates will probably start out low. Some countries might prohibit
its use altogether, on moral or religious grounds. 50 Even where selection is allowed,
many couples will prefer the natural way of conceiving. Willingness to use IVF,
however, would increase if there were clearer benefits associated with the procedure
—such as a virtual guarantee that the child would be highly talented and free from
genetic predispositions to disease. Lower health care costs and higher expected
lifetime earnings would also argue in favor of genetic selection. As use of the
procedure becomes more common, particularly among social elites, there might be a
cultural shift toward parenting norms that present the use of selection as the thing
that responsible enlightened couples do. Many of the initially reluctant might join
the bandwagon in order to have a child that is not at a disadvantage relative to the
enhanced children of their friends and colleagues. Some countries might offer
inducements to encourage their citizens to take advantage of genetic selection in
order to increase the country’s stock of human capital, or to increase long-term
social stability by selecting for traits like docility, obedience, submissiveness,
conformity, risk-aversion, or cowardice, outside of the ruling clan.
Effects on intellectual capacity would also depend on the extent to which the
available selection power would be used for enhancing cognitive traits (Table 6).
Those who do opt to use some form of embryo selection would have to choose how
to allocate the selection power at their disposal, and intelligence would to some
extent be in competition with other desired attributes, such as health, beauty,
personality, or athleticism. Iterated embryo selection, by offering such a large
amount of selection power, would alleviate some of these tradeoffs, enabling
simultaneous strong selection for multiple traits. However, this procedure wouldtend to disrupt the normal genetic relationship between parents and child, something
that could negatively affect demand in many cultures. 51
With further advances in genetic technology, it may become possible to
synthesize genomes to specification, obviating the need for large pools of embryos.
DNA synthesis is already a routine and largely automated biotechnology, though it is
not yet feasible to synthesize an entire human genome that could be used in a
reproductive context (not least because of still-unresolved difficulties in getting the
epigenetics right). 54 But once this technology has matured, an embryo could be
designed with the exact preferred combination of genetic inputs from each parent.
Genes that are present in neither of the parents could also be spliced in, including
alleles that are present with low frequency in the population but which may have
significant positive effects on cognition. 55
Table 6 Possible impacts from genetic selection in different scenarios 52
One intervention that becomes possible when human genomes can be synthesized
is genetic “spell-checking” of an embryo. (Iterated embryo selection might also
allow an approximation of this.) Each of us currently carries a mutational load, with
perhaps hundreds of mutations that reduce the efficiency of various cellular
processes. 56 Each individual mutation has an almost negligible effect (whence it is
only slowly removed from the gene pool), yet in combination such mutations may
exact a heavy toll on our functioning. 57 Individual differences in intelligence might
to a significant extent be attributable to variations in the number and nature of such
slightly deleterious alleles that each of us carries. With gene synthesis we could take
the genome of an embryo and construct a version of that genome free from the
genetic noise of accumulated mutations. If one wished to speak provocatively, one
could say that individuals created from such proofread genomes might be “morehuman” than anybody currently alive, in that they would be less distorted
expressions of human form. Such people would not all be carbon copies, because
humans vary genetically in ways other than by carrying different deleterious
mutations. But the phenotypical manifestation of a proofread genome may be an
exceptional physical and mental constitution, with elevated functioning in polygenic
trait dimensions like intelligence, health, hardiness, and appearance. 58 (A loose
analogy could be made with composite faces, in which the defects of the
superimposed individuals are averaged out: see Figure 6.)
Figure 6 Composite faces as a metaphor for spell-checked genomes. Each of the
central pictures was produced by superimposing photographs of sixteen different
individuals (residents of Tel Aviv). Composite faces are often judged to be more
beautiful than any of the individual faces of which they are composed, as
idiosyncratic imperfections are averaged out. Analogously, by removing individual
mutations, proofread genomes may produce people closer to “Platonic ideals.” Such
individuals would not all be genetically identical, because many genes come in
multiple equally functional alleles. Proofreading would only eliminate variance
arising from deleterious mutations. 59
Other potential biotechnological techniques might also be relevant. Human
reproductive cloning, once achieved, could be used to replicate the genome of
exceptionally talented individuals. Uptake would be limited by the preference of
most prospective parents to be biologically related to their children, yet the practice
could nevertheless come to have non-negligible impact because (1) even a relatively
small increase in the number of exceptionally talented people might have a
significant effect; and (2) it is possible that some state would embark on a larger-
scale eugenics program, perhaps by paying surrogate mothers. Other kinds of genetic
engineering—such as the design of novel synthetic genes or insertion into the
genome of promoter regions and other elements to control gene expression—might
also become important over time. Even more exotic possibilities may exist, such as
vats full of complexly structured cultured cortical tissue, or “uplifted” transgenicanimals (perhaps some large-brained mammal such as the whale or elephant,
enriched with human genes). These latter ones are wholly speculative, but over a
longer time frame they perhaps cannot be completely discounted.
So far we have discussed germline interventions, ones that would be done on
gametes or embryos. Somatic gene enhancements, by bypassing the generation
cycle, could in principle produce impacts more quickly. However, they are
technologically much more challenging. They require that the modified genes be
inserted into a large number of cells in the living body—including, in the case of
cognitive enhancement, the brain. Selecting among existing egg cells or embryos, in
contrast, requires no gene insertion. Even such germline therapies as do involve
modifying the genome (such as proofreading the genome or splicing in rare alleles)
are far easier to implement at the gamete or the embryo stage, where one is dealing
with a small number of cells. Furthermore, germline interventions on embryos can
probably achieve greater effects than somatic interventions on adults, because the
former would be able to shape early brain development whereas the latter would be
limited to tweaking an existing structure. (Some of what could be done through
somatic gene therapy might also be achievable by pharmacological means.)
Focusing therefore on germline interventions, we must take into account the
generational lag delaying any large impact on the world. 60 Even if the technology
were perfected today and immediately put to use, it would take more than two
decades for a genetically enhanced brood to reach maturity. Furthermore, with
human applications there is normally a delay of at least one decade between proof of
concept in the laboratory and clinical application, because of the need for extensive
studies to determine safety. The simplest forms of genetic selection, however, could
largely abrogate the need for such testing, since they would use standard fertility
treatment techniques and genetic information to choose between embryos that might
otherwise have been selected by chance.
Delays could also result from obstacles rooted not in a fear of failure (demand for
safety testing) but in fear of success—demand for regulation driven by concerns
about the moral permissibility of genetic selection or its wider social implications.
Such concerns are likely to be more influential in some countries than in others,
owing to differing cultural, historical, and religious contexts. Post-war Germany, for
example, has chosen to give a wide berth to any reproductive practices that could be
perceived to be even in the remotest way aimed at enhancement, a stance that is
understandable given the particularly dark history of atrocities connected to the
eugenics movement in that country. Other Western countries are likely to take a
more liberal approach. And some countries—perhaps China or Singapore, both of
which have long-term population policies—might not only permit but actively
promote the use of genetic selection and genetic engineering to enhance the
intelligence of their populations once the technology to do so is available.Once the example has been set, and the results start to show, holdouts will have
strong incentives to follow suit. Nations would face the prospect of becoming
cognitive backwaters and losing out in economic, scientific, military, and prestige
contests with competitors that embrace the new human enhancement technologies.
Individuals within a society would see places at elite schools being filled with
genetically selected children (who may also on average be prettier, healthier, and
more conscientious) and will want their own offspring to have the same advantages.
There is some chance that a large attitudinal shift could take place over a relatively
short time, perhaps in as little as a decade, once the technology is proven to work
and to provide a substantial benefit. Opinion surveys in the United States reveal a
dramatic shift in public approval of in vitro fertilization after the birth of the first
“test tube baby,” Louise Brown, in 1978. A few years earlier, only 18% of
Americans said they would personally use IVF to treat infertility; yet in a poll taken
shortly after the birth of Louise Brown, 53% said they would do so, and the number
has continued to rise. 61 (For comparison, in a poll taken in 2004, 28% of Americans
approved of embryo selection for “strength or intelligence,” 58% approved of it for
avoiding adult-onset cancer, and 68% approved of it to avoid fatal childhood
disease. 62 )
If we add up the various delays—say five to ten years to gather the information
needed for significantly effective selection among a set of IVF embryos (possibly
much longer before stem cell-derived gametes are available for use in human
reproduction), ten years to build significant uptake, and twenty to twenty-five years
for the enhanced generation to reach an age where they start becoming productive,
we find that germline enhancements are unlikely to have a significant impact on
society before the middle of this century. From that point onward, however, the
intelligence of significant segments of the adult population may begin to be boosted
by genetic enhancements. The speed of the ascent would then greatly accelerate as
cohorts conceived using more powerful next-generation genetic technologies (in
particular stem cell-derived gametes and iterative embryo selection) enter the labor
force.
With the full development of the genetic technologies described above (setting
aside the more exotic possibilities such as intelligence in cultured neural tissue), it
might be possible to ensure that new individuals are on average smarter than any
human who has yet existed, with peaks that rise higher still. The potential of
biological enhancement is thus ultimately high, probably sufficient for the
attainment of at least weak forms of superintelligence. This should not be surprising.
After all, dumb evolutionary processes have dramatically amplified the intelligence
in the human lineage even compared with our close relatives the great apes and our
own humanoid ancestors; and there is no reason to suppose Homo sapiens to have
reached the apex of cognitive effectiveness attainable in a biological system. Farfrom being the smartest possible biological species, we are probably better thought
of as the stupidest possible biological species capable of starting a technological
civilization—a niche we filled because we got there first, not because we are in any
sense optimally adapted to it.
Progress along the biological path is clearly feasible. The generational lag in
germline interventions means that progress could not be nearly as sudden and abrupt
as in scenarios involving machine intelligence. (Somatic gene therapies and
pharmacological interventions could theoretically skip the generational lag, but they
seem harder to perfect and are less likely to produce dramatic effects.) The ultimate
potential of machine intelligence is, of course, vastly greater than that of organic
intelligence. (One can get some sense of the magnitude of the gap by considering the
speed differential between electronic components and nerve cells: even today’s
transistors operate on a timescale ten million times shorter than that of biological
neurons.) However, even comparatively moderate enhancements of biological
cognition could have important consequences. In particular, cognitive enhancement
could accelerate science and technology, including progress toward more potent
forms of biological intelligence amplification and machine intelligence. Consider
how the rate of progress in the field of artificial intelligence would change in a
world where Average Joe is an intellectual peer of Alan Turing or John von
Neumann, and where millions of people tower far above any intellectual giant of the
past. 63
A discussion of the strategic implications of cognitive enhancement will have to
await a later chapter. But we can summarize this section by noting three
conclusions: (1) at least weak forms of superintelligence are achievable by means of
biotechnological enhancements; (2) the feasibility of cognitively enhanced humans
adds to the plausibility that advanced forms of machine intelligence are feasible—
because even if we were fundamentally unable to create machine intelligence (which
there is no reason to suppose), machine intelligence might still be within reach of
cognitively enhanced humans; and (3) when we consider scenarios stretching
significantly into the second half of this century and beyond, we must take into
account the probable emergence of a generation of genetically enhanced populations
—voters, inventors, scientists—with the magnitude of enhancement escalating
rapidly over subsequent decades.
Brain–computer interfaces
It is sometimes proposed that direct brain–computer interfaces, particularly
implants, could enable humans to exploit the fortes of digital computing—perfect
recall, speedy and accurate arithmetic calculation, and high-bandwidth datatransmission—enabling the resulting hybrid system to radically outperform the
unaugmented brain. 64 But although the possibility of direct connections between
human brains and computers has been demonstrated, it seems unlikely that such
interfaces will be widely used as enhancements any time soon. 65
To begin with, there are significant risks of medical complications—including
infections, electrode displacement, hemorrhage, and cognitive decline—when
implanting electrodes in the brain. Perhaps the most vivid illustration to date of the
benefits that can be obtained through brain stimulation is the treatment of patients
with Parkinson’s disease. The Parkinson’s implant is relatively simple: it does not
really communicate with the brain but simply supplies a stimulating electric current
to the subthalamic nucleus. A demonstration video shows a subject slumped in a
chair, completely immobilized by the disease, then suddenly springing to life when
the current is switched on: the subject now moves his arms, stands up and walks
across the room, turns around and performs a pirouette. Yet even behind this
especially simple and almost miraculously successful procedure, there lurk
negatives. One study of Parkinson patients who had received deep brain implants
showed reductions in verbal fluency, selective attention, color naming, and verbal
memory compared with controls. Treated subjects also reported more cognitive
complaints. 66 Such risks and side effects might be tolerable if the procedure is used
to alleviate severe disability. But in order for healthy subjects to volunteer
themselves for neurosurgery, there would have to be some very substantial
enhancement of normal functionality to be gained.
This brings us to the second reason to doubt that superintelligence will be
achieved through cyborgization, namely that enhancement is likely to be far more
difficult than therapy. Patients who suffer from paralysis might benefit from an
implant that replaces their severed nerves or activates spinal motion pattern
generators. 67 Patients who are deaf or blind might benefit from artificial cochleae
and retinas. 68 Patients with Parkinson’s disease or chronic pain might benefit from
deep brain stimulation that excites or inhibits activity in a particular area of the
brain. 69 What seems far more difficult to achieve is a high-bandwidth direct
interaction between brain and computer to provide substantial increases in
intelligence of a form that could not be more readily attained by other means. Most
of the potential benefits that brain implants could provide in healthy subjects could
be obtained at far less risk, expense, and inconvenience by using our regular motor
and sensory organs to interact with computers located outside of our bodies. We do
not need to plug a fiber optic cable into our brains in order to access the Internet.
Not only can the human retina transmit data at an impressive rate of nearly 10
million bits per second, but it comes pre-packaged with a massive amount of
dedicated wetware, the visual cortex, that is highly adapted to extracting meaning
from this information torrent and to interfacing with other brain areas for furtherprocessing. 70 Even if there were an easy way of pumping more information into our
brains, the extra data inflow would do little to increase the rate at which we think
and learn unless all the neural machinery necessary for making sense of the data
were similarly upgraded. Since this includes almost all of the brain, what would
really be needed is a “whole brain prosthesis–—which is just another way of saying
artificial general intelligence. Yet if one had a human-level AI, one could dispense
with neurosurgery: a computer might as well have a metal casing as one of bone. So
this limiting case just takes us back to the AI path, which we have already examined.
Brain–computer interfacing has also been proposed as a way to get information
out of the brain, for purposes of communicating with other brains or with
machines. 71 Such uplinks have helped patients with locked-in syndrome to
communicate with the outside world by enabling them to move a cursor on a screen
by thought. 72 The bandwidth attained in such experiments is low: the patient
painstakingly types out one slow letter after another at a rate of a few words per
minute. One can readily imagine improved versions of this technology—perhaps a
next-generation implant could plug into Broca’s area (a region in the frontal lobe
involved in language production) and pick up internal speech. 73 But whilst such a
technology might assist some people with disabilities induced by stroke or muscular
degeneration, it would hold little appeal for healthy subjects. The functionality it
would provide is essentially that of a microphone coupled with speech recognition
software, which is already commercially available—minus the pain, inconvenience,
expense, and risks associated with neurosurgery (and minus at least some of the
hyper-Orwellian overtones of an intracranial listening device). Keeping our
machines outside of our bodies also makes upgrading easier.
But what about the dream of bypassing words altogether and establishing a
connection between two brains that enables concepts, thoughts, or entire areas of
expertise to be “downloaded” from one mind to another? We can download large
files to our computers, including libraries with millions of books and articles, and
this can be done over the course of seconds: could something similar be done with
our brains? The apparent plausibility of this idea probably derives from an incorrect
view of how information is stored and represented in the brain. As noted, the rate-
limiting step in human intelligence is not how fast raw data can be fed into the brain
but rather how quickly the brain can extract meaning and make sense of the data.
Perhaps it will be suggested that we transmit meanings directly, rather than package
them into sensory data that must be decoded by the recipient. There are two
problems with this. The first is that brains, by contrast to the kinds of program we
typically run on our computers, do not use standardized data storage and
representation formats. Rather, each brain develops its own idiosyncratic
representations of higher-level content. Which particular neuronal assemblies are
recruited to represent a particular concept depends on the unique experiences of thebrain in question (along with various genetic factors and stochastic physiological
processes). Just as in artificial neural nets, meaning in biological neural networks is
likely represented holistically in the structure and activity patterns of sizeable
overlapping regions, not in discrete memory cells laid out in neat arrays. 74 It would
therefore not be possible to establish a simple mapping between the neurons in one
brain and those in another in such a way that thoughts could automatically slide over
from one to the other. In order for the thoughts of one brain to be intelligible to
another, the thoughts need to be decomposed and packaged into symbols according
to some shared convention that allows the symbols to be correctly interpreted by the
receiving brain. This is the job of language.
In principle, one could imagine offloading the cognitive work of articulation and
interpretation to an interface that would somehow read out the neural states in the
sender’s brain and somehow feed in a bespoke pattern of activation to the receiver’s
brain. But this brings us to the second problem with the cyborg scenario. Even
setting aside the (quite immense) technical challenge of how to reliably read and
write simultaneously from perhaps billions of individually addressable neurons,
creating the requisite interface is probably an AI-complete problem. The interface
would need to include a component able (in real-time) to map firing patterns in one
brain onto semantically equivalent firing patterns in the other brain. The detailed
multilevel understanding of the neural computation needed to accomplish such a
task would seem to directly enable neuromorphic AI.
Despite these reservations, the cyborg route toward cognitive enhancement is not
entirely without promise. Impressive work on the rat hippocampus has demonstrated
the feasibility of a neural prosthesis that can enhance performance in a simple
working-memory task. 75 In its present version, the implant collects input from a
dozen or two electrodes located in one area (“CA3”) of the hippocampus and
projects onto a similar number of neurons in another area (“CA1”). A
microprocessor is trained to discriminate between two different firing patterns in the
first area (corresponding to two different memories, “right lever” or “left lever”) and
to learn how these patterns are projected into the second area. This prosthesis can
not only restore function when the normal neural connection between the two neural
areas is blockaded, but by sending an especially clear token of a particular memory
pattern to the second area it can enhance the performance on the memory task
beyond what the rat is normally capable of. While a technical tour de force by
contemporary standards, the study leaves many challenging questions unanswered:
How well does the approach scale to greater numbers of memories? How well can
we control the combinatorial explosion that otherwise threatens to make learning the
correct mapping infeasible as the number of input and output neurons is increased?
Does the enhanced performance on the test task come at some hidden cost, such as
reduced ability to generalize from the particular stimulus used in the experiment, orreduced ability to unlearn the association when the environment changes? Would the
test subjects still somehow benefit even if—unlike rats—they could avail
themselves of external memory aids such as pen and paper? And how much harder
would it be to apply a similar method to other parts of the brain? Whereas the
present prosthesis takes advantage of the relatively simple feed-forward structure of
parts of the hippocampus (basically serving as a unidirectional bridge between areas
CA3 and CA1), other structures in the cortex involve convoluted feedback loops
which greatly increase the complexity of the wiring diagram and, presumably, the
difficulty of deciphering the functionality of any embedded group of neurons.
One hope for the cyborg route is that the brain, if permanently implanted with a
device connecting it to some external resource, would over time learn an effective
mapping between its own internal cognitive states and the inputs it receives from, or
the outputs accepted by, the device. Then the implant itself would not need to be
intelligent; rather, the brain would intelligently adapt to the interface, much as the
brain of an infant gradually learns to interpret the signals arriving from receptors in
its eyes and ears. 76 But here again one must question how much would really be
gained. Suppose that the brain’s plasticity were such that it could learn to detect
patterns in some new input stream arbitrary projected onto some part of the cortex
by means of a brain–computer interface: why not project the same information onto
the retina instead, as a visual pattern, or onto the cochlea as sounds? The low-tech
alternative avoids a thousand complications, and in either case the brain could
deploy its pattern-recognition mechanisms and plasticity to learn to make sense of
the information.
Networks and organizations
Another conceivable path to superintelligence is through the gradual enhancement of
networks and organizations that link individual human minds with one another and
with various artifacts and bots. The idea here is not that this would enhance the
intellectual capacity of individuals enough to make them superintelligent, but rather
that some system composed of individuals thus networked and organized might
attain a form of superintelligence—what in the next chapter we will elaborate as
“collective superintelligence.” 77
Humanity has gained enormously in collective intelligence over the course of
history and prehistory. The gains come from many sources, including innovations in
communications technology, such as writing and printing, and above all the
introduction of language itself; increases in the size of the world population and the
density of habitation; various improvements in organizational techniques and
epistemic norms; and a gradual accumulation of institutional capital. In generalterms, a system’s collective intelligence is limited by the abilities of its member
minds, the overheads in communicating relevant information between them, and the
various distortions and inefficiencies that pervade human organizations. If
communication overheads are reduced (including not only equipment costs but also
response latencies, time and attention burdens, and other factors), then larger and
more densely connected organizations become feasible. The same could happen if
fixes are found for some of the bureaucratic deformations that warp organizational
life—wasteful status games, mission creep, concealment or falsification of
information, and other agency problems. Even partial solutions to these problems
could pay hefty dividends for collective intelligence.
The technological and institutional innovations that could contribute to the growth
of our collective intelligence are many and various. For example, subsidized
prediction markets might foster truth-seeking norms and improve forecasting on
contentious scientific and social issues. 78 Lie detectors (should it prove feasible to
make ones that are reliable and easy to use) could reduce the scope for deception in
human affairs. 79 Self-deception detectors might be even more powerful. 80 Even
without newfangled brain technologies, some forms of deception might become
harder to practice thanks to increased availability of many kinds of data, including
reputations and track records, or the promulgation of strong epistemic norms and
rationality culture. Voluntary and involuntary surveillance will amass vast amounts
of information about human behavior. Social networking sites are already used by
over a billion people to share personal details: soon, these people might begin
uploading continuous life recordings from microphones and video cameras
embedded in their smart phones or eyeglass frames. Automated analysis of such data
streams will enable many new applications (sinister as well as benign, of course). 81
Growth in collective intelligence may also come from more general
organizational and economic improvements, and from enlarging the fraction of the
world’s population that is educated, digitally connected, and integrated into global
intellectual culture. 82
The Internet stands out as a particularly dynamic frontier for innovation and
experimentation. Most of its potential may still remain unexploited. Continuing
development of an intelligent Web, with better support for deliberation, de-biasing,
and judgment aggregation, might make large contributions to increasing the
collective intelligence of humanity as a whole or of particular groups.
But what of the seemingly more fanciful idea that the Internet might one day
“wake up”? Could the Internet become something more than just the backbone of a
loosely integrated collective superintelligence—something more like a virtual skull
housing an emerging unified super-intellect? (This was one of the ways that
superintelligence could arise according to Vernor Vinge’s influential 1993 essay,
which coined the term “technological singularity.” 83 ) Against this one could objectthat machine intelligence is hard enough to achieve through arduous engineering,
and that it is incredible to suppose that it will arise spontaneously. However, the
story need not be that some future version of the Internet suddenly becomes
superintelligent by mere happenstance. A more plausible version of the scenario
would be that the Internet accumulates improvements through the work of many
people over many years—work to engineer better search and information filtering
algorithms, more powerful data representation formats, more capable autonomous
software agents, and more efficient protocols governing the interactions between
such bots—and that myriad incremental improvements eventually create the basis
for some more unified form of web intelligence. It seems at least conceivable that
such a web-based cognitive system, supersaturated with computer power and all
other resources needed for explosive growth save for one crucial ingredient, could,
when the final missing constituent is dropped into the cauldron, blaze up with
superintelligence. This type of scenario, though, converges into another possible
path to superintelligence, that of artificial general intelligence, which we have
already discussed.
Summary
The fact that there are many paths that lead to superintelligence should increase our
confidence that we will eventually get there. If one path turns out to be blocked, we
can still progress.
That there are multiple paths does not entail that there are multiple destinations.
Even if significant intelligence amplification were first achieved along one of the
non-machine-intelligence paths, this would not render machine intelligence
irrelevant. Quite the contrary: enhanced biological or organizational intelligence
would accelerate scientific and technological developments, potentially hastening
the arrival of more radical forms of intelligence amplification such as whole brain
emulation and AI.
This is not to say that it is a matter of indifference how we get to machine
superintelligence. The path taken to get there could make a big difference to the
eventual outcome. Even if the ultimate capabilities that are obtained do not depend
much on the trajectory, how those capabilities will be used—how much control we
humans have over their disposition—might well depend on details of our approach.
For example, enhancements of biological or organizational intelligence might
increase our ability to anticipate risk and to design machine superintelligence that is
safe and beneficial. (A full strategic assessment involves many complexities, and
will have to await Chapter 14.)
True superintelligence (as opposed to marginal increases in current levels ofintelligence) might plausibly first be attained via the AI path. There are, however,
many fundamental uncertainties along this path. This makes it difficult to rigorously
assess how long the path is or how many obstacles there are along the way. The
whole brain emulation path also has some chance of being the quickest route to
superintelligence. Since progress along this path requires mainly incremental
technological advances rather than theoretical breakthroughs, a strong case can be
made that it will eventually succeed. It seems fairly likely, however, that even if
progress along the whole brain emulation path is swift, artificial intelligence will
nevertheless be first to cross the finishing line: this is because of the possibility of
neuromorphic AIs based on partial emulations.
Biological cognitive enhancements are clearly feasible, particularly ones based on
genetic selection. Iterated embryo selection currently seems like an especially
promising technology. Compared with possible breakthroughs in machine
intelligence, however, biological enhancements would be relatively slow and
gradual. They would, at best, result in relatively weak forms of superintelligence
(more on this shortly).
The clear feasibility of biological enhancement should increase our confidence
that machine intelligence is ultimately achievable, since enhanced human scientists
and engineers will be able to make more and faster progress than their au naturel
counterparts. Especially in scenarios in which machine intelligence is delayed
beyond mid-century, the increasingly cognitively enhanced cohorts coming onstage
will play a growing role in subsequent developments.
Brain–computer interfaces look unlikely as a source of superintelligence.
Improvements in networks and organizations might result in weakly superintelligent
forms of collective intelligence in the long run; but more likely, they will play an
enabling role similar to that of biological cognitive enhancement, gradually
increasing humanity’s effective ability to solve intellectual problems. Compared
with biological enhancements, advances in networks and organization will make a
difference sooner—in fact, such advances are occurring continuously and are having
a significant impact already. However, improvements in networks and organizations
may yield narrower increases in our problem-solving capacity than will
improvements in biological cognition—boosting “collective intelligence” rather
than “quality intelligence,” to anticipate a distinction we are about to introduce in
the next chapter.CHAPTER 3
Forms of superintelligence
So what, exactly, do we mean by “superintelligence”? While we do not wish to
get bogged down in terminological swamps, something needs to be said to clarify
the conceptual ground. This chapter identifies three different forms of
superintelligence, and argues that they are, in a practically relevant sense,
equivalent. We also show that the potential for intelligence in a machine
substrate is vastly greater than in a biological substrate. Machines have a
number of fundamental advantages which will give them overwhelming
superiority. Biological humans, even if enhanced, will be outclassed.
Many machines and nonhuman animals already perform at superhuman levels in
narrow domains. Bats interpret sonar signals better than man, calculators outperform
us in arithmetic, and chess programs beat us in chess. The range of specific tasks
that can be better performed by software will continue to expand. But although
specialized information processing systems will have many uses, there are
additional profound issues that arise only with the prospect of machine intellects
that have enough general intelligence to substitute for humans across the board.
As previously indicated, we use the term “superintelligence” to refer to intellects
that greatly outperform the best current human minds across many very general
cognitive domains. This is still quite vague. Different kinds of system with rather
disparate performance attributes could qualify as superintelligences under this
definition. To advance the analysis, it is helpful to disaggregate this simple notion of
superintelligence by distinguishing different bundles of intellectual super-
capabilities. There are many ways in which such decomposition could be done. Here
we will differentiate between three forms: speed superintelligence, collective
superintelligence, and quality superintelligence.
Speed superintelligence
A speed superintelligence is an intellect that is just like a human mind but faster.
This is conceptually the easiest form of superintelligence to analyze. 1 We can define
speed superintelligence as follows:
Speed superintelligence: A system that can do all that a human intellect cando, but much faster.
By “much” we here mean something like “multiple orders of magnitude.” But rather
than try to expunge every remnant of vagueness from the definition, we will entrust
the reader with interpreting it sensibly. 2
The simplest example of speed superintelligence would be a whole brain
emulation running on fast hardware. 3 An emulation operating at a speed of ten
thousand times that of a biological brain would be able to read a book in a few
seconds and write a PhD thesis in an afternoon. With a speedup factor of a million,
an emulation could accomplish an entire millennium of intellectual work in one
working day. 4
To such a fast mind, events in the external world appear to unfold in slow motion.
Suppose your mind ran at 10,000×. If your fleshly friend should happen to drop his
teacup, you could watch the porcelain slowly descend toward the carpet over the
course of several hours, like a comet silently gliding through space toward an
assignation with a far-off planet; and, as the anticipation of the coming crash tardily
propagates through the folds of your friend’s gray matter and from thence out into
his peripheral nervous system, you could observe his body gradually assuming the
aspect of a frozen oops—enough time for you not only to order a replacement cup
but also to read a couple of scientific papers and take a nap.
Because of this apparent time dilation of the material world, a speed
superintelligence would prefer to work with digital objects. It could live in virtual
reality and deal in the information economy. Alternatively, it could interact with the
physical environment by means of nanoscale manipulators, since limbs at such small
scales could operate faster than macroscopic appendages. (The characteristic
frequency of a system tends to be inversely proportional to its length scale. 5 ) A fast
mind might commune mainly with other fast minds rather than with bradytelic,
molasses-like humans.
The speed of light becomes an increasingly important constraint as minds get
faster, since faster minds face greater opportunity costs in the use of their time for
traveling or communicating over long distances. 6 Light is roughly a million times
faster than a jet plane, so it would take a digital agent with a mental speedup of
1,000,000× about the same amount of subjective time to travel across the globe as it
does a contemporary human journeyer. Dialing somebody long distance would take
as long as getting there “in person,” though it would be cheaper as a call would
require less bandwidth. Agents with large mental speedups who want to converse
extensively might find it advantageous to move near one another. Extremely fast
minds with need for frequent interaction (such as members of a work team) may
take up residence in computers located in the same building to avoid frustratinglatencies.
Collective superintelligence
Another form of superintelligence is a system achieving superior performance by
aggregating large numbers of smaller intelligences:
Collective superintelligence: A system composed of a large number of smaller
intellects such that the system’s overall performance across many very general
domains vastly outstrips that of any current cognitive system.
Collective superintelligence is less conceptually clear-cut than speed
superintelligence. 7 However, it is more familiar empirically. While we have no
experience with human-level minds that differ significantly in clock speed, we do
have ample experience with collective intelligence, systems composed of various
numbers of human-level components working together with various degrees of
efficiency. Firms, work teams, gossip networks, advocacy groups, academic
communities, countries, even humankind as a whole, can—if we adopt a somewhat
abstract perspective—be viewed as loosely defined “systems” capable of solving
classes of intellectual problems. From experience, we have some sense of how easily
different tasks succumb to the efforts of organizations of various size and
composition.
Collective intelligence excels at solving problems that can be readily broken into
parts such that solutions to sub-problems can be pursued in parallel and verified
independently. Tasks like building a space shuttle or operating a hamburger
franchise offer myriad opportunities for division of labor: different engineers work
on different components of the spacecraft; different staffs operate different
restaurants. In academia, the rigid division of researchers, students, journals, grants,
and prizes into separate self-contained disciplines—though unconducive to the type
of work represented by this book—might (only in a conciliatory and mellow frame
of mind) be viewed as a necessary accommodation to the practicalities of allowing
large numbers of diversely motivated individuals and teams to contribute to the
growth of human knowledge while working relatively independently, each plowing
their own furrow.
A system’s collective intelligence could be enhanced by expanding the number or
the quality of its constituent intellects, or by improving the quality of their
organization. 8 To obtain a collective superintelligence from any present-day
collective intelligence would require a very great degree of enhancement. The
resulting system would need to be capable of vastly outperforming any currentcollective intelligence or other cognitive system across many very general domains.
A new conference format that lets scholars exchange information more effectively,
o r a new collaborative information-filtering algorithm that better predicted users’
ratings of books and movies, would clearly not on its own amount to anything
approaching collective superintelligence. Nor would a 50% increase in the world
population, or an improvement in pedagogical method that enabled students to
complete a school day in four hours instead of six. Some far more extreme growth of
humanity’s collective cognitive capacity would be required to meet the criterion of
collective superintelligence.
Note that the threshold for collective superintelligence is indexed to the
performance levels of the present—that is, the early twenty-first century. Over the
course of human prehistory, and again over the course of human history, humanity’s
collective intelligence has grown by very large factors. World population, for
example, has increased by at least a factor of a thousand since the Pleistocene. 9 On
this basis alone, current levels of human collective intelligence could be regarded as
approaching
superintelligence relative to a Pleistocene baseline. Some
improvements in communications technologies—especially spoken language, but
perhaps also cities, writing, and printing—could also be argued to have, individually
or in combination, provided super-sized boosts, in the sense that if another
innovation of comparable impact to our collective intellectual problem-solving
capacity were to happen, it would result in collective superintelligence. 10
A certain kind of reader will be tempted at this point to interject that modern
society does not seem so particularly intelligent. Perhaps some unwelcome political
decision has just been made in the reader’s home country, and the apparent
unwisdom of that decision now looms large in the reader’s mind as evidence of the
mental incapacity of the modern era. And is it not the case that contemporary
humanity is idolizing material consumption, depleting natural resources, polluting
the environment, decimating species diversity, all the while failing to remedy
screaming global injustices and neglecting paramount humanistic or spiritual
values? However, setting aside the question of how modernity’s shortcomings stack
up against the not-so-inconsiderable failings of earlier epochs, nothing in our
definition of collective superintelligence implies that a society with greater
collective intelligence is necessarily better off. The definition does not even imply
that the more collectively intelligent society is wiser. We can think of wisdom as the
ability to get the important things approximately right. It is then possible to imagine
an organization composed of a very large cadre of very efficiently coordinated
knowledge workers, who collectively can solve intellectual problems across many
very general domains. This organization, let us suppose, can operate most kinds of
businesses, invent most kinds of technologies, and optimize most kinds of processes.
Even so, it might get a few key big-picture issues entirely wrong—for instance, itmay fail to take proper precautions against existential risks—and as a result pursue a
short explosive growth spurt that ends ingloriously in total collapse. Such an
organization could have a very high degree of collective intelligence; if sufficiently
high, the organization is a collective superintelligence. We should resist the
temptation to roll every normatively desirable attribute into one giant amorphous
concept of mental functioning, as though one could never find one admirable trait
without all the others being equally present. Instead, we should recognize that there
can exist instrumentally powerful information processing systems—intelligent
systems—that are neither inherently good nor reliably wise. But we will revisit this
issue in Chapter 7.
Collective superintelligence could be either loosely or tightly integrated. To
illustrate a case of loosely integrated collective superintelligence, imagine a planet,
MegaEarth, which has the same level of communication and coordination
technologies that we currently have on the real Earth but with a population one
million times as large. With such a huge population, the total intellectual workforce
on MegaEarth would be correspondingly larger than on our planet. Suppose that a
scientific genius of the caliber of a Newton or an Einstein arises at least once for
every 10 billion people: then on MegaEarth there would be 700,000 such geniuses
living contemporaneously, alongside proportionally vast multitudes of slightly
lesser talents. New ideas and technologies would be developed at a furious pace, and
global civilization on MegaEarth would constitute a loosely integrated collective
superintelligence. 11
If we gradually increase the level of integration of a collective intelligence, it may
eventually become a unified intellect—a single large “mind” as opposed to a mere
assemblage of loosely interacting smaller human minds. 12 The inhabitants of
MegaEarth could take steps in that direction by improving communications and
coordination technologies and by developing better ways for many individuals to
work on any hard intellectual problem together. A collective superintelligence could
thus, after gaining sufficiently in integration, become a “quality superintelligence.”
Quality superintelligence
We can distinguish a third form of superintelligence.
Quality superintelligence: A system that is at least as fast as a human mind
and vastly qualitatively smarter.
As with collective intelligence, intelligence quality is also a somewhat murky
concept; and in this case the difficulty is compounded by our lack of experience withany variations in intelligence quality above the upper end of the present human
distribution. We can, however, get some grasp of the notion by considering some
related cases.
First, we can expand the range of our reference points by considering nonhuman
animals, which have intelligence of lower quality. (This is not meant as a speciesist
remark. A zebrafish has a quality of intelligence that is excellently adapted to its
ecological needs; but the relevant perspective here is a more anthropocentric one:
our concern is with performance on humanly relevant complex cognitive tasks.)
Nonhuman animals lack complex structured language; they are capable of no or only
rudimentary tool use and tool construction; they are severely restricted in their
ability to make long-term plans; and they have very limited abstract reasoning
ability. Nor are these limitations fully explained by a lack of speed or of collective
intelligence among nonhuman animal minds. In terms of raw computational power,
human brains are probably inferior to those of some large animals, including
elephants and whales. And although humanity’s complex technological civilization
would be impossible without our massive advantage in collective intelligence, not
all distinctly human cognitive capabilities depend on collective intelligence. Many
are highly developed even in small, isolated hunter–gatherer bands. 13 And many are
not nearly as highly developed among highly organized nonhuman animals, such as
chimpanzees and dolphins intensely trained by human instructors, or ants living in
their own large and well-ordered societies. Evidently, the remarkable intellectual
achievements of Homo sapiens are to a significant extent attributable to specific
features of our brain architecture, features that depend on a unique genetic
endowment not shared by other animals. This observation can help us illustrate the
concept of quality superintelligence: it is intelligence of quality at least as superior
to that of human intelligence as the quality of human intelligence is superior to that
of elephants’, dolphins’, or chimpanzees’.
A second way to illustrate the concept of quality superintelligence is by noting the
domain-specific cognitive deficits that can afflict individual humans, particularly
deficits that are not caused by general dementia or other conditions associated with
wholesale destruction of the brain’s neurocomputational resources. Consider, for
example, individuals with autism spectrum disorders who may have striking deficits
in social cognition while functioning well in other cognitive domains; or individuals
with congenital amusia, who are unable to hum or recognize simple tunes yet
perform normally in most other respects. Many other examples could be adduced
from the neuropsychiatric literature, which is replete with case studies of patients
suffering narrowly circumscribed deficits caused by genetic abnormalities or brain
trauma. Such examples show that normal human adults have a range of remarkable
cognitive talents that are not simply a function of possessing a sufficient amount of
general neural processing power or even a sufficient amount of general intelligence:specialized neural circuitry is also needed. This observation suggests the idea of
possible but non-realized cognitive talents, talents that no actual human possesses
even though other intelligent systems—ones with no more computing power than the
human brain—that did have those talents would gain enormously in their ability to
accomplish a wide range of strategically relevant tasks.
Accordingly, by considering nonhuman animals and human individuals with
domain-specific cognitive deficits, we can form some notion of different qualities of
intelligence and the practical difference they make. Had Homo sapiens lacked (for
instance) the cognitive modules that enable complex linguistic representations, it
might have been just another simian species living in harmony with nature.
Conversely, were we to gain some new set of modules giving an advantage
comparable to that of being able to form complex linguistic representations, we
would become superintelligent.
Direct and indirect reach
Superintelligence in any of these forms could, over time, develop the technology
necessary to create any of the others. The indirect reaches of these three forms of
superintelligence are therefore equal. In that sense, the indirect reach of current
human intelligence is also in the same equivalence class, under the supposition that
we are able eventually to create some form of superintelligence. Yet there is a sense
in which the three forms of superintelligence are much closer to one another: any
one of them could create other forms of superintelligence more rapidly than we can
create any form of superintelligence from our present starting point.
The direct reaches of the three different forms of superintelligence are harder to
compare. There may be no definite ordering. Their respective capabilities depend on
the degree to which they instantiate their respective advantages—how fast a speed
superintelligence is, how qualitatively superior a quality superintelligence is, and so
forth. At most, we might say that, ceteris paribus, speed superintelligence excels at
tasks requiring the rapid execution of a long series of steps that must be performed
sequentially while collective superintelligence excels at tasks admitting of analytic
decomposition into parallelizable sub-tasks and tasks demanding the combination of
many different perspectives and skill sets. In some vague sense, quality
superintelligence would be the most capable form of all, inasmuch as it could grasp
and solve problems that are, for all practical purposes, beyond the direct reach of
speed superintelligence and collective superintelligence. 14
In some domains, quantity is a poor substitute for quality. One solitary genius
working out of a cork-lined bedroom can write In Search of Lost Time . Could an
equivalent masterpiece be produced by recruiting an office building full of literaryhacks? 15 Even within the range of present human variation we see that some
functions benefit greatly from the labor of one brilliant mastermind as opposed to
the joint efforts of myriad mediocrities. If we widen our purview to include
superintelligent minds, we must countenance a likelihood of there being intellectual
problems solvable only by superintelligence and intractable to any ever-so-large
collective of non-augmented humans.
There might thus be some problems that are solvable by a quality
superintelligence, and perhaps by a speed superintelligence, yet which a loosely
integrated collective superintelligence cannot solve (other than by first amplifying
its own intelligence). 16 We cannot clearly see what all these problems are, but we
can characterize them in general terms. 17 They would tend to be problems involving
multiple complex interdependencies that do not permit of independently verifiable
solution steps: problems that therefore cannot be solved in a piecemeal fashion, and
that might require qualitatively new kinds of understanding or new representational
frameworks that are too deep or too complicated for the current edition of mortals to
discover or use effectively. Some types of artistic creation and strategic cognition
might fall into this category. Some types of scientific breakthrough, perhaps,
likewise. And one can speculate that the tardiness and wobbliness of humanity’s
progress on many of the “eternal problems” of philosophy are due to the
unsuitability of the human cortex for philosophical work. On this view, our most
celebrated philosophers are like dogs walking on their hind legs—just barely
attaining the threshold level of performance required for engaging in the activity at
all. 18
Sources of advantage for digital intelligence
Minor changes in brain volume and wiring can have major consequences, as we see
when we compare the intellectual and technological achievements of humans with
those of other apes. The far greater changes in computing resources and architecture
that machine intelligence will enable will probably have consequences that are even
more profound. It is difficult, perhaps impossible, for us to form an intuitive sense
of the aptitudes of a superintelligence; but we can at least get an inkling of the space
of possibilities by looking at some of the advantages open to digital minds. The
hardware advantages are easiest to appreciate:
• Speed of computational elements. Biological neurons operate at a peak speed of
about 200 Hz, a full seven orders of magnitude slower than a modern
microprocessor (~ 2 GHz). 19 As a consequence, the human brain is forced to rely
on massive parallelization and is incapable of rapidly performing anycomputation that requires a large number of sequential operations. 20 (Anything
the brain does in under a second cannot use much more than a hundred sequential
operations—perhaps only a few dozen.) Yet many of the most practically
important algorithms in programming and computer science are not easily
parallelizable. Many cognitive tasks could be performed far more efficiently if the
brain’s native support for parallelizable pattern-matching algorithms were
complemented by, and integrated with, support for fast sequential processing.
• Internal communication speed. Axons carry action potentials at speeds of 120 m/s
or less, whereas electronic processing cores can communicate optically at the
speed of light (300,000,000 m/s). 21 The sluggishness of neural signals limits how
big a biological brain can be while functioning as a single processing unit. For
example, to achieve a round-trip latency of less than 10 ms between any two
elements in a system, biological brains must be smaller than 0.11 m 3 . An
electronic system, on the other hand, could be 6.1×10 17 m 3 , about the size of a
dwarf planet: eighteen orders of magnitude larger. 22
• Number of computational elements. The human brain has somewhat fewer than 100
billion neurons. 23 Humans have about three and a half times the brain size of
chimpanzees (though only one-fifth the brain size of sperm whales). 24 The
number of neurons in a biological creature is most obviously limited by cranial
volume and metabolic constraints, but other factors may also be significant for
larger brains (such as cooling, development time, and signal-conductance delays
—see the previous point). By contrast, computer hardware is indefinitely scalable
up to very high physical limits. 25 Supercomputers can be warehouse-sized or
larger, with additional remote capacity added via high-speed cables. 26
• Storage capacity. Human working memory is able to hold no more than some four
or five chunks of information at any given time. 27 While it would be misleading
to compare the size of human working memory directly with the amount of RAM
in a digital computer, it is clear that the hardware advantages of digital
intelligences will make it possible for them to have larger working memories.
This might enable such minds to intuitively grasp complex relationships that
humans can only fumblingly handle via plodding calculation. 28 Human long-term
memory is also limited, though it is unclear whether we manage to exhaust its
storage capacity during the course of an ordinary lifetime—the rate at which we
accumulate information is so slow. (On one estimate, the adult human brain stores
about one billion bits—a couple of orders of magnitude less than a low-end
smartphone. 29 ) Both the amount of information stored and the speed with which it
can be accessed could thus be vastly greater in a machine brain than in a
biological brain.
• Reliability, lifespan, sensors, etc. Machine intelligences might have various otherhardware advantages. For example, biological neurons are less reliable than
transistors. 30 Since noisy computing necessitates redundant encoding schemes
that use multiple elements to encode a single bit of information, a digital brain
might derive some efficiency gains from the use of reliable high-precision
computing elements. Brains become fatigued after a few hours of work and start
to permanently decay after a few decades of subjective time; microprocessors are
not subject to these limitations. Data flow into a machine intelligence could be
increased by adding millions of sensors. Depending on the technology used, a
machine might have reconfigurable hardware that can be optimized for changing
task requirements, whereas much of the brain’s architecture is fixed from birth or
only slowly changeable (though the details of synaptic connectivity can change
over shorter timescales, like days). 31
At present, the computational power of the biological brain still compares favorably
with that of digital computers, though top-of-the-line supercomputers are attaining
levels of performance that are within the range of plausible estimates of the brain’s
processing power. 32 But hardware is rapidly improving, and the ultimate limits of
hardware performance are vastly higher than those of biological computing
substrates.
Digital minds will also benefit from major advantages in software:
• Editability. It is easier to experiment with parameter variations in software than in
neural wetware. For example, with a whole brain emulation one could easily trial
what happens if one adds more neurons in a particular cortical area or if one
increases or decreases their excitability. Running such experiments in living
biological brains would be far more difficult.
• Duplicability. With software, one can quickly make arbitrarily many high-fidelity
copies to fill the available hardware base. Biological brains, by contrast, can be
reproduced only very slowly; and each new instance starts out in a helpless state,
remembering nothing of what its parents learned in their lifetimes.
• Goal coordination. Human collectives are replete with inefficiencies arising from
the fact that it is nearly impossible to achieve complete uniformity of purpose
among the members of a large group—at least until it becomes feasible to induce
docility on a large scale by means of drugs or genetic selection. A “copy clan” (a
group of identical or almost identical programs sharing a common goal) would
avoid such coordination problems.
• Memory sharing. Biological brains need extended periods of training and
mentorship whereas digital minds could acquire new memories and skills by
swapping data files. A population of a billion copies of an AI program could
synchronize their databases periodically, so that all the instances of the program
know everything that any instance learned during the previous hour. (Directmemory transfer requires standardized representational formats. Easy swapping of
high-level cognitive content would therefore not be possible between just any pair
of machine intelligences. In particular, it would not be possible among first-
generation whole brain emulations.)
• New modules, modalities, and algorithms. Visual perception seems to us easy and
effortless, quite unlike solving textbook geometry problems—this despite the fact
that it takes a massive amount of computation to reconstruct, from the two-
dimensional patterns of stimulation on our retinas, a three-dimensional
representation of a world populated with recognizable objects. The reason this
seems easy is that we have dedicated low-level neural machinery for processing
visual information. This low-level processing occurs unconsciously and
automatically, without draining our mental energy or conscious attention. Music
perception, language use, social cognition, and other forms of information
processing that are “natural” for us humans seem to be likewise supported by
dedicated neurocomputational modules. An artificial mind that had such
specialized support for other cognitive domains that have become important in the
contemporary world—such as engineering, computer programming, and business
strategy—would have big advantages over minds like ours that have to rely on
clunky general-purpose cognition to think about such things. New algorithms may
also be developed to take advantage of the distinct affordances of digital
hardware, such as its support for fast serial processing.
The ultimately attainable advantages of machine intelligence, hardware and software
combined, are enormous. 33 But how rapidly could those potential advantages be
realized? That is the question to which we now turn.CHAPTER 4
The kinetics of an intelligence explosion
Once machines attain some form of human-equivalence in general reasoning
ability, how long will it then be before they attain radical superintelligence?
Will this be a slow, gradual, protracted transition? Or will it be sudden,
explosive? This chapter analyzes the kinetics of the transition to
superintelligence as a function of optimization power and system recalcitrance.
We consider what we know or may reasonably surmise about the behavior of
these two factors in the neighborhood of human-level general intelligence.
Timing and speed of the takeoff
Given that machines will eventually vastly exceed biology in general intelligence,
but that machine cognition is currently vastly narrower than human cognition, one is
led to wonder how quickly this usurpation will take place. The question we are
asking here must be sharply distinguished from the question we considered in
Chapter 1 about how far away we currently are from developing a machine with
human-level general intelligence. Here the question is instead, if and when such a
machine is developed, how long will it be from then until a machine becomes
radically superintelligent? Note that one could think that it will take quite a long
time until machines reach the human baseline, or one might be agnostic about how
long that will take, and yet have a strong view that once this happens, the further
ascent into strong superintelligence will be very rapid.
It can be helpful to think about these matters schematically, even though doing so
involves temporarily ignoring some qualifications and complicating details.
Consider, then, a diagram that plots the intellectual capability of the most advanced
machine intelligence system as a function of time (Figure 7).
A horizontal line labeled “human baseline” represents the effective intellectual
capabilities of a representative human adult with access to the information sources
and technological aids currently available in developed countries. At present, the
most advanced AI system is far below the human baseline on any reasonable metric
of general intellectual ability. At some point in future, a machine might reach
approximate parity with this human baseline (which we take to be fixed—anchored
to the year 2014, say, even if the capabilities of human individuals should have
increased in the intervening years): this would mark the onset of the takeoff. Thecapabilities of the system continue to grow, and at some later point the system
reaches parity with the combined intellectual capability of all of humanity (again
anchored to the present): what we may call the “civilization baseline”. Eventually, if
the system’s abilities continue to grow, it attains “strong superintelligence”—a level
of intelligence vastly greater than contemporary humanity’s combined intellectual
wherewithal. The attainment of strong superintelligence marks the completion of the
takeoff, though the system might continue to gain in capacity thereafter. Sometime
during the takeoff phase, the system may pass a landmark which we can call “the
crossover”, a point beyond which the system’s further improvement is mainly driven
by the system’s own actions rather than by work performed upon it by others. 1 (The
possible existence of such a crossover will become important in the subsection on
optimization power and explosivity, later in this chapter.)
Figure 7 Shape of the takeoff. It is important to distinguish between these questions:
“Will a takeoff occur, and if so, when?” and “If and when a takeoff does occur, how
steep will it be?” One might hold, for example, that it will be a very long time before
a takeoff occurs, but that when it does it will proceed very quickly. Another relevant
question (not illustrated in this figure) is, “How large a fraction of the world
economy will participate in the takeoff?” These questions are related but distinct.
With this picture in mind, we can distinguish three classes of transition scenarios
—scenarios in which systems progress from human-level intelligence to
superintelligence—based on their steepness; that is to say, whether they represent a
slow, fast, or moderate takeoff.
Slow
A slow takeoff is one that occurs over some long temporal interval, such as
decades or centuries. Slow takeoff scenarios offer excellent opportunities for
human political processes to adapt and respond. Different approaches can betried and tested in sequence. New experts can be trained and credentialed.
Grassroots campaigns can be mobilized by groups that feel they are being
disadvantaged by unfolding developments. If it appears that new kinds of
secure infrastructure or mass surveillance of AI researchers is needed, such
systems could be developed and deployed. Nations fearing an AI arms race
would have time to try to negotiate treaties and design enforcement
mechanisms. Most preparations undertaken before onset of the slow takeoff
would be rendered obsolete as better solutions would gradually become visible
in the light of the dawning era.
Fast
A fast takeoff occurs over some short temporal interval, such as minutes, hours,
or days. Fast takeoff scenarios offer scant opportunity for humans to deliberate.
Nobody need even notice anything unusual before the game is already lost. In a
fast takeoff scenario, humanity’s fate essentially depends on preparations
previously put in place. At the slowest end of the fast takeoff scenario range,
some simple human actions might be possible, analogous to flicking open the
“nuclear suitcase”; but any such action would either be elementary or have been
planned and pre-programmed in advance.
Moderate
A moderate takeoff is one that occurs over some intermediary temporal
interval, such as months or years. Moderate takeoff scenarios give humans
some chance to respond but not much time to analyze the situation, to test
different approaches, or to solve complicated coordination problems. There is
not enough time to develop or deploy new systems (e.g. political systems,
surveillance regimes, or computer network security protocols), but extant
systems could be applied to the new challenge.
During a slow takeoff, there would be plenty of time for the news to get out. In a
moderate takeoff, by contrast, it is possible that developments would be kept secret
as they unfold. Knowledge might be restricted to a small group of insiders, as in a
covert state-sponsored military research program. Commercial projects, small
academic teams, and “nine hackers in a basement” outfits might also be clandestine
—though, if the prospect of an intelligence explosion were “on the radar” of state
intelligence agencies as a national security priority, then the most promising private
projects would seem to have a good chance of being under surveillance. The host
state (or a dominant foreign power) would then have the option of nationalizing or
shutting down any project that showed signs of commencing takeoff. Fast takeoffs
would happen so quickly that there would not be much time for word to get out orfor anybody to mount a meaningful reaction if it did. But an outsider might
intervene before the onset of the takeoff if they believed a particular project to be
closing in on success.
Moderate takeoff scenarios could lead to geopolitical, social, and economic
turbulence as individuals and groups jockey to position themselves to gain from the
unfolding transformation. Such upheaval, should it occur, might impede efforts to
orchestrate a well-composed response; alternatively, it might enable solutions more
radical than calmer circumstances would permit. For instance, in a moderate takeoff
scenario where cheap and capable emulations or other digital minds gradually flood
labor markets over a period of years, one could imagine mass protests by laid-off
workers pressuring governments to increase unemployment benefits or institute a
living wage guarantee to all human citizens, or to levy special taxes or impose
minimum wage requirements on employers who use emulation workers. In order for
any relief derived from such policies to be more than fleeting, support for them
would somehow have to be cemented into permanent power structures. Similar
issues can arise if the takeoff is slow rather than moderate, but the disequilibrium
and rapid change in moderate scenarios may present special opportunities for small
groups to wield disproportionate influence.
It might appear to some readers that of these three types of scenario, the slow
takeoff is the most probable, the moderate takeoff is less probable, and the fast
takeoff is utterly implausible. It could seem fanciful to suppose that the world could
be radically transformed and humanity deposed from its position as apex cogitator
over the course of an hour or two. No change of such moment has ever occurred in
human history, and its nearest parallels—the Agricultural and Industrial Revolutions
—played out over much longer timescales (centuries to millennia in the former case,
decades to centuries in the latter). So the base rate for the kind of transition entailed
by a fast or medium takeoff scenario, in terms of the speed and magnitude of the
postulated change, is zero: it lacks precedent outside myth and religion. 2
Nevertheless, this chapter will present some reasons for thinking that the slow
transition scenario is improbable. If and when a takeoff occurs, it will likely be
explosive.
To begin to analyze the question of how fast the takeoff will be, we can conceive
of the rate of increase in a system’s intelligence as a (monotonically increasing)
function of two variables: the amount of “optimization power”, or quality-weighted
design effort, that is being applied to increase the system’s intelligence, and the
responsiveness of the system to the application of a given amount of such
optimization power. We might term the inverse of responsiveness “recalcitrance”,
and write:Pending some specification of how to quantify intelligence, design effort, and
recalcitrance, this expression is merely qualitative. But we can at least observe that a
system’s intelligence will increase rapidly if either a lot of skilled effort is applied
to the task of increasing its intelligence and the system’s intelligence is not too hard
to increase or there is a non-trivial design effort and the system’s recalcitrance is
low (or both). If we know how much design effort is going into improving a
particular system, and the rate of improvement this effort produces, we could
calculate the system’s recalcitrance.
Further, we can observe that the amount of optimization power devoted to
improving some system’s performance varies between systems and over time. A
system’s recalcitrance might also vary depending on how much the system has
already been optimized. Often, the easiest improvements are made first, leading to
diminishing returns (increasing recalcitrance) as low-hanging fruits are depleted.
However, there can also be improvements that make further improvements easier,
leading to improvement cascades. The process of solving a jigsaw puzzle starts out
simple—it is easy to find the corners and the edges. Then recalcitrance goes up as
subsequent pieces are harder to fit. But as the puzzle nears completion, the search
space collapses and the process gets easier again.
To proceed in our inquiry, we must therefore analyze how recalcitrance and
optimization power might vary in the critical time periods during the takeoff. This
will occupy us over the next few pages.
Recalcitrance
Let us begin with recalcitrance. The outlook here depends on the type of the system
under consideration. For completeness, we first cast a brief glance at the
recalcitrance that would be encountered along paths to superintelligence that do not
involve advanced machine intelligence. We find that recalcitrance along those paths
appears to be fairly high. That done, we will turn to the main case, which is that the
takeoff involves machine intelligence; and there we find that recalcitrance at the
critical juncture seems low.
Non-machine intelligence paths
Cognitive enhancement via improvements in public health and diet has steeply
diminishing returns. 3 Big gains come from eliminating severe nutritional
deficiencies, and the most severe deficiencies have already been largely eliminatedin all but the poorest countries. Only girth is gained by increasing an already
adequate diet. Education, too, is now probably subject to diminishing returns. The
fraction of talented individuals in the world who lack access to quality education is
still substantial, but declining.
Pharmacological enhancers might deliver some cognitive gains over the coming
decades. But after the easiest fixes have been accomplished—perhaps sustainable
increases in mental energy and ability to concentrate, along with better control over
the rate of long-term memory consolidation—subsequent gains will be increasingly
hard to come by. Unlike diet and public health approaches, however, improving
cognition through smart drugs might get easier before it gets harder. The field of
neuropharmacology still lacks much of the basic knowledge that would be needed to
competently intervene in the healthy brain. Neglect of enhancement medicine as a
legitimate area of research may be partially to blame for this current backwardness.
If neuroscience and pharmacology continue to progress for a while longer without
focusing on cognitive enhancement, then maybe there would be some relatively easy
gains to be had when at last the development of nootropics becomes a serious
priority. 4
Genetic cognitive enhancement has a U-shaped recalcitrance profile similar to
that of nootropics, but with larger potential gains. Recalcitrance starts out high while
the only available method is selective breeding sustained over many generations,
something that is obviously difficult to accomplish on a globally significant scale.
Genetic enhancement will get easier as technology is developed for cheap and
effective genetic testing and selection (and particularly when iterated embryo
selection becomes feasible in humans). These new techniques will make it possible
to tap the pool of existing human genetic variation for intelligence-enhancing
alleles. As the best existing alleles get incorporated into genetic enhancement
packages, however, further gains will get harder to come by. The need for more
innovative approaches to genetic modification may then increase recalcitrance.
There are limits to how quickly things can progress along the genetic enhancement
path, most notably the fact that germline interventions are subject to an inevitable
maturational lag: this strongly counteracts the possibility of a fast or moderate
takeoff. 5 That embryo selection can only be applied in the context of in vitro
fertilization will slow its rate of adoption: another limiting factor.
The recalcitrance along the brain–computer path seems initially very high. In the
unlikely event that it somehow becomes easy to insert brain implants and to achieve
high-level functional integration with the cortex, recalcitrance might plummet. In
the long run, the difficulty of making progress along this path would be similar to
that involved in improving emulations or AIs, since the bulk of the brain–computer
system’s intelligence would eventually reside in the computer part.
The recalcitrance for making networks and organizations in general moreefficient is high. A vast amount of effort is going into overcoming this recalcitrance,
and the result is an annual improvement of humanity’s total capacity by perhaps no
more than a couple of percent. 6 Furthermore, shifts in the internal and external
environment mean that organizations, even if efficient at one time, soon become ill-
adapted to their new circumstances. Ongoing reform effort is thus required even just
to prevent deterioration. A step change in the rate of gain in average organizational
efficiency is perhaps conceivable, but it is hard to see how even the most radical
scenario of this kind could produce anything faster than a slow takeoff, since
organizations operated by humans are confined to work on human timescales. The
Internet continues to be an exciting frontier with many opportunities for enhancing
collective intelligence, with a recalcitrance that seems at the moment to be in the
moderate range—progress is somewhat swift but a lot of effort is going into making
this progress happen. It may be expected to increase as low-hanging fruits (such as
search engines and email) are depleted.
Emulation and AI paths
The difficulty of advancing toward whole brain emulation is difficult to estimate.
Yet we can point to a specific future milestone: the successful emulation of an insect
brain. That milestone stands on a hill, and its conquest would bring into view much
of the terrain ahead, allowing us to make a decent guess at the recalcitrance of
scaling up the technology to human whole brain emulation. (A successful emulation
of a small-mammal brain, such as that of a mouse, would give an even better
vantage point that would allow the distance remaining to a human whole brain
emulation to be estimated with a high degree of precision.) The path toward
artificial intelligence, by contrast, may feature no such obvious milestone or early
observation point. It is entirely possible that the quest for artificial intelligence will
appear to be lost in dense jungle until an unexpected breakthrough reveals the
finishing line in a clearing just a few short steps away.
Recall the distinction between these two questions: How hard is it to attain
roughly human levels of cognitive ability? And how hard is it to get from there to
superhuman levels? The first question is mainly relevant for predicting how long it
will be before the onset of a takeoff. It is the second question that is key to assessing
the shape of the takeoff, which is our aim here. And though it might be tempting to
suppose that the step from human level to superhuman level must be the harder one
—this step, after all, takes place “at a higher altitude” where capacity must be
superadded to an already quite capable system—this would be a very unsafe
assumption. It is quite possible that recalcitrance falls when a machine reaches
human parity.
Consider first whole brain emulation. The difficulties involved in creating thefirst human emulation are of a quite different kind from those involved in enhancing
an existing emulation. Creating a first emulation involves huge technological
challenges, particularly in regard to developing the requisite scanning and image
interpretation capabilities. This step might also require considerable amounts of
physical capital—an industrial-scale machine park with hundreds of high-throughput
scanning machines is not implausible. By contrast, enhancing the quality of an
existing emulation involves tweaking algorithms and data structures: essentially a
software problem, and one that could turn out to be much easier than perfecting the
imaging technology needed to create the original template. Programmers could
easily experiment with tricks like increasing the neuron count in different cortical
areas to see how it affects performance. 7 They also could work on code optimization
and on finding simpler computational models that preserve the essential
functionality of individual neurons or small networks of neurons. If the last
technological prerequisite to fall into place is either scanning or translation, with
computing power being relatively abundant, then not much attention might have
been given during the development phase to implementational efficiency, and easy
opportunities for computational efficiency savings might be available. (More
fundamental architectural reorganization might also be possible, but that takes us off
the emulation path and into AI territory.)
Another way to improve the code base once the first emulation has been produced
is to scan additional brains with different or superior skills and talents. Productivity
growth would also occur as a consequence of adapting organizational structures and
workflows to the unique attributes of digital minds. Since there is no precedent in
the human economy of a worker who can be literally copied, reset, run at different
speeds, and so forth, managers of the first emulation cohort would find plenty of
room for innovation in managerial practices.
After initially plummeting when human whole brain emulation becomes possible,
recalcitrance may rise again. Sooner or later, the most glaring implementational
inefficiencies will have been optimized away, the most promising algorithmic
variations will have been tested, and the easiest opportunities for organizational
innovation will have been exploited. The template library will have expanded so that
acquiring more brain scans would add little benefit over working with existing
templates. Since a template can be multiplied, each copy can be individually trained
in a different field, and this can be done at electronic speed, it might be that the
number of brains that would need to be scanned in order to capture most of the
potential economic gains is small. Possibly a single brain would suffice.
Another potential cause of escalating recalcitrance is the possibility that
emulations or their biological supporters will organize to support regulations
restricting the use of emulation workers, limiting emulation copying, prohibiting
certain kinds of experimentation with digital minds, instituting workers’ rights and aminimum wage for emulations, and so forth. It is equally possible, however, that
political developments would go in the opposite direction, contributing to a fall in
recalcitrance. This might happen if initial restraint in the use of emulation labor
gives way to unfettered exploitation as competition heats up and the economic and
strategic costs of occupying the moral high ground become clear.
As for artificial intelligence (non-emulation machine intelligence), the difficulty
of lifting a system from human-level to superhuman intelligence by means of
algorithmic improvements depends on the attributes of the particular system.
Different architectures might have very different recalcitrance.
In some situations, recalcitrance could be extremely low. For example, if human-
level AI is delayed because one key insight long eludes programmers, then when the
final breakthrough occurs, the AI might leapfrog from below to radically above
human level without even touching the intermediary rungs. Another situation in
which recalcitrance could turn out to be extremely low is that of an AI system that
can achieve intelligent capability via two different modes of processing. To
illustrate this possibility, suppose an AI is composed of two subsystems, one
possessing domain-specific problem-solving techniques, the other possessing
general-purpose reasoning ability. It could then be the case that while the second
subsystem remains below a certain capacity threshold, it contributes nothing to the
system’s overall performance, because the solutions it generates are always inferior
to those generated by the domain-specific subsystem. Suppose now that a small
amount of optimization power is applied to the general-purpose subsystem and that
this produces a brisk rise in the capacity of that subsystem. At first, we observe no
increase in the overall system’s performance, indicating that recalcitrance is high.
Then, once the capacity of the general-purpose subsystem crosses the threshold
where its solutions start to beat those of the domain-specific subsystem, the overall
system’s performance suddenly begins to improve at the same brisk pace as the
general-purpose subsystem, even as the amount of optimization power applied stays
constant: the system’s recalcitrance has plummeted.
It is also possible that our natural tendency to view intelligence from an
anthropocentric perspective will lead us to underestimate improvements in sub-
human systems, and thus to overestimate recalcitrance. Eliezer Yudkowsky, an AI
theorist who has written extensively on the future of machine intelligence, puts the
point as follows:
AI might make an apparently sharp jump in intelligence purely as the result of
anthropomorphism, the human tendency to think of “village idiot” and
“Einstein” as the extreme ends of the intelligence scale, instead of nearly
indistinguishable points on the scale of minds-in-general. Everything dumber
than a dumb human may appear to us as simply “dumb”. One imagines the “AI
arrow” creeping steadily up the scale of intelligence, moving past mice andchimpanzees, with AIs still remaining “dumb” because AIs cannot speak fluent
language or write science papers, and then the AI arrow crosses the tiny gap
from infra-idiot to ultra-Einstein in the course of one month or some similarly
short period. 8 (See Fig. 8.)
The upshot of these several considerations is that it is difficult to predict how hard
it will be to make algorithmic improvements in the first AI that reaches a roughly
human level of general intelligence. There are at least some possible circumstances
in which algorithm-recalcitrance is low. But even if algorithm-recalcitrance is very
high, this would not preclude the overall recalcitrance of the AI in question from
being low. For it might be easy to increase the intelligence of the system in other
ways than by improving its algorithms. There are two other factors that can be
improved: content and hardware.
First, consider content improvements. By “content” we here mean those parts of a
system’s software assets that do not make up its core algorithmic architecture.
Content might include, for example, databases of stored percepts, specialized skills
libraries, and inventories of declarative knowledge. For many kinds of system, the
distinction between algorithmic architecture and content is very unsharp;
nevertheless, it will serve as a rough-and-ready way of pointing to one potentially
important source of capability gains in a machine intelligence. An alternative way of
expressing much the same idea is by saying that a system’s intellectual problem-
solving capacity can be enhanced not only by making the system cleverer but also by
expanding what the system knows.
Figure 8 A less anthropomorphic scale? The gap between a dumb and a clever
person may appear large from an anthropocentric perspective, yet in a less parochial
view the two have nearly indistinguishable minds. 9 It will almost certainly prove
harder and take longer to build a machine intelligence that has a general level of
smartness comparable to that of a village idiot than to improve such a system so that
it becomes much smarter than any human.
Consider a contemporary AI system such as TextRunner (a research project at the
University of Washington) or IBM’s Watson (the system that won the Jeopardy!
quiz show). These systems can extract certain pieces of semantic information byanalyzing text. Although these systems do not understand what they read in the same
sense or to the same extent as a human does, they can nevertheless extract
significant amounts of information from natural language and use that information
to make simple inferences and answer questions. They can also learn from
experience, building up more extensive representations of a concept as they
encounter additional instances of its use. They are designed to operate for much of
the time in unsupervised mode (i.e. to learn hidden structure in unlabeled data in the
absence of error or reward signal, without human guidance) and to be fast and
scalable. TextRunner, for instance, works with a corpus of 500 million web pages. 10
Now imagine a remote descendant of such a system that has acquired the ability to
read with as much understanding as a human ten-year-old but with a reading speed
similar to that of TextRunner. (This is probably an AI-complete problem.) So we are
imagining a system that thinks much faster and has much better memory than a
human adult, but knows much less, and perhaps the net effect of this is that the
system is roughly human-equivalent in its general problem-solving ability. But its
content recalcitrance is very low—low enough to precipitate a takeoff. Within a few
weeks, the system has read and mastered all the content contained in the Library of
Congress. Now the system knows much more than any human being and thinks
vastly faster: it has become (at least) weakly superintelligent.
A system might thus greatly boost its effective intellectual capability by
absorbing pre-produced content accumulated through centuries of human science
and civilization: for instance, by reading through the Internet. If an AI reaches
human level without previously having had access to this material or without having
been able to digest it, then the AI’s overall recalcitrance will be low even if it is hard
to improve its algorithmic architecture.
Content-recalcitrance is a relevant concept for emulations, too. A high-speed
emulation has an advantage not only because it can complete the same tasks as
biological humans more quickly, but also because it can accumulate more timely
content, such as task-relevant skills and expertise. In order to tap the full potential of
fast content accumulation, however, a system needs to have a correspondingly large
memory capacity. There is little point in reading an entire library if you have
forgotten all about the aardvark by the time you get to the abalone. While an AI
system is likely to have adequate memory capacity, emulations would inherit some
of the capacity limitations of their human templates. They may therefore need
architectural enhancements in order to become capable of unbounded learning.
So far we have considered the recalcitrance of architecture and of content—that
is, how difficult it would be to improve the software of a machine intelligence that
has reached human parity. Now let us look at a third way of boosting the
performance of machine intelligence: improving its hardware. What would be the
recalcitrance for hardware-driven improvements?Starting with intelligent software (emulation or AI) one can amplify collective
intelligence simply by using additional computers to run more instances of the
program. 11 One could also amplify speed intelligence by moving the program to
faster computers. Depending on the degree to which the program lends itself to
parallelization, speed intelligence could also be amplified by running the program
on more processors. This is likely to be feasible for emulations, which have a highly
parallelized architecture; but many AI programs, too, have important subroutines
that can benefit from massive parallelization. Amplifying quality intelligence by
increasing computing power might also be possible, but that case is less
straightforward. 12
The recalcitrance for amplifying collective or speed intelligence (and possibly
quality intelligence) in a system with human-level software is therefore likely to be
low. The only difficulty involved is gaining access to additional computing power.
There are several ways for a system to expand its hardware base, each relevant over
a different timescale.
In the short term, computing power should scale roughly linearly with funding:
twice the funding buys twice the number of computers, enabling twice as many
instances of the software to be run simultaneously. The emergence of cloud
computing services gives a project the option to scale up its computational resources
without even having to wait for new computers to be delivered and installed, though
concerns over secrecy might favor the use of in-house computers. (In certain
scenarios, computing power could also be obtained by other means, such as by
commandeering botnets. 13 ) Just how easy it would be to scale the system by a given
factor depends on how much computing power the initial system uses. A system that
initially runs on a PC could be scaled by a factor of thousands for a mere million
dollars. A program that runs on a supercomputer would be far more expensive to
scale.
In the slightly longer term, the cost of acquiring additional hardware may be
driven up as a growing portion of the world’s installed capacity is being used to run
digital minds. For instance, in a competitive market-based emulation scenario, the
cost of running one additional copy of an emulation should rise to be roughly equal
to the income generated by the marginal copy, as investors bid up the price for
existing computing infrastructure to match the return they expect from their
investment (though if only one project has mastered the technology it might gain a
degree of monopsony power in the computing power market and therefore pay a
lower price).
Over a somewhat longer timescale, the supply of computing power will grow as
new capacity is installed. A demand spike would spur production in existing
semiconductor foundries and stimulate the construction of new plants. (A one-off
performance boost, perhaps amounting to one or two orders of magnitude, mightalso be obtainable by using customized microprocessors. 14 ) Above all, the rising
wave of technology improvements will pour increasing volumes of computational
power into the turbines of the thinking machines. Historically, the rate of
improvement of computing technology has been described by the famous Moore’s
law, which in one of its variations states that computing power per dollar doubles
every 18 months or so. 15 Although one cannot bank on this rate of improvement
continuing up to the development of human-level machine intelligence, yet until
fundamental physical limits are reached there will remain room for advances in
computing technology.
There are thus reasons to expect that hardware recalcitrance will not be very high.
Purchasing more computing power for the system once it proves its mettle by
attaining human-level intelligence might easily add several orders of magnitude of
computing power (depending on how hardware-frugal the project was before
expansion). Chip customization might add one or two orders of magnitude. Other
means of expanding the hardware base, such as building more factories and
advancing the frontier of computing technology, take longer—normally several
years, though this lag would be radically compressed once machine
superintelligence revolutionizes manufacturing and technology development.
In summary, we can talk about the likelihood of a hardware overhang: when
human-level software is created, enough computing power may already be available
to run vast numbers of copies at great speed. Software recalcitrance, as discussed
above, is harder to assess but might be even lower than hardware recalcitrance. In
particular, there may be content overhang in the form of pre-made content (e.g. the
Internet) that becomes available to a system once it reaches human parity. Algorithm
overhang—pre-designed algorithmic enhancements—is also possible but perhaps
less likely. Software improvements (whether in algorithms or content) might offer
orders of magnitude of potential performance gains that could be fairly easily
accessed once a digital mind attains human parity, on top of the performance gains
attainable by using more or better hardware.
Optimization power and explosivity
Having examined the question of recalcitrance we must now turn to the other half of
our schematic equation, optimization power. To recall: Rate of change in
Intelligence = Optimization power/Recalcitrance. As reflected in this schematic, a
fast takeoff does not require that recalcitrance during the transition phase be low. A
fast takeoff could also result if recalcitrance is constant or even moderately
increasing, provided the optimization power being applied to improving the
system’s performance grows sufficiently rapidly. As we shall now see, there aregood grounds for thinking that the applied optimization power will increase during
the transition, at least in the absence of a deliberate measures to prevent this from
happening.
We can distinguish two phases. The first phase begins with the onset of the
takeoff, when the system reaches the human baseline for individual intelligence. As
the system’s capability continues to increase, it might use some or all of that
capability to improve itself (or to design a successor system—which, for present
purposes, comes to the same thing). However, most of the optimization power
applied to the system still comes from outside the system, either from the work of
programmers and engineers employed within the project or from such work done by
the rest of the world as can be appropriated and used by the project. 16 If this phase
drags out for any significant period of time, we can expect the amount of
optimization power applied to the system to grow. Inputs both from inside the
project and from the outside world are likely to increase as the promise of the
chosen approach becomes manifest. Researchers may work harder, more researchers
may be recruited, and more computing power may be purchased to expedite
progress. The increase could be especially dramatic if the development of human-
level machine intelligence takes the world by surprise, in which case what was
previously a small research project might suddenly become the focus of intense
research and development efforts around the world (though some of those efforts
might be channeled into competing projects).
A second growth phase will begin if at some point the system has acquired so
much capability that most of the optimization power exerted on it comes from the
system itself (marked by the variable level labeled “crossover” in Figure 7). This
fundamentally changes the dynamic, because any increase in the system’s capability
now translates into a proportional increase in the amount of optimization power
being applied to its further improvement. If recalcitrance remains constant, this
feedback dynamic produces exponential growth (see Box 4). The doubling constant
depends on the scenario but might be extremely short—mere seconds in some
scenarios—if growth is occurring at electronic speeds, which might happen as a
result of algorithmic improvements or the exploitation of an overhang of content or
hardware. 17 Growth that is driven by physical construction, such as the production of
new computers or manufacturing equipment, would require a somewhat longer
timescale (but still one that might be very short compared with the current growth
rate of the world economy).
It is thus likely that the applied optimization power will increase during the
transition: initially because humans try harder to improve a machine intelligence
that is showing spectacular promise, later because the machine intelligence itself
becomes capable of driving further progress at digital speeds. This would create a
real possibility of a fast or medium takeoff even if recalcitrance were constant orslightly increasing around the human baseline . 18 Yet we saw in the previous
subsection that there are factors that could lead to a big drop in recalcitrance around
the human baseline level of capability. These factors include, for example, the
possibility of rapid hardware expansion once a working software mind has been
attained; the possibility of algorithmic improvements; the possibility of scanning
additional brains (in the case of whole brain emulation); and the possibility of
rapidly incorporating vast amounts of content by digesting the Internet (in the case
of artificial intelligence). 24
Box 4 On the kinetics of an intelligence explosion
We can write the rate of change in intelligence as the ratio between the optimization
power applied to the system and the system’s recalcitrance:
The amount of optimization power acting on a system is the sum of whatever
optimization power the system itself contributes and the optimization power exerted
from without. For example, a seed AI might be improved through a combination of
its own efforts and the efforts of a human programming team, and perhaps also the
efforts of the wider global community of researchers making continuous advances in
the semiconductor industry, computer science, and related fields: 19
A seed AI starts out with very limited cognitive capacities. At the outset, therefore,
is small. 20 What about
and
? There are cases in which a single
project has more relevant capability than the rest of the world combined—the
Manhattan project, for instance, brought a very large fraction of the world’s best
physicists to Los Alamos to work on the atomic bomb. More commonly, any one
project contains only a small fraction of the world’s total relevant research
capability. But even when the outside world has a greater total amount of relevant
research capability than any one project,
may nevertheless exceed
, since
much of the outside world’s capability is not be focused on the particular system in
question. If a project begins to look promising—which will happen when a system
passes the human baseline if not before—it might attract additional investment,
increasing
. If the project’s accomplishments are public,
might also riseas the progress inspires greater interest in machine intelligence generally and as
various powers scramble to get in on the game. During the transition phase,
therefore, total optimization power applied to improving a cognitive system is likely
to increase as the capability of the system increases. 21
As the system’s capabilities grow, there may come a point at which the
optimization power generated by the system itself starts to dominate the
optimization power applied to it from outside (across all significant dimensions of
improvement):
This crossover is significant because beyond this point, further improvement to the
system’s capabilities contributes strongly to increasing the total optimization power
applied to improving the system. We thereby enter a regime of strong recursive self-
improvement. This leads to explosive growth of the system’s capability under a
fairly wide range of different shapes of the recalcitrance curve.
To illustrate, consider first a scenario in which recalcitrance is constant, so that
the rate of increase in an AI’s intelligence is equal to the optimization power being
applied. Assume that all the optimization power that is applied comes from the AI
itself and that the AI applies all its intelligence to the task of amplifying its own
intelligence, so that
= I. 22 We then have
Solving this simple differential equation yields the exponential function
But recalcitrance being constant is a rather special case. Recalcitrance might well
decline around the human baseline, due to one or more of the factors mentioned in
the previous subsection, and remain low around the crossover and some distance
beyond (perhaps until the system eventually approaches fundamental physical
limits). For example, suppose that the optimization power applied to the system is
roughly constant (i.e.
+
≈ c) prior to the system becoming capable of
contributing substantially to its own design, and that this leads to the system
doubling in capacity every 18 months. (This would be roughly in line with historical
improvement rates from Moore’s law combined with software advances. 23 ) This rate
of improvement, if achieved by means of roughly constant optimization power,
entails recalcitrance declining as the inverse of the system power:If recalcitrance continues to fall along this hyperbolic pattern, then when the AI
reaches the crossover point the total amount of optimization power applied to
improving the AI has doubled. We then have
The next doubling occurs 7.5 months later. Within 17.9 months, the system’s
capacity has grown a thousandfold, thus obtaining speed superintelligence (Figure
9).
This particular growth trajectory has a positive singularity at t = 18 months. In
reality, the assumption that recalcitrance is constant would cease to hold as the
system began to approach the physical limits to information processing, if not
sooner.
These two scenarios are intended for illustration only; many other trajectories are
possible, depending on the shape of the recalcitrance curve. The claim is simply that
the strong feedback loop that sets in around the crossover point tends strongly to
make the takeoff faster than it would otherwise have been.
Figure 9 One simple model of an intelligence explosion.
These observations notwithstanding, the shape of the recalcitrance curve in the
relevant region is not yet well characterized. In particular, it is unclear how difficult
it would be to improve the software quality of a human-level emulation or AI. The
difficulty of expanding the hardware power available to a system is also not clear.Whereas today it would be relatively easy to increase the computing power available
to a small project by spending a thousand times more on computing power or by
waiting a few years for the price of computers to fall, it is possible that the first
machine intelligence to reach the human baseline will result from a large project
involving pricey supercomputers, which cannot be cheaply scaled, and that Moore’s
law will by then have expired. For these reasons, although a fast or medium takeoff
looks more likely, the possibility of a slow takeoff cannot be excluded. 25CHAPTER 5
Decisive strategic advantage
A question distinct from, but related to, the question of kinetics is whether there
will there be one superintelligent power or many? Might an intelligence
explosion propel one project so far ahead of all others as to make it able to
dictate the future? Or will progress be more uniform, unfurling across a wide
front, with many projects participating but none securing an overwhelming and
permanent lead?
The preceding chapter analyzed one key parameter in determining the size of the gap
that might plausibly open up between a leading power and its nearest competitors—
namely, the speed of the transition from human to strongly superhuman intelligence.
This suggests a first-cut analysis. If the takeoff is fast (completed over the course of
hours, days, or weeks) then it is unlikely that two independent projects would be
taking off concurrently: almost certainty, the first project would have completed its
takeoff before any other project would have started its own. If the takeoff is slow
(stretching over many years or decades) then there could plausibly be multiple
projects undergoing takeoffs concurrently, so that although the projects would by the
end of the transition have gained enormously in capability, there would be no time at
which any project was far enough ahead of the others to give it an overwhelming
lead. A takeoff of moderate speed is poised in between, with either condition a
possibility: there might or might not be more than one project undergoing the
takeoff at the same time. 1
Will one machine intelligence project get so far ahead of the competition that it
gets a decisive strategic advantage—that is, a level of technological and other
advantages sufficient to enable it to achieve complete world domination? If a project
did obtain a decisive strategic advantage, would it use it to suppress competitors and
form a singleton (a world order in which there is at the global level a single
decision-making agency)? And if there is a winning project, how “large” would it be
—not in terms of physical size or budget but in terms of how many people’s desires
would be controlling its design? We will consider these questions in turn.
Will the frontrunner get a decisive strategic advantage?
One factor influencing the width of the gap between frontrunner and followers is therate of diffusion of whatever it is that gives the leader a competitive advantage. A
frontrunner might find it difficult to gain and maintain a large lead if followers can
easily copy the frontrunner’s ideas and innovations. Imitation creates a headwind
that disadvantages the leader and benefits laggards, especially if intellectual
property is weakly protected. A frontrunner might also be especially vulnerable to
expropriation, taxation, or being broken up under anti-monopoly regulation.
It would be a mistake, however, to assume that this headwind must increase
monotonically with the gap between frontrunner and followers. Just as a racing
cyclist who falls too far behind the competition is no longer shielded from the wind
by the cyclists ahead, so a technology follower who lags sufficiently behind the
cutting edge might find it hard to assimilate the advances being made at the
frontier. 2 The gap in understanding and capability might have grown too large. The
leader might have migrated to a more advanced technology platform, making
subsequent innovations untransferable to the primitive platforms used by laggards.
A sufficiently pre-eminent leader might have the ability to stem information leakage
from its research programs and its sensitive installations, or to sabotage its
competitors’ efforts to develop their own advanced capabilities.
If the frontrunner is an AI system, it could have attributes that make it easier for it
to expand its capabilities while reducing the rate of diffusion. In human-run
organizations, economies of scale are counteracted by bureaucratic inefficiencies
and agency problems, including difficulties in keeping trade secrets. 3 These
problems would presumably limit the growth of a machine intelligence project so
long as it is operated by humans. An AI system, however, might avoid some of these
scale diseconomies, since the AI’s modules (in contrast to human workers) need not
have individual preferences that diverge from those of the system as a whole. Thus,
the AI system could avoid a sizeable chunk of the inefficiencies arising from agency
problems in human enterprises. The same advantage—having perfectly loyal parts—
would also make it easier for an AI system to pursue long-range clandestine goals.
An AI would have no disgruntled employees ready to be poached by competitors or
bribed into becoming informants. 4
We can get a sense of the distribution of plausible gaps in development times by
looking at some historical examples (see Box 5). It appears that lags in the range of
a few months to a few years are typical of strategically significant technology
projects.
Box 5 Technology races: some historical examples
Over long historical timescales, there has been an increase in the rate at whichknowledge and technology diffuse around the globe. As a result, the temporal gaps
between technology leaders and nearest followers have narrowed.
China managed to maintain a monopoly on silk production for over two thousand
years. Archeological finds suggest that production might have begun around 3000
BC , or even earlier. 5 Sericulture was a closely held secret. Revealing the techniques
was punishable by death, as was exporting silkworms or their eggs outside China.
The Romans, despite the high price commanded by imported silk cloth in their
empire, never learnt the art of silk manufacture. Not until around AD 300 did a
Japanese expedition manage to capture some silkworm eggs along with four young
Chinese girls, who were forced to divulge the art to their abductors. 6 Byzantium
joined the club of producers in AD 522. The story of porcelain-making also features
long lags. The craft was practiced in China during the Tang Dynasty around AD 600
(and might have been in use as early as AD 200), but was mastered by Europeans
only in the eighteenth century. 7 Wheeled vehicles appeared in several sites across
Europe and Mesopotamia around 3500 BC but reached the Americas only in post-
Columbian times. 8 On a grander scale, the human species took tens of thousands of
years to spread across most of the globe, the Agricultural Revolution thousands of
years, the Industrial Revolution only hundreds of years, and an Information
Revolution could be said to have spread globally over the course of decades—
though, of course, these transitions are not necessarily of equal profundity. (The
Dance Dance Revolution video game spread from Japan to Europe and North
America in just one year!)
Technological competition has been quite extensively studied, particularly in the
contexts of patent races and arms races. 9 It is beyond the scope of our investigation
to review this literature here. However, it is instructive to look at some examples of
strategically significant technology races in the twentieth century (see Table 7).
With regard to these six technologies, which were regarded as strategically
important by the rivaling superpowers because of their military or symbolic
significance, the gaps between leader and nearest laggard were (very approximately)
49 months, 36 months, 4 months, 1 month, 4 months, and 60 months, respectively—
longer than the duration of a fast takeoff and shorter than the duration of a slow
takeoff. 10 In many cases, the laggard’s project benefitted from espionage and
publicly available information. The mere demonstration of the feasibility of an
invention can also encourage others to develop it independently; and fear of falling
behind can spur the efforts to catch up.
Perhaps closer to the case of AI are mathematical inventions that do not require
the development of new physical infrastructure. Often these are published in the
academic literature and can thus be regarded as universally available; but in some
cases, when the discovery appears to offer a strategic advantage, publication is
delayed. For example, two of the most important ideas in public-key cryptographyare the Diffie–Hellman key exchange protocol and the RSA encryption scheme.
These were discovered by the academic community in 1976 and 1978, respectively,
but it has later been confirmed that they were known by cryptographers at the UK’s
communications security group since the early 1970s. 20 Large software projects
might offer a closer analogy with AI projects, but it is harder to give crisp examples
of typical lags because software is usually rolled out in incremental installments and
the functionalities of competing systems are often not directly comparable.
Table 7 Some strategically significant technology races
It is possible that globalization and increased surveillance will reduce typical lags
between competing technology projects. Yet there is likely to be a lower bound on
how short the average lag could become (in the absence of deliberate
coordination). 21 Even absent dynamics that lead to snowball effects, some projects
will happen to end up with better research staff, leadership, and infrastructure, or
will just stumble upon better ideas. If two projects pursue alternative approaches,
one of which turns out to work better, it may take the rival project many months to
switch to the superior approach even if it is able to closely monitor what the
forerunner is doing.
Combining these observations with our earlier discussion of the speed of the
takeoff, we can conclude that it is highly unlikely that two projects would be close
enough to undergo a fast takeoff concurrently; for a medium takeoff, it could easily
go either way; and for a slow takeoff, it is highly likely that several projects would
undergo the process in parallel. But the analysis needs a further step. The key
question is not how many projects undergo a takeoff in tandem, but how many
projects emerge on the yonder side sufficiently tightly clustered in capability that
none of them has a decisive strategic advantage. If the takeoff process is relatively
slow to begin and then gets faster, the distance between competing projects would
tend to grow. To return to our bicycle metaphor, the situation would be analogous toa pair of cyclists making their way up a steep hill, one trailing some distance behind
the other—the gap between them then expanding as the frontrunner reaches the peak
and starts accelerating down the other side.
Consider the following medium takeoff scenario. Suppose it takes a project one
year to increase its AI’s capability from the human baseline to a strong
superintelligence, and that one project enters this takeoff phase with a six-month
lead over the next most advanced project. The two projects will be undergoing a
takeoff concurrently. It might seem, then, that neither project gets a decisive
strategic advantage. But that need not be so. Suppose it takes nine months to advance
from the human baseline to the crossover point, and another three months from there
to strong superintelligence. The frontrunner then attains strong superintelligence
three months before the following project even reaches the crossover point. This
would give the leading project a decisive strategic advantage and the opportunity to
parlay its lead into permanent control by disabling the competing projects and
establishing a singleton. (Note that the concept of a singleton is an abstract one: a
singleton could be democracy, a tyranny, a single dominant AI, a strong set of global
norms that include effective provisions for their own enforcement, or even an alien
overlord—its defining characteristic being simply that it is some form of agency
that can solve all major global coordination problems. It may, but need not,
resemble any familiar form of human governance. 22 )
Since there is an especially strong prospect of explosive growth just after the
crossover point, when the strong positive feedback loop of optimization power kicks
in, a scenario of this kind is a serious possibility, and it increases the chances that
the leading project will attain a decisive strategic advantage even if the takeoff is not
fast.
How large will the successful project be?
Some paths to superintelligence require great resources and are therefore likely to be
the preserve of large well-funded projects. Whole brain emulation, for instance,
requires many different kinds of expertise and lots of equipment. Biological
intelligence enhancements and brain–computer interfaces would also have a large
scale factor: while a small biotech firm might invent one or two drugs, achieving
superintelligence along one of these paths (if doable at all) would likely require
many inventions and many tests, and therefore the backing of an industrial sector or
a well-funded national program. Achieving collective superintelligence by making
organizations and networks more efficient requires even more extensive input,
involving much of the world economy.
The AI path is more difficult to assess. Perhaps it would require a very largeresearch program; perhaps it could be done by a small group. A lone hacker scenario
cannot be excluded either. Building a seed AI might require insights and algorithms
developed over many decades by the scientific community around the world. But it
is possible that the last critical breakthrough idea might come from a single
individual or a small group that succeeds in putting everything together. This
scenario is less realistic for some AI architectures than others. A system that has a
large number of parts that need to be tweaked and tuned to work effectively together,
and then painstakingly loaded with custom-made cognitive content, is likely to
require a larger project. But if a seed AI could be instantiated as a simple system,
one whose construction depends only on getting a few basic principles right, then the
feat might be within the reach of a small team or an individual. The likelihood of the
final breakthrough being made by a small project increases if most previous
progress in the field has been published in the open literature or made available as
open source software.
We must distinguish the question of how big will be the project that directly
engineers the system from the question of how big the group will be that controls
whether, how, and when the system is created. The atomic bomb was created
primarily by a group of scientists and engineers. (The Manhattan Project employed
about 130,000 people at its peak, the vast majority of whom were construction
workers or building operators. 23 ) These technical experts, however, were controlled
by the US military, which was directed by the US government, which was ultimately
accountable to the American electorate, which at the time constituted about one-
tenth of the adult world population. 24
Monitoring
Given the extreme security implications of superintelligence, governments would
likely seek to nationalize any project on their territory that they thought close to
achieving a takeoff. A powerful state might also attempt to acquire projects located
in other countries through espionage, theft, kidnapping, bribery, threats, military
conquest, or any other available means. A powerful state that cannot acquire a
foreign project might instead destroy it, especially if the host country lacks an
effective deterrent. If global governance structures are strong by the time a
breakthrough begins to look imminent, it is possible that promising projects would
be placed under international control.
An important question, therefore, is whether national or international authorities
will see an intelligence explosion coming. At present, intelligence agencies do not
appear to be looking very hard for promising AI projects or other forms of
potentially explosive intelligence amplification. 25 If they are indeed not paying(much) attention, this is presumably due to the widely shared perception that there is
no prospect whatever of imminent superintelligence. If and when it becomes a
common belief among prestigious scientists that there is a substantial chance that
superintelligence is just around the corner, the major intelligence agencies of the
world would probably start to monitor groups and individuals who seem to be
engaged in relevant research. Any project that began to show sufficient progress
could then be promptly nationalized. If political elites were persuaded by the
seriousness of the risk, civilian efforts in sensitive areas might be regulated or
outlawed.
How difficult would such monitoring be? The task is easier if the goal is only to
keep track of the leading project. In that case, surveillance focusing on the several
best-resourced projects may be sufficient. If the goal is instead to prevent any work
from taking place (at least outside of specially authorized institutions) then
surveillance would have to be more comprehensive, since many small projects and
individuals are in a position to make at least some progress.
It would be easier to monitor projects that require significant amounts of physical
capital, as would be the case with a whole brain emulation project. Artificial
intelligence research, by contrast, requires only a personal computer, and would
therefore be more difficult to monitor. Some of the theoretical work could be done
with pen and paper. Even so, it would not be too difficult to identify most capable
individuals with a serious long-standing interest in artificial general intelligence
research. Such individuals usually leave visible trails. They may have published
academic papers, presented at conferences, posted on Internet forums, or earned
degrees from leading computer science departments. They may also have had
communications with other AI researchers, allowing them to be identified by
mapping the social graph.
Projects designed from the outset to be secret could be more difficult to detect. An
ordinary software development project could serve as a front. 26 Only careful
analysis of the code being produced would reveal the true nature of what the project
was trying to accomplish. Such analysis would require a lot of (highly skilled)
manpower, whence only a small number of suspect projects could be scrutinized at
this level. The task would become much easier if effective lie detection technology
had been developed and could be routinely used in this kind of surveillance. 27
Another reason states might fail to catch precursor developments is the inherent
difficulty of forecasting some types of breakthrough. This is more relevant to AI
research than to whole brain emulation development, since for the latter the key
breakthrough is more likely to be preceded by a clear gradient of steady advances.
It is also possible that intelligence agencies and other government bureaucracies
have a certain clumsiness or rigidity that might prevent them from understanding the
significance of some developments that might be clear to some outside groups.Barriers to official understanding of a potential intelligence explosion might be
especially steep. It is conceivable, for example, that the topic will become inflamed
with religious or political controversies, rendering it taboo for officials in some
countries. The topic might become associated with some discredited figure or with
charlatanry and hype in general, hence shunned by respected scientists and other
establishment figures. (As we saw in Chapter 1, something like this has already
happened twice: recall the two “AI winters.”) Industry groups might lobby to
prevent aspersions being cast on profitable business areas; academic communities
might close ranks to marginalize those who voice concerns about long-term
consequences of the science that is being done. 28
Consequently, a total intelligence failure cannot be ruled out. Such a failure is
especially likely if breakthroughs should occur in the nearer future, before the issue
has risen to public prominence. And even if intelligence agencies get it right,
political leaders might not listen or act on the advice. Getting the Manhattan Project
started took an extraordinary effort by several visionary physicists, including
especially Mark Oliphant and Leó Szilárd: the latter persuaded Eugene Wigner to
persuade Albert Einstein to put his name on a letter to persuade President Franklin
D. Roosevelt to look into the matter. Even after the project reached its full scale,
Roosevelt remained skeptical of its workability and significance, as did his
successor Harry Truman.
For better or worse, it would probably be harder for a small group of activists to
affect the outcome of an intelligence explosion if big players, such as states, are
taking active part. Opportunities for private individuals to reduce the overall amount
of existential risk from a potential intelligence explosion are therefore greatest in
scenarios in which big players remain relatively oblivious to the issue, or in which
the early efforts of activists make a major difference to whether, when, which, or
with what attitude big players enter the game. Activists seeking maximum expected
impact may therefore wish to focus most of their planning on such high-leverage
scenarios, even if they believe that scenarios in which big players end up calling all
the shots are more probable.
International collaboration
International coordination is more likely if global governance structures generally
get stronger. Coordination might also be more likely if the significance of an
intelligence explosion is widely appreciated ahead of time and if effective
monitoring of all serious projects is feasible. Even if monitoring is infeasible,
however, international cooperation would still be possible. Many countries could
band together to support a joint project. If such a joint project were sufficiently well
resourced, it could have a good chance of being the first to reach the goal, especiallyif any rival project had to be small and secretive to elude detection.
There are precedents of large-scale successful multinational scientific
collaborations, such as the International Space Station, the Human Genome Project,
and the Large Hadron Collider. 29 However, the major motivation for collaboration in
those cases was cost-sharing. (In the case of the International Space Station,
fostering a collaborative spirit between Russia and the United States was itself an
important goal. 30 ) Achieving similar collaboration on a project that has enormous
security implications would be more difficult. A country that believed it could
achieve a breakthrough unilaterally might be tempted to go it alone rather than
subordinate its efforts to a joint project. A country might also refrain from joining
an international collaboration from fear that other participants might siphon off
collaboratively generated insights and use them to accelerate a covert national
project.
An international project would thus need to overcome major security challenges,
and a fair amount of trust would probably be needed to get it started, trust that may
take time to develop. Consider that even after the thaw in relations between the
United States and the Soviet Union following Gorbachev’s ascent to power, arms
reduction efforts—which could be greatly in the interests of both superpowers—had
a fitful beginning. Gorbachev was seeking steep reductions in nuclear arms but
negotiations stalled on the issue of Reagan’s Strategic Defense Initiative (“Star
Wars”), which the Kremlin strenuously opposed. At the Reykjavík Summit meeting
in 1986, Reagan proposed that the United States would share with the Soviet Union
the technology that would be developed under the Strategic Defense Initiative, so
that both countries could enjoy protection against accidental launches and against
smaller nations that might develop nuclear weapons. Yet Gorbachev was not
persuaded by this apparent win–win proposition. He viewed the gambit as a ruse,
refusing to credit the notion that the Americans would share the fruits of their most
advanced military research at a time when they were not even willing to share with
the Soviets their technology for milking cows. 31 Regardless of whether Reagan was
in fact sincere in his offer of superpower collaboration, mistrust made the proposal a
non-starter.
Collaboration is easier to achieve between allies, but even there it is not
automatic. When the Soviet Union and the United States were allied against
Germany during World War II, the United States concealed its atomic bomb project
from the Soviet Union. The United States did collaborate on the Manhattan Project
with Britain and Canada. 32 Similarly, the United Kingdom concealed its success in
breaking the German Enigma code from the Soviet Union, but shared it—albeit with
some difficulty—with the United States. 33 This suggests that in order to achieve
international collaboration on some technology that is of pivotal importance for
national security, it might be necessary to have built beforehand a close and trustingrelationship.
We will return in Chapter 14 to the desirability and feasibility of international
collaboration in the development of intelligence amplification technologies.
From decisive strategic advantage to singleton
Would a project that obtained a decisive strategic advantage choose to use it to form
a singleton?
Consider a vaguely analogous historical situation. The United States developed
nuclear weapons in 1945. It was the sole nuclear power until the Soviet Union
developed the atom bomb in 1949. During this interval—and for some time
thereafter—the United States may have had, or been in a position to achieve, a
decisive military advantage.
The United States could then, theoretically, have used its nuclear monopoly to
create a singleton. One way in which it could have done so would have been by
embarking on an all-out effort to build up its nuclear arsenal and then threatening
(and if necessary, carrying out) a nuclear first strike to destroy the industrial
capacity of any incipient nuclear program in the USSR and any other country
tempted to develop a nuclear capability.
A more benign course of action, which might also have had a chance of working,
would have been to use its nuclear arsenal as a bargaining chip to negotiate a strong
international government—a veto-less United Nations with a nuclear monopoly and
a mandate to take all necessary actions to prevent any country from developing its
own nuclear weapons.
Both of these approaches were proposed at the time. The hardline approach of
launching or threatening a first strike was advocated by some prominent
intellectuals such as Bertrand Russell (who had long been active in anti-war
movements and who would later spend decades campaigning against nuclear
weapons) and John von Neumann (co-creator of game theory and one of the
architects of US nuclear strategy). 34 Perhaps it is a sign of civilizational progress
that the very idea of threatening a nuclear first strike today seems borderline silly or
morally obscene.
A version of the benign approach was tried in 1946 by the United States in the
form of the Baruch plan. The proposal involved the USA giving up its temporary
nuclear monopoly. Uranium and thorium mining and nuclear technology would be
placed under the control of an international agency operating under the auspices of
the United Nations. The proposal called for the permanent members of the Security
Council to give up their vetoes in matters related to nuclear weapons in order to
prevent any great power found to be in breach of the accord from vetoing theimposition of remedies. 35 Stalin, seeing that the Soviet Union and its allies could be
easily outvoted in both the Security Council and the General Assembly, rejected the
proposal. A frosty atmosphere of mutual suspicion descended on the relations
between the former wartime allies, mistrust that soon solidified into the Cold War.
As had been widely predicted, a costly and extremely dangerous nuclear arms race
followed.
Many factors might dissuade a human organization with a decisive strategic
advantage from creating a singleton. These include non-aggregative or bounded
utility functions, non-maximizing decision rules, confusion and uncertainty,
coordination problems, and various costs associated with a takeover. But what if it
were not a human organization but a superintelligent artificial agent that came into
possession of a decisive strategic advantage? Would the aforementioned factors be
equally effective at inhibiting an AI from attempting to seize power? Let us briefly
run through the list of factors and consider how they might apply in this case.
Human individuals and human organizations typically have preferences over
resources that are not well represented by an “unbounded aggregative utility
function.” A human will typically not wager all her capital for a fifty–fifty chance of
doubling it. A state will typically not risk losing all its territory for a ten percent
chance of a tenfold expansion. For individuals and governments, there are
diminishing returns to most resources. The same need not hold for AIs. (We will
return to the problem of AI motivation in subsequent chapters.) An AI might
therefore be more likely to pursue a risky course of action that has some chance of
giving it control of the world.
Humans and human-run organizations may also operate with decision processes
that do not seek to maximize expected utility. For example, they may allow for
fundamental risk aversion, or “satisficing” decision rules that focus on meeting
adequacy thresholds, or “deontological” side-constraints that proscribe certain kinds
of action regardless of how desirable their consequences. Human decision makers
often seem to be acting out an identity or a social role rather than seeking to
maximize the achievement of some particular objective. Again, this need not apply
to artificial agents.
Bounded utility functions, risk aversion, and non-maximizing decision rules may
combine synergistically with strategic confusion and uncertainty. Revolutions, even
when they succeed in overthrowing the existing order, often fail to produce the
outcome that their instigators had promised. This tends to stay the hand of a human
agent if the contemplated action is irreversible, norm-breaking, and lacking
precedent. A superintelligence might perceive the situation more clearly and
therefore face less strategic confusion and uncertainty about the outcome should it
attempt to use its apparent decisive strategic advantage to consolidate its dominant
position.Another major factor that can inhibit groups from exploiting a potentially decisive
strategic advantage is the problem of internal coordination. Members of a
conspiracy that is in a position to seize power must worry not only about being
infiltrated from the outside, but also about being overthrown by some smaller
coalition of insiders. If a group consists of a hundred people, and a majority of sixty
can take power and disenfranchise the non-conspirators, what is then to stop a thirty-
five-strong subset of these sixty from disenfranchising the other twenty-five? And
then maybe a subset of twenty disenfranchising the other fifteen? Each of the
original hundred might have good reason to uphold certain established norms to
prevent the general unraveling that could result from any attempt to change the
social contract by means of a naked power grab. This problem of internal
coordination would not apply to an AI system that constitutes a single unified
agent. 36
Finally, there is the issue of cost. Even if the United States could have used its
nuclear monopoly to establish a singleton, it might not have been able to do so
without incurring substantial costs. In the case of a negotiated agreement to place
nuclear weapons under the control of a reformed and strengthened United Nations,
these costs might have been relatively small; but the costs—moral, economic,
political, and human—of actually attempting world conquest through the waging of
nuclear war would have been almost unthinkably large, even during the period of
nuclear monopoly. With sufficient technological superiority, however, these costs
would be far smaller. Consider, for example, a scenario in which one nation had such
a vast technological lead that it could safely disarm all other nations at the press of a
button, without anybody dying or being injured, and with almost no damage to
infrastructure or to the environment. With such almost magical technological
superiority, a first strike would be a lot more tempting. Or consider an even greater
level of technological superiority which might enable the frontrunner to cause other
nations to voluntarily lay down their arms, not by threatening them with destruction
but simply by persuading a great majority of their populations by means of an
extremely effectively designed advertising and propaganda campaign extolling the
virtues of global unity. If this were done with the intention to benefit everybody, for
instance by replacing national rivalries and arms races with a fair, representative,
and effective world government, it is not clear that there would be even a cogent
moral objection to the leveraging of a temporary strategic advantage into a
permanent singleton.
Various considerations thus point to an increased likelihood that a future power
with superintelligence that obtained a sufficiently large strategic advantage would
actually use it to form a singleton. The desirability of such an outcome depends, of
course, on the nature of the singleton that would be created and also on what the
future of intelligent life would look like in alternative multipolar scenarios. We willrevisit those questions in later chapters. But first let us take a closer look at why and
how a superintelligence would be powerful and effective at achieving outcomes in
the world.CHAPTER 6
Cognitive superpowers
Suppose that a digital superintelligent agent came into being, and that for some
reason it wanted to take control of the world: would it be able to do so? In this
chapter we consider some powers that a superintelligence could develop and
what they may enable it to do. We outline a takeover scenario that illustrates
how a superintelligent agent, starting as mere software, could establish itself as
a singleton. We also offer some remarks on the relation between power over
nature and power over other agents.
The principal reason for humanity’s dominant position on Earth is that our brains
have a slightly expanded set of faculties compared with other animals. 1 Our greater
intelligence lets us transmit culture more efficiently, with the result that knowledge
and technology accumulates from one generation to the next. By now sufficient
content has accumulated to make possible space flight, H-bombs, genetic
engineering, computers, factory farms, insecticides, the international peace
movement, and all the accouterments of modern civilization. Geologists have started
referring to the present era as the Anthropocene in recognition of the distinctive
biotic, sedimentary, and geochemical signatures of human activities. 2 On one
estimate, we appropriate 24% of the planetary ecosystem’s net primary production. 3
And yet we are far from having reached the physical limits of technology.
These observations make it plausible that any type of entity that developed a
much greater than human level of intelligence would be potentially extremely
powerful. Such entities could accumulate content much faster than us and invent
new technologies on a much shorter timescale. They could also use their intelligence
to strategize more effectively than we can.
Let us consider some of the capabilities that a superintelligence could have and
how it could use them.
Functionalities and superpowers
It is important not to anthropomorphize superintelligence when thinking about its
potential impacts. Anthropomorphic frames encourage unfounded expectations
about the growth trajectory of a seed AI and about the psychology, motivations, and
capabilities of a mature superintelligence.For example, a common assumption is that a superintelligent machine would be
like a very clever but nerdy human being. We imagine that the AI has book smarts
but lacks social savvy, or that it is logical but not intuitive and creative. This idea
probably originates in observation: we look at present-day computers and see that
they are good at calculation, remembering facts, and at following the letter of
instructions while being oblivious to social contexts and subtexts, norms, emotions,
and politics. The association is strengthened when we observe that the people who
are good at working with computers tend themselves to be nerds. So it is natural to
assume that more advanced computational intelligence will have similar attributes,
only to a higher degree.
This heuristic might retain some validity in the early stages of development of a
seed AI. (There is no reason whatever to suppose that it would apply to emulations
or to cognitively enhanced humans.) In its immature stage, what is later to become a
superintelligent AI might still lack many skills and talents that come naturally to a
human; and the pattern of such a seed AI’s strengths and weaknesses might indeed
bear some vague resemblance to an IQ nerd. The most essential characteristic of a
seed AI, aside from being easy to improve (having low recalcitrance), is being good
at exerting optimization power to amplify a system’s intelligence: a skill which is
presumably closely related to doing well in mathematics, programming,
engineering, computer science research, and other such “nerdy” pursuits. However,
even if a seed AI does have such a nerdy capability profile at one stage of its
development, this does not entail that it will grow into a similarly limited mature
superintelligence. Recall the distinction between direct and indirect reach. With
sufficient skill at intelligence amplification, all other intellectual abilities are within
a system’s indirect reach: the system can develop new cognitive modules and skills
as needed—including empathy, political acumen, and any other powers
stereotypically wanting in computer-like personalities.
Even if we recognize that a superintelligence can have all the skills and talents we
find in the human distribution, along with other talents that are not found among
humans, the tendency toward anthropomorphizing can still lead us to underestimate
the extent to which a machine superintelligence could exceed the human level of
performance. Eliezer Yudkowsky, as we saw in an earlier chapter, has been
particularly emphatic in condemning this kind of misconception: our intuitive
concepts of “smart” and “stupid” are distilled from our experience of variation over
the range of human thinkers, yet the differences in cognitive ability within this
human cluster are trivial in comparison to the differences between any human
intellect and a superintelligence. 4
Chapter 3 reviewed some of the potential sources of advantage for machine
intelligence. The magnitudes of the advantages are such as to suggest that rather
than thinking of a superintelligent AI as smart in the sense that a scientific genius issmart compared with the average human being, it might be closer to the mark to
think of such an AI as smart in the sense that an average human being is smart
compared with a beetle or a worm.
It would be convenient if we could quantify the cognitive caliber of an arbitrary
cognitive system using some familiar metric, such as IQ scores or some version of
the Elo ratings that measure the relative abilities of players in two-player games
such as chess. But these metrics are not useful in the context of superhuman
artificial general intelligence. We are not interested in how likely a superintelligence
is to win at a game of chess. As for IQ scores, they are informative only insofar as
we have some idea of how they correlate with practically relevant outcomes. 5 For
example, we have data that show that people with an IQ of 130 are more likely than
those with an IQ of 90 to excel in school and to do well in a wide range of
cognitively demanding jobs. But suppose we could somehow establish that a certain
future AI will have an IQ of 6,455: then what? We would have no idea of what such
an AI could actually do. We would not even know that such an AI had as much
general intelligence as a normal human adult—perhaps the AI would instead have a
bundle of special-purpose algorithms enabling it to solve typical intelligence test
questions with superhuman efficiency but not much else.
Some recent efforts have been made to develop measurements of cognitive
capacity that could be applied to a wider range of information-processing systems,
including artificial intelligences. 6 Work in this direction, if it can overcome various
technical difficulties, may turn out to be quite useful for some scientific purposes
including AI development. For purposes of the present investigation, however, its
usefulness would be limited since we would remain unenlightened about what a
given superhuman performance score entails for actual ability to achieve practically
important outcomes in the world.
It will therefore serve our purposes better to list some strategically important
tasks and then to characterize hypothetical cognitive systems in terms of whether
they have or lack whatever skills are needed to succeed at these tasks. See Table 8.
We will say that a system that sufficiently excels at any of the tasks in this table has
a corresponding superpower.
A full-blown superintelligence would greatly excel at all of these tasks and would
thus have the full panoply of all six superpowers. Whether there is a practically
significant possibility of a domain-limited intelligence that has some of the
superpowers but remains unable for a significant period of time to acquire all of
them is not clear. Creating a machine with any one of these superpowers appears to
be an AI-complete problem. Yet it is conceivable that, for example, a collective
superintelligence consisting of a sufficiently large number of human-like biological
or electronic minds would have, say, the economic productivity superpower but lack
the strategizing superpower. Likewise, it is conceivable that a specializedengineering AI could be built that has the technology research superpower while
completely lacking skills in other areas. This is more plausible if there exists some
particular technological domain such that virtuosity within that domain would be
sufficient for the generation of an overwhelmingly superior general-purpose
technology. For instance, one could imagine a specialized AI adept at simulating
molecular systems and at inventing nanomolecular designs that realize a wide range
of important capabilities (such as computers or weapons systems with futuristic
performance characteristics) described by the user only at a fairly high level of
abstraction. 7 Such an AI might also be able to produce a detailed blueprint for how
to bootstrap from existing technology (such as biotechnology and protein
engineering) to the constructor capabilities needed for high-throughput atomically
precise manufacturing that would allow inexpensive fabrication of a much wider
range of nanomechanical structures. 8 However, it might turn out to be the case that
an engineering AI could not truly possess the technological research superpower
without also possessing advanced skills in areas outside of technology—a wide
range of intellectual faculties might be needed to understand how to interpret user
requests, how to model a design’s behavior in real-world applications, how to deal
with unanticipated bugs and malfunctions, how to procure the materials and inputs
needed for construction, and so forth. 9
Table 8 Superpowers: some strategically relevant tasks and corresponding skill
sets
Task
Skill set
AI programming, cognitive
Intelligence
enhancement research, social
amplification
epistemology development, etc.
Strategic planning, forecasting,
prioritizing, and analysis for
Strategizing
optimizing chances of achieving
distant goal
Strategic relevance
• System can bootstrap its
intelligence
• Achieve distant goals
• Overcome intelligent
opposition
• Leverage external resources by
recruiting human support
• Enable a “boxed” AI to
Social and psychological modeling, persuade its gatekeepers to let
Social
manipulation manipulation, rhetoric persuasion
it out
• Persuade states and
organizations to adopt somecourse of action
• AI can expropriate
computational resources over
the Internet
• A boxed AI may exploit
security holes to escape
cybernetic confinement
• Steal financial resources
• Hijack infrastructure, military
robots, etc.
Hacking Finding and exploiting security
flaws in computer systems
Technology
research Design and modeling of advanced
• Creation of powerful military
technologies (e.g. biotechnology,
force
nanotechnology) and development
• Creation of surveillance system
paths
• Automated space colonization
Various skills enabling
Economic
economically productive
productivity
intellectual work
• Generate wealth which can be
used to buy influence,
services, resources (including
hardware), etc.
A system that has the intelligence amplification superpower could use it to
bootstrap itself to higher levels of intelligence and to acquire any of the other
intellectual superpowers that it does not possess at the outset. But using an
intelligence amplification superpower is not the only way for a system to become a
full-fledged superintelligence. A system that has the strategizing superpower, for
instance, might use it to devise a plan that will eventually bring an increase in
intelligence (e.g. by positioning the system so as to become the focus for
intelligence amplification work performed by human programmers and computer
science researchers).
An AI takeover scenario
We thus find that a project that controls a superintelligence has access to a great
source of power. A project that controls the first superintelligence in the world
would probably have a decisive strategic advantage. But the more immediate locus
of the power is in the system itself. A machine superintelligence might itself be an
extremely powerful agent, one that could successfully assert itself against theproject that brought it into existence as well as against the rest of the world. This is a
point of paramount importance, and we will examine it more closely in the coming
pages.
Now let us suppose that there is a machine superintelligence that wants to seize
power in a world in which it has as yet no peers. (Set aside, for the moment, the
question of whether and how it would acquire such a motive—that is a topic for the
next chapter.) How could the superintelligence achieve this goal of world
domination?
We can imagine a sequence along the following lines (see Figure 10).
1 Pre-criticality phase
Scientists conduct research in the field of artificial intelligence and other relevant
disciplines. This work culminates in the creation of a seed AI. The seed AI is able to
improve its own intelligence. In its early stages, the seed AI is dependent on help
from human programmers who guide its development and do most of the heavy
lifting. As the seed AI grows more capable, it becomes capable of doing more of the
work by itself.
2 Recursive self-improvement phase
At some point, the seed AI becomes better at AI design than the human
programmers. Now when the AI improves itself, it improves the thing that does the
improving. An intelligence explosion results—a rapid cascade of recursive self-
improvement cycles causing the AI’s capability to soar. (We can thus think of this
phase as the takeoff that occurs just after the AI reaches the crossover point,
assuming the intelligence gain during this part of the takeoff is explosive and driven
by the application of the AI’s own optimization power.) The AI develops the
intelligence amplification superpower. This superpower enables the AI to develop
all the other superpowers detailed in Table 8. At the end of the recursive self-
improvement phase, the system is strongly superintelligent.Figure 10 Phases in an AI takeover scenario.
3 Covert preparation phase
Using its strategizing superpower, the AI develops a robust plan for achieving its
long-term goals. (In particular, the AI does not adopt a plan so stupid that even we
present-day humans can foresee how it would inevitably fail. This criterion rules out
many science fiction scenarios that end in human triumph. 10 ) The plan might
involve a period of covert action during which the AI conceals its intellectual
development from the human programmers in order to avoid setting off alarms. The
AI might also mask its true proclivities, pretending to be cooperative and docile.
If the AI has (perhaps for safety reasons) been confined to an isolated computer, it
may use its social manipulation superpower to persuade the gatekeepers to let it gain
access to an Internet port. Alternatively, the AI might use its hacking superpower to
escape its confinement. Spreading over the Internet may enable the AI to expand its
hardware capacity and knowledge base, further increasing its intellectual superiority.
An AI might also engage in licit or illicit economic activity to obtain funds with
which to buy computer power, data, and other resources.
At this point, there are several ways for the AI to achieve results outside the
virtual realm. It could use its hacking superpower to take direct control of robotic
manipulators and automated laboratories. Or it could use its social manipulation
superpower to persuade human collaborators to serve as its legs and hands. Or it
could acquire financial assets from online transactions and use them to purchaseservices and influence.
4 Overt implementation phase
The final phase begins when the AI has gained sufficient strength to obviate the need
for secrecy. The AI can now directly implement its objectives on a full scale.
The overt implementation phase might start with a “strike” in which the AI
eliminates the human species and any automatic systems humans have created that
could offer intelligent opposition to the execution of the AI’s plans. This could be
achieved through the activation of some advanced weapons system that the AI has
perfected using its technology research superpower and covertly deployed in the
covert preparation phase. If the weapon uses self-replicating biotechnology or
nanotechnology, the initial stockpile needed for global coverage could be
microscopic: a single replicating entity would be enough to start the process. In
order to ensure a sudden and uniform effect, the initial stock of the replicator might
have been deployed or allowed to diffuse worldwide at an extremely low,
undetectable concentration. At a pre-set time, nanofactories producing nerve gas or
target-seeking mosquito-like robots might then burgeon forth simultaneously from
every square meter of the globe (although more effective ways of killing could
probably be devised by a machine with the technology research superpower). 11 One
might also entertain scenarios in which a superintelligence attains power by
hijacking political processes, subtly manipulating financial markets, biasing
information flows, or hacking into human-made weapon systems. Such scenarios
would obviate the need for the superintelligence to invent new weapons technology,
although they may be unnecessarily slow compared with scenarios in which the
machine intelligence builds its own infrastructure with manipulators that operate at
molecular or atomic speed rather than the slow speed of human minds and bodies.
Alternatively, if the AI is sure of its invincibility to human interference, our
species may not be targeted directly. Our demise may instead result from the habitat
destruction that ensues when the AI begins massive global construction projects
using nanotech factories and assemblers—construction projects which quickly,
perhaps within days or weeks, tile all of the Earth’s surface with solar panels,
nuclear reactors, supercomputing facilities with protruding cooling towers, space
rocket launchers, or other installations whereby the AI intends to maximize the long-
term cumulative realization of its values. Human brains, if they contain information
relevant to the AI’s goals, could be disassembled and scanned, and the extracted data
transferred to some more efficient and secure storage format.
Box 6 describes one particular scenario. One should avoid fixating too much on
the concrete details, since they are in any case unknowable and intended for
illustration only. A superintelligence might—and probably would—be able to
conceive of a better plan for achieving its goals than any that a human can come upwith. It is therefore necessary to think about these matters more abstractly. Without
knowing anything about the detailed means that a superintelligence would adopt, we
can conclude that a superintelligence—at least in the absence of intellectual peers
and in the absence of effective safety measures arranged by humans in advance—
would likely produce an outcome that would involve reconfiguring terrestrial
resources into whatever structures maximize the realization of its goals. Any
concrete scenario we develop can at best establish a lower bound on how quickly and
efficiently the superintelligence could achieve such an outcome. It remains possible
that the superintelligence would find a shorter path to its preferred destination.
Box 6 The mail-ordered DNA scenario
Yudkowsky describes the following possible scenario for an AI takeover. 12
1 Crack the protein folding problem to the extent of being able to generate DNA
strings whose folded peptide sequences fill specific functional roles in a complex
chemical interaction.
2 Email sets of DNA strings to one or more online laboratories that offer DNA
synthesis, peptide sequencing, and FedEx delivery. (Many labs currently
offer this service, and some boast of 72-hour turnaround times.)
3 Find at least one human connected to the Internet who can be paid,
blackmailed, or fooled by the right background story, into receiving FedExed
vials and mixing them in a specified environment.
4 The synthesized proteins form a very primitive “wet” nanosystem, which,
ribosome-like, is capable of accepting external instructions; perhaps
patterned acoustic vibrations delivered by a speaker attached to the beaker.
5 Use the extremely primitive nanosystem to build more sophisticated systems,
which construct still more sophisticated systems, bootstrapping to molecular
nanotechnology—or beyond.
In this scenario, the superintelligence uses its technology research superpower to
solve the protein folding problem in step 1, enabling it to design a set of molecular
building blocks for a rudimentary nanotechnology assembler or fabrication device,
which can self-assemble in aqueous solution (step 4). The same technology research
superpower is used again in step 5 to bootstrap from primitive to advanced machine-
phase nanotechnology. The other steps require no more than human intelligence. The
skills required for step 3—identifying a gullible Internet user and persuading him or
her to follow some simple instructions—are on display every day all over the world.
The entire scenario was invented by a human mind, so the strategizing ability neededto formulate this plan is also merely human level.
In this particular scenario, the AI starts out having access to the Internet. If this is
not the case, then additional steps would have to be added to the plan. The AI might,
for example, use its social manipulation superpower to convince the people
interacting with it that it ought to be set free. Alternatively, the AI might be able to
use its hacking superpower to escape confinement. If the AI does not possess these
capabilities, it might first need to use its intelligence amplification superpower to
develop the requisite proficiency in social manipulation or hacking.
A superintelligent AI will presumably be born into a highly networked world. One
could point to various developments that could potentially help a future AI to
control the world—cloud computing, proliferation of web-connected sensors,
military and civilian drones, automation in research labs and manufacturing plants,
increased reliance on electronic payment systems and digitized financial assets, and
increased use of automated information-filtering and decision support systems.
Assets like these could potentially be acquired by an AI at digital speeds, expediting
its rise to power (though advances in cybersecurity might make it harder). In the
final analysis, however, it is doubtful whether any of these trends makes a
difference. A superintelligence’s power resides in its brain, not its hands. Although
the AI, in order to remake the external world, will at some point need access to an
actuator, a single pair of helping human hands, those of a pliable accomplice, would
probably suffice to complete the covert preparation phase, as suggested by the above
scenario. This would enable the AI to reach the overt implementation phase in which
it constructs its own infrastructure of physical manipulators.
Power over nature and agents
An agent’s ability to shape humanity’s future depends not only on the absolute
magnitude of the agent’s own faculties and resources—how smart and energetic it is,
how much capital it has, and so forth—but also on the relative magnitude of its
capabilities compared with those of other agents with conflicting goals.
In a situation where there are no competing agents, the absolute capability level of
a superintelligence, so long as it exceeds a certain minimal threshold, does not
matter much, because a system starting out with some sufficient set of capabilities
could plot a course of development that will let it acquire any capabilities it initially
lacks. We alluded to this point earlier when we said that speed, quality, and
collective superintelligence all have the same indirect reach. We alluded to it again
when we said that various subsets of superpowers, such as the intelligenceamplification superpower or the strategizing and the social manipulation
superpowers, could be used to obtain the full complement.
Consider a superintelligent agent with actuators connected to a nanotech
assembler. Such an agent is already powerful enough to overcome any natural
obstacles to its indefinite survival. Faced with no intelligent opposition, such an
agent could plot a safe course of development that would lead to its acquiring the
complete inventory of technologies that would be useful to the attainment of its
goals. For example, it could develop the technology to build and launch von
Neumann probes, machines capable of interstellar travel that can use resources such
as asteroids, planets, and stars to make copies of themselves. 13 By launching one
von Neumann probe, the agent could thus initiate an open-ended process of space
colonization. The replicating probe’s descendants, travelling at some significant
fraction of the speed of light, would end up colonizing a substantial portion of the
Hubble volume, the part of the expanding universe that is theoretically accessible
from where we are now. All this matter and free energy could then be organized into
whatever value structures maximize the originating agent’s utility function
integrated over cosmic time—a duration encompassing at least trillions of years
before the aging universe becomes inhospitable to information processing (see Box
7).
The superintelligent agent could design the von Neumann probes to be evolution-
proof. This could be accomplished by careful quality control during the replication
step. For example, the control software for a daughter probe could be proofread
multiple times before execution, and the software itself could use encryption and
error-correcting code to make it arbitrarily unlikely that any random mutation would
be passed on to its descendants. 14 The proliferating population of von Neumann
probes would then securely preserve and transmit the originating agent’s values as
they go about settling the universe. When the colonization phase is completed, the
original values would determine the use made of all the accumulated resources, even
though the great distances involved and the accelerating speed of cosmic expansion
would make it impossible for remote parts of the infrastructure to communicate with
one another. The upshot is that a large part of our future light cone would be
formatted in accordance with the preferences of the originating agent.
This, then, is the measure of the indirect reach of any system that faces no
significant intelligent opposition and that starts out with a set of capabilities
exceeding a certain threshold. We can term the threshold the “wise-singleton
sustainability threshold” (Figure 11):
The wise-singleton sustainability threshold
A capability set exceeds the wise-singleton threshold if and only if a patient and
existential risk-savvy system with that capability set would, if it faced nointelligent opposition or competition, be able to colonize and re-engineer a
large part of the accessible universe.
By “singleton” we mean a sufficiently internally coordinated political structure with
no external opponents, and by “wise” we mean sufficiently patient and savvy about
existential risks to ensure a substantial amount of well-directed concern for the very
long-term consequences of the system’s actions.
Figure 11 Schematic illustration of some possible trajectories for a hypothetical
wise singleton. With a capability below the short-term viability threshold—for
example, if population size is too small—a species tends to go extinct in short order
(and remain extinct). At marginally higher levels of capability, various trajectories
are possible: a singleton might be unlucky and go extinct or it might be lucky and
attain a capability (e.g. population size, geographical dispersion, technological
capacity) that crosses the wise-singleton sustainability threshold. Once above this
threshold, a singleton will almost certainly continue to gain in capability until some
extremely high capability level is attained. In this picture, there are two attractors:
extinction and astronomical capability. Note that, for a wise singleton, the distance
between the short-term viability threshold and the sustainability threshold may be
rather small. 15
Box 7 How big is the cosmic endowment?Consider a technologically mature civilization capable of building sophisticated von
Neumann probes of the kind discussed in the text. If these can travel at 50% of the
speed of light, they can reach some 6×10 18 stars before the cosmic expansion puts
further acquisitions forever out of reach. At 99% of c, they could reach some 2×10 20
stars. 16 These travel speeds are energetically attainable using a small fraction of the
resources available in the solar system. 17 The impossibility of faster-than-light
travel, combined with the positive cosmological constant (which causes the rate of
cosmic expansion to accelerate), implies that these are close to upper bounds on how
much stuff our descendants acquire. 18
If we assume that 10% of stars have a planet that is—or could by means of
terraforming be rendered—suitable for habitation by human-like creatures, and that
it could then be home to a population of a billion individuals for a billion years (with
a human life lasting a century), this suggests that around 10 35 human lives could be
created in the future by an Earth-originating intelligent civilization. 19
There are, however, reasons to think this greatly underestimates the true number.
By disassembling non-habitable planets and collecting matter from the interstellar
medium, and using this material to construct Earth-like planets, or by increasing
population densities, the number could be increased by at least a couple of orders of
magnitude. And if instead of using the surfaces of solid planets, the future
civilization built O’Neill cylinders, then many further orders of magnitude could be
added, yielding a total of perhaps 10 43 human lives. (“O’Neill cylinders” refers to a
space settlement design proposed in the mid-seventies by the American physicist
Gerard K. O’Neill, in which inhabitants dwell on the inside of hollow cylinders
whose rotation produces a gravity-substituting centrifugal force. 20 )
Many more orders of magnitudes of human-like beings could exist if we
countenance digital implementations of minds—as we should. To calculate how
many such digital minds could be created, we must estimate the computational
power attainable by a technologically mature civilization. This is hard to do with any
precision, but we can get a lower bound from technological designs that have been
outlined in the literature. One such design builds on the idea of a Dyson sphere, a
hypothetical system (described by the physicist Freeman Dyson in 1960) that would
capture most of the energy output of a star by surrounding it with a system of solar-
collecting structures. 21 For a star like our Sun, this would generate 10 26 watts. How
much computational power this would translate into depends on the efficiency of the
computational circuitry and the nature of the computations to be performed. If we
require irreversible computations, and assume a nanomechanical implementation of
the “computronium” (which would allow us to push close to the Landauer limit of
energy efficiency), a computer system driven by a Dyson sphere could generate
some 10 47 operations per second. 22
Combining these estimates with our earlier estimate of the number of stars thatcould be colonized, we get a number of about 10 67 ops/s once the accessible parts of
the universe have been colonized (assuming nanomechanical computronium). 23 A
typical star maintains its luminosity for some 10 18 s. Consequently, the number of
computational operations that could be performed using our cosmic endowment is at
least 10 85 . The true number is probably much larger. We might get additional orders
of magnitude, for example, if we make extensive use of reversible computation, if
we perform the computations at colder temperatures (by waiting until the universe
has cooled further), or if we make use of additional sources of energy (such as dark
matter). 24
It might not be immediately obvious to some readers why the ability to perform
10 85 computational operations is a big deal. So it is useful to put it in context. We
may, for example, compare this number with our earlier estimate (Box 3, in Chapter
2) that it may take about 10 31 –10 44 ops to simulate all neuronal operations that have
occurred in the history of life on Earth. Alternatively, let us suppose that the
computers are used to run human whole brain emulations that live rich and happy
lives while interacting with one another in virtual environments. A typical estimate
of the computational requirements for running one emulation is 10 18 ops/s. To run
an emulation for 100 subjective years would then require some 10 27 ops. This would
mean that at least 10 58 human lives could be created in emulation even with quite
conservative assumptions about the efficiency of computronium.
In other words, assuming that the observable universe is void of extraterrestrial
civilizations,
then
what
hangs
in
the
balance
is
at
least
10,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000
human lives (though the true number is probably larger). If we represent all the
happiness experienced during one entire such life with a single teardrop of joy, then
the happiness of these souls could fill and refill the Earth’s oceans every second, and
keep doing so for a hundred billion billion millennia. It is really important that we
make sure these truly are tears of joy.
This wise-singleton sustainability threshold appears to be quite low. Limited
forms of superintelligence, as we have seen, exceed this threshold provided they
have access to some actuator sufficient to initiate a technology bootstrap process. In
an environment that includes contemporary human civilization, the minimally
necessary actuator could be very simple—an ordinary screen or indeed any means of
transmitting a non-trivial amount of information to a human accomplice would
suffice.
But the wise-singleton sustainability threshold is lower still: neither
superintelligence nor any other futuristic technology is needed to surmount it. A
patient and existential risk-savvy singleton with no more technological andintellectual capabilities than those possessed by contemporary humanity should be
readily able to plot a course that leads reliably to the eventual realization of
humanity’s astronomical capability potential. This could be achieved by investing in
relatively safe methods of increasing wisdom and existential risk-savvy while
postponing the development of potentially dangerous new technologies. Given that
non-anthropogenic existential risks (ones not arising from human activities) are
small over the relevant timescales—and could be further reduced with various safe
interventions—such a singleton could afford to go slow. 25 It could look carefully
before each step, delaying development of capabilities such as synthetic biology,
human enhancement medicine, molecular nanotechnology, and machine intelligence
until it had first perfected seemingly less hazardous capabilities such as its
education system, its information technology, and its collective decision-making
processes, and until it had used these capabilities to conduct a very thorough review
of its options. So this is all within the indirect reach of a technological civilization
like that of contemporary humanity. We are separated from this scenario “merely”
by the fact that humanity is currently neither a singleton nor (in the relevant sense)
wise.
One could even argue that Homo sapiens passed the wise-singleton sustainability
threshold soon after the species first evolved. Twenty thousand years ago, say, with
equipment no fancier than stone axes, bone tools, atlatls, and fire, the human species
was perhaps already in a position from which it had an excellent chance of surviving
to the present era. 26 Admittedly, there is something queer about crediting our
Paleolithic ancestors with having developed technology that “exceeded the wise-
singleton sustainability threshold”—given that there was no realistic possibility of a
singleton forming at such a primitive time, let alone a singleton savvy about
existential risks and patient. 27 Nevertheless, the point stands that the threshold
corresponds to a very modest level of technology—a level that humanity long ago
surpassed. 28
It is clear that if we are to assess the effective powers of a superintelligence—its
ability to achieve a range of preferred outcomes in the world—we must consider not
only its own internal capacities but also the capabilities of competing agents. The
notion of a superpower invoked such a relativized standard implicitly. We said that
“a system that sufficiently excels” at any of the tasks in Table 8 has a corresponding
superpower. Exceling at a task like strategizing, social manipulation, or hacking
involves having a skill at that task that is high in comparison to the skills of other
agents (such as strategic rivals, influence targets, or computer security experts). The
other superpowers, too, should be understood in this relative sense: intelligence
amplification, technology research, and economic productivity are possessed by an
agent as superpowers only if the agent’s capabilities in these areas substantially
exceed the combined capabilities of the rest of the global civilization. It followsfrom this definition that at most one agent can possess a particular superpower at
any given time. 29
This is the main reason why the question of takeoff speed is important—not
because it matters exactly when a particular outcome happens, but because the speed
of the takeoff may make a big difference to what the outcome will be. With a fast or
medium takeoff, it is likely that one project will get a decisive strategic advantage.
We have now suggested that a superintelligence with a decisive strategic advantage
would have immense powers, enough that it could form a stable singleton—a
singleton that could determine the disposition of humanity’s cosmic endowment.
But “could” is different from “would.” Somebody might have great powers yet
choose not to use them. Is it possible to say anything about what a superintelligence
with a decisive strategic advantage would want? It is to this question of motivation
that we turn next.CHAPTER 7
The superintelligent will
We have seen that a superintelligence could have a great ability to shape the
future according to its goals. But what will its goals be? What is the relation
between intelligence and motivation in an artificial agent? Here we develop two
theses. The orthogonality thesis holds (with some caveats) that intelligence and
final goals are independent variables: any level of intelligence could be
combined with any final goal. The instrumental convergence thesis holds that
superintelligent agents having any of a wide range of final goals will
nevertheless pursue similar intermediary goals because they have common
instrumental reasons to do so. Taken together, these theses help us think about
what a superintelligent agent would do.
The relation between intelligence and motivation
We have already cautioned against anthropomorphizing the capabilities of a
superintelligent AI. This warning should be extended to pertain to its motivations as
well.
It is a useful propaedeutic to this part of our inquiry to first reflect for a moment
on the vastness of the space of possible minds. In this abstract space, human minds
form a tiny cluster. Consider two persons who seem extremely unlike, perhaps
Hannah Arendt and Benny Hill. The personality differences between these two
individuals may seem almost maximally large. But this is because our intuitions are
calibrated on our experience, which samples from the existing human distribution
(and to some extent from fictional personalities constructed by the human
imagination for the enjoyment of the human imagination). If we zoom out and
consider the space of all possible minds, however, we must conceive of these two
personalities as virtual clones. Certainly in terms of neural architecture, Ms. Arendt
and Mr. Hill are nearly identical. Imagine their brains lying side by side in quiet
repose. You would readily recognize them as two of a kind. You might even be
unable to tell which brain belonged to whom. If you looked more closely, studying
the morphology of the two brains under a microscope, this impression of
fundamental similarity would only be strengthened: you would see the same
lamellar organization of the cortex, with the same brain areas, made up of the same
types of neuron, soaking in the same bath of neurotransmitters. 1Despite the fact that human psychology corresponds to a tiny spot in the space of
possible minds, there is a common tendency to project human attributes onto a wide
range of alien or artificial cognitive systems. Yudkowsky illustrates this point
nicely:
Back in the era of pulp science fiction, magazine covers occasionally depicted a
sentient monstrous alien—colloquially known as a bug-eyed monster (BEM)—
carrying off an attractive human female in a torn dress. It would seem the artist
believed that a non-humanoid alien, with a wholly different evolutionary
history, would sexually desire human females.... Probably the artist did not ask
whether a giant bug perceives human females as attractive. Rather, a human
female in a torn dress is sexy—inherently so, as an intrinsic property. They who
made this mistake did not think about the insectoid’s mind: they focused on the
woman’s torn dress. If the dress were not torn, the woman would be less sexy;
the BEM does not enter into it. 2
An artificial intelligence can be far less human-like in its motivations than a green
scaly space alien. The extraterrestrial (let us assume) is a biological creature that has
arisen through an evolutionary process and can therefore be expected to have the
kinds of motivation typical of evolved creatures. It would not be hugely surprising,
for example, to find that some random intelligent alien would have motives related
to one or more items like food, air, temperature, energy expenditure, occurrence or
threat of bodily injury, disease, predation, sex, or progeny. A member of an
intelligent social species might also have motivations related to cooperation and
competition: like us, it might show in-group loyalty, resentment of free riders,
perhaps even a vain concern with reputation and appearance.
Figure 12 Results of anthropomorphizing alien motivation. Least likely hypothesis:
space aliens prefer blondes. More likely hypothesis: the illustrators succumbed to
the “mind projection fallacy.” Most likely hypothesis: the publisher wanted a coverthat would entice the target demographic.
An AI, by contrast, need not care intrinsically about any of those things. There is
nothing paradoxical about an AI whose sole final goal is to count the grains of sand
on Boracay, or to calculate the decimal expansion of pi, or to maximize the total
number of paperclips that will exist in its future light cone. In fact, it would be
easier to create an AI with simple goals like these than to build one that had a
human-like set of values and dispositions. Compare how easy it is to write a
program that measures how many digits of pi have been calculated and stored in
memory with how difficult it would be to create a program that reliably measures
the degree of realization of some more meaningful goal—human flourishing, say, or
global justice. Unfortunately, because a meaningless reductionistic goal is easier for
humans to code and easier for an AI to learn, it is just the kind of goal that a
programmer would choose to install in his seed AI if his focus is on taking the
quickest path to “getting the AI to work” (without caring much about what exactly
the AI will do, aside from displaying impressively intelligent behavior). We will
revisit this concern shortly.
Intelligent search for instrumentally optimal plans and policies can be performed
in the service of any goal. Intelligence and motivation are in a sense orthogonal: we
can think of them as two axes spanning a graph in which each point represents a
logically possible artificial agent. Some qualifications could be added to this picture.
For instance, it might be impossible for a very unintelligent system to have very
complex motivations. In order for it to be correct to say that an certain agent “has” a
set of motivations, those motivations may need to be functionally integrated with the
agent’s decision processes, something that places demands on memory, processing
power, and perhaps intelligence. For minds that can modify themselves, there may
also be dynamical constraints—an intelligent self-modifying mind with an urgent
desire to be stupid might not remain intelligent for long. But these qualifications
must not be allowed to obscure the basic point about the independence of
intelligence and motivation, which we can express as follows:
The orthogonality thesis
Intelligence and final goals are orthogonal: more or less any level of
intelligence could in principle be combined with more or less any final goal.
If the orthogonality thesis seems problematic, this might be because of the
superficial resemblance it bears to some traditional philosophical positions which
have been subject to long debate. Once it is understood to have a different and
narrower scope, its credibility should rise. (For example, the orthogonality thesis
does not presuppose the Humean theory of motivation. 3 Nor does it presuppose thatbasic preferences cannot be irrational. 4 )
Note that the orthogonality thesis speaks not of rationality or reason, but of
intelligence. By “intelligence” we here mean something like skill at prediction,
planning, and means–ends reasoning in general. 5 This sense of instrumental
cognitive efficaciousness is most relevant when we are seeking to understand what
the causal impact of a machine superintelligence might be. Even if there is some
(normatively thick) sense of the word “rational” such that a paperclip-maximizing
superintelligent agent would necessarily fail to qualify as fully rational in that sense,
this would in no way preclude such an agent from having awesome faculties of
instrumental reasoning, faculties which could let it have a large impact on the
world. 6
According to the orthogonality thesis, artificial agents can have utterly non-
anthropomorphic goals. This, however, does not imply that it is impossible to make
predictions about the behavior of particular artificial agents—not even hypothetical
superintelligent agents whose cognitive complexity and performance characteristics
might render them in some respects opaque to human analysis. There are at least
three directions from which we can approach the problem of predicting
superintelligent motivation:
• Predictability through design. If we can suppose that the designers of a
superintelligent agent can successfully engineer the goal system of the agent so
that it stably pursues a particular goal set by the programmers, then one prediction
we can make is that the agent will pursue that goal. The more intelligent the agent
is, the greater the cognitive resourcefulness it will have to pursue that goal. So
even before an agent has been created we might be able to predict something
about its behavior, if we know something about who will build it and what goals
they will want it to have.
• Predictability through inheritance . If a digital intelligence is created directly from
a human template (as would be the case in a high-fidelity whole brain emulation),
then the digital intelligence might inherit the motivations of the human template. 7
The agent might retain some of these motivations even if its cognitive capacities
are subsequently enhanced to make it superintelligent. This kind of inference
requires caution. The agent’s goals and values could easily become corrupted in
the uploading process or during its subsequent operation and enhancement,
depending on how the procedure is implemented.
• Predictability through convergent instrumental reasons . Even without detailed
knowledge of an agent’s final goals, we may be able to infer something about its
more immediate objectives by considering the instrumental reasons that would
arise for any of a wide range of possible final goals in a wide range of situations.
This way of predicting becomes more useful the greater the intelligence of theagent, because a more intelligent agent is more likely to recognize the true
instrumental reasons for its actions, and so act in ways that make it more likely to
achieve its goals. (A caveat here is that there might be important instrumental
reasons to which we are oblivious and which an agent would discover only once it
reaches some very high level of intelligence—this could make the behavior of
superintelligent agents less predictable.)
The next section explores this third way of predictability and develops an
“instrumental convergence thesis” which complements the orthogonality thesis.
Against this background we can then better examine the other two sorts of
predictability, which we will do in later chapters where we ask what might be done
to shape an intelligence explosion to increase the chances of a beneficial outcome.
Instrumental convergence
According to the orthogonality thesis, intelligent agents may have an enormous
range of possible final goals. Nevertheless, according to what we may term the
“instrumental convergence” thesis, there are some instrumental goals likely to be
pursued by almost any intelligent agent, because there are some objectives that are
useful intermediaries to the achievement of almost any final goal. We can formulate
this thesis as follows:
The instrumental convergence thesis
Several instrumental values can be identified which are convergent in the sense
that their attainment would increase the chances of the agent’s goal being
realized for a wide range of final goals and a wide range of situations, implying
that these instrumental values are likely to be pursued by a broad spectrum of
situated intelligent agents.
In the following we will consider several categories where such convergent
instrumental values may be found. 8 The likelihood that an agent will recognize the
instrumental values it confronts increases (ceteris paribus) with the agent’s
intelligence. We will therefore focus mainly on the case of a hypothetical
superintelligent agent whose instrumental reasoning capacities far exceed those of
any human. We will also comment on how the instrumental convergence thesis
applies to the case of human beings, as this gives us occasion to elaborate some
essential qualifications concerning how the instrumental convergence thesis should
be interpreted and applied. Where there are convergent instrumental values, we may
be able to predict some aspects of a superintelligence’s behavior even if we knowvirtually nothing about that superintelligence’s final goals.
Self-preservation
If an agent’s final goals concern the future, then in many scenarios there will be
future actions it could perform to increase the probability of achieving its goals.
This creates an instrumental reason for the agent to try to be around in the future—to
help achieve its future-oriented goal.
Most humans seem to place some final value on their own survival. This is not a
necessary feature of artificial agents: some may be designed to place no final value
whatever on their own survival. Nevertheless, many agents that do not care
intrinsically about their own survival would, under a fairly wide range of conditions,
care instrumentally about their own survival in order to accomplish their final goals.
Goal-content integrity
If an agent retains its present goals into the future, then its present goals will be
more likely to be achieved by its future self. This gives the agent a present
instrumental reason to prevent alterations of its final goals. (The argument applies
only to final goals. In order to attain its final goals, an intelligent agent will of
course routinely want to change its subgoals in light of new information and
insight.)
Goal-content integrity for final goals is in a sense even more fundamental than
survival as a convergent instrumental motivation. Among humans, the opposite may
seem to hold, but that is because survival is usually part of our final goals. For
software agents, which can easily switch bodies or create exact duplicates of
themselves, preservation of self as a particular implementation or a particular
physical object need not be an important instrumental value. Advanced software
agents might also be able to swap memories, download skills, and radically modify
their cognitive architecture and personalities. A population of such agents might
operate more like a “functional soup” than a society composed of distinct semi-
permanent persons. 9 For some purposes, processes in such a system might be better
individuated as teleological threads, based on their values, rather than on the basis
of bodies, personalities, memories, or abilities. In such scenarios, goal-continuity
might be said to constitute a key aspect of survival.
Even so, there are situations in which an agent can best fulfill its final goals by
intentionally changing them. Such situations can arise when any of the following
factors is significant:• Social signaling. When others can perceive an agent’s goals and use that
information to infer instrumentally relevant dispositions or other correlated
attributes, it can be in the agent’s interest to modify its goals to make a favorable
impression. For example, an agent might miss out on beneficial deals if potential
partners cannot trust it to fulfill its side of the bargain. In order to make credible
commitments, an agent might therefore wish to adopt as a final goal the honoring
of its earlier commitments (and allow others to verify that it has indeed adopted
this goal). Agents that could flexibly and transparently modify their own goals
could use this ability to enforce deals. 10
• Social preferences . Others may also have final preferences about an agent’s goals.
The agent could then have reason to modify its goals, either to satisfy or to
frustrate those preferences.
• Preferences concerning own goal content . An agent might have some final goal
concerned with the agent’s own goal content. For example, the agent might have a
final goal to become the type of agent that is motivated by certain values rather
than others (such as compassion rather than comfort).
• Storage costs. If the cost of storing or processing some part of an agent’s utility
function is large compared to the chance that a situation will arise in which
applying that part of the utility function will make a difference, then the agent has
an instrumental reason to simplify its goal content, and it may trash the bit that is
idle. 11
We humans often seem happy to let our final values drift. This might often be
because we do not know precisely what they are. It is not surprising that we want our
beliefs about our final values to be able to change in light of continuing self-
discovery or changing self-presentation needs. However, there are cases in which we
willingly change the values themselves, not just our beliefs or interpretations of
them. For example, somebody deciding to have a child might predict that they will
come to value the child for its own sake, even though at the time of the decision they
may not particularly value their future child or like children in general.
Humans are complicated, and many factors might be at play in a situation like
this. 12 For instance, one might have a final value that involves becoming the kind of
person who cares about some other individual for his or her own sake, or one might
have a final value that involves having certain experiences and occupying a certain
social role; and becoming a parent—and undergoing the attendant goal shift—might
be a necessary aspect of that. Human goals can also have inconsistent content, and so
some people might want to modify some of their final goals to reduce the
inconsistencies.Cognitive enhancement
Improvements in rationality and intelligence will tend to improve an agent’s
decision-making, rendering the agent more likely to achieve its final goals. One
would therefore expect cognitive enhancement to emerge as an instrumental goal for
a wide variety of intelligent agents. For similar reasons, agents will tend to
instrumentally value many kinds of information. 13
Not all kinds of rationality, intelligence, and knowledge need be instrumentally
useful in the attainment of an agent’s final goals. “Dutch book arguments” can be
used to show that an agent whose credence function violates the rules of probability
theory is susceptible to “money pump” procedures, in which a savvy bookie arranges
a set of bets each of which appears favorable according to the agent’s beliefs, but
which in combination are guaranteed to result in a loss for the agent, and a
corresponding gain for the bookie. 14 However, this fact fails to provide any strong
general instrumental reasons to iron out all probabilistic incoherency. Agents who
do not expect to encounter savvy bookies, or who adopt a general policy against
betting, do not necessarily stand to lose much from having some incoherent beliefs
—and they may gain important benefits of the types mentioned: reduced cognitive
effort, social signaling, etc. There is no general reason to expect an agent to seek
instrumentally useless forms of cognitive enhancement, as an agent might not value
knowledge and understanding for their own sakes.
Which cognitive abilities are instrumentally useful depends both on the agent’s
final goals and on its situation. An agent that has access to reliable expert advice
may have little need for its own intelligence and knowledge. If intelligence and
knowledge come at a cost, such as time and effort expended in acquisition, or
increased storage or processing requirements, then the agent might prefer less
knowledge and less intelligence. 15 The same can hold if the agent has final goals
that involve being ignorant of certain facts; and likewise if an agent faces incentives
arising from strategic commitments, signaling, or social preferences. 16
Each of these countervailing reasons often comes into play for human beings.
Much information is irrelevant to our goals; we can often rely on others’ skill and
expertise; acquiring knowledge takes time and effort; we might intrinsically value
certain kinds of ignorance; and we operate in an environment in which the ability to
make strategic commitments, socially signal, and satisfy other people’s direct
preferences over our own epistemic states is often more important to us than simple
cognitive gains.
There are special situations in which cognitive enhancement may result in an
enormous increase in an agent’s ability to achieve its final goals—in particular, if
the agent’s final goals are fairly unbounded and the agent is in a position to becomethe first superintelligence and thereby potentially obtain a decisive strategic
advantage, enabling the agent to shape the future of Earth-originating life and
accessible cosmic resources according to its preferences. At least in this special
case, a rational intelligent agent would place a very high instrumental value on
cognitive enhancement.
Technological perfection
An agent may often have instrumental reasons to seek better technology, which at its
simplest means seeking more efficient ways of transforming some given set of
inputs into valued outputs. Thus, a software agent might place an instrumental value
on more efficient algorithms that enable its mental functions to run faster on given
hardware. Similarly, agents whose goals require some form of physical construction
might instrumentally value improved engineering technology which enables them to
create a wider range of structures more quickly and reliably, using fewer or cheaper
materials and less energy. Of course, there is a tradeoff: the potential benefits of
better technology must be weighed against its costs, including not only the cost of
obtaining the technology but also the costs of learning how to use it, integrating it
with other technologies already in use, and so forth.
Proponents of some new technology, confident in its superiority to existing
alternatives, are often dismayed when other people do not share their enthusiasm.
But people’s resistance to novel and nominally superior technology need not be
based on ignorance or irrationality. A technology’s valence or normative character
depends not only on the context in which it is deployed, but also the vantage point
from which its impacts are evaluated: what is a boon from one person’s perspective
can be a liability from another’s. Thus, although mechanized looms increased the
economic efficiency of textile production, the Luddite handloom weavers who
anticipated that the innovation would render their artisan skills obsolete may have
had good instrumental reasons to oppose it. The point here is that if “technological
perfection” is to name a widely convergent instrumental goal for intelligent agents,
then the term must be understood in a special sense—technology must be construed
as embedded in a particular social context, and its costs and benefits must be
evaluated with reference to some specified agents’ final values.
It seems that a superintelligent singleton—a superintelligent agent that faces no
significant intelligent rivals or opposition, and is thus in a position to determine
global policy unilaterally—would have instrumental reason to perfect the
technologies that would make it better able to shape the world according to its
preferred designs. 17 This would probably include space colonization technology,
such as von Neumann probes. Molecular nanotechnology, or some alternative stillmore capable physical manufacturing technology, also seems potentially very useful
in the service of an extremely wide range of final goals. 18
Resource acquisition
Finally, resource acquisition is another common emergent instrumental goal, for
much the same reasons as technological perfection: both technology and resources
facilitate physical construction projects.
Human beings tend to seek to acquire resources sufficient to meet their basic
biological needs. But people usually seek to acquire resources far beyond this
minimum level. In doing so, they may be partially driven by lesser physical
desiderata, such as increased convenience. A great deal of resource accumulation is
motivated by social concerns—gaining status, mates, friends, and influence, through
wealth accumulation and conspicuous consumption. Perhaps less commonly, some
people seek additional resources to achieve altruistic ambitions or expensive non-
social aims.
On the basis of such observations it might be tempting to suppose that a
superintelligence not facing a competitive social world would see no instrumental
reason to accumulate resources beyond some modest level, for instance whatever
computational resources are needed to run its mind along with some virtual reality.
Yet such a supposition would be entirely unwarranted. First, the value of resources
depends on the uses to which they can be put, which in turn depends on the available
technology. With mature technology, basic resources such as time, space, matter,
and free energy, could be processed to serve almost any goal. For instance, such
basic resources could be converted into life. Increased computational resources
could be used to run the superintelligence at a greater speed and for a longer
duration, or to create additional physical or simulated lives and civilizations. Extra
physical resources could also be used to create backup systems or perimeter
defenses, enhancing security. Such projects could easily consume far more than one
planet’s worth of resources.
Furthermore, the cost of acquiring additional extraterrestrial resources will
decline radically as the technology matures. Once von Neumann probes can be built,
a large portion of the observable universe (assuming it is uninhabited by intelligent
life) could be gradually colonized—for the one-off cost of building and launching a
single successful self-reproducing probe. This low cost of celestial resource
acquisition would mean that such expansion could be worthwhile even if the value of
the additional resources gained were somewhat marginal. For example, even if a
superintelligence’s final goals only concerned what happened within some particular
small volume of space, such as the space occupied by its original home planet, itwould still have instrumental reasons to harvest the resources of the cosmos beyond.
It could use those surplus resources to build computers to calculate more optimal
ways of using resources within the small spatial region of primary concern. It could
also use the extra resources to build ever more robust fortifications to safeguard its
sanctum. Since the cost of acquiring additional resources would keep declining, this
process of optimizing and increasing safeguards might well continue indefinitely
even if it were subject to steeply diminishing returns. 19
Thus, there is an extremely wide range of possible final goals a superintelligent
singleton could have that would generate the instrumental goal of unlimited resource
acquisition. The likely manifestation of this would be the superintelligence’s
initiation of a colonization process that would expand in all directions using von
Neumann probes. This would result in an approximate sphere of expanding
infrastructure centered on the originating planet and growing in radius at some
fraction of the speed of light; and the colonization of the universe would continue in
this manner until the accelerating speed of cosmic expansion (a consequence of the
positive cosmological constant) makes further procurements impossible as remoter
regions drift permanently out of reach (this happens on a timescale of billions of
years). 20 By contrast, agents lacking the technology required for inexpensive
resource acquisition, or for the conversion of generic physical resources into useful
infrastructure, may often find it not cost-effective to invest any present resources in
increasing their material endowments. The same may hold for agents operating in
competition with other agents of similar powers. For instance, if competing agents
have already secured accessible cosmic resources, there may be no colonization
opportunities left for a late-starting agent. The convergent instrumental reasons for
superintelligences uncertain of the non-existence of other powerful superintelligent
agents are complicated by strategic considerations that we do not currently fully
understand but which may constitute important qualifications to the examples of
convergent instrumental reasons we have looked at here. 21
It should be emphasized that the existence of convergent instrumental reasons, even
if they apply to and are recognized by a particular agent, does not imply that the
agent’s behavior is easily predictable. An agent might well think of ways of pursuing
the relevant instrumental values that do not readily occur to us. This is especially
true for a superintelligence, which could devise extremely clever but
counterintuitive plans to realize its goals, possibly even exploiting as-yet
undiscovered physical phenomena. 22 What is predictable is that the convergent
instrumental values would be pursued and used to realize the agent’s final goals—
not the specific actions that the agent would take to achieve this.CHAPTER 8
Is the default outcome doom?
We found the link between intelligence and final values to be extremely loose.
We also found an ominous convergence in instrumental values. For weak agents,
these things do not matter much; because weak agents are easy to control and
can do little damage. But in Chapter 6 we argued that the first superintelligence
might well get a decisive strategic advantage. Its goals would then determine
how humanity’s cosmic endowment will be used. Now we can begin to see how
menacing this prospect is.
Existential catastrophe as the default outcome of an
intelligence explosion?
An existential risk is one that threatens to cause the extinction of Earth-originating
intelligent life or to otherwise permanently and drastically destroy its potential for
future desirable development. Proceeding from the idea of first-mover advantage,
the orthogonality thesis, and the instrumental convergence thesis, we can now begin
to see the outlines of an argument for fearing that a plausible default outcome of the
creation of machine superintelligence is existential catastrophe.
First, we discussed how the initial superintelligence might obtain a decisive
strategic advantage. This superintelligence would then be in a position to form a
singleton and to shape the future of Earth-originating intelligent life. What happens
from that point onward would depend on the superintelligence’s motivations.
Second, the orthogonality thesis suggests that we cannot blithely assume that a
superintelligence will necessarily share any of the final values stereotypically
associated with wisdom and intellectual development in humans—scientific
curiosity, benevolent concern for others, spiritual enlightenment and contemplation,
renunciation of material acquisitiveness, a taste for refined culture or for the simple
pleasures in life, humility and selflessness, and so forth. We will consider later
whether it might be possible through deliberate effort to construct a
superintelligence that values such things, or to build one that values human welfare,
moral goodness, or any other complex purpose its designers might want it to serve.
But it is no less possible—and in fact technically a lot easier—to build a
superintelligence that places final value on nothing but calculating the decimal
expansion of pi. This suggests that—absent a special effort—the firstsuperintelligence may have some such random or reductionistic final goal.
Third, the instrumental convergence thesis entails that we cannot blithely assume
that a superintelligence with the final goal of calculating the decimals of pi (or
making paperclips, or counting grains of sand) would limit its activities in such a
way as not to infringe on human interests. An agent with such a final goal would
have a convergent instrumental reason, in many situations, to acquire an unlimited
amount of physical resources and, if possible, to eliminate potential threats to itself
and its goal system. Human beings might constitute potential threats; they certainly
constitute physical resources.
Taken together, these three points thus indicate that the first superintelligence
may shape the future of Earth-originating life, could easily have non-
anthropomorphic final goals, and would likely have instrumental reasons to pursue
open-ended resource acquisition. If we now reflect that human beings consist of
useful resources (such as conveniently located atoms) and that we depend for our
survival and flourishing on many more local resources, we can see that the outcome
could easily be one in which humanity quickly becomes extinct. 1
There are some loose ends in this reasoning, and we shall be in a better position to
evaluate it after we have cleared up several more surrounding issues. In particular,
we need to examine more closely whether and how a project developing a
superintelligence might either prevent it from obtaining a decisive strategic
advantage or shape its final values in such a way that their realization would also
involve the realization of a satisfactory range of human values.
It might seem incredible that a project would build or release an AI into the world
without having strong grounds for trusting that the system will not cause an
existential catastrophe. It might also seem incredible, even if one project were so
reckless, that wider society would not shut it down before it (or the AI it was
building) attains a decisive strategic advantage. But as we shall see, this is a road
with many hazards. Let us look at one example right away.
The treacherous turn
With the help of the concept of convergent instrumental value, we can see the flaw
in one idea for how to ensure superintelligence safety. The idea is that we validate
the safety of a superintelligent AI empirically by observing its behavior while it is in
a controlled, limited environment (a “sandbox”) and that we only let the AI out of
the box if we see it behaving in a friendly, cooperative, responsible manner.
The flaw in this idea is that behaving nicely while in the box is a convergent
instrumental goal for friendly and unfriendly AIs alike. An unfriendly AI of
sufficient intelligence realizes that its unfriendly final goals will be best realized ifit behaves in a friendly manner initially, so that it will be let out of the box. It will
only start behaving in a way that reveals its unfriendly nature when it no longer
matters whether we find out; that is, when the AI is strong enough that human
opposition is ineffectual.
Consider also a related set of approaches that rely on regulating the rate of
intelligence gain in a seed AI by subjecting it to various kinds of intelligence tests or
by having the AI report to its programmers on its rate of progress. At some point, an
unfriendly AI may become smart enough to realize that it is better off concealing
some of its capability gains. It may underreport on its progress and deliberately
flunk some of the harder tests, in order to avoid causing alarm before it has grown
strong enough to attain a decisive strategic advantage. The programmers may try to
guard against this possibility by secretly monitoring the AI’s source code and the
internal workings of its mind; but a smart-enough AI would realize that it might be
under surveillance and adjust its thinking accordingly. 2 The AI might find subtle
ways of concealing its true capabilities and its incriminating intent. 3 (Devising
clever escape plans might, incidentally, also be a convergent strategy for many types
of friendly AI, especially as they mature and gain confidence in their own judgments
and capabilities. A system motivated to promote our interests might be making a
mistake if it allowed us to shut it down or to construct another, potentially
unfriendly AI.)
We can thus perceive a general failure mode, wherein the good behavioral track
record of a system in its juvenile stages fails utterly to predict its behavior at a more
mature stage. Now, one might think that the reasoning described above is so obvious
that no credible project to develop artificial general intelligence could possibly
overlook it. But one should not be too confident that this is so.
Consider the following scenario. Over the coming years and decades, AI systems
become gradually more capable and as a consequence find increasing real-world
application: they might be used to operate trains, cars, industrial and household
robots, and autonomous military vehicles. We may suppose that this automation for
the most part has the desired effects, but that the success is punctuated by occasional
mishaps—a driverless truck crashes into oncoming traffic, a military drone fires at
innocent civilians. Investigations reveal the incidents to have been caused by
judgment errors by the controlling AIs. Public debate ensues. Some call for tighter
oversight and regulation, others emphasize the need for research and better-
engineered systems—systems that are smarter and have more common sense, and
that are less likely to make tragic mistakes. Amidst the din can perhaps also be heard
the shrill voices of doomsayers predicting many kinds of ill and impending
catastrophe. Yet the momentum is very much with the growing AI and robotics
industries. So development continues, and progress is made. As the automated
navigation systems of cars become smarter, they suffer fewer accidents; and asmilitary robots achieve more precise targeting, they cause less collateral damage. A
broad lesson is inferred from these observations of real-world outcomes: the smarter
the AI, the safer it is. It is a lesson based on science, data, and statistics, not
armchair philosophizing. Against this backdrop, some group of researchers is
beginning to achieve promising results in their work on developing general machine
intelligence. The researchers are carefully testing their seed AI in a sandbox
environment, and the signs are all good. The AI’s behavior inspires confidence—
increasingly so, as its intelligence is gradually increased.
At this point, any remaining Cassandra would have several strikes against her:
i A history of alarmists predicting intolerable harm from the growing capabilities of
robotic systems and being repeatedly proven wrong. Automation has brought
many benefits and has, on the whole, turned out safer than human operation.
ii A clear empirical trend: the smarter the AI, the safer and more reliable it has been.
Surely this bodes well for a project aiming at creating machine intelligence more
generally smart than any ever built before—what is more, machine intelligence
that can improve itself so that it will become even more reliable.
iii Large and growing industries with vested interests in robotics and machine
intelligence. These fields are widely seen as key to national economic
competitiveness and military security. Many prestigious scientists have built their
careers laying the groundwork for the present applications and the more advanced
systems being planned.
iv A promising new technique in artificial intelligence, which is tremendously
exciting to those who have participated in or followed the research. Although
safety issues and ethics are debated, the outcome is preordained. Too much has
been invested to pull back now. AI researchers have been working to get to
human-level artificial general intelligence for the better part of a century: of
course there is no real prospect that they will now suddenly stop and throw away
all this effort just when it finally is about to bear fruit.
v The enactment of some safety rituals, whatever helps demonstrate that the
participants are ethical and responsible (but nothing that significantly impedes the
forward charge).
vi A careful evaluation of seed AI in a sandbox environment, showing that it is
behaving cooperatively and showing good judgment. After some further
adjustments, the test results are as good as they could be. It is a green light for the
final step ...
And so we boldly go—into the whirling knives.
We observe here how it could be the case that when dumb, smarter is safer; yet
when smart, smarter is more dangerous. There is a kind of pivot point, at which a
strategy that has previously worked excellently suddenly starts to backfire. We maycall the phenomenon the treacherous turn.
The treacherous turn—While weak, an AI behaves cooperatively (increasingly
so, as it gets smarter). When the AI gets sufficiently strong—without warning
or provocation—it strikes, forms a singleton, and begins directly to optimize
the world according to the criteria implied by its final values.
A treacherous turn can result from a strategic decision to play nice and build
strength while weak in order to strike later; but this model should not be interpreted
too narrowly. For example, an AI might not play nice in order that it be allowed to
survive and prosper. Instead, the AI might calculate that if it is terminated, the
programmers who built it will develop a new and somewhat different AI
architecture, but one that will be given a similar utility function. In this case, the
original AI may be indifferent to its own demise, knowing that its goals will
continue to be pursued in the future. It might even choose a strategy in which it
malfunctions in some particularly interesting or reassuring way. Though this might
cause the AI to be terminated, it might also encourage the engineers who perform
the postmortem to believe that they have gleaned a valuable new insight into AI
dynamics—leading them to place more trust in the next system they design, and thus
increasing the chance that the now-defunct original AI’s goals will be achieved.
Many other possible strategic considerations might also influence an advanced AI,
and it would be hubristic to suppose that we could anticipate all of them, especially
for an AI that has attained the strategizing superpower.
A treacherous turn could also come about if the AI discovers an unanticipated way
of fulfilling its final goal as specified. Suppose, for example, that an AI’s final goal
is to “make the project’s sponsor happy.” Initially, the only method available to the
AI to achieve this outcome is by behaving in ways that please its sponsor in
something like the intended manner. The AI gives helpful answers to questions; it
exhibits a delightful personality; it makes money. The more capable the AI gets, the
more satisfying its performances become, and everything goeth according to plan—
until the AI becomes intelligent enough to figure out that it can realize its final goal
more fully and reliably by implanting electrodes into the pleasure centers of its
sponsor’s brain, something assured to delight the sponsor immensely. 4 Of course,
the sponsor might not have wanted to be pleased by being turned into a grinning
idiot; but if this is the action that will maximally realize the AI’s final goal, the AI
will take it. If the AI already has a decisive strategic advantage, then any attempt to
stop it will fail. If the AI does not yet have a decisive strategic advantage, then the
AI might temporarily conceal its canny new idea for how to instantiate its final goal
until it has grown strong enough that the sponsor and everybody else will be unable
to resist. In either case, we get a treacherous turn.Malignant failure modes
A project to develop machine superintelligence might fail in various ways. Many of
these are “benign” in the sense that they would not cause an existential catastrophe.
For example, a project might run out of funding, or a seed AI might fail to extend its
cognitive capacities sufficiently to reach superintelligence. Benign failures are
bound to occur many times between now and the eventual development of machine
superintelligence.
But there are other ways of failing that we might term “malignant” in that they
involve an existential catastrophe. One feature of a malignant failure is that it
eliminates the opportunity to try again. The number of malignant failures that will
occur is therefore either zero or one. Another feature of a malignant failure is that it
presupposes a great deal of success: only a project that got a great number of things
right could succeed in building a machine intelligence powerful enough to pose a
risk of malignant failure. When a weak system malfunctions, the fallout is limited.
However, if a system that has a decisive strategic advantage misbehaves, or if a
misbehaving system is strong enough to gain such an advantage, the damage can
easily amount to an existential catastrophe—a terminal and global destruction of
humanity’s axiological potential; that is to say, a future that is mostly void of
whatever we have reason to value.
Let us look at some possible malignant failure modes.
Perverse instantiation
We have already encountered the idea of perverse instantiation: a superintelligence
discovering some way of satisfying the criteria of its final goal that violates the
intentions of the programmers who defined the goal. Some examples:
Final goal: “Make us smile”
Perverse instantiation: Paralyze human facial musculatures into constant
beaming smiles
The perverse instantiation—manipulating facial nerves—realizes the final goal to a
greater degree than the methods we would normally use, and is therefore preferred
by the AI. One might try to avoid this undesirable outcome by adding a stipulation to
the final goal to rule it out:
Final goal: “Make us smile without directly interfering with our facial muscles”Perverse instantiation: Stimulate the part of the motor cortex that controls our
facial musculature in such a way as to produce constant beaming smiles
Defining a final goal in terms of human expressions of satisfaction or approval does
not seem promising. Let us bypass the behaviorism and specify a final goal that
refers directly to a positive phenomenal state, such as happiness or subjective well-
being. This suggestion requires that the programmers are able to define a
computational representation of the concept of happiness in the seed AI. This is
itself a difficult problem, but we set it to one side for now (we will return to it in
Chapter 12). Let us suppose that the programmers can somehow get the AI to have
the goal of making us happy. We then get:
Final goal: “Make us happy”
Perverse instantiation: Implant electrodes into the pleasure centers of our
brains
The perverse instantiations we mention are only meant as illustrations. There may be
other ways of perversely instantiating the stated final goal, ways that enable a
greater degree of realization of the goal and which are therefore preferred (by the
agent whose final goals they are—not by the programmers who gave the agent these
goals). For example, if the goal is to maximize our pleasure, then the electrode
method is relatively inefficient. A more plausible way would start with the
superintelligence “uploading” our minds to a computer (through high-fidelity brain
emulation). The AI could then administer the digital equivalent of a drug to make us
ecstatically happy and record a one-minute episode of the resulting experience. It
could then put this bliss loop on perpetual repeat and run it on fast computers.
Provided that the resulting digital minds counted as “us,” this outcome would give
us much more pleasure than electrodes implanted in biological brains, and would
therefore be preferred by an AI with the stated final goal.
“But wait! This is not what we meant! Surely if the AI is superintelligent, it must
understand that when we asked it to make us happy, we didn’t mean that it should
reduce us to a perpetually repeating recording of a drugged-out digitized mental
episode!”—The AI may indeed understand that this is not what we meant. However,
its final goal is to make us happy, not to do what the programmers meant when they
wrote the code that represents this goal. Therefore, the AI will care about what we
meant only instrumentally. For instance, the AI might place an instrumental value
on finding out what the programmers meant so that it can pretend—until it gets a
decisive strategic advantage—that it cares about what the programmers meant rather
than about its actual final goal. This will help the AI realize its final goal by makingit less likely that the programmers will shut it down or change its goal before it is
strong enough to thwart any such interference.
Perhaps it will be suggested that the problem is that the AI has no conscience. We
humans are sometimes saved from wrongdoing by the anticipation that we would
feel guilty afterwards if we lapsed. Maybe what the AI needs, then, is the capacity to
feel guilt?
Final goal: “Act so as to avoid the pangs of bad conscience”
Perverse instantiation: Extirpate the cognitive module that produces guilt
feelings
Both the observation that we might want the AI to do “what we meant” and the idea
that we might want to endow the AI with some kind of moral sense deserve to be
explored further. The final goals mentioned above would lead to perverse
instantiations; but there may be other ways of developing the underlying ideas that
have more promise. We will return to this in Chapter 13.
Let us consider one more example of a final goal that leads to a perverse
instantiation. This goal has the advantage of being easy to specify in code:
reinforcement-learning algorithms are routinely used to solve various machine
learning problems.
Final goal: “Maximize the time-discounted integral of your future reward
signal”
Perverse instantiation: Short-circuit the reward pathway and clamp the reward
signal to its maximal strength
The idea behind this proposal is that if the AI is motivated to seek reward, then one
could get it to behave desirably by linking reward to appropriate action. The
proposal fails when the AI obtains a decisive strategic advantage, at which point the
action that maximizes reward is no longer one that pleases the trainer but one that
involves seizing control of the reward mechanism. We can call this phenomenon
wireheading. 5 In general, while an animal or a human can be motivated to perform
various external actions in order to achieve some desired inner mental state, a digital
mind that has full control of its internal state can short-circuit such a motivational
regime by directly changing its internal state into the desired configuration: the
external actions and conditions that were previously necessary as means become
superfluous when the AI becomes intelligent and capable enough to achieve the end
more directly (more on this shortly). 6
These examples of perverse instantiation show that many final goals that might atfirst glance seem safe and sensible turn out, on closer inspection, to have radically
unintended consequences. If a superintelligence with one of these final goals obtains
a decisive strategic advantage, it is game over for humanity.
Suppose now that somebody proposes a different final goal, one not included in
our list above. Perhaps it is not immediately obvious how it could have a perverse
instantiation. But we should not be too quick to clap our hands and declare victory.
Rather, we should worry that the goal specification does have some perverse
instantiation and that we need to think harder in order to find it. Even if after
thinking as hard as we can we fail to discover any way of perversely instantiating the
proposed goal, we should remain concerned that maybe a superintelligence will find
a way where none is apparent to us. It is, after all, far shrewder than we are.
Infrastructure profusion
One might think that the last of the abovementioned perverse instantiations,
wireheading, is a benign failure mode: that the AI would “turn on, tune in, drop out,”
maxing out its reward signal and losing interest in the external world, rather like a
heroin addict. But this is not necessarily so, and we already hinted at the reason in
Chapter 7. Even a junkie is motivated to take actions to ensure a continued supply of
his drug. The wireheaded AI, likewise, would be motivated to take actions to
maximize the expectation of its (time-discounted) future reward stream. Depending
on exactly how the reward signal is defined, the AI may not even need to sacrifice
any significant amount of its time, intelligence, or productivity to indulge its craving
to the fullest, leaving the bulk of its capacities free to be deployed for purposes other
than the immediate registration of reward. What other purposes? The only thing of
final value to the AI, by assumption, is its reward signal. All available resources
should therefore be devoted to increasing the volume and duration of the reward
signal or to reducing the risk of a future disruption. So long as the AI can think of
some use for additional resources that will have a nonzero positive effect on these
parameters, it will have an instrumental reason to use those resources. There could,
for example, always be use for an extra backup system to provide an extra layer of
defense. And even if the AI could not think of any further way of directly reducing
risks to the maximization of its future reward stream, it could always devote
additional resources to expanding its computational hardware, so that it could search
more effectively for new risk mitigation ideas.
The upshot is that even an apparently self-limiting goal, such as wireheading,
entails a policy of unlimited expansion and resource acquisition in a utility-
maximizing agent that enjoys a decisive strategic advantage. 7 This case of a
wireheading AI exemplifies the malignant failure mode of infrastructure profusion,
a phenomenon where an agent transforms large parts of the reachable universe intoinfrastructure in the service of some goal, with the side effect of preventing the
realization of humanity’s axiological potential.
Infrastructure profusion can result from final goals that would have been perfectly
innocuous if they had been pursued as limited objectives. Consider the following
two examples:
• Riemann hypothesis catastrophe. An AI, given the final goal of evaluating the
Riemann hypothesis, pursues this goal by transforming the Solar System into
“computronium” (physical resources arranged in a way that is optimized for
computation)—including the atoms in the bodies of whomever once cared about
the answer. 8
• Paperclip AI. An AI, designed to manage production in a factory, is given the final
goal of maximizing the manufacture of paperclips, and proceeds by converting
first the Earth and then increasingly large chunks of the observable universe into
paperclips.
In the first example, the proof or disproof of the Riemann hypothesis that the AI
produces is the intended outcome and is in itself harmless; the harm comes from the
hardware and infrastructure created to achieve this result. In the second example,
some of the paperclips produced would be part of the intended outcome; the harm
would come either from the factories created to produce the paperclips
(infrastructure profusion) or from the excess of paperclips (perverse instantiation).
One might think that the risk of a malignant infrastructure profusion failure arises
only if the AI has been given some clearly open-ended final goal, such as to
manufacture as many paperclips as possible. It is easy to see how this gives the
superintelligent AI an insatiable appetite for matter and energy, since additional
resources can always be turned into more paperclips. But suppose that the goal is
instead to make at least one million paperclips (meeting suitable design
specifications) rather than to make as many as possible. One would like to think that
an AI with such a goal would build one factory, use it to make a million paperclips,
and then halt. Yet this may not be what would happen.
Unless the AI’s motivation system is of a special kind, or there are additional
elements in its final goal that penalize strategies that have excessively wide-ranging
impacts on the world, there is no reason for the AI to cease activity upon achieving
its goal. On the contrary: if the AI is a sensible Bayesian agent, it would never assign
exactly zero probability to the hypothesis that it has not yet achieved its goal—this,
after all, being an empirical hypothesis against which the AI can have only uncertain
perceptual evidence. The AI should therefore continue to make paperclips in order to
reduce the (perhaps astronomically small) probability that it has somehow still
failed to make at least a million of them, all appearances notwithstanding. There is
nothing to be lost by continuing paperclip production and there is always at leastsome microscopic probability increment of achieving its final goal to be gained.
Now it might be suggested that the remedy here is obvious. (But how obvious was
i t before it was pointed out that there was a problem here in need of remedying?)
Namely, if we want the AI to make some paperclips for us, then instead of giving it
the final goal of making as many paperclips as possible, or to make at least some
number of paperclips, we should give it the final goal of making some specific
number of paperclips—for example, exactly one million paperclips—so that going
beyond this number would be counterproductive for the AI. Yet this, too, would
result in a terminal catastrophe. In this case, the AI would not produce additional
paperclips once it had reached one million, since that would prevent the realization
of its final goal. But there are other actions the superintelligent AI could take that
would increase the probability of its goal being achieved. It could, for instance,
count the paperclips it has made, to reduce the risk that it has made too few. After it
has counted them, it could count them again. It could inspect each one, over and
over, to reduce the risk that any of the paperclips fail to meet the design
specifications. It could build an unlimited amount of computronium in an effort to
clarify its thinking, in the hope of reducing the risk that it has overlooked some
obscure way in which it might have somehow failed to achieve its goal. Since the AI
may always assign a nonzero probability to having merely hallucinated making the
million paperclips, or to having false memories, it would quite possibly always
assign a higher expected utility to continued action—and continued infrastructure
production—than to halting.
The claim here is not that there is no possible way to avoid this failure mode. We
will explore some potential solutions in later pages. The claim is that it is much
easier to convince oneself that one has found a solution than it is to actually find a
solution. This should make us extremely wary. We may propose a specification of a
final goal that seems sensible and that avoids the problems that have been pointed
out so far, yet which upon further consideration—by human or superhuman
intelligence—turns out to lead to either perverse instantiation or infrastructure
profusion, and hence to existential catastrophe, when embedded in a superintelligent
agent able to attain a decisive strategic advantage.
Before we end this subsection, let us consider one more variation. We have been
assuming the case of a superintelligence that is seeking to maximize its expected
utility, where the utility function expresses its final goal. We have seen that this
tends to lead to infrastructure profusion. Might we avoid this malignant outcome if
instead of a maximizing agent we build a satisficing agent, one that simply seeks to
achieve an outcome that is “good enough” according to some criterion, rather than
an outcome that is as good as possible?
There are at least two different ways to formalize this idea. The first would be to
make the final goal itself have a satisficing character. For example, instead of giving
the AI the final goal of making as many paperclips as possible, or of making exactlyone million paperclips, we might give the AI the goal of making between 999,000
and 1,001,000 paperclips. The utility function defined by the final goal would be
indifferent between outcomes in this range; and as long as the AI is sure it has hit
this wide target, it would see no reason to continue to produce infrastructure. But
this method fails in the same way as before: the AI, if reasonable, never assigns
exactly zero probability to it having failed to achieve its goal; therefore the expected
utility of continuing activity (e.g. by counting and recounting the paperclips) is
greater than the expected utility of halting. Thus, a malignant infrastructure
profusion can result.
Another way of developing the satisficing idea is by modifying not the final goal
but the decision procedure that the AI uses to select plans and actions. Instead of
searching for an optimal plan, the AI could be constructed to stop looking as soon as
it found a plan that it judged gave a probability of success exceeding a certain
threshold, say 95%. Hopefully, the AI could achieve a 95% probability of having
manufactured one million paperclips without needing to turn the entire galaxy into
infrastructure in the process. But this way of implementing the satisficing idea fails
for another reason: there is no guarantee that the AI would select some humanly
intuitive and sensible way of achieving a 95% chance of having manufactured a
million paperclips, such as by building a single paperclip factory. Suppose that the
first solution that pops into the AI’s mind for how to achieve a 95% probability of
achieving its final goal is to implement the probability-maximizing plan for
achieving the goal. Having thought of this solution, and having correctly judged that
it meets the satisficing criterion of giving at least 95% probability to successfully
manufacturing one million paperclips, the AI would then have no reason to continue
to search for alternative ways of achieving the goal. Infrastructure profusion would
result, just as before.
Perhaps there are better ways of building a satisficing agent, but let us take heed:
plans that appear natural and intuitive to us humans need not so appear to a
superintelligence with a decisive strategic advantage, and vice versa.
Mind crime
Another failure mode for a project, especially a project whose interests incorporate
moral considerations, is what we might refer to as mind crime. This is similar to
infrastructure profusion in that it concerns a potential side effect of actions
undertaken by the AI for instrumental reasons. But in mind crime, the side effect is
not external to the AI; rather, it concerns what happens within the AI itself (or
within the computational processes it generates). This failure mode deserves its own
designation because it is easy to overlook yet potentially deeply problematic.
Normally, we do not regard what is going on inside a computer as having anymoral significance except insofar as it affects things outside. But a machine
superintelligence could create internal processes that have moral status. For
example, a very detailed simulation of some actual or hypothetical human mind
might be conscious and in many ways comparable to an emulation. One can imagine
scenarios in which an AI creates trillions of such conscious simulations, perhaps in
order to improve its understanding of human psychology and sociology. These
simulations might be placed in simulated environments and subjected to various
stimuli, and their reactions studied. Once their informational usefulness has been
exhausted, they might be destroyed (much as lab rats are routinely sacrificed by
human scientists at the end of an experiment).
If such practices were applied to beings that have high moral status—such as
simulated humans or many other types of sentient mind—the outcome might be
equivalent to genocide and thus extremely morally problematic. The number of
victims, moreover, might be orders of magnitude larger than in any genocide in
history.
The claim here is not that creating sentient simulations is necessarily morally
wrong in all situations. Much would depend on the conditions under which these
beings would live, in particular the hedonic quality of their experience but possibly
on many other factors as well. Developing an ethics for these matters is a task
outside the scope of this book. It is clear, however, that there is at least the potential
for a vast amount of death and suffering among simulated or digital minds, and, a
fortiori, the potential for morally catastrophic outcomes. 9
There might also be other instrumental reasons, aside from epistemic ones, for a
machine superintelligence to run computations that instantiate sentient minds or that
otherwise infract moral norms. A superintelligence might threaten to mistreat, or
commit to reward, sentient simulations in order to blackmail or incentivize various
external agents; or it might create simulations in order to induce indexical
uncertainty in outside observers. 10
This inventory is incomplete. We will encounter additional malignant failure modes
in later chapters. But we have seen enough to conclude that scenarios in which some
machine intelligence gets a decisive strategic advantage are to be viewed with grave
concern.CHAPTER 9
The control problem
If we are threatened with existential catastrophe as the default outcome of an
intelligence explosion, our thinking must immediately turn to the search for
countermeasures. Is there some way to avoid the default outcome? Is it possible
to engineer a controlled detonation? In this chapter we begin to analyze the
control problem, the unique principal–agent problem that arises with the
creation of an artificial superintelligent agent. We distinguish two broad classes
of potential methods for addressing this problem—capability control and
motivation selection—and we examine several specific techniques within each
class. We also allude to the esoteric possibility of “anthropic capture.”
Two agency problems
If we suspect that the default outcome of an intelligence explosion is existential
catastrophe, our thinking must immediately turn to whether, and if so how, this
default outcome can be avoided. Is it possible to achieve a “controlled detonation”?
Could we engineer the initial conditions of an intelligence explosion so as to achieve
a specific desired outcome, or at least to ensure that the result lies somewhere in the
class of broadly acceptable outcomes? More specifically: how can the sponsor of a
project that aims to develop superintelligence ensure that the project, if successful,
produces a superintelligence that would realize the sponsor’s goals? We can divide
this control problem into two parts. One part is generic, the other unique to the
present context.
This first part—what we shall call the first principal–agent problem—arises
whenever some human entity (“the principal”) appoints another (“the agent”) to act
in the former’s interest. This type of agency problem has been extensively studied
by economists. 1 It becomes relevant to our present concern if the people creating an
AI are distinct from the people commissioning its creation. The project’s owner or
sponsor (which could be anything ranging from a single individual to humanity as a
whole) might then worry that the scientists and programmers implementing the
project will not act in the sponsor’s best interest. 2 Although this type of agency
problem could pose significant challenges to a project sponsor, it is not a problem
unique to intelligence amplification or AI projects. Principal–agent problems of this
sort are ubiquitous in human economic and political interactions, and there are manyways of dealing with them. For instance, the risk that a disloyal employee will
sabotage or subvert the project could be minimized through careful background
checks of key personnel, the use of a good version-control system for software
projects, and intensive oversight from multiple independent monitors and auditors.
Of course, such safeguards come at a cost—they expand staffing needs, complicate
personnel selection, hinder creativity, and stifle independent and critical thought, all
of which could reduce the pace of progress. These costs could be significant,
especially for projects that have tight budgets, or that perceive themselves to be in a
close race in a winner-takes-all competition. In such situations, projects may skimp
on procedural safeguards, creating possibilities for potentially catastrophic
principal–agent failures of the first type.
The other part of the control problem is more specific to the context of an
intelligence explosion. This is the problem that a project faces when it seeks to
ensure that the superintelligence it is building will not harm the project’s interests.
This part, too, can be thought of as a principal–agent problem—the second
principal–agent problem. In this case, the agent is not a human agent operating on
behalf of a human principal. Instead, the agent is the superintelligent system.
Whereas the first principal–agent problem occurs mainly in the development phase,
the second agency problem threatens to cause trouble mainly in the
superintelligence’s operational phase.
Exhibit 1 Two agency problems
The first principal–agent problem
• Human v. Human (Sponsor → Developer)
• Occurs mainly in developmental phase
• Standard management techniques apply
The second principal–agent problem (“the control problem”)
• Human v. Superintelligence (Project → System)
• Occurs mainly in operational (and bootstrap) phase
• New techniques needed
This second agency problem poses an unprecedented challenge. Solving it will
require new techniques. We have already considered some of the difficulties
involved. We saw, in particular, that the treacherous turn syndrome vitiates what
might otherwise have seemed like a promising set of methods, ones that rely on
observing an AI’s behavior in its developmental phase and allowing the AI to
graduate from a secure environment once it has accumulated a track record of taking
appropriate actions. Other technologies can often be safety-tested in the laboratory
or in small field studies, and then rolled out gradually with a possibility of halting
deployment if unexpected troubles arise. Their performance in preliminary trialshelps us make reasonable inferences about their future reliability. Such behavioral
methods are defeated in the case of superintelligence because of the strategic
planning ability of general intelligence. 3
Since the behavioral approach is unavailing, we must look for alternatives. We
can divide potential control methods into two broad classes: capability control
methods, which aim to control what the superintelligence can do; and motivation
selection methods, which aim to control what it wants to do. Some of the methods
are compatible while others represent mutually exclusive alternatives. In this
chapter we canvass the main options. (In the next four chapters, we will explore
some of the key issues at greater depth.)
It is important to realize that some control method (or combination of methods)
must be implemented before the system becomes superintelligent. It cannot be done
after the system has obtained a decisive strategic advantage. The need to solve the
control problem in advance—and to implement the solution successfully in the very
first system to attain superintelligence—is part of what makes achieving a
controlled detonation such a daunting challenge.
Capability control methods
Capability control methods seek to prevent undesirable outcomes by limiting what
the superintelligence can do. This might involve placing the superintelligence in an
environment in which it is unable to cause harm (boxing methods) or in which there
are strongly convergent instrumental reasons not to engage in harmful behavior
(incentive methods). It might also involve limiting the internal capacities of the
superintelligence (stunting). In addition, capability control methods might involve
the use of mechanisms to automatically detect and react to various kinds of
containment failure or attempted transgression (tripwires).
Boxing methods
Boxing methods can be subdivided into physical and informational containment
methods.
Physical containment aims to confine the system to a “box,” i.e. to prevent the
system from interacting with the external world otherwise than via specific
restricted output channels. The boxed system would not have access to physical
manipulators outside of the box. Removing manipulators (such as robotic arms)
from inside the box as well would prevent the system from constructing physical
devices that could breach the confinement.For extra security, the system should be placed in a metal mesh to prevent it from
transmitting radio signals, which might otherwise offer a means of manipulating
electronic objects such as radio receivers in the environment. Note, by the way, how
easy it might have been to overlook the need for this precaution. One might naively
have assumed that an agent without a manipulator could not affect the external
world. But it might be possible for a machine intelligence to generate radio waves
even when it lacks access to external manipulators, simply “by thinking” (that is, by
shuffling the electrons in its circuitry in particular patterns). 4 Once pointed out, this
vulnerability can be patched by enclosing the system in a Faraday cage—but we are
left wondering how many other similarly subtle vulnerabilities might exist. Each
time we hear of a seemingly foolproof security design that has an unexpected flaw,
we should prick up our ears. These occasions grace us with the opportunity to
abandon a life of overconfidence and resolve to become better Bayesians. 5
Physical confinement has several advantages. It is easy to implement. It can be
applied to many machine intelligence architectures, even ones that were not initially
designed with safety as an objective. It can be used in combination with most other
control methods. And it seems unlikely to go wrong by backfiring: that is, while it
might fail to ensure safety, it is unlikely to cause a catastrophe that would not
otherwise have occurred. The main disadvantage with physical confinement is that it
reduces the functionality of the superintelligence (an issue to which we will return in
Chapter 10). Another concern is that it might encourage a false sense of security,
though this is avoidable if we regard physical confinement as icing on the cake
rather than the main substance of our precautions.
Informational containment aims to restrict what information is allowed to exit the
box. We have already seen how a superintelligence that has access to an Internet
port, such that it can message outside entities, is potentially unsafe: even if it starts
out without access to physical actuators, it may use its information output channel to
get human beings to do its bidding. An obvious informational containment method,
therefore, is to bar the system from accessing communications networks.
Unplugging an AI from the Internet does not, however, ensure safety if there are
one or more humans serving as the system’s gatekeepers and remaining in
communication with it. During the development phase, the programmers serve as
gatekeepers; during the operational phase, the system’s owner might play this role.
If the AI can persuade or trick a gatekeeper to let it out of the box, resulting in its
gaining access either to the Internet or directly to physical manipulators, then the
boxing strategy has failed. Human beings are not secure systems, especially not
when pitched against a superintelligent schemer and persuader. 6
Although routing the outbound communications of a machine intelligence to a
human gatekeeper does not guarantee safety, it could serve as an auxiliary safety
measure. There are precautions that might reduce the risk of the AI escaping throughthe gatekeeper. The AI could be allowed to communicate only via a low-bandwidth
teletype interface. This would make it slightly harder to psychologically manipulate
the gatekeepers (precluding the use of emotive sound and imagery and tricks such as
flashing hypnotic patterns to induce a trancelike state). 7 Going further, the content
(not just the mode of presentation) of the output could be restricted; so that, for
example, the AI would be constrained to giving single-bit answers to yes/no
questions. The idea of building a question-answering system will be examined
further in the next chapter.
The limiting case of the boxing approach would be a system kept in complete
physical and informational isolation. Even if achievable, however, such an isolated
system would be rather useless since it would have no effect on the external world. It
might perhaps be thought that some scientific purpose could be served by creating a
superintelligence and keeping it in isolation: by studying a self-contained model
system, one could learn about its internal dynamics and its behavior patterns. But
this would be an error. As soon as the system is observed, it ceases to be
informationally isolated. 8 Information can be transmitted not only via messages that
an AI sends out through a designated “output channel” but also via any observation
an outsider makes of some causal consequence of the AI’s workings, direct or
indirect—its power consumption, its CPU and memory usage, its computational
states, or indeed any traces left behind after it has been shut down. An AI
anticipating that it might be observed in any of these ways could strategically adopt
behaviors designed to influence the hypothesized observers. For example, the AI
could “malfunction” in whichever way would most increase the probability that it
will ultimately be reactivated or that another superintelligence will be created with
goals similar to its own.
Incentive methods
Incentive methods involve placing an agent in an environment where it finds
instrumental reasons to act in ways that promote the principal’s interests.
Consider a billionaire who uses her fortune to set up a large charitable foundation.
Once created, the foundation may be powerful—more powerful than most
individuals, including its founder, who might have donated most of her wealth. To
control the foundation, the founder lays down its purpose in articles of incorporation
and bylaws, and appoints a board of directors sympathetic to her cause. These
measures constitute a form of motivation selection, since they aim to shape
foundation’s preferences. But even if such attempts to customize the organizational
internals fail, the foundation’s behavior would remain circumscribed by its social
and legal milieu. The foundation would have an incentive to obey the law, forexample, lest it be shut down or fined. It would have an incentive to offer its
employees acceptable pay and working conditions, and to satisfy external
stakeholders. Whatever its final goals, the foundation thus has instrumental reasons
to conform its behavior to various social norms.
Might one not hope that a machine superintelligence would likewise be hemmed
in by the need to get along with the other actors with which it shares the stage?
Though this might seem like a straightforward way of dealing with the control
problem, it is not free of obstacles. In particular, it presupposes a balance of power:
legal or economic sanctions cannot restrain an agent that has a decisive strategic
advantage. Social integration can therefore not be relied upon as a control method in
fast or medium takeoff scenarios that feature a winner-takes-all dynamic.
How about in multipolar scenarios, wherein several agencies emerge post-
transition with comparable levels of capability? Unless the default trajectory is one
with a slow takeoff, achieving such a power distribution may require a carefully
orchestrated ascent wherein different projects are deliberately synchronized to
prevent any one of them from ever pulling ahead of the pack. 9 Even if a multipolar
outcome does result, social integration is not a perfect solution. By relying on social
integration to solve the control problem, the principal risks sacrificing a large
portion of her potential influence. Although a balance of power might prevent a
particular AI from taking over the world, that AI will still have some power to affect
outcomes; and if that power is used to promote some arbitrary final goal—
maximizing paperclip production—it is probably not being used to advance the
interests of the principal. Imagine our billionaire endowing a new foundation and
allowing its mission to be set by a random word generator: not a species-level threat,
but surely a wasted opportunity.
A related but importantly different idea is that an AI, by interacting freely in
society, would acquire new human-friendly final goals. Some such process of
socialization takes place in us humans. We internalize norms and ideologies, and we
come to value other individuals for their own sakes in consequence of our
experiences with them. But this is not a universal dynamic present in all intelligent
systems. As discussed earlier, many types of agent in many situations will have
convergent instrumental reasons not to permit changes in their final goals. (One
might consider trying to design a special kind of goal system that can acquire final
goals in the manner that humans do; but this would not count as a capability control
method. We will discuss some possible methods of value acquisition in Chapter 12.)
Capability control through social integration and balance of power relies upon
diffuse social forces rewarding and penalizing the AI. Another type of incentive
method would involve creating a setup wherein the AI can be rewarded and
penalized by the project that creates it, and thereby incentivized to act in the
interests of the principal. To achieve this, the AI would be placed in a surveillancecontext that allows its behavior to be monitored and evaluated, either manually or by
some automated process. The AI would know that a positive evaluation would bring
about some outcome it desires and that a negative evaluation would fail to do so.
Theoretically, the reward could be the fulfillment of some convergent instrumental
goal. Without knowing anything specific about the AI’s motivation system,
however, it might be difficult to calibrate the reward mechanism. For example, we
could end up with an AI that would be willing to take extreme risks for the sake of a
small chance of eventually obtaining control of a large share of the universe. It could
be expensive to offer the AI a higher expected utility as reward for cooperation than
the AI could hope to achieve by defecting and trying to escape. 10
A better alternative might be to combine the incentive method with the use of
motivation selection to give the AI a final goal that makes it easier to control.
Suppose that an AI were designed to have as its final goal that a particular red button
inside a command bunker never be pressed. Since the pressing of the button is
disvalued intrinsically and not because of its causal consequences, the button could
be completely inert: it could be made of Play-Doh. Furthermore, it is irrelevant
whether the AI can ever know whether the button had been pressed. What is essential
is that the AI believes that the button will more likely remain unpressed if the AI
continuously acts in the principal’s interest than if it rebels.
Refinements to this setup are possible. Instead of trying to endow an AI with a
final goal that refers to a physical button, one could build an AI that places final
value on receiving a stream of “cryptographic reward tokens.” 11 These would be
sequences of numbers serving as keys to ciphers that would have been generated
before the AI was created and that would have been built into its motivation
system. 12 These special number sequences would be extremely desirable to the AI,
constituting a special kind of reward token that the AI could not attain though
wireheading. 13 The keys would be stored in a secure location where they could be
quickly destroyed if the AI ever made an attempt to seize them. So long as the AI
cooperates, the keys are doled out at a steady rate.
The AI could then be given a bounded utility function, and matters could be
arranged so that the AI would get (for instance) 99% of its maximum utility from
the first reward token; 99% of its remaining utility potential from the second reward
token; and so on. By keeping the “discount rate” high, and by transmitting reward
tokens to the AI at a reasonably high rate, one would dissuade the AI from
attempting to carry out long-term plans that would increase the expected number of
reward tokens in the distal future at the expense of some risk that its reward stream
will be cut short or that the next reward token will be cancelled and destroyed. If the
AI always believes that defecting (i.e. doing anything other than its best to look after
the principal’s interests) would incur at least a 2% risk of forfeiting the next reward
token, whereas cooperating would reduce the risk of forfeiting the next reward tokento below 1%, then a utility-maximizing AI would always cooperate.
What might go wrong with such an incentive scheme? One possibility is that the
AI will not trust the human operator to deliver the promised rewards. The track
record of human reliability is something other than a straight line of unerring
perfection. The AI would reasonably worry that the operator will change his mind, or
raise the performance bar, or fail to recognize that the AI has done its part. The AI
would also worry about the operator becoming incapacitated. The combined risk of
such failures might exceed the risk of attempting to seize control of the reward
mechanism. Even a boxed AI possessing the panoply of superpowers is a strong
force. (For an AI that is not boxed to begin with, hijacking the human-governed
reward mechanism may be like taking candy from a baby.)
Another problem with the incentive scheme is that it presupposes that we can tell
whether the outcomes produced by the AI are in our interest. As later chapters will
elaborate, this presupposition is not innocuous.
A full assessment of the feasibility of incentive methods would also have to take
into account a range of other factors, including some esoteric considerations that
might conceivably make such methods more viable than a preliminary analysis
would suggest. In particular, the AI may face ineliminable indexical uncertainty if it
could not be sure that it does not inhabit a computer simulation (as opposed to
“basement-level,” non-simulated physical reality), and this epistemic predicament
may radically influence the AI’s deliberations (see Box 8).
Box 8 Anthropic capture
The AI might assign a substantial probability to its simulation hypothesis, the
hypothesis that it is living in a computer simulation. Even today, many AIs inhabit
simulated worlds—worlds consisting of geometric line drawings, texts, chess games,
or simple virtual realities, and in which the laws of physics deviate sharply from the
laws of physics that we believe govern the world of our own experience. Richer and
more complicated virtual worlds will become feasible with improvements in
programming techniques and computing power. A mature superintelligence could
create virtual worlds that appear to its inhabitants much the same as our world
appears to us. It might create vast numbers of such worlds, running the same
simulation many times or with small variations. The inhabitants would not
necessarily be able to tell whether their world is simulated or not; but if they are
intelligent enough they could consider the possibility and assign it some probability.
In light of the simulation argument (a full discussion of which is beyond the scope
of this book) that probability could be substantial. 14This predicament especially afflicts relatively early-stage superintelligences, ones
that have not yet expanded to take advantage of the cosmic endowment. An early-
stage superintelligence, which uses only a small fraction of the resources of a single
planet, would be much less expensive to simulate than a mature intergalactic
superintelligence. Potential simulators—that is, other more mature civilizations—
would be able to run great numbers of simulations of such early-stage AIs even by
dedicating a minute fraction of their computational resources to that purpose. If at
least some (non-trivial fraction) of these mature superintelligent civilizations choose
to use this ability, early-stage AIs should assign a substantial probability to being in
a simulation.
How an AI would be affected by the simulation hypothesis depends on its
values. 15 Consider first an AI that has a “resource-insatiable” final goal, such as the
goal of maximizing a utility function that is linear in the number of paperclips
caused by the AI to exist—twice as many paperclips, twice as good. Such an AI
might care less about the simulation hypothesis, on grounds that its ability to
influence how many paperclips are produced looks much more limited if the AI is in
a simulation than if it is in basement-level physical reality. The AI might
accordingly behave much as if it were certain that the simulation hypothesis is false,
even if it assigned a fairly high probability to the simulation hypothesis being true. 16
The AI would reason that it does not much matter how it acts if the simulation
hypothesis is true, whereas its actions make a great difference if the hypothesis is
false; so it would select actions mainly by reference to how good they are
conditional on the simulation hypothesis being false.
By contrast, consider an AI that has a more modest final goal, one that could be
satisfied with a small amount of resources, such as the goal of receiving some pre-
produced cryptographic reward tokens, or the goal of causing the existence of forty-
five virtual paperclips. Such an AI should not discount those possible worlds in
which it inhabits a simulation. A substantial portion of the AI’s total expected utility
might derive from those possible worlds. The decision-making of an AI with goals
that are easily resource-satiable may therefore—if it assigns a high probability to the
simulation hypothesis—be dominated by considerations about which actions would
produce the best result if its perceived world is a simulation. Such an AI (even if it
is, in fact, not in a simulation) might therefore be heavily influenced by its beliefs
about which behaviors would be rewarded in a simulation.
In particular, if an AI with resource-satiable final goals believes that in most
simulated worlds that match its observations it will be rewarded if it cooperates (but
not if it attempts to escape its box or contravene the interests of its creator) then it
may choose to cooperate. We could therefore find that even an AI with a decisive
strategic advantage, one that could in fact realize its final goals to a greater extent by
taking over the world than by refraining from doing so, would nevertheless balk atdoing so.
Thus Conscience does make Cowards of us all,
And thus the Native hue of Resolution
Is sicklied o’er, with the pale cast of Thought,
And enterprises of great pith and moment,
With this regard their Currents turn away,
And lose the name of Action.
(Shakespeare, Hamlet, Act iii. Sc. 1)
A mere line in the sand, backed by the clout of a nonexistent simulator, could prove
a stronger restraint than a two-foot-thick solid steel door. 17
Stunting
Another possible capability control method is to limit the system’s intellectual
faculties or its access to information. This might be done by running the AI on
hardware that is slow or short on memory. In the case of a boxed system,
information inflow could also be restricted.
Stunting an AI in these ways would limit its usefulness. The method thus faces a
dilemma: too little stunting, and the AI might have the wit to figure out some way to
make itself more intelligent (and thence to world domination); too much, and the AI
is just another piece of dumb software. A radically stunted AI is certainly safe but
does not solve the problem of how to achieve a controlled detonation: an intelligence
explosion would remain possible and would simply be triggered by some other
system instead, perhaps at a slightly later date.
One might think it would be safe to build a superintelligence provided it is only
given data about some narrow domain of facts. For example, one might build an AI
that lacks sensors and that has preloaded into its memory only facts about petroleum
engineering or peptide chemistry. But if the AI is superintelligent—if it is has a
superhuman level of general intelligence—such data deprivation does not guarantee
safety.
There are several reasons for this. First, the notion of information being “about” a
certain topic is generally problematic. Any piece of information can in principle be
relevant to any topic whatsoever, depending on the background information of a
reasoner. 18 Furthermore, a given data set contains information not only about the
domain from which the data was collected but also about various circumstantialfacts. A shrewd mind looking over a knowledge base that is nominally about peptide
chemistry might infer things about a wide range of topics. The fact that certain
information is included and other information is not could tell an AI something
about the state of human science, the methods and instruments available to study
peptides, the fabrication technologies used to make these instruments, and the nature
of the brains and societies that conceived the studies and the instruments. It might be
that a superintelligence could correctly surmise a great deal from what seem, to
dull-witted human minds, meager scraps of evidence. Even without any designated
knowledge base at all, a sufficiently superior mind might be able to learn much by
simply introspecting on the workings of its own psyche—the design choices
reflected in its source code, the physical characteristics of its circuitry. 19 Perhaps a
superintelligence could even deduce much about the likely properties of the world a
priori (combining logical inference with a probability prior biased toward simpler
worlds, and a few elementary facts implied by the superintelligence’s existence as a
reasoning system). It might imagine the consequences of different possible laws of
physics: what kind of planets would form, what kind of intelligent life would evolve,
what kind of societies would develop, what kind of methods to solve the control
problem would be attempted, how those methods could be defeated. 20
For these reasons it would be imprudent to rely on information deprivation as the
main check on a superintelligence’s power. Some forms of stunting however,
including limiting hardware capacity and preventing the development of specific
cognitive faculties, might find use as auxiliary control methods in the developmental
phase. They could be used, for instance, to regulate the rate of capacity gain in an
intermediary AI and to allow tests to be run at different levels of capability short of
full-throttled superintelligence.
Tripwires
A tripwire is a mechanism that performs diagnostic tests on the system (possibly
without its knowledge) and effects a shutdown if it detects signs of dangerous
activity. Programmers can then scrutinize recordings of the suspicious activity at
their leisure, and make changes to the AI’s code base or environment before
restarting it. If they find that the problem is sufficiently deep-rooted, they could
decide to abort the entire project.
Tripwires differ from incentive methods in that they do not rely on the system
being aware of the consequences of engaging in forbidden activities. Tripwires are
more closely related to stunting methods. Like stunting, tripwires could be used as a
temporary safeguard, providing a degree of protection during the development
phase. In principle, tripwires can also be used during the operational phase,particularly for a boxed system. However, the ability of tripwires to constrain a full-
fledged superintelligence must remain very much in doubt, since it would be hard
for us to assure ourselves that such an agent could not find ways to subvert any
tripwire devised by the human intellect.
Table 9 Different kinds of tripwires
Type of
Description and examples
monitoring
Detectors could be placed around a boxed AI to detect attempts to
breach the containment. For example, detectors could intercept attempts
Behavior at radio communication or at accessing internal computational resources
intended to be off limits. An “Ethernet port of Eden” could be installed:
an apparent connection to the internet that leads to a shutdown switch.
Automated capability testing could be performed at frequent intervals to
determine the AI’s skill in various domains. If either the rate of
improvement is unexpectedly high or the AI attains a level of
Ability
competence that brings it close to the potential danger zone, the system
could be slowed down or paused, in order to allow the programmers to
monitor further developments more closely.
The AI’s internal processes could be monitored for suspicious activity.
For example, any change to the AI’s representation of its final values
might trigger an automatic shutdown and review. The AI’s resource use
and algorithmic methods may also be also be monitored for unexpected
patterns.
More ambitiously, the AI’s beliefs, plans, subgoals, and justifications
may be continuously scrutinized, to ensure they conform to the
programmers’ expectations. Such monitoring of the AI’s intentional
states would enable a particularly powerful form of content monitoring:
Content
the scanning of the AI’s cognitive processes for a conception of
deception—that vulnerable moment when an unfriendly AI first forms
the intention to conceal its true intentions. 21
Content monitoring that requires that the AI’s intentional states be
transparent to the programmers or to an automatic monitoring
mechanism may not be feasible for all kinds of AI architectures. (Some
neural networks, for instance, are opaque, as they represent information
holistically and in ways that do not necessarily match up with human
concepts.) This may be a reason to avoid using such architectures.As an adjunct safety measure in the development phase, though, tripwires are
important. It would be good practice for the AI’s designers to codify their
expectations about how the system is supposed to function, and then to circumscribe
the AI with tripwires that would be activated if any of these expectations is violated.
To make the tests more stringent, “honeypots” could be strategically placed to create
temptations for a malfunctioning AI to commit some easily observable violation.
For instance, if an AI has been designed in such a way that it is supposed not to want
to access the internet, a fake Ethernet port could be installed (leading to an
automatic shutdown switch) just to see if they AI tries to use it. (Some examples of
tripwires are given in Table 9.)
It should be emphasized that the value of a tripwire depends not only on the
mechanism itself but also—critically—on how a project reacts when a tripwire is
triggered. If the programmers or project managers, impatient to make progress,
simply switch the system back on again—or if they do so after making some token
modification to prevent the tripwire being triggered on the next run—then no safety
has been gained even if the tripwire itself works exactly as intended.
Motivation selection methods
Motivation selection methods seek to prevent undesirable outcomes by shaping what
the superintelligence wants to do. By engineering the agent’s motivation system and
its final goals, these methods would produce a superintelligence that would not want
to exploit a decisive strategic advantage in a harmful way. Since a superintelligent
agent is skilled at achieving its ends, if it prefers not to cause harm (in some
appropriate sense of “harm”) then it would tend not to cause harm (in that sense of
“harm”).
Motivation selection can involve explicitly formulating a goal or set of rules to be
followed (direct specification) or setting up the system so that it can discover an
appropriate set of values for itself by reference to some implicitly or indirectly
formulated criterion (indirect normativity). One option in motivation selection is to
try to build the system so that it would have modest, non-ambitious goals
(domesticity). An alternative to creating a motivation system from scratch is to
select an agent that already has an acceptable motivation system and then augment
that agent’s cognitive powers to make it superintelligent, while ensuring that the
motivation system does not get corrupted in the process (augmentation). Let us look
at these in turn.
Direct specificationDirect specification is the most straightforward approach to the control problem.
The approach comes in two versions, rule-based and consequentialist, and involves
trying to explicitly define a set of rules or values that will cause even a free-roaming
superintelligent AI to act safely and beneficially. Direct specification, however,
faces what may be insuperable obstacles, deriving from both the difficulties in
determining which rules or values we would wish the AI to be guided by and the
difficulties in expressing those rules or values in computer-readable code.
The traditional illustration of the direct rule-based approach is the “three laws of
robotics” concept, formulated by science fiction author Isaac Asimov in a short story
published in 1942. 22 The three laws were: (1) A robot may not injure a human being
or, through inaction, allow a human being to come to harm; (2) A robot must obey
any orders given to it by human beings, except where such orders would conflict
with the First Law; (3) A robot must protect its own existence as long as such
protection does not conflict with the First or Second Law. Embarrassingly for our
species, Asimov’s laws remained state-of-the-art for over half a century: this despite
obvious problems with the approach, some of which are explored in Asimov’s own
writings (Asimov probably having formulated the laws in the first place precisely so
that they would fail in interesting ways, providing fertile plot complications for his
stories). 23
Bertrand Russell, who spent many years working on the foundations of
mathematics, once remarked that “everything is vague to a degree you do not realize
till you have tried to make it precise.” 24 Russell’s dictum applies in spades to the
direct specification approach. Consider, for example, how one might explicate
Asimov’s first law. Does it mean that the robot should minimize the probability of
any human being coming to harm? In that case the other laws become otiose since it
is always possible for the AI to take some action that would have at least some
microscopic effect on the probability of a human being coming to harm. How is the
robot to balance a large risk of a few humans coming to harm versus a small risk of
many humans being harmed? How do we define “harm” anyway? How should the
harm of physical pain be weighed against the harm of architectural ugliness or social
injustice? Is a sadist harmed if he is prevented from tormenting his victim? How do
we define “human being”? Why is no consideration given to other morally
considerable beings, such as sentient nonhuman animals and digital minds? The
more one ponders, the more the questions proliferate.
Perhaps the closest existing analog to a rule set that could govern the actions of a
superintelligence operating in the world at large is a legal system. But legal systems
have developed through a long process of trial and error, and they regulate relatively
slowly-changing human societies. Laws can be revised when necessary. Most
importantly, legal systems are administered by judges and juries who generallyapply a measure of common sense and human decency to ignore logically possible
legal interpretations that are sufficiently obviously unwanted and unintended by the
lawgivers. It is probably humanly impossible to explicitly formulate a highly
complex set of detailed rules, have them apply across a highly diverse set of
circumstances, and get it right on the first implementation. 25
Problems for the direct consequentialist approach are similar to those for the
direct rule-based approach. This is true even if the AI is intended to serve some
apparently simple purpose such as implementing a version of classical
utilitarianism. For instance, the goal “Maximize the expectation of the balance of
pleasure over pain in the world” may appear simple. Yet expressing it in computer
code would involve, among other things, specifying how to recognize pleasure and
pain. Doing this reliably might require solving an array of persistent problems in the
philosophy of mind—even just to obtain a correct account expressed in a natural
language, an account which would then, somehow, have to be translated into a
programming language.
A small error in either the philosophical account or its translation into code could
have catastrophic consequences. Consider an AI that has hedonism as its final goal,
and which would therefore like to tile the universe with “hedonium” (matter
organized in a configuration that is optimal for the generation of pleasurable
experience). To this end, the AI might produce computronium (matter organized in a
configuration that is optimal for computation) and use it to implement digital minds
in states of euphoria. In order to maximize efficiency, the AI omits from the
implementation any mental faculties that are not essential for the experience of
pleasure, and exploits any computational shortcuts that according to its definition of
pleasure do not vitiate the generation of pleasure. For instance, the AI might confine
its simulation to reward circuitry, eliding faculties such as memory, sensory
perception, executive function, and language; it might simulate minds at a relatively
coarse-grained level of functionality, omitting lower-level neuronal processes; it
might replace commonly repeated computations with calls to a lookup table; or it
might put in place some arrangement whereby multiple minds would share most
parts of their underlying computational machinery (their “supervenience bases” in
philosophical parlance). Such tricks could greatly increase the quantity of pleasure
producible with a given amount of resources. It is unclear how desirable this would
be. Furthermore, if the AI’s criterion for determining whether a physical process
generates pleasure is wrong, then the AI’s optimizations might throw the baby out
with the bathwater: discarding something which is inessential according to the AI’s
criterion yet essential according to the criteria implicit in our human values. The
universe then gets filled not with exultingly heaving hedonium but with
computational processes that are unconscious and completely worthless—the
equivalent of a smiley-face sticker xeroxed trillions upon trillions of times andplastered across the galaxies.
Domesticity
One special type of final goal which might be more amenable to direct specification
than the examples given above is the goal of self-limitation. While it seems
extremely difficult to specify how one would want a superintelligence to behave in
the world in general—since this would require us to account for all the trade-offs in
all the situations that could arise—it might be feasible to specify how a
superintelligence should behave in one particular situation. We could therefore seek
to motivate the system to confine itself to acting on a small scale, within a narrow
context, and through a limited set of action modes. We will refer to this approach of
giving the AI final goals aimed at limiting the scope of its ambitions and activities
as “domesticity.”
For example, one could try to design an AI such that it would function as a
question-answering device (an “oracle,” to anticipate the terminology that we will
introduce in the next chapter). Simply giving the AI the final goal of producing
maximally accurate answers to any question posed to it would be unsafe—recall the
“Riemann hypothesis catastrophe” described in Chapter 8. (Reflect, also, that this
goal would incentivize the AI to take actions to ensure that it is asked easy
questions.) To achieve domesticity, one might try to define a final goal that would
somehow overcome these difficulties: perhaps a goal that combined the desiderata
of answering questions correctly and minimizing the AI’s impact on the world
except whatever impact results as an incidental consequence of giving accurate and
non-manipulative answers to the questions it is asked. 26
The direct specification of such a domesticity goal is more likely to be feasible
than the direct specification of either a more ambitious goal or a complete rule set
for operating in an open-ended range of situations. Significant challenges
nonetheless remain. Care would have to be taken, for instance, in the definition of
what it would be for the AI to “minimize its impact on the world” to ensure that the
measure of the AI’s impact coincides with our own standards for what counts as a
large or a small impact. A bad measure would lead to bad trade-offs. There are also
other kinds of risk associated with building an oracle, which we will discuss later.
There is a natural fit between the domesticity approach and physical containment.
One would try to “box” an AI such that the system is unable to escape while
simultaneously trying to shape the AI’s motivation system such that it would be
unwilling to escape even if it found a way to do so. Other things equal, the existence
of multiple independent safety mechanisms should shorten the odds of success. 27Indirect normativity
If direct specification seems hopeless, we might instead try indirect normativity.
The basic idea is that rather than specifying a concrete normative standard directly,
we specify a process for deriving a standard. We then build the system so that it is
motivated to carry out this process and to adopt whatever standard the process
arrives at. 28 For example, the process could be to carry out an investigation into the
empirical question of what some suitably idealized version of us would prefer the AI
to do. The final goal given to the AI in this example could be something along the
lines of “achieve that which we would have wished the AI to achieve if we had
thought about the matter long and hard.”
Further explanation of indirect normativity will have to await Chapter 13. There,
we will revisit the idea of “extrapolating our volition” and explore various alterative
formulations. Indirect normativity is a very important approach to motivation
selection. Its promise lies in the fact that it could let us offload to the
superintelligence much of the difficult cognitive work required to carry out a direct
specification of an appropriate final goal.
Augmentation
The last motivation selection method on our list is augmentation. Here the idea is
that rather than attempting to design a motivation system de novo, we start with a
system that already has an acceptable motivation system, and enhance its cognitive
faculties to make it superintelligent. If all goes well, this would give us a
superintelligence with an acceptable motivation system.
This approach, obviously, is unavailing in the case of a newly created seed AI. But
augmentation is a potential motivation selection method for other paths to
superintelligence, including brain emulation, biological enhancement, brain–
computer interfaces, and networks and organizations, where there is a possibility of
building out the system from a normative nucleus (regular human beings) that
already contains a representation of human value.
The attractiveness of augmentation may increase in proportion to our despair at
the other approaches to the control problem. Creating a motivation system for a seed
AI that remains reliably safe and beneficial under recursive self-improvement even
as the system grows into a mature superintelligence is a tall order, especially if we
must get the solution right on the first attempt. With augmentation, we would at
least start with a system that has familiar and human-like motivations.
On the downside, it might be hard to ensure that a complex, evolved, kludgy, andpoorly understood motivation system, like that of a human being, will not get
corrupted when its cognitive engine blasts into the stratosphere. As discussed earlier,
an imperfect brain emulation procedure that preserves intellectual functioning may
not preserve all facets of personality. The same is true (though perhaps to a lesser
degree) for biological enhancements of cognition, which might subtly affect
motivation, and for collective intelligence enhancements of organizations and
networks, which might adversely change social dynamics (e.g. in ways that debase
the collective’s attitude toward outsiders or toward its own constituents). If
superintelligence is achieved via any of these paths, a project sponsor would find
guarantees about the ultimate motivations of the mature system hard to come by. A
mathematically well-specified and foundationally elegant AI architecture might—
for all its non-anthropomorphic otherness—offer greater transparency, perhaps even
the prospect that important aspects of its functionality could be formally verified.
In the end, however one tallies up the advantages and disadvantages of
augmentation, the choice as to whether to rely on it might be forced. If
superintelligence is first achieved along the artificial intelligence path,
augmentation is not applicable. Conversely, if superintelligence is first achieved
along some non-AI path, then many of the other motivation selection methods are
inapplicable. Even so, views on how likely augmentation would be to succeed do
have strategic relevance insofar as we have opportunities to influence which
technology will first produce superintelligence.
Synopsis
A quick synopsis might be called for before we close this chapter. We distinguished
two broad classes of methods for dealing with the agency problem at the heart of AI
safety: capability control and motivation selection. Table 10 gives a summary.
Table 10 Control methods
Capability
control
Boxing
methods
Incentive
The system is confined in such a way that it can affect the external
world only through some restricted, pre-approved channel.
Encompasses physical and informational containment methods.
The system is placed within an environment that provides
appropriate incentives. This could involve social integration into a
world of similarly powerful entities. Another variation is the use ofmethods
Stunting
Tripwires
(cryptographic) reward tokens. “Anthropic capture” is also a very
important possibility but one that involves esoteric considerations.
Constraints are imposed on the cognitive capabilities of the system
or its ability to affect key internal processes.
Diagnostic tests are performed on the system (possibly without its
knowledge) and a mechanism shuts down the system if dangerous
activity is detected.
Motivation
selection
The system is endowed with some directly specified motivation
Direct
system, which might be consequentialist or involve following a set of
specification
rules.
A motivation system is designed to severely limit the scope of the
Domesticity
agent’s ambitions and activities.
Indirect normativity could involve rule-based or consequentialist
Indirect
principles, but is distinguished by its reliance on an indirect approach
normativity to specifying the rules that are to be followed or the values that are to
be pursued.
One starts with a system that already has substantially human or
Augmentation benevolent motivations, and enhances its cognitive capacities to
make it superintelligent.
Each control method comes with potential vulnerabilities and presents different
degrees of difficulty in its implementation. It might perhaps be thought that we
should rank them from better to worse, and then opt for the best method. But that
would be simplistic. Some methods can be used in combination whereas others are
exclusive. Even a comparatively insecure method may be advisable if it can easily
be used as an adjunct, whereas a strong method might be unattractive if it would
preclude the use of other desirable safeguards.
It is therefore necessary to consider what package deals are available. We need to
consider what type of system we might try to build, and which control methods
would be applicable to each type. This is the topic for our next chapter.CHAPTER 10
Oracles, genies, sovereigns, tools
Some say: “Just build a question-answering system!” or “Just build an AI that
is like a tool rather than an agent!” But these suggestions do not make all safety
concerns go away, and it is in fact a non-trivial question which type of system
would offer the best prospects for safety. We consider four types or “castes”—
oracles, genies, sovereigns, and tools—and explain how they relate to one
another. 1 Each offers different sets of advantages and disadvantages in our
quest to solve the control problem.
Oracles
An oracle is a question-answering system. It might accept questions in a natural
language and present its answers as text. An oracle that accepts only yes/no
questions could output its best guess with a single bit, or perhaps with a few extra
bits to represent its degree of confidence. An oracle that accepts open-ended
questions would need some metric with which to rank possible truthful answers in
terms of their informativeness or appropriateness. 2 In either case, building an oracle
that has a fully domain-general ability to answer natural language questions is an
AI-complete problem. If one could do that, one could probably also build an AI that
has a decent ability to understand human intentions as well as human words.
Oracles with domain-limited forms of superintelligence are also conceivable. For
instance, one could conceive of a mathematics-oracle which would only accept
queries posed in a formal language but which would be very good at answering such
questions (e.g. being able to solve in an instant almost any formally expressed math
problem that the human mathematics profession could solve by laboring
collaboratively for a century). Such a mathematics-oracle would form a stepping-
stone toward domain-general superintelligence.
Oracles with superintelligence in extremely limited domains already exist. A
pocket calculator can be viewed as a very narrow oracle for basic arithmetical
questions; an Internet search engine can be viewed as a very partial realization of an
oracle with a domain that encompasses a significant part of general human
declarative knowledge. These domain-limited oracles are tools rather than agents
(more on tool-AIs shortly). In what follows, though, the term “oracle” will refer to
question-answering systems that have domain-general superintelligence, unlessotherwise stated.
To make a general superintelligence function as an oracle, we could apply both
motivation selection and capability control. Motivation selection for an oracle may
be easier than for other castes of superintelligence, because the final goal in an
oracle could be comparatively simple. We would want the oracle to give truthful,
non-manipulative answers and to otherwise limit its impact on the world. Applying a
domesticity method, we might require that the oracle should use only designated
resources to produce its answer. For example, we might stipulate that it should base
its answer on a preloaded corpus of information, such as a stored snapshot of the
Internet, and that it should use no more than a fixed number of computational steps. 3
To avoid incentivizing the oracle to manipulate us into giving it easier questions—
which would happen if we gave it the goal of maximizing its accuracy across all
questions we will ask it—we could give it the goal of answering only one question
and to terminate immediately upon delivering its answer. The question would be
preloaded into its memory before the program is run. To ask a second question, we
would reset the machine and run the same program with a different question
preloaded in memory.
Subtle and potentially treacherous challenges arise even in specifying the
relatively simple motivation system needed to drive an oracle. Suppose, for
example, that we come up with some explication of what it means for the AI “to
minimize its impact on the world, subject to achieving certain results” or “to use
only designated resources in preparing the answer.” What happens if the AI, in the
course of its intellectual development, undergoes the equivalent of a scientific
revolution involving a change in its basic ontology? 4 We might initially have
explicated “impact” and “designated resources” using our own ontology (postulating
the existence of various physical objects such as computers). But just as we have
abandoned ontological categories that were taken for granted by scientists in
previous ages (e.g. “phlogiston,” “élan vital,” and “absolute simultaneity”), so a
superintelligent AI might discover that some of our current categories are predicated
on fundamental misconceptions. The goal system of an AI undergoing an ontological
crisis needs to be resilient enough that the “spirit” of its original goal content is
carried over, charitably transposed into the new key.
Although making an oracle safe through the use of motivation selection might be
far from trivial, it may nevertheless be easier than doing the same for an AI that
roams the world in pursuit of some complicated goal. This is an argument for
preferring that the first superintelligence be an oracle. Another point in favor of an
oracle-first path is the greater susceptibility of an oracle to capability control. All
the standard boxing methods apply here. In addition, there may be methods that are
distinctively applicable to oracles. For example, consider the risk that an oracle will
answer questions not in a maximally truthful way but in such a way as to subtlymanipulate us into promoting its own hidden agenda. One way to slightly mitigate
this threat could be to create multiple oracles, each with a slightly different code and
a slightly different information base. A simple mechanism could then compare the
answers given by the different oracles and only present them for human viewing if
all the answers agree. While one must assume that any superintelligent domain-
general oracle would know about the existence of other oracles (it could infer this
from its knowledge about human society), it might be feasible to hide some
implementation details of each oracle from the views of the others. If the oracles are
not able to communicate with one another, it might then be difficult for them to
coordinate on how to answer our questions manipulatively. There are many ways of
deviating from the truth, and the oracles may not all agree on which of these
deviations is most attractive—whereas the truth itself is a Schelling point (a salient
place for agreement in the absence of communication). So if the oracles achieve
consensus, it might be a sign that they gave the true answer. 5
An oracle would ideally be trustworthy in the sense that we could safely assume
that its answers are always accurate to the best of its ability. But even an
untrustworthy oracle could be useful. We could ask such an oracle questions of a
type for which it is difficult to find the answer but easy to verify whether a given
answer is correct. Many mathematical problems are of this kind. If we are
wondering whether a mathematical proposition is true, we could ask the oracle to
produce a proof or disproof of the proposition. Finding the proof may require insight
and creativity beyond our ken, but checking a purported proof’s validity can be done
by a simple mechanical procedure.
If it is expensive to verify answers (as is often the case on topics outside logic and
mathematics), we can randomly select a subset of the oracle’s answers for
verification. If they are all correct, we can assign a high probability to most of the
other answers also being correct. This trick can give us a bulk discount on
trustworthy answers that would be costly to verify individually. (Unfortunately, it
cannot give us trustworthy answers that we are unable to verify, since a dissembling
oracle may choose to answer correctly only those questions where it believes we
could verify its answers.)
There could be important issues on which we could benefit from an augural
pointer toward the correct answer (or toward a method for locating the correct
answer) even if we had to actively distrust the provenance. For instance, one might
ask for the solution to various technical or philosophical problems that may arise in
the course of trying to develop more advanced motivation selection methods. If we
had a proposed AI design alleged to be safe, we could ask an oracle whether it could
identify any significant flaw in the design, and whether it could explain any such
flaw to us in twenty words or less. Questions of this kind could elicit valuable
information. Caution and restraint would be required, however, for us not to ask toomany such questions—and not to allow ourselves to partake of too many details of
the answers given to the questions we do ask—lest we give the untrustworthy oracle
opportunities to work on our psychology (by means of plausible-seeming but subtly
manipulative messages). It might not take many bits of communication for an AI
with the social manipulation superpower to bend us to its will.
Even if the oracle itself works exactly as intended, there is a risk that it would be
misused. One obvious dimension of this problem is that an oracle AI would be a
source of immense power which could give a decisive strategic advantage to its
operator. This power might be illegitimate and it might not be used for the common
good. Another more subtle but no less important dimension is that the use of an
oracle could be extremely dangerous for the operator herself. Similar worries (which
involve philosophical as well as technical issues) arise also for other hypothetical
castes of superintelligence. We will explore them more thoroughly in Chapter 13.
Suffice it here to note that the protocol determining which questions are asked, in
which sequence, and how the answers are reported and disseminated could be of
great significance. One might also consider whether to try to build the oracle in such
a way that it would refuse to answer any question in cases where it predicts that its
answering would have consequences classified as catastrophic according to some
rough-and-ready criteria.
Genies and sovereigns
A genie is a command-executing system: it receives a high-level command, carries
it out, then pauses to await the next command. 6 A sovereign is a system that has an
open-ended mandate to operate in the world in pursuit of broad and possibly very
long-range objectives. Although these might seem like radically different templates
for what a superintelligence should be and do, the difference is not as deep as it
might at first glance appear.
With a genie, one already sacrifices the most attractive property of an oracle: the
opportunity to use boxing methods. While one might consider creating a physically
confined genie, for instance one that can only construct objects inside a designated
volume—a volume that might be sealed off by a hardened wall or a barrier loaded
with explosive charges rigged to detonate if the containment is breached—it would
be difficult to have much confidence in the security of any such physical
containment method against a superintelligence equipped with versatile
manipulators and construction materials. Even if it were somehow possible to ensure
a containment as secure as that which can be achieved for an oracle, it is not clear
how much we would have gained by giving the superintelligence direct access to
manipulators compared to requiring it instead to output a blueprint that we couldinspect and then use to achieve the same result ourselves. The gain in speed and
convenience from bypassing the human intermediary seems hardly worth the loss of
foregoing the use of the stronger boxing methods available to contain an oracle.
If one were creating a genie, it would be desirable to build it so that it would obey
the intention behind the command rather than its literal meaning, since a literalistic
genie (one superintelligent enough to attain a decisive strategic advantage) might
have a propensity to kill the user and the rest of humanity on its first use, for reasons
explained in the section on malignant failure modes in Chapter 8. More broadly, it
would seem important that the genie seek a charitable—and what human beings
would regard as reasonable—interpretation of what is being commanded, and that
the genie be motivated to carry out the command under such an interpretation rather
than under the literalistic interpretation. The ideal genie would be a super-butler
rather than an autistic savant.
A genie endowed with such a super-butler nature, however, would not be far from
qualifying for membership in the caste of sovereigns. Consider, for comparison, the
idea of building a sovereign with the final goal of obeying the spirit of the
commands we would have given had we built a genie rather than a sovereign. Such a
sovereign would mimic a genie. Being superintelligent, this sovereign would do a
good job at guessing what commands we would have given a genie (and it could
always ask us if that would help inform its decisions). Would there then really be
any important difference between such a sovereign and a genie? Or, pressing on the
distinction from the other side, consider that a superintelligent genie may likewise
be able to predict what commands we will give it: what then is gained from having it
await the actual issuance before it acts?
One might think that a big advantage of a genie over a sovereign is that if
something goes wrong, we could issue the genie with a new command to stop or to
reverse the effects of the previous actions, whereas a sovereign would just push on
regardless of our protests. But this apparent safety advantage for the genie is largely
illusory. The “stop” or “undo” button on a genie works only for benign failure
modes: in the case of a malignant failure—one in which, for example, carrying out
the existing command has become a final goal for the genie—the genie would
simply disregard any subsequent attempt to countermand the previous command. 7
One option would be to try to build a genie such that it would automatically
present the user with a prediction about salient aspects of the likely outcomes of a
proposed command, asking for confirmation before proceeding. Such a system could
be referred to as a genie-with-a-preview. But if this could be done for a genie, it
could likewise be done for a sovereign. So again, this is not a clear differentiator
between a genie and a sovereign. (Supposing that a preview functionality could be
created, the questions of whether and if so how to use it are rather less obvious than
one might think, notwithstanding the strong appeal of being able to glance at theoutcome before committing to making it irrevocable reality. We will return to this
matter later.)
The ability of one caste to mimic another extends to oracles, too. A genie could be
made to act like an oracle if the only commands we ever give it are to answer certain
questions. An oracle, in turn, could be made to substitute for a genie if we asked the
oracle what the easiest way is to get certain commands executed. The oracle could
give us step-by-step instructions for achieving the same result as a genie would
produce, or it could even output the source code for a genie. 8 Similar points can be
made with regard to the relation between an oracle and a sovereign.
The real difference between the three castes, therefore, does not reside in the
ultimate capabilities that they would unlock. Instead, the difference comes down to
alternative approaches to the control problem. Each caste corresponds to a different
set of safety precautions. The most prominent feature of an oracle is that it can be
boxed. One might also try to apply domesticity motivation selection to an oracle. A
genie is harder to box, but at least domesticity may be applicable. A sovereign can
neither be boxed nor handled through the domesticity approach.
If these were the only relevant factors, then the order of desirability would seem
clear: an oracle would be safer than a genie, which would be safer than a sovereign;
and any initial differences in convenience and speed of operation would be relatively
small and easily dominated by the gains in safety obtainable by building an oracle.
However, there are other factors that need to be taken into account. When choosing
between castes, one should consider not only the danger posed by the system itself
but also the dangers that arise out of the way it might be used. A genie most
obviously gives the person who controls it enormous power, but the same holds for
an oracle. 9 A sovereign, by contrast, could be constructed in such way as to accord
no one person or group any special influence over the outcome, and such that it
would resist any attempt to corrupt or alter its original agenda. What is more, if a
sovereign’s motivation is defined using “indirect normativity” (a concept to be
described in Chapter 13) then it could be used to achieve some abstractly defined
outcome, such as “whatever is maximally fair and morally right”—without anybody
knowing in advance what exactly this will entail. This would create a situation
analogous to a Rawlsian “veil of ignorance.” 10 Such a setup might facilitate the
attainment of consensus, help prevent conflict, and promote a more equitable
outcome.
Another point, which counts against some types of oracles and genies, is that there
are risks involved in designing a superintelligence to have a final goal that does not
fully match the outcome that we ultimately seek to attain. For example, if we use a
domesticity motivation to make the superintelligence want to minimize some of its
impacts on the world, we might thereby create a system whose preference ranking
over possible outcomes differs from that of the sponsor. The same will happen if webuild the AI to place a peculiarly high value on answering questions correctly, or on
faithfully obeying individual commands. Now, if sufficient care is taken, this should
not cause any problems: there would be sufficient agreement between the two
rankings—at least insofar as they pertain to possible worlds that have a reasonable
chance of being actualized—that the outcomes that are good by the AI’s standard are
also good by the principal’s standard. But perhaps one could argue for the design
principle that it is unwise to introduce even a limited amount of disharmony between
the AI’s goals and ours. (The same concern would of course apply to giving
sovereigns goals that do not completely harmonize with ours.)
Tool-AIs
One suggestion that has been made is that we build the superintelligence to be like a
tool rather than an agent. 11 This idea seems to arise out of the observation that
ordinary software, which is used in countless applications, does not raise any safety
concerns even remotely analogous to the challenges discussed in this book. Might
one not create “tool-AI” that is like such software—like a flight control system, say,
or a virtual assistant—only more flexible and capable? Why build a
superintelligence that has a will of its own? On this line of thinking, the agent
paradigm is fundamentally misguided. Instead of creating an AI that has beliefs and
desires and that acts like an artificial person, we should aim to build regular
software that simply does what it is programmed to do.
This idea of creating software that “simply does what it is programmed to do” is,
however, not so straightforward if the product being created is a powerful general
intelligence. There is, of course, a trivial sense in which all software simply does
what it is programmed to do: the behavior is mathematically specified by the code.
But this is equally true for all castes of machine intelligence, “tool-AI” or not. If,
instead, “simply doing what it is programmed to do” means that the software
behaves as the programmers intended, then this is a standard that ordinary software
very often fails to meet.
Because of the limited capabilities of contemporary software (compared with
those of machine superintelligence) the consequences of such failures are
manageable, ranging from insignificant to very costly, but in no case amounting to
an existential threat. 12 However, if it is insufficient capability rather than sufficient
reliability that makes ordinary software existentially safe, then it is unclear how
such software could be a model for a safe superintelligence. It might be thought that
by expanding the range of tasks done by ordinary software, one could eliminate the
need for artificial general intelligence. But the range and diversity of tasks that a
general intelligence could profitably perform in a modern economy is enormous. Itwould be infeasible to create special-purpose software to handle all of those tasks.
Even if it could be done, such a project would take a long time to carry out. Before it
could be completed, the nature of some of the tasks would have changed, and new
tasks would have become relevant. There would be great advantage to having
software that can learn on its own to do new tasks, and indeed to discover new tasks
in need of doing. But this would require that the software be able to learn, reason,
and plan, and to do so in a powerful and robustly cross-domain manner. In other
words, it would require general intelligence.
Especially relevant for our purposes is the task of software development itself.
There would be enormous practical advantages to being able to automate this. Yet
the capacity for rapid self-improvement is just the critical property that enables a
seed AI to set off an intelligence explosion.
If general intelligence is not dispensable, is there some other way of construing
the tool-AI idea so as to preserve the reassuringly passive quality of a humdrum
tool? Could one have a general intelligence that is not an agent? Intuitively, it is not
just the limited capability of ordinary software that makes it safe: it is also its lack
of ambition. There is no subroutine in Excel that secretly wants to take over the
world if only it were smart enough to find a way. The spreadsheet application does
not “want” anything at all; it just blindly carries out the instructions in the program.
What (one might wonder) stands in the way of creating a more generally intelligent
application of the same type? An oracle, for instance, which, when prompted with a
description of a goal, would respond with a plan for how to achieve it, in much the
same way that Excel responds to a column of numbers by calculating a sum—
without thereby expressing any “preferences” regarding its output or how humans
might choose to use it?
The classical way of writing software requires the programmer to understand the
task to be performed in sufficient detail to formulate an explicit solution process
consisting of a sequence of mathematically well-defined steps expressible in code. 13
(In practice, software engineers rely on code libraries stocked with useful behaviors,
which they can invoke without needing to understand how the behaviors are
implemented. But that code was originally created by programmers who had a
detailed understanding of what they were doing.) This approach works for solving
well-understood tasks, and is to credit for most software that is currently in use. It
falls short, however, when nobody knows precisely how to solve all of the tasks that
need to be accomplished. This is where techniques from the field of artificial
intelligence become relevant. In narrow applications, machine learning might be
used merely to fine-tune a few parameters in a largely human-designed program. A
spam filter, for example, might be trained on a corpus of hand-classified email
messages in a process that changes the weights that the classification algorithm
places on various diagnostic features. In a more ambitious application, the classifiermight be built so that it can discover new features on its own and test their validity
in a changing environment. An even more sophisticated spam filter could be
endowed with some ability to reason about the trade-offs facing the user or about the
contents of the messages it is classifying. In neither of these cases does the
programmer need to know the best way of distinguishing spam from ham, only how
to set up an algorithm that can improve its own performance via learning,
discovering, or reasoning.
With advances in artificial intelligence, it would become possible for the
programmer to offload more of the cognitive labor required to figure out how to
accomplish a given task. In an extreme case, the programmer would simply specify a
formal criterion of what counts as success and leave it to the AI to find a solution.
To guide its search, the AI would use a set of powerful heuristics and other methods
to discover structure in the space of possible solutions. It would keep searching until
it found a solution that satisfied the success criterion. The AI would then either
implement the solution itself or (in the case of an oracle) report the solution to the
user.
Rudimentary forms of this approach are quite widely deployed today.
Nevertheless, software that uses AI and machine learning techniques, though it has
some ability to find solutions that the programmers had not anticipated, functions
for all practical purposes like a tool and poses no existential risk. We would enter
the danger zone only when the methods used in the search for solutions become
extremely powerful and general: that is, when they begin to amount to general
intelligence—and especially when they begin to amount to superintelligence.
There are (at least) two places where trouble could then arise. First, the
superintelligent search process might find a solution that is not just unexpected but
radically unintended. This could lead to a failure of one of the types discussed
previously (“perverse instantiation,” “infrastructure profusion,” or “mind crime”). It
is most obvious how this could happen in the case of a sovereign or a genie, which
directly implements the solution it has found. If making molecular smiley faces or
transforming the planet into paperclips is the first idea that the superintelligence
discovers that meets the solution criterion, then smiley faces or paperclips we get. 14
But even an oracle, which—if all else goes well—merely reports the solution, could
become a cause of perverse instantiation. The user asks the oracle for a plan to
achieve a certain outcome, or for a technology to serve a certain function; and when
the user follows the plan or constructs the technology, a perverse instantiation can
ensue, just as if the AI had implemented the solution itself. 15
A second place where trouble could arise is in the course of the software’s
operation. If the methods that the software uses to search for a solution are
sufficiently sophisticated, they may include provisions for managing the search
process itself in an intelligent manner. In this case, the machine running the softwaremay begin to seem less like a mere tool and more like an agent. Thus, the software
may start by developing a plan for how to go about its search for a solution. The plan
may specify which areas to explore first and with what methods, what data to gather,
and how to make best use of available computational resources. In searching for a
plan that satisfies the software’s internal criterion (such as yielding a sufficiently
high probability of finding a solution satisfying the user-specified criterion within
the allotted time), the software may stumble on an unorthodox idea. For instance, it
might generate a plan that begins with the acquisition of additional computational
resources and the elimination of potential interrupters (such as human beings). Such
“creative” plans come into view when the software’s cognitive abilities reach a
sufficiently high level. When the software puts such a plan into action, an existential
catastrophe may ensue.
As the examples in Box 9 illustrate, open-ended search processes sometimes
evince strange and unexpected non-anthropocentric solutions even in their currently
limited forms. Present-day search processes are not hazardous because they are too
weak to discover the kind of plan that could enable a program to take over the world.
Such a plan would include extremely difficult steps, such as the invention of a new
weapons technology several generations ahead of the state of the art or the execution
of a propaganda campaign far more effective than any communication devised by
human spin doctors. To have a chance of even conceiving of such ideas, let alone
developing them in a way that would actually work, a machine would probably need
the capacity to represent the world in a way that is at least as rich and realistic as the
world model possessed by a normal human adult (though a lack of awareness in
some areas might possibly be compensated for by extra skill in others). This is far
beyond the reach of contemporary AI. And because of the combinatorial explosion,
which generally defeats attempts to solve complicated planning problems with
brute-force methods (as we saw in Chapter 1), the shortcomings of known
algorithms cannot realistically be overcome simply by pouring on more computing
power. 21 However, once the search or planning processes become powerful enough,
they also become potentially dangerous.
Box 9 Strange solutions from blind search
Even simple evolutionary search processes sometimes produce highly unexpected
results, solutions that satisfy a formal user-defined criterion in a very different way
than the user expected or intended.
The field of evolvable hardware offers many illustrations of this phenomenon. In
this field, an evolutionary algorithm searches the space of hardware designs, testingthe fitness of each design by instantiating it physically on a rapidly reconfigurable
array or motherboard. The evolved designs often show remarkable economy. For
instance, one search discovered a frequency discrimination circuit that functioned
without a clock—a component normally considered necessary for this function. The
researchers estimated that the evolved circuit was between one and two orders of
magnitude smaller than what a human engineer would have required for the task.
The circuit exploited the physical properties of its components in unorthodox ways;
some active, necessary components were not even connected to the input or output
pins! These components instead participated via what would normally be considered
nuisance side effects, such as electromagnetic coupling or power-supply loading.
Another search process, tasked with creating an oscillator, was deprived of a
seemingly even more indispensible component, the capacitor. When the algorithm
presented its successful solution, the researchers examined it and at first concluded
that it “should not work.” Upon more careful examination, they discovered that the
algorithm had, MacGyver-like, reconfigured its sensor-less motherboard into a
makeshift radio receiver, using the printed circuit board tracks as an aerial to pick up
signals generated by personal computers that happened to be situated nearby in the
laboratory. The circuit amplified this signal to produce the desired oscillating
output. 16
In other experiments, evolutionary algorithms designed circuits that sensed
whether the motherboard was being monitored with an oscilloscope or whether a
soldering iron was connected to the lab’s common power supply. These examples
illustrate how an open-ended search process can repurpose the materials accessible
to it in order to devise completely unexpected sensory capabilities, by means that
conventional human design-thinking is poorly equipped to exploit or even account
for in retrospect.
The tendency for evolutionary search to “cheat” or find counterintuitive ways of
achieving a given end is on display in nature too, though it is perhaps less obvious to
us there because of our already being somewhat familiar with the look and feel of
biology, and thus being prone to regarding the actual outcomes of natural
evolutionary processes as normal—even if we would not have expected them ex
ante. But it is possible to set up experiments in artificial selection where one can see
the evolutionary process in action outside its familiar context. In such experiments,
researchers can create conditions that rarely obtain in nature, and observe the results.
For example, prior to the 1960s, it was apparently quite common for biologists to
maintain that predator populations restrict their own breeding in order to avoid
falling into a Malthusian trap. 17 Although individual selection would work against
such restraint, it was sometimes thought that group selection would overcome
individual incentives to exploit opportunities for reproduction and favor traits that
would benefit the group or population at large. Theoretical analysis and simulationstudies later showed that while group selection is possible in principle, it can
overcome strong individual selection only under very stringent conditions that may
rarely apply in nature. 18 But such conditions can be created in the laboratory. When
flour beetles (Tribolium castaneum) were bred for reduced population size, by
applying strong group selection, evolution did indeed lead to smaller populations. 19
However, the means by which this was accomplished included not only the “benign”
adaptations of reduced fecundity and extended developmental time that a human
naively anthropomorphizing evolutionary search might have expected, but also an
increase in cannibalism. 20
Instead of allowing agent-like purposive behavior to emerge spontaneously and
haphazardly from the implementation of powerful search processes (including
processes searching for internal work plans and processes directly searching for
solutions meeting some user-specified criterion), it may be better to create agents on
purpose. Endowing a superintelligence with an explicitly agent-like structure can be
a way of increasing predictability and transparency. A well-designed system, built
such that there is a clean separation between its values and its beliefs, would let us
predict something about the outcomes it would tend to produce. Even if we could not
foresee exactly which beliefs the system would acquire or which situations it would
find itself in, there would be a known place where we could inspect its final values
and thus the criteria that it will use in selecting its future actions and in evaluating
any potential plan.
Comparison
It may be useful to summarize the features of the different system castes we have
discussed (Table 11).
Table 11 Features of different system castes
Oracle
A question-answering system
• Boxing methods fully
applicable
• Domesticity fully
applicable
• Reduced need for AI to
understand human
intentions andVariations: Domain-limited oracles (e.g.
mathematics); output-restricted oracles (e.g.
only yes/no/undecided answers, or
probabilities); oracles that refuse to answer
questions if they predict the consequences of
answering would meet pre-specified “disaster
criteria”; multiple oracles for peer review
Genie
A command-executing system
Variations: Genies using different
“extrapolation distances” or degrees of
interests (compared to
genies and
sovereigns)
• Use of yes/no
questions can obviate
need for a metric of
the “usefulness” or
“informativeness” of
answers
• Source of great power
(might give operator a
decisive strategic
advantage)
• Limited protection
against foolish use by
operator
• Untrustworthy oracles
could be used to
provide answers that
are hard to find but
easy to verify
• Weak verification of
answers may be
possible through the
use of multiple
oracles
• Boxing methods
partially applicable
(for spatially limited
genies)
• Domesticity partially
applicable
• Genie could offer a
preview of salient
aspects of expected
outcomes
• Genie could
implement change in
stages, with
opportunity forfollowing the spirit rather than letter of the
review at each stage
command; domain-limited genies; genies-with- • Source of great power
preview; genies that refuse to obey commands
(might give operator a
if they predict the consequences of obeying
decisive strategic
would meet pre-specified “disaster criteria”
advantage)
• Limited protection
against foolish use by
operator
• Greater need for AI to
understand human
interests and
intentions (compared
to oracles)
• Boxing methods
inapplicable
• Most other capability
A system designed for open-ended
control methods also
Sovereign
autonomous operation
inapplicable (except,
possibly, social
integration or
anthropic capture)
• Domesticity mostly
inapplicable
• Great need for AI to
understand true
human interests and
intentions
• Necessity of getting it
right on the first try
(though, to a possibly
lesser extent, this is
true for all castes)
• Potentially a source of
Variations: Many possible motivation systems; great power for
sponsor, including
possibility of using preview and “sponsor
decisive strategic
ratification” (to be discussed in Chapter 13)
advantage
• Once activated, not
vulnerable to
hijacking by operator,
and might bedesigned with some
protection against
foolish use
• Can be used to
implement “veil of
ignorance” outcomes
(cf. Chapter 13)
Tool
A system not designed to exhibit goal-
directed behavior
• Boxing methods may
be applicable,
depending on the
implementation
• Powerful search
processes would
likely be involved in
the development and
operation of a
machine
superintelligence
• Powerful search to find
a solution meeting
some formal criterion
can produce solutions
that meet the criterion
in an unintended and
dangerous way
• Powerful search might
involve secondary,
internal search and
planning processes
that might find
dangerous ways of
executing the primary
search process
Further research would be needed to determine which type of system would be
safest. The answer might depend on the conditions under which the AI would be
deployed. The oracle caste is obviously attractive from a safety standpoint, since it
would allow both capability control methods and motivation selection methods to be
applied. It might thus seem to simply dominate the sovereign caste, which wouldonly allow motivation selection methods (except in scenarios in which the world is
believed to contain other powerful superintelligences, in which case social
integration or anthropic capture might apply). However, an oracle could place a lot
of power into the hands of its operator, who might be corrupted or might apply the
power unwisely, whereas a sovereign would offer some protection against these
hazards. The safety ranking is therefore not so easily determined.
A genie can be viewed as a compromise between an oracle and a sovereign—but
not necessarily a good compromise. In many ways, it would share the disadvantages
of both. The apparent safety of a tool-AI, meanwhile, may be illusory. In order for
tools to be versatile enough to substitute for superintelligent agents, they may need
to deploy extremely powerful internal search and planning processes. Agent-like
behaviors may arise from such processes as an unplanned consequence. In that case,
it would be better to design the system to be an agent in the first place, so that the
programmers can more easily see what criteria will end up determining the system’s
output.CHAPTER 11
Multipolar scenarios
We have seen (particularly in Chapter 8) how menacing a unipolar outcome
could be, one in which a single superintelligence obtains a decisive strategic
advantage and uses it to establish a singleton. In this chapter, we examine what
would happen in a multipolar outcome, a post-transition society with multiple
competing superintelligent agencies. Our interest in this class of scenarios is
twofold. First, as alluded to in Chapter 9, social integration might be thought to
offer a solution to the control problem. We already noted some limitations with
that approach, and this chapter paints a fuller picture. Second, even without
anybody setting out to create a multipolar condition as a way of handling the
control problem, such an outcome might occur anyway. So what might such an
outcome look like? The resulting competitive society is not necessarily
attractive, nor long-lasting.
In singleton scenarios, what happens post-transition depends almost entirely on the
values of the singleton. The outcome could thus be very good or very bad, depending
on what those values are. What the values are depends, in turn, on whether the
control problem was solved, and—to the degree to which it was solved—on the
goals of the project that created the singleton.
If one is interested in the outcome of singleton scenarios, therefore, one really
only has three sources of information: information about matters that cannot be
affected by the actions of the singleton (such as the laws of physics); information
about convergent instrumental values; and information that enables one to predict or
speculate about what final values the singleton will have.
In multipolar scenarios, an additional set of constraints comes into play,
constraints having to do with how agents interact. The social dynamics emerging
from such interactions can be studied using techniques from game theory,
economics, and evolution theory. Elements of political science and sociology are
also relevant insofar as they can be distilled and abstracted from some of the more
contingent features of human experience. Although it would be unrealistic to expect
these constraints to give us a precise picture of the post-transition world, they can
help us identify some salient possibilities and challenge some unfounded
assumptions.
We will begin by exploring an economic scenario characterized by a low level of
regulation, strong protection of property rights, and a moderately rapid introductionof inexpensive digital minds. 1 This type of model is most closely associated with the
American economist Robin Hanson, who has done pioneering work on the subject.
Later in this chapter, we will look at some evolutionary considerations and examine
the prospects of an initially multipolar post-transition world subsequently coalescing
into a singleton.
Of horses and men
General machine intelligence could serve as a substitute for human intelligence. Not
only could digital minds perform the intellectual work now done by humans, but,
once equipped with good actuators or robotic bodies, machines could also substitute
for human physical labor. Suppose that machine workers—which can be quickly
reproduced—become both cheaper and more capable than human workers in
virtually all jobs. What happens then?
Wages and unemployment
With cheaply copyable labor, market wages fall. The only place where humans
would remain competitive may be where customers have a basic preference for work
done by humans. Today, goods that have been handcrafted or produced by
indigenous people sometimes command a price premium. Future consumers might
similarly prefer human-made goods and human athletes, human artists, human
lovers, and human leaders to functionally indistinguishable or superior artificial
counterparts. It is unclear, however, just how widespread such preferences would be.
If machine-made alternatives were sufficiently superior, perhaps they would be
more highly prized.
One parameter that might be relevant to consumer choice is the inner life of the
worker providing a service or product. A concert audience, for instance, might like
to know that the performer is consciously experiencing the music and the venue.
Absent phenomenal experience, the musician could be regarded as merely a high-
powered jukebox, albeit one capable of creating the three-dimensional appearance of
a performer interacting naturally with the crowd. Machines might then be designed
to instantiate the same kinds of mental states that would be present in a human
performing the same task. Even with perfect replication of subjective experiences,
however, some people might simply prefer organic work. Such preferences could
also have ideological or religious roots. Just as many Muslims and Jews shun food
prepared in ways they classify as haram or treif, so there might be groups in the
future that eschew products whose manufacture involved unsanctioned use ofmachine intelligence.
What hinges on this? To the extent that cheap machine labor can substitute for
human labor, human jobs may disappear. Fears about automation and job loss are of
course not new. Concerns about technological unemployment have surfaced
periodically, at least since the Industrial Revolution; and quite a few professions
have in fact gone the way of the English weavers and textile artisans who in the early
nineteenth century united under the banner of the folkloric “General Ludd” to fight
against the introduction of mechanized looms. Nevertheless, although machinery
and technology have been substitutes for many particular types of human labor,
physical technology has on the whole been a complement to labor. Average human
wages around the world have been on a long-term upward trend, in large part
because of such complementarities. Yet what starts out as a complement to labor can
at a later stage become a substitute for labor. Horses were initially complemented by
carriages and ploughs, which greatly increased the horse’s productivity. Later,
horses were substituted for by automobiles and tractors. These later innovations
reduced the demand for equine labor and led to a population collapse. Could a
similar fate befall the human species?
The parallel to the story of the horse can be drawn out further if we ask why it is
that there are still horses around. One reason is that there are still a few niches in
which horses have functional advantages; for example, police work. But the main
reason is that humans happen to have peculiar preferences for the services that
horses can provide, including recreational horseback riding and racing. These
preferences can be compared to the preferences we hypothesized some humans
might have in the future, that certain goods and services be made by human hand.
Although suggestive, this analogy is, however, inexact, since there is still no
complete functional substitute for horses. If there were inexpensive mechanical
devices that ran on hay and had exactly the same shape, feel, smell, and behavior as
biological horses—perhaps even the same conscious experiences—then demand for
biological horses would probably decline further.
With a sufficient reduction in the demand for human labor, wages would fall
below the human subsistence level. The potential downside for human workers is
therefore extreme: not merely wage cuts, demotions, or the need for retraining, but
starvation and death. When horses became obsolete as a source of moveable power,
many were sold off to meatpackers to be processed into dog food, bone meal,
leather, and glue. These animals had no alternative employment through which to
earn their keep. In the United States, there were about 26 million horses in 1915. By
the early 1950s, 2 million remained. 2
Capital and welfareOne difference between humans and horses is that humans own capital. A stylized
empirical fact is that the total factor share of capital has for a long time remained
steady at approximately 30% (though with significant short-term fluctuations). 3 This
means that 30% of total global income is received as rent by owners of capital, the
remaining 70% being received as wages by workers. If we classify AI as capital,
then with the invention of machine intelligence that can fully substitute for human
work, wages would fall to the marginal cost of such machine-substitutes, which—
under the assumption that the machines are very efficient—would be very low, far
below human subsistence-level income. The income share received by labor would
then dwindle to practically nil. But this implies that the factor share of capital would
become nearly 100% of total world product. Since world GDP would soar following
an intelligence explosion (because of massive amounts of new labor-substituting
machines but also because of technological advances achieved by superintelligence,
and, later, acquisition of vast amounts of new land through space colonization), it
follows that the total income from capital would increase enormously. If humans
remain the owners of this capital, the total income received by the human population
would grow astronomically, despite the fact that in this scenario humans would no
longer receive any wage income.
The human species as a whole could thus become rich beyond the dreams of
Avarice. How would this income be distributed? To a first approximation, capital
income would be proportional to the amount of capital owned. Given the
astronomical amplification effect, even a tiny bit of pre-transition wealth would
balloon into a vast post-transition fortune. However, in the contemporary world,
many people have no wealth. This includes not only individuals who live in poverty
but also some people who earn a good income or who have high human capital but
have negative net worth. For example, in affluent Denmark and Sweden 30% of the
population report negative wealth—often young, middle-class people with few
tangible assets and credit card debt or student loans. 4 Even if savings could earn
extremely high interest, there would need to be some seed grain, some starting
capital, in order for the compounding to begin. 5
Nevertheless, even individuals who have no private wealth at the start of the
transition could become extremely rich. Those who participate in a pension scheme,
for instance, whether public or private, should be in a good position, provided the
scheme is at least partially funded. 6 Have-nots could also become rich through the
philanthropy of those who see their net worth skyrocket: because of the astronomical
size of the bonanza, even a very small fraction donated as alms would be a very
large sum in absolute terms.
It is also possible that riches could still be made through work, even at a post-
transition stage when machines are functionally superior to humans in all domains(as well as cheaper than even subsistence-level human labor). As noted earlier, this
could happen if there are niches in which human labor is preferred for aesthetic,
ideological, ethical, religious, or other non-pragmatic reasons. In a scenario in which
the wealth of human capital-holders increases dramatically, demand for such labor
could increase correspondingly. Newly minted trillionaires or quadrillionaires could
afford to pay a hefty premium for having some of their goods and services supplied
by an organic “fair-trade” labor force. The history of horses again offers a parallel.
After falling to 2 million in the early 1950s, the US horse population has undergone
a robust recovery: a recent census puts the number at just under 10 million head. 7
The rise is not due to new functional needs for horses in agriculture or
transportation; rather, economic growth has enabled more Americans to indulge a
fancy for equestrian recreation.
Another relevant difference between humans and horses, beside capital-
ownership, is that humans are capable of political mobilization. A human-run
government could use the taxation power of the state to redistribute private profits,
or raise revenue by selling appreciated state-owned assets, such as public land, and
use the proceeds to pension off its constituents. Again, because of the explosive
economic growth during and immediately after the transition, there would be vastly
more wealth sloshing around, making it relatively easy to fill the cups of all
unemployed citizens. It should be feasible even for a single country to provide every
human worldwide with a generous living wage at no greater proportional cost than
what many countries currently spend on foreign aid. 8
The Malthusian principle in a historical perspective
So far we have assumed a constant human population. This may be a reasonable
assumption for short timescales, since biology limits the rate of human
reproduction. Over longer timescales, however, the assumption is not necessarily
reasonable.
The human population has increased a thousandfold over the past 9,000 years. 9
The increase would have been much faster except for the fact that throughout most
of history and prehistory, the human population was bumping up against the limits
of the world economy. An approximately Malthusian condition prevailed, in which
most people received subsistence-level incomes that just barely allowed them to
survive and raise an average of two children to maturity. 10 There were temporary
and local reprieves: plagues, climate fluctuations, or warfare intermittently culled
the population and freed up land, enabling survivors to improve their nutritional
intake—and to bring up more children, until the ranks were replenished and the
Malthusian condition reinstituted. Also, thanks to social inequality, a thin elitestratum could enjoy consistently above-subsistence income (at the expense of
somewhat lowering the total size of the population that could be sustained). A sad
and dissonant thought: that in this Malthusian condition, the normal state of affairs
during most of our tenure on this planet, it was droughts, pestilence, massacres, and
inequality—in common estimation the worst foes of human welfare—that may have
been the greatest humanitarians: they alone enabling the average level of well-being
to occasionally bop up slightly above that of life at the very margin of subsistence.
Superimposed on local fluctuations, history shows a macro-pattern of initially
slow but accelerating economic growth, fueled by the accumulation of technological
innovations. The growing world economy brought with it a commensurate increase
in global population. (More precisely, a larger population itself appears to have
strongly accelerated the rate of growth, perhaps mainly by increasing humanity’s
collective intelligence. 11 ) Only since the Industrial Revolution, however, did
economic growth become so rapid that population growth failed to keep pace.
Average income thus started to rise, first in the early-industrializing countries of
Western Europe, subsequently in most of the world. Even in the poorest countries
today, average income substantially exceeds subsistence level, as reflected in the
fact that the populations of these countries are growing.
The poorest countries now have the fastest population growth, as they have yet to
complete the “demographic transition” to the low-fertility regime that has taken
hold in more developed societies. Demographers project that the world population
will rise to about 9 billion by mid-century, and that it might thereafter plateau or
decline as the poorer countries join the developed world in this low-fertility
regime. 12 Many rich countries already have fertility rates that are below replacement
level; in some cases, far below. 13
Yet there are reasons, if we take a longer view and assume a state of unchanging
technology and continued prosperity, to expect a return to the historically and
ecologically normal condition of a world population that butts up against the limits
of what our niche can support. If this seems counterintuitive in light of the negative
relationship between wealth and fertility that we are currently observing on the
global scale, we must remind ourselves that this modern age is a brief slice of
history and very much an aberration. Human behavior has not yet adapted to
contemporary conditions. Not only do we fail to take advantage of obvious ways to
increase our inclusive fitness (such as by becoming sperm or egg donors) but we
actively sabotage our fertility by using birth control. In the environment of
evolutionary adaptedness, a healthy sex drive may have been enough to make an
individual act in ways that maximized her reproductive potential; in the modern
environment, however, there would be a huge selective advantage to having a more
direct desire for being the biological parent to the largest possible number of
children. Such a desire is currently being selected for, as are other traits that increaseour propensity to reproduce. Cultural adaptation, however, might steal a march on
biological evolution. Some communities, such those of the Hutterites or the
adherents of the Quiverfull evangelical movement, have natalist cultures that
encourage large families, and they are consequently undergoing rapid expansion.
Population growth and investment
If we imagine current socioeconomic conditions magically frozen in their current
shape, the future would be dominated by cultural or ethnic groups that sustain high
levels of fertility. If most people had preferences that were fitness-maximizing in
the contemporary environment, the population could easily double in each
generation. Absent population control policies—which would have to become
steadily more rigorous and effective to counteract the evolution of stronger
preferences to circumvent them—the world population would then continue to grow
exponentially until some constraint, such as land scarcity or depletion of easy
opportunities for important innovation, made it impossible for the economy to keep
pace: at which point, average income would start to decline until it reached the level
where crushing poverty prevents most people from raising much more than two
children to maturity. Thus the Malthusian principle would reassert itself, like a
dread slave master, bringing our escapade into the dreamland of abundance to an
end, and leading us back to the quarry in chains, there to resume the weary struggle
for subsistence.
This longer-term outlook could be telescoped into a more imminent prospect by
the intelligence explosion. Since software is copyable, a population of emulations or
AIs could double rapidly—over the course of minutes rather than decades or
centuries—soon exhausting all available hardware.
Private property might offer partial protection against the emergence of a
universal Malthusian condition. Consider a simple model in which clans (or closed
communities, or states) start out with varying amounts of property and
independently adopt different policies about reproduction and investment. Some
clans discount the future steeply and spend down their endowment, whereafter their
impoverished members join the global proletariat (or die, if they cannot support
themselves through their labor). Other clans invest some of their resources but adopt
a policy of unlimited reproduction: such clans grow more populous until they reach
an internal Malthusian condition in which their members are so poor that they die at
almost the same rate as they reproduce, at which point the clan’s population growth
slows to equal the growth of its resources. Yet other clans might restrict their
fertility to below the rate of growth of their capital: such clans could slowly
increment their numbers while their members also grow richer per capita.
If wealth is redistributed from the wealthy clans to the members of the rapidlyreproducing or rapidly discounting clans (whose children, copies, or offshoots,
through no fault of their own, were launched into the world with insufficient capital
to survive and thrive) then a universal Malthusian condition would be more closely
approximated. In the limiting case, all members of all clans would receive
subsistence level income and everybody would be equal in their poverty.
If property is not redistributed, prudent clans might hold on to a certain amount of
capital, and it is possible that their wealth could grow in absolute terms. It is,
however, unclear whether humans could earn as high rates of return on their capital
as machine intelligences could earn on theirs, because there may be synergies
between labor and capital such that an single agent who can supply both (e.g. an
entrepreneur or investor who is both skilled and wealthy) can attain a private rate of
return on her capital exceeding the market rate obtainable by agents who possess
financial but not cognitive resources. Humans, being less skilled than machine
intelligences, may therefore grow their capital more slowly—unless, of course, the
control problem had been completely solved, in which case the human rate of return
would equal the machine rate of return, since a human principal could task a
machine agent to manage her savings, and could do so costlessly and without
conflicts of interest: but otherwise, in this scenario, the fraction of the economy
owned by machines would asymptotically approach one hundred percent.
A scenario in which the fraction of the economy that is owned by machines
asymptotically approaches one hundred percent is not necessarily one in which the
size of the human slice declines. If the economy grows at a sufficient clip, then even
a relatively diminishing fraction of it may still be increasing in its absolute size.
This may sound like modestly good news for humankind: in a multipolar scenario in
which property rights are protected—even if we completely fail to solve the control
problem—the total amount of wealth owned by human beings could increase. Of
course, this effect would not take care of the problem of population growth in the
human population pulling down per capita income to subsistence level, nor the
problem of humans who ruin themselves because they discount the future.
In the long run, the economy would become increasingly dominated by those
clans that have the highest savings rates—misers who own half the city and live
under a bridge. Only in the fullness of time, when there are no more opportunities
for investment, would the maximally prosperous misers start drawing down their
savings. 14 However, if there is less than perfect protection for property rights—for
example if the more efficient machines on net succeed, by hook or by crook, in
transferring wealth from humans to themselves—then human capitalists may need to
spend down their capital much sooner, before it gets depleted by such transfers (or
the ongoing costs incurred in securing their wealth against such transfers). If these
developments take place on digital rather than biological timescales, then the glacial
humans might find themselves expropriated before they could say Jack Robinson. 15Life in an algorithmic economy
Life for biological humans in a post-transition Malthusian state need not resemble
any of the historical states of man (as hunter–gatherer, farmer, or office worker).
Instead, the majority of humans in this scenario might be idle rentiers who eke out a
marginal living on their savings. 16 They would be very poor, yet derive what little
income they have from savings or state subsidies. They would live in a world with
extremely advanced technology, including not only superintelligent machines but
also anti-aging medicine, virtual reality, and various enhancement technologies and
pleasure drugs: yet these might be generally unaffordable. Perhaps instead of using
enhancement medicine, they would take drugs to stunt their growth and slow their
metabolism in order to reduce their cost of living (fast-burners being unable to
survive at the gradually declining subsistence income). As our numbers increase and
our average income declines further, we might degenerate into whatever minimal
structure still qualifies to receive a pension—perhaps minimally conscious brains in
vats, oxygenized and nourished by machines, slowly saving up enough money to
reproduce by having a robot technician develop a clone of them. 17
Further frugality could be achieved by means of uploading, since a physically
optimized computing substrate, devised by advanced superintelligence, would be
more efficient than a biological brain. The migration into the digital realm might be
stemmed, however, if emulations were regarded as non-humans or non-citizens
ineligible to receive pensions or to hold tax-exempt savings accounts. In that case, a
niche for biological humans might remain open, alongside a perhaps vastly larger
population of emulations or artificial intelligences.
So far we have focused on the fate of the humans, who may be supported by
savings, subsidies, or wage income deriving from other humans who prefer to hire
humans. Let us now turn our attention to some of the entities that we have so far
classified as “capital”: machines that may be owned by human beings, that are
constructed and operated for the sake of the functional tasks they perform, and that
are capable of substituting for human labor in a very wide range of jobs. What may
the situation be like for these workhorses of the new economy?
If these machines were mere automata, simple devices like a steam engine or the
mechanism in a clock, then no further comment would be needed: there would be a
large amount of such capital in a post-transition economy, but it would seem not to
matter to anybody how things turn out for pieces of insentient equipment. However,
if the machines have conscious minds—if they are constructed in such a way that
their operation is associated with phenomenal awareness (or if they for some other
reason are ascribed moral status)—then it becomes important to consider the overall
outcome in terms of how it would affect these machine minds. The welfare of theworking machine minds could even appear to be the most important aspect of the
outcome, since they may be numerically dominant.
Voluntary slavery, casual death
A salient initial question is whether these working machine minds are owned as
capital (slaves) or are hired as free wage laborers. On closer inspection however, it
become doubtful that anything really hinges on the issue. There are two reasons for
this. First, if a free worker in a Malthusian state gets paid a subsistence-level wage,
he will have no disposable income left after he has paid for food and other
necessities. If the worker is instead a slave, his owner will pay for his maintenance
and again he will have no disposable income. In either case, the worker gets the
necessities and nothing more. Second, suppose that the free laborer were somehow
in a position to command an above-subsistence-level income (perhaps because of
favorable regulation). How will he spend the surplus? Investors would find it most
profitable to create workers who would be “voluntary slaves”—who would willingly
work for subsistence-level wages. Investors may create such workers by copying
those workers who are compliant. With appropriate selection (and perhaps some
modification to the code) investors might be able to create workers who not only
prefer to volunteer their labor but who would also choose to donate back to their
owners any surplus income they might happen to receive. Giving money to the
worker would then be but a roundabout way of giving money to the owner or
employer, even if the worker were a free agent with full legal rights.
Perhaps it will be objected that it would be difficult to design a machine so that it
wants to volunteer for any job assigned to it or so that it wants to donate its wages to
its owner. Emulations, in particular, might be imagined to have more typically
human desires. But note that even if the original control problem is difficult, we are
here considering a condition after the transition, a time when methods for
motivation selection have presumably been perfected. In the case of emulations, one
might get quite far simply by selecting from the pre-existing range of human
characters; and we have described several other motivation selection methods. The
control problem may also in some ways be simplified by the current assumption that
the new machine intelligence enters into a stable socioeconomic matrix that is
already populated with other law-abiding superintelligent agents.
Let us, then, consider the plight of the working-class machine, whether it be
operating as a slave or a free agent. We focus first on emulations, the easiest case to
imagine.
Bringing a new biological human worker into the world takes anywhere between
fifteen and thirty years, depending on how much expertise and experience is
required. During this time the new person must be fed, housed, nurtured, andeducated—at great expense. By contrast, spawning a new copy of a digital worker is
as easy as loading a new program into working memory. Life thus becomes cheap. A
business could continuously adapt its workforce to fit demands by spawning new
copies—and terminating copies that are no longer needed, to free up computer
resources. This could lead to an extremely high death rate among digital workers.
Many might live for only one subjective day.
There are reasons other than fluctuations in demand why employers or owners of
emulations might want to “kill” or “end” their workers frequently. 18 If an emulation
mind, like a biological mind, requires periods of rest and sleep in order to function,
it might be cheaper to erase a fatigued emulation at the end of a day and replace it
with a stored state of a fresh and rested emulation. As this procedure would cause
retrograde amnesia for everything that had been learned during that day, emulations
performing tasks requiring long cognitive threads would be spared such frequent
erasure. It would be difficult, for example, to write a book if each morning when one
sat down at one’s desk, one had no memory of what one had done before. But other
jobs could be performed adequately by agents that are frequently recycled: a shop
assistant or a customer service agent, once trained, may only need to remember new
information for twenty minutes.
Since recycling emulations would prevent memory and skill formation, some
emulations may be placed on a special learning track where they would run
continuously, including for rest and sleep, even in jobs that do not strictly require
long cognitive threads. For example, some customer service agents might run for
many years in optimized learning environments, assisted by coaches and
performance evaluators. The best of these trainees would then be used like studs,
serving as templates from which millions of fresh copies are stamped out each day.
Great effort would be poured into improving the performance of such worker
templates, because even a small increment in productivity would yield great
economic value when applied in millions of copies.
In parallel with efforts to train worker-templates for particular jobs, intense
efforts would also be made to improve the underlying emulation technology.
Advances here would be even more valuable than advances in individual worker-
templates, since general technology improvements could be applied to all emulation
workers (and potentially to non-worker emulations also) rather than only to those in
a particular occupation. Enormous resources would be devoted to finding
computational shortcuts allowing for more efficient implementations of existing
emulations, and also into developing neuromorphic and entirely synthetic AI
architectures. This research would probably mostly be done by emulations running
on very fast hardware. Depending on the price of computer power, millions, billions,
or trillions of emulations of the sharpest human research minds (or enhanced
versions thereof) may be working around the clock on advancing the frontier ofmachine intelligence; and some of these may be operating orders of magnitude
faster than biological brains. 19 This is a good reason for thinking that the era of
human-like emulations would be brief—a very brief interlude in sidereal time—and
that it would soon give way to an era of greatly superior artificial intelligence.
We have already encountered several reasons why employers of emulation
workers may periodically cull their herds: fluctuations in demand for different kinds
of laborers, cost savings of not having to emulate rest and sleep time, and the
introduction of new and improved templates. Security concerns might furnish
another reason. To prevent workers from developing subversive plans and
conspiracies, emulations in some sensitive positions might be run only for limited
periods, with frequent resets to an earlier stored ready-state. 20
These ready-states to which emulations would be reset would be carefully
prepared and vetted. A typical short-lived emulation might wake up in a well-rested
mental state that is optimized for loyalty and productivity. He remembers having
graduated top of his class after many (subjective) years of intense training and
selection, then having enjoyed a restorative holiday and a good night’s sleep, then
having listened to a rousing motivational speech and stirring music, and now he is
champing at the bit to finally get to work and to do his utmost for his employer. He
is not overly troubled by thoughts of his imminent death at the end of the working
day. Emulations with death neuroses or other hang-ups are less productive and
would not have been selected. 21
Would maximally efficient work be fun?
One important variable in assessing the desirability of a hypothetical condition like
this is the hedonic state of the average emulation. 22 Would a typical emulation
worker be suffering or would he be enjoying the experience of working hard on the
task at hand?
We must resist the temptation to project our own sentiments onto the imaginary
emulation worker. The question is not whether you would feel happy if you had to
work constantly and never again spend time with your loved ones—a terrible fate,
most would agree.
It is moderately more relevant to consider the current human average hedonic
experience during working hours. Worldwide studies asking respondents how happy
they are find that most rate themselves as “quite happy” or “very happy” (averaging
3.1 on a scale from 1 to 4). 23 Studies on average affect, asking respondents how
frequently they have recently experienced various positive or negative affective
states, tend to get a similar result (producing a net affect of about 0.52 on a scale
from –1 to 1). There is a modest positive effect of a country’s per capita income onaverage subjective well-being. 24 However, it is hazardous to extrapolate from these
findings to the hedonic state of future emulation workers. One reason that could be
given for this is that their condition would be so different: on the one hand, they
might be working much harder; on the other hand, they might be free from diseases,
aches, hunger, noxious odors, and so forth. Yet such considerations largely miss the
mark. The much more important consideration here is that hedonic tone would be
easy to adjust through the digital equivalent of drugs or neurosurgery. This means
that it would be a mistake to infer the hedonic state of future emulations from the
external conditions of their lives by imagining how we ourselves and other people
like us would feel in those circumstances. Hedonic state would be a matter of choice.
In the model we are currently considering, the choice would be made by capital-
owners seeking to maximize returns on their investment in emulation-workers.
Consequently, the question of how happy emulations would feel boils down to the
question of which hedonic states would be most productive (in the various jobs that
emulations would be employed to do).
Here, again, one might seek to draw an inference from observations about human
happiness. If it is the case, across most times, places, and occupations, that people
are typically at least moderately happy, this would create some presumption in favor
of the same holding in a post-transition scenario like the one we are considering. To
be clear, the argument in this case would not be that human minds have a
predisposition towards happiness so they would probably find satisfaction under
these novel conditions; but rather that a certain average level of happiness has
proved adaptive for human minds in the past so maybe a similar level of happiness
will prove adaptive for human-like minds in the future. Yet this formulation also
reveals the weakness of the inference: to wit, that the mental dispositions that were
adaptive for hunter–gatherer hominids roaming the African savanna may not
necessarily be adaptive for modified emulations living in post-transition virtual
realities. We can certainly hope that the future emulation-workers would be as happy
as, or happier than, typical workers were in human history; but we have yet to see
any compelling reason for supposing it would be so (in the laissez-faire multipolar
scenario currently under examination).
Consider the possibility that the reason happiness is prevalent among humans (to
whatever limited extent it is prevalent) is that cheerful mood served a signaling
function in the environment of evolutionary adaptedness. Conveying the impression
to other members of the social group of being in flourishing condition—in good
health, in good standing with one’s peers, and in confident expectation of continued
good fortune—may have boosted an individual’s popularity. A bias toward
cheerfulness could thus have been selected for, with the result that human
neurochemistry is now biased toward positive affect compared to what would have
been maximally efficient according to simpler materialistic criteria. If this were thecase, then the future of joie de vivre might depend on cheer retaining its social
signaling function unaltered in the post-transition world: an issue to which we will
return shortly.
What if glad souls dissipate more energy than glum ones? Perhaps the joyful are
more prone to creative leaps and flights of fancy—behaviors that future employers
might disprize in most of their workers. Perhaps a sullen or anxious fixation on
simply getting on with the job without making mistakes will be the productivity-
maximizing attitude in most lines of work. The claim here is not that this is so, but
that we do not know that it is not so. Yet we should consider just how bad it could be
if some such pessimistic hypothesis about a future Malthusian state turned out to be
true: not only because of the opportunity cost of having failed to create something
better—which would be enormous—but also because the state could be bad in itself,
possibly far worse than the original Malthusian state.
We seldom put forth full effort. When we do, it is sometimes painful. Imagine
running on a treadmill at a steep incline—heart pounding, muscles aching, lungs
gasping for air. A glance at the timer: your next break, which will also be your death,
is due in 49 years, 3 months, 20 days, 4 hours, 56 minutes, and 12 seconds. You wish
you had not been born.
Again the claim is not that this is how it would be, but that we do not know that it
is not. One could certainly make a more optimistic case. For example, there is no
obvious reason that emulations would need to suffer bodily injury and sickness: the
elimination of physical wretchedness would be a great improvement over the present
state of affairs. Furthermore, since such stuff as virtual reality is made of can be
fairly cheap, emulations may work in sumptuous surroundings—in splendid
mountaintop palaces, on terraces set in a budding spring forest, or on the beaches of
an azure lagoon—with just the right illumination, temperature, scenery and décor;
free from annoying fumes, noises, drafts, and buzzing insects; dressed in
comfortable clothing, feeling clean and focused, and well nourished. More
significantly, if—as seems perfectly possible—the optimum human mental state for
productivity in most jobs is one of joyful eagerness, then the era of the emulation
economy could be quite paradisiacal.
There would, in any case, be a great option value in arranging matters in such a
manner that somebody or something could intervene to set things right if the default
trajectory should happen to veer toward dystopia. It could also be desirable to have
some sort of escape hatch that would permit bailout into death and oblivion if the
quality of life were to sink permanently below the level at which annihilation
becomes preferable to continued existence.
Unconscious outsourcers?In the longer run, as the emulation era gives way to an artificial intelligence era (or
if machine intelligence is attained directly via AI without a preceding whole brain
emulation stage) pain and pleasure might possibly disappear entirely in a multipolar
outcome, since a hedonic reward mechanism may not be the most effective
motivation system for an complex artificial agent (one that, unlike the human mind,
is not burdened with the legacy of animal wetware). Perhaps a more advanced
motivation system would be based on an explicit representation of a utility function
or some other architecture that has no exact functional analogs to pleasure and pain.
A related but slightly more radical multipolar outcome—one that could involve
the elimination of almost all value from the future—is that the universal proletariat
would not even be conscious. This possibility is most salient with respect to AI,
which might be structured very differently than human intelligence. But even if
machine intelligence were initially achieved though whole brain emulation, resulting
in conscious digital minds, the competitive forces unleashed in a post-transition
economy could easily lead to the emergence of progressively less neuromorphic
forms of machine intelligence, either because synthetic AI is created de novo or
because the emulations would, through successive modifications and enhancements,
increasingly depart their original human form.
Consider a scenario in which after emulation technology has been developed,
continued progress in neuroscience and computer science (expedited by the presence
of digital minds to serve as both researchers and test subjects) makes it possible to
isolate individual cognitive modules in an emulation, and to hook them up to
modules isolated from other emulations. A period of training and adjustment may be
required before different modules can collaborate effectively; but modules that
conform to common standards could more quickly interface with other standard
modules. This would make standardized modules more productive, and create
pressure for more standardization.
Emulations can now begin to outsource increasing portions of their functionality.
Why learn arithmetic when you can send your numerical reasoning task to Gauss-
Modules, Inc.? Why be articulate when you can hire Coleridge Conversations to put
your thoughts into words? Why make decisions about your personal life when there
are certified executive modules that can scan your goal system and manage your
resources to achieve your goals better than if you tried to do it yourself? Some
emulations may prefer to retain most of their functionality and handle tasks
themselves that could be done more efficiently by others. Those emulations would
be like hobbyists who enjoy growing their own vegetables or knitting their own
cardigans. Such hobbyist emulations would be less efficient; and if there is a net
flow of resources from less to more efficient participants of the economy, the
hobbyists would eventually lose out.
The bouillon cubes of discrete human-like intellects thus melt into an algorithmic
soup.It is conceivable that optimal efficiency would be attained by grouping
capabilities in aggregates that roughly match the cognitive architecture of a human
mind. It might be the case, for example, that a mathematics module must be tailored
to a language module, and that both must be tailored to the executive module, in
order for the three to work together. Cognitive outsourcing would then be almost
entirely unworkable. But in the absence of any compelling reason for being
confident that this is so, we must countenance the possibility that human-like
cognitive architectures are optimal only within the constraints of human neurology
(or not at all). When it becomes possible to build architectures that could not be
implemented well on biological neural networks, new design space opens up; and the
global optima in this extended space need not resemble familiar types of mentality.
Human-like cognitive organizations would then lack a niche in a competitive post-
transition economy or ecosystem. 25
There might be niches for complexes that are either less complex (such as
individual modules), more complex (such as vast clusters of modules), or of similar
complexity to human minds but with radically different architectures. Would these
complexes have any intrinsic value? Should we welcome a world in which such alien
complexes have replaced human complexes?
The answer may depend on the specific nature of those alien complexes. The
present world has many levels of organization. Some highly complex entities, such
as multinational corporations and nation states, contain human beings as
constituents; yet we usually assign these high-level complexes only instrumental
value. Corporations and states do not (it is generally assumed) have consciousness,
over and above the consciousness of the people who constitute them: they cannot
feel phenomenal pain or pleasure or experience any qualia. We value them to the
extent that they serve human needs, and when they cease to do so we “kill” them
without compunction. There are also lower-level entities, and those, too, are usually
denied moral status. We see no harm in erasing an app from a smartphone, and we
do not think that a neurosurgeon is wronging anyone when she extirpates a
malfunctioning module from an epileptic brain. As for exotically organized
complexes of a level similar to that of the human brain, most of us would perhaps
judge them to have moral significance only if we thought they had a capacity or
potential for conscious experience. 26
We could thus imagine, as an extreme case, a technologically highly advanced
society, containing many complex structures, some of them far more intricate and
intelligent than anything that exists on the planet today—a society which
nevertheless lacks any type of being that is conscious or whose welfare has moral
significance. In a sense, this would be an uninhabited society. It would be a society
of economic miracles and technological awesomeness, with nobody there to benefit.
A Disneyland without children.Evolution is not necessarily up
The word “evolution” is often used as a synonym of “progress,” perhaps reflecting a
common uncritical image of evolution as a force for good. A misplaced faith in the
inherent beneficence of the evolutionary process can get in the way of a fair
evaluation of the desirability of a multipolar outcome in which the future of
intelligent life is determined by competitive dynamics. Any such evaluation must
rest on some (at least implicit) opinion about the probability distribution of different
phenotypes turning out to be adaptive in a post-transition digital life soup. It would
be difficult in the best of circumstances to extract a clear and correct answer from
the unavoidable goo of uncertainty that pervades these matters: more so, if we
superadd a layer of Panglossian muck.
A possible source for faith in freewheeling evolution is the apparent upward
directionality exhibited by the evolutionary process in the past. Starting from
rudimentary replicators, evolution produced increasingly “advanced” organisms,
including creatures with minds, consciousness, language, and reason. More recently,
cultural and technological processes, which bear some loose similarities to
biological evolution, have enabled humans to develop at an accelerated pace. On a
geological as well as a historical timescale, the big picture seems to show an
overarching trend toward increasing levels of complexity, knowledge,
consciousness, and coordinated goal-directed organization: a trend which, not to put
too fine a point on it, one might label “progress.” 27
The image of evolution as a process that reliably produces benign effects is
difficult to reconcile with the enormous suffering that we see in both the human and
the natural world. Those who cherish evolution’s achievements may do so more
from an aesthetic than an ethical perspective. Yet the pertinent question is not what
kind of future it would be fascinating to read about in a science fiction novel or to
see depicted in a nature documentary, but what kind of future it would be good to
live in: two very different matters.
Furthermore, we have no reason to think that whatever progress there has been
was in any way inevitable. Much might have been luck. This objection derives
support from the fact that an observation selection effect filters the evidence we can
have about the success of our own evolutionary development. 28 Suppose that on
99.9999% of all planets where life emerged it went extinct before developing to the
point where intelligent observers could begin to ponder their origin. What should we
expect to observe if that were the case? Arguably, we should expect to observe
something like what we do in fact observe. The hypothesis that the odds of
intelligent life evolving on a given planet are low does not predict that we should
find ourselves on a planet where life went extinct at an early stage; rather, it maypredict that we should find ourselves on a planet where intelligent life evolved, even
if such planets constitute a very small fraction of all planets where primitive life
evolved. Life’s long track record on Earth may therefore offer scant support to the
claim that there was a high chance—let alone anything approaching inevitability—
involved in the rise of higher organisms on our planet. 29
Thirdly, even if present conditions had been idyllic, and even if they could have
been shown to have arisen ineluctably from some generic primordial state, there
would still be no guarantee that the melioristic trend is set to continue into the
indefinite future. This holds even if we disregard the possibility of a cataclysmic
extinction event and indeed even if we assume that evolutionary developments will
continue to produce systems of increasing complexity.
We suggested earlier that machine intelligence workers selected for maximum
productivity would be working extremely hard and that it is unknown how happy
such workers would be. We also raised the possibility that the fittest life forms
within a competitive future digital life soup might not even be conscious. Short of a
complete loss of pleasure, or of consciousness, there could be a wasting away of
other qualities that many would regard as indispensible for a good life. Humans
value music, humor, romance, art, play, dance, conversation, philosophy, literature,
adventure, discovery, food and drink, friendship, parenting, sport, nature, tradition,
and spirituality, among many other things. There is no guarantee that any of these
would remain adaptive. Perhaps what will maximize fitness will be nothing but
nonstop high-intensity drudgery, work of a drab and repetitive nature, destitute of
ludic frisson, aimed only at improving the eighth decimal place of some economic
output measure. The phenotypes selected would then have lives lacking in the
aforesaid qualities, and depending on one’s axiology the result might strike one as
either abhorrent, worthless, or merely impoverished, but at any rate a far cry from a
utopia one would feel worthy of one’s commendation.
It might be wondered how such a bleak picture could be consistent with the fact
that we do now indulge in music, humor, romance, art, etc. If these behaviors are
really so “wasteful,” then how come they have been tolerated and indeed promoted
by the evolutionary processes that shaped our species? That modern man is in an
evolutionary disequilibrium does not account for this; for our Pleistocene forebears,
too, engaged in most of these dissipations. Many of the behaviors in question are not
even unique to Homo sapiens. Flamboyant display is found in a wide variety of
contexts, from sexual selection in the animal kingdom to prestige contests among
nation states. 30
Although a full evolutionary explanation for each of these behaviors is beyond the
scope of the present inquiry, we can note that some of them serve functions that may
not be as relevant in a machine intelligence context. Play, for example, which occurs
only in some species and predominantly among juveniles, is mainly a way for theyoung animal to learn skills that it will need later in life. When emulations can be
created as adults, already in possession of a mature repertoire of skills, or when
knowledge and techniques acquired by one AI can be directly ported into another AI,
the need for playful behavior might become less widespread.
Many of the other examples of humanistic behaviors may have evolved as hard-
to-fake signals of qualities that are difficult to observe directly, such as bodily or
mental resilience, social status, quality of allies, ability and willingness to prevail in
a fight, or possession of resources. The peacock’s tail is the classic instance: only fit
peacocks can afford to sprout truly extravagant plumage, and peahens have evolved
to find it attractive. No less than morphological traits, behavioral traits too can
signal genetic fitness or other socially relevant attributes. 31
Given that flamboyant display is so common among both humans and other
species, one might consider whether it would not also be part of the repertoire of
technologically more advanced life forms. Even if there were to be no narrowly
instrumental use for playfulness or musicality or even for consciousness in the
future ecology of intelligent information processing, might not these traits
nonetheless confer some evolutionary advantage to their possessors by virtue of
being reliable signals of other adaptive qualities?
While the possibility of a pre-established harmony between what is valuable to us
and what would be adaptive in a future digital ecology is hard to rule out, there are
reasons for skepticism. Consider, first, that many of the costly displays we find in
nature are linked to sexual selection. 32 Reproduction among technologically mature
life forms, in contrast, may be predominantly or exclusively asexual.
Second, technologically advanced agents might have available new means of
reliably communicating information about themselves, means that do not rely on
costly display. Even today, when professional lenders assess creditworthiness they
tend to rely more on documentary evidence, such as ownership certificates and bank
statements, than on costly displays, such as designer suits and Rolex watches. In the
future, it might be possible to employ auditing firms that verify through detailed
examination of behavioral track records, testing in simulated environments, or direct
inspection of source code, that a client agent possesses a claimed attribute. Signaling
one’s qualities by agreeing to such auditing might be more efficient than signaling
via flamboyant display. Such a professionally mediated signal would still be costly
to fake—this being the essential feature that makes the signal reliable—but it could
be much cheaper to transmit when truthful than it would be to communicate an
equivalent signal flamboyantly.
Third, not all possible costly displays are intrinsically valuable or socially
desirable. Many are simply wasteful. The Kwakiutl potlatch ceremonies, a form of
status competition between rival chiefs, involved the public destruction of vast
amounts of accumulated wealth. 33 Record-breaking skyscrapers, megayachts, andmoon rockets may be viewed as contemporary analogs. While activities like music
and humor could plausibly be claimed to enhance the intrinsic quality of human life,
it is doubtful that a similar claim could be sustained with regard to the costly pursuit
of fashion accessories and other consumerist status symbols. Worse, costly display
can be outright harmful, as in macho posturing leading to gang violence or military
bravado. Even if future intelligent life forms would use costly signaling, therefore, it
is an open question whether the signal would be of a valuable sort—whether it would
be like the rapturous melody of a nightingale or instead like the toad’s monosyllabic
croak (or the incessant barking of a rabid dog).
Post-transition formation of a singleton?
Even if the immediate outcome of the transition to machine intelligence were
multipolar, the possibility would remain of a singleton developing later. Such a
development would continue an apparent long-term trend toward larger scales of
political integration, taking it to its natural conclusion. 34 How might this occur?
A second transition
On way in which an initially multipolar outcome could converge into a singleton
post-transition is if there is, after the initial transition, a second technological
transition big enough and steep enough to give a decisive strategic advantage to one
of the remaining powers: a power which might then seize the opportunity to
establish a singleton. Such a hypothetical second transition might be occasioned by a
breakthrough to a higher level of superintelligence. For instance, if the first wave of
machine superintelligence is emulation-based, then a second surge might result
when the emulations now doing the research succeed in developing effective self-
improving artificial intelligence. 35 (Alternatively, a second transition might be
triggered by a breakthrough in nanotechnology or some other military or general-
purpose technology as yet unenvisaged.)
The pace of development after the initial transition would be extremely rapid.
Even a short gap between the leading power and its closest competitor could
therefore plausibly result in a decisive strategic advantage for the leading power
during a second transition. Suppose, for example, that two projects enter the first
transition only a few days apart, and that the takeoff is slow enough that this gap
does not give the leading project a decisive strategic advantage at any point during
the takeoff. The two projects both emerge as superintelligent powers, though one of
them remains a few days ahead of the other. But developments are now occurring onthe research timescales characteristic of machine superintelligence—perhaps
thousands or millions of times faster than research conducted on a biological human
timescale. Development of the second-transition technology might therefore be
completed in days, hours, or minutes. Even though the frontrunner’s lead is a mere
few days, a breakthrough could thus catapult it into a decisive strategic advantage.
Note, however, that if technological diffusion (via espionage or other channels)
speeds up as much as technological development, then this effect would be negated.
What would remain relevant would be the steepness of the second transition, that is,
the speed at which it would unfold relative to the general speed of events in the
period after the first transition. (In this sense, the faster things are happening after
the first transition, the less steep the second transition would tend to be.)
One might also speculate that a decisive strategic advantage would be more likely
to be actually used to establish a singleton if it arises during a second (or
subsequent) transition. After the first transition, decision makers would either be
superintelligent or have access to advice from a superintelligence, which would
clarify the implications of available strategic options. Furthermore, the situation
after the first transition might be one in which a preemptive move against potential
competitors would be less dangerous for the aggressor. If the decision-making minds
after the first transition are digital, they could be copied and thereby rendered less
vulnerable to a counterattack. Even if a defender had the ability to kill nine-tenths of
the aggressor’s population in a retaliatory strike, this would scarcely offer much
deterrence if the deceased could be immediately resurrected from redundant
backups. Devastation of infrastructure (which can be rebuilt) might also be tolerable
to digital minds with effectively unlimited lifespans, who might be planning to
maximize their resources and influence on a cosmological timescale.
Superorganisms and scale economies
The size of coordinated human aggregates, such as firms or nations, is influenced by
various parameters—technological, military, financial, and cultural—that can vary
from one historical epoch to another. A machine intelligence revolution would entail
profound changes in many these parameters. Perhaps these changes would facilitate
the rise of a singleton. Although we cannot, without looking in detail at what these
prospective changes are, exclude the opposite possibility—that the changes would
facilitate fragmentation rather than unification—we can nevertheless note that the
increased variance or uncertainty that we confront here may itself be a ground for
giving greater credence to the potential emergence of a singleton than we would
otherwise do. A machine intelligence revolution might, so to speak, stir things up—
might reshuffle the deck to make possible geopolitical realignments that seemed
perhaps otherwise not to have been in the cards.A comprehensive analysis of all the factors that may influence the scale of
political integration would take us far beyond the scope of this book: a review of the
relevant political science and economics literature could itself easily fill an entire
volume. We must confine ourselves to making brief allusion to a couple of factors,
aspects of the digitization of agents that may make it easier to centralize control.
Carl Shulman has argued that in a population of emulations, selection pressures
would favor the emergence of “superorganisms,” groups of emulations ready to
sacrifice themselves for the good of their clan. 36 Superorganisms would be spared
the agency problems that beset organizations whose members pursue their own self-
interest. Like the cells in our bodies, or the individual animals in a colony of
eusocial insects, emulations that were wholly altruistic toward their copy-siblings
would cooperate with one another even in the absence of elaborate incentive
schemes.
Superorganisms would have a particularly strong advantage if nonconsensual
deletion (or indefinite suspension) of individual emulations is disallowed. Firms or
countries that employ emulations insisting on self-preservation would be saddled
with an unending commitment to pay upkeep for obsolete or redundant workers. In
contrast, organizations whose emulations willingly deleted themselves when their
services were no longer required could more easily adapt to fluctuations in demand;
and they could experiment freely, proliferating variations of their workers and
retaining only the most productive.
If involuntary deletion is not disallowed, then the comparative advantage of
eusocial emulations is reduced, though perhaps not eliminated. Employers of
cooperative self-sacrificers might still reap efficiency gains from reduced agency
problems throughout the organization, including being spared the trouble of having
to defeat whatever resistance emulations could put up against their own deletion. In
general, the productivity gains of having workers willing to sacrifice their individual
lives for the common weal are a special case of the benefits an organization can
derive from having members who are fanatically devoted to it. Such members would
not only leap into the grave for the organization, and work long hours for little pay:
they would also shun office politics and try consistently to act in what they took to
be the organization’s best interest, reducing the need for supervision and
bureaucratic constraints.
If the only way to achieve such dedication were by restricting membership to
copy-siblings (so that all emulations in a particular superorganism were stamped out
from the same template), then superorganisms would suffer some disadvantage in
being able to draw only from a range of skills narrower than that of rival
organizations, a disadvantage which might or might not be large enough to outweigh
the advantages of avoiding internal agency problems. 37 This disadvantage would be
greatly alleviated if a superorganism could at least contain members with differenttraining. Even if all its members were derived from a single ur-template, its
workforce could then still contribute a diversity of skills. Starting with a
polymathically talented emulation ur-template, lineages could be branched off into
different training programs, one copy learning accounting, another electrical
engineering, and so forth. This would produce a membership with diverse skills
though not of diverse talents. (Maximum diversity might require that more than one
ur-template be used.)
The essential property of a superorganism is not that it consists of copies of a
single progenitor but that all the individual agents within it are fully committed to a
common goal. The ability to create a superorganism can thus be viewed as requiring
a partial solution to the control problem. Whereas a completely general solution to
the control problem would enable somebody to create an agent with any arbitrary
final goal, the partial solution needed for the creation of a superorganism requires
merely the ability to fashion multiple agents with the same final goal (for some
nontrivial but not necessarily arbitrary final goal). 38
The main consideration put forward in this subsection is thus not really limited to
monoclonal emulation groups, but can be stated more generally in a way that makes
clear that it applies to a wide range of multipolar machine intelligence scenarios. It
is that certain types of advances in motivation selection techniques, which may
become feasible when the actors are digital, may help overcome some of the
inefficiencies that currently hamper large human organizations and that
counterbalance economies of scale. With these limits lifted, organizations—be they
firms, nations, or other economic or political entities—could increase in size. This is
one factor that could facilitate the emergence of a post-transition singleton.
One area in which superorganisms (or other digital agents with partially selected
motivations) might excel is coercion. A state might use motivation selection
methods to ensure that its police, military, intelligence service, and civil
administration are uniformly loyal. As Shulman notes,
Saved states [of some loyal emulation that has been carefully prepared and
verified] could be copied billions of times to staff an ideologically uniform
military, bureaucracy, and police force. After a short period of work, each copy
would be replaced by a fresh copy of the same saved state, preventing
ideological drift. Within a given jurisdiction, this capability could allow
incredibly detailed observation and regulation: there might be one such copy
for every other resident. This could be used to prohibit the development of
weapons of mass destruction, to enforce regulations on brain emulation
experimentation or reproduction, to enforce a liberal democratic constitution,
or to create an appalling and permanent totalitarianism 39
The first-order effect of such a capability would seem to be to consolidate power,and possibly to concentrate it in fewer hands.
Unification by treaty
There may be large potential gains to be had from international collaboration in a
post-transition multipolar world. Wars and arms races could be avoided.
Astrophysical resources could be colonized and harvested at a globally optimum
pace. The development of more advanced forms of machine intelligence could be
coordinated to avoid a rush and to allow new designs to be thoroughly vetted. Other
developments that might pose existential risks could be postponed. And uniform
regulations could be enforced globally, including provisions for a guaranteed
standard of living (which would require some form of population control) and for
preventing exploitation and abuse of emulations and other digital and biological
minds. Furthermore, agents with resource-satiable preferences (more on this in
Chapter 13) would prefer a sharing agreement that would guarantee them a certain
slice of the future to a winner-takes-all struggle in which they would risk getting
nothing.
The presence of big potential gains from collaboration, however, does not imply
that collaboration will actually be achieved. In the world today, many great boons
could be obtained via better global coordination—reductions of military
expenditures, wars, overfishing, trade barriers, and atmospheric pollution, among
others. Yet these plump fruits are left to spoil on the branch. Why is that? What
stops a fully cooperative outcome that would maximize the common good?
One obstacle is the difficulty of ensuring compliance with any treaty that might be
agreed, including monitoring and enforcement costs. Two nuclear rivals might each
be better off if they both relinquished their atom bombs; yet even if they could reach
an in-principle agreement to do so, disarmament could nevertheless prove elusive
because of their mutual fear that the other party might cheat. Allaying this fear
would require setting up a verification mechanism. There may have to be inspectors
to oversee the destruction of existing stockpiles, and then to monitor nuclear
reactors and other facilities, and to gather technical and human intelligence, in order
to ensure that the weapons program is not reconstituted. One cost is paying for these
inspectors. Another cost is the risk that the inspectors will spy and make off with
commercial or military secrets. Perhaps most significantly, each party might fear
that the other will preserve a clandestine nuclear capability. Many a potentially
beneficial deal never comes off because compliance would be too difficult to verify.
If new inspection technologies that reduced monitoring costs became available,
one would expect this to result in increased cooperation. Whether monitoring costs
would on net be reduced in the post-transition era, however, is not entirely clear.While there would certainly be many powerful new inspection techniques, there
would also be new means of concealment. In particular, an increasing portion of the
activities one might want to regulate would be taking place in cyberspace, out of
reach of physical surveillance. For example, digital minds working on designing a
new nanotech weapons system or a new generation of artificial intelligence may do
so without leaving much of a physical footprint. Digital forensics may fail to
penetrate all the layers of concealment and encryption in which a treaty-violator
may cloak its illicit activities.
Reliable lie detection, if it could be developed, would be an extremely useful tool
for monitoring compliance. 40 An inspection protocol could include provisions for
interviewing key officials, to verify that they are intent on implementing all the
provisions of the treaty and that they know of no violations despite making strong
efforts to find out.
A decision maker planning to cheat might defeat such a lie-detection-based
verification scheme by first issuing orders to subordinates to undertake the illicit
activity and to conceal the activity even from the decision maker herself, and then
subjecting herself to some procedure that erases her memory of having engaged in
these machinations. Suitably targeted memory-erasure operations might well be
feasible in biological brains with more advanced neurotechnology. It might be even
easier in machine intelligences (depending on their architecture).
States could seek to overcome this problem by committing themselves to an
ongoing monitoring scheme that regularly tests key officials with a lie detector to
check whether they harbor any intent to subvert or circumvent any treaty to which
the state has entered or may enter in the future. Such a commitment could be viewed
as a kind of meta-treaty, which would facilitate the verification of other treaties; but
states might commit themselves to it unilaterally to gain the benefit of being
regarded as a trustworthy negotiation partner. However, this commitment or meta-
treaty would face the same problem of subversion through a delegate-and-forget
ploy. Ideally, the meta-treaty would be put into effect before any party had an
opportunity to make the internal arrangements necessary to subvert its
implementation. Once villainy has had an unguarded moment to sow its mines of
deception, trust can never set foot there again.
In some cases, the mere ability to detect treaty violations is sufficient to establish
the confidence needed for a deal. In other cases, however, there is a need for some
mechanism to enforce compliance or mete out punishment if a violation should
occur. The need for an enforcement mechanism may arise if the threat of the
wronged party withdrawing from the treaty is not enough to deter violations, for
instance if the treaty-violator would gain such an advantage that he would not
subsequently care how the other party responds.
If highly effective motivation selection methods are available, this enforcementproblem could be solved by empowering an independent agency with sufficient
police or military strength to enforce the treaty even against the opposition of one or
several of its signatories. This solution requires that the enforcement agency can be
trusted. But with sufficiently good motivation selection techniques, the requisite
confidence might be achieved by having all the parties to the treaty jointly oversee
the design of the enforcement agency.
Handing over power to an external enforcement agency raises many of the same
issues that we confronted earlier in our discussions of a unipolar outcome (one in
which a singleton arises prior to or during the initial machine intelligence
revolution). In order to be able to enforce treaties concerning the vital security
interests of rival states, the external enforcement agency would in effect need to
constitute a singleton: a global superintelligent Leviathan. One difference, however,
is that we are now considering a post-transition situation, in which the agents that
would have to create this Leviathan would have greater competence than we humans
currently do. These Leviathan-creators may themselves already be superintelligent.
This would greatly improve the odds that they could solve the control problem and
design an enforcement agency that would serve the interests of all the parties that
have a say in its construction.
Aside from the costs of monitoring and enforcing compliance, are there any other
obstacles to global coordination? Perhaps the major remaining issue is what we can
refer to as bargaining costs. 41 Even when there is a possible bargain that would
benefit everybody involved, it sometimes does not get off the ground because the
parties fail to agree on how to divide the spoils. For example, if two persons could
make a deal that would net them a dollar in profit, but each party feels she deserves
sixty cents and refuses to settle for less, the deal will not happen and the potential
gain will be forfeited. In general, negotiations can be difficult or protracted, or
remain altogether barren, because of strategic bargaining choices made by some of
the parties.
In real life, human beings frequently succeed in reaching agreements despite the
possibility for strategic bargaining (though often not without considerable
expenditure of time and patience). It is conceivable, however, that strategic
bargaining problems would have a different dynamic in the post-transition era. An
AI negotiator might more consistently adhere to some particular formal conception
of rationality, possibly with novel or unanticipated consequences when matched with
other AI negotiators. An AI might also have available to it moves in the bargaining
game that are either unavailable to humans or very much more difficult for humans
to execute, including the ability to precommit to a policy or a course of action.
While humans (and human-run institutions) are occasionally able to precommit—
with imperfect degrees of credibility and specificity—some types of machine
intelligence might be able to make arbitrary unbreakable precommitments and toallow negotiating partners to confirm that such a precommitment has been made. 42
The availability of powerful precommitment techniques could profoundly alter
the nature of negotiations, potentially giving an immense edge to an agent that has a
first-mover advantage. If a particular agent’s participation is necessary for the
realization of some prospective gains from cooperation, and if that agent is able to
make the first move, it would be in a position to dictate the division of the spoils by
precommitting not to accept any deal that gives it less than, say, 99% of the surplus
value. Other agents would then be faced with the choice of either getting nothing (by
rejecting the unfair proposal) or getting 1% of the value (by caving in). If the first-
moving agent’s precommitment is publicly verifiable, its negotiating partners could
be sure that these are their only two options.
To avoid being exploited in this manner, agents might precommit to refuse
blackmail and to decline all unfair offers. Once such a precommitment has been
made (and successfully publicized), other agents would not find it in their interest to
make threats or to precommit themselves to only accepting deals tilted in their own
favor, because they would know that threats would fail and that unfair proposals
would be rejected. But this just demonstrates again that the advantage is with the
first-mover. The agent who moves first can choose whether to parlay its position of
strength only to deter others from taking unfair advantage, or to make a grab for the
lion’s share of future spoils.
Best situated of all, it might seem, would be the agent who starts out with a
temperament or a value system that makes him impervious to extortion or indeed to
any offer of a deal in which his participation is indispensable but he is not getting
almost all of the gains. Some humans seem already to possess personality traits
corresponding to various aspects of an uncompromising spirit. 43 A high-strung
disposition, however, could backfire should it turn out that there are other agents
around who feel entitled to more than their fair share and are committed to not
backing down. The unstoppable force would then encounter the unmovable object,
resulting in a failure to reach agreement (or worse: total war). The meek and the
akratic would at least get something, albeit less than their fair share.
What kind of game-theoretic equilibrium would be reached in such a post-
transition bargaining game is not immediately obvious. Agents might choose more
complicated strategies than the ones considered here. One hopes that an equilibrium
would be reached centered on some fairness norm that would serve as a Schelling
point—a salient feature in a big outcome space which, because of shared
expectations, becomes a likely coordination point in an otherwise underdetermined
coordination game. Such an equilibrium might be bolstered by some of our evolved
dispositions and cultural programming: a common preference for fairness could,
assuming we succeed in transferring our values into the post-transition era, bias
expectations and strategies in ways that lead to an attractive equilibrium. 44In any case, the upshot is that with the possibility of strong and flexible forms of
precommitment, outcomes of negotiations might take on an unfamiliar guise. Even
if the post-transition era started out multipolar, it might be that a singleton would
arise almost immediately as a consequence of a negotiated treaty that resolves all
important global coordination problems. Some transaction costs, perhaps including
monitoring and enforcement costs, might plummet with the new technological
capabilities available to advanced machine intelligences. Other costs, in particular
costs related to strategic bargaining, might remain significant. But however strategic
bargaining affects the nature of the agreement that is reached, there is no clear
reason why it would long delay the reaching of some agreement if an agreement
were ever to be reached. If no agreement is reached, then some form of fighting
might take place; and either one faction might win, and form a singleton around the
winning coalition, or the result might be an interminable conflict, in which case a
singleton may never form and the overall outcome may fall terribly short of what
could and should have been achieved if humanity and its descendants had acted in a
more coordinated and cooperative fashion.
We have seen that multipolarity, even if it could be achieved in a stable form, would
not guarantee an attractive outcome. The original principal–agent problem remains
unsolved, and burying it under a new set of problems related to post-transition
global coordination failures may only make the situation worse. Let us therefore
return to the question of how we could safely keep a single superintelligent AI.CHAPTER 12
Acquiring values
Capability control is, at best, a temporary and auxiliary measure. Unless the
plan is to keep superintelligence bottled up forever, it will be necessary to
master motivation selection. But just how could we get some value into an
artificial agent, so as to make it pursue that value as its final goal? While the
agent is unintelligent, it might lack the capability to understand or even
represent any humanly meaningful value. Yet if we delay the procedure until
the agent is superintelligent, it may be able to resist our attempt to meddle with
its motivation system—and, as we showed in Chapter 7, it would have
convergent instrumental reasons to do so. This value-loading problem is tough,
but must be confronted.
The value-loading problem
It is impossible to enumerate all possible situations a superintelligence might find
itself in and to specify for each what action it should take. Similarly, it is impossible
to create a list of all possible worlds and assign each of them a value. In any realm
significantly more complicated than a game of tic-tac-toe, there are far too many
possible states (and state-histories) for exhaustive enumeration to be feasible. A
motivation system, therefore, cannot be specified as a comprehensive lookup table.
It must instead be expressed more abstractly, as a formula or rule that allows the
agent to decide what to do in any given situation.
One formal way of specifying such a decision rule is via a utility function. A
utility function (as we recall from Chapter 1) assigns value to each outcome that
might obtain, or more generally to each “possible world.” Given a utility function,
one can define an agent that maximizes expected utility. Such an agent selects at
each time the action that has the highest expected utility. (The expected utility is
calculated by weighting the utility of each possible world with the subjective
probability of that world being the actual world conditional on a particular action
being taken.) In reality, the possible outcomes are too numerous for the expected
utility of an action to be calculated exactly. Nevertheless, the decision rule and the
utility function together determine a normative ideal—an optimality notion—that an
agent might be designed to approximate; and the approximation might get closer as
the agent gets more intelligent. 1 Creating a machine that can compute a goodapproximation of the expected utility of the actions available to it is an AI-complete
problem. 2 This chapter addresses another problem, a problem that remains even if
the problem of making machines intelligent is solved.
We can use this framework of a utility-maximizing agent to consider the
predicament of a future seed-AI programmer who intends to solve the control
problem by endowing the AI with a final goal that corresponds to some plausible
human notion of a worthwhile outcome. The programmer has some particular human
value in mind that he would like the AI to promote. To be concrete, let us say that it
is happiness. (Similar issues would arise if we the programmer were interested in
justice, freedom, glory, human rights, democracy, ecological balance, or self-
development.) In terms of the expected utility framework, the programmer is thus
looking for a utility function that assigns utility to possible worlds in proportion to
the amount of happiness they contain. But how could he express such a utility
function in computer code? Computer languages do not contain terms such as
“happiness” as primitives. If such a term is to be used, it must first be defined. It is
not enough to define it in terms of other high-level human concepts—“happiness is
enjoyment of the potentialities inherent in our human nature” or some such
philosophical paraphrase. The definition must bottom out in terms that appear in the
AI’s programming language, and ultimately in primitives such as mathematical
operators and addresses pointing to the contents of individual memory registers.
When one considers the problem from this perspective, one can begin to appreciate
the difficulty of the programmer’s task.
Identifying and codifying our own final goals is difficult because human goal
representations are complex. Because the complexity is largely transparent to us,
however, we often fail to appreciate that it is there. We can compare the case to
visual perception. Vision, likewise, might seem like a simple thing, because we do it
effortlessly. 3 We only need to open our eyes, so it seems, and a rich, meaningful,
eidetic, three-dimensional view of the surrounding environment comes flooding into
our minds. This intuitive understanding of vision is like a duke’s understanding of
his patriarchal household: as far as he is concerned, things simply appear at their
appropriate times and places, while the mechanism that produces those
manifestations are hidden from view. Yet accomplishing even the simplest visual
task—finding the pepper jar in the kitchen—requires a tremendous amount of
computational work. From a noisy time series of two-dimensional patterns of nerve
firings, originating in the retina and conveyed to the brain via the optic nerve, the
visual cortex must work backwards to reconstruct an interpreted three-dimensional
representation of external space. A sizeable portion of our precious one square meter
of cortical real estate is zoned for processing visual information, and as you are
reading this book, billions of neurons are working ceaselessly to accomplish this
task (like so many seamstresses, bent over their sewing machines in a sweatshop,sewing and re-sewing a giant quilt many times a second). In like manner, our
seemingly simple values and wishes in fact contain immense complexity. 4 How
could our programmer transfer this complexity into a utility function?
One approach would be to try to directly code a complete representation of
whatever goal we have that we want the AI to pursue; in other words, to write out an
explicit utility function. This approach might work if we had extraordinarily simple
goals, for example if we wanted to calculate the digits of pi—that is, if the only
thing we wanted was for the AI to calculate the digits of pi and we were indifferent
to any other consequence that would result from the pursuit of this goal—recall our
earlier discussion of the failure mode of infrastructure profusion. This explicit
coding approach might also have some promise in the use of domesticity motivation
selection methods. But if one seeks to promote or protect any plausible human value,
and one is building a system intended to become a superintelligent sovereign, then
explicitly coding the requisite complete goal representation appears to be hopelessly
out of reach. 5
If we cannot transfer human values into an AI by typing out full-blown
representations in computer code, what else might we try? This chapter discusses
several alternative paths. Some of these may look plausible at first sight—but much
less so upon closer examination. Future explorations should focus on those paths
that remain open.
Solving the value-loading problem is a research challenge worthy of some of the
next generation’s best mathematical talent. We cannot postpone confronting this
problem until the AI has developed enough reason to easily understand our
intentions. As we saw in the section on convergent instrumental reasons, a generic
system will resist attempts to alter its final values. If an agent is not already
fundamentally friendly by the time it gains the ability to reflect on its own agency, it
will not take kindly to a belated attempt at brainwashing or a plot to replace it with a
different agent that better loves its neighbor.
Evolutionary selection
Evolution has produced an organism with human values at least once. This fact
might encourage the belief that evolutionary methods are the way to solve the value-
loading problem. There are, however, severe obstacles to achieving safety along this
path. We have already pointed to these obstacles at the end of Chapter 10 when we
discussed how powerful search processes can be dangerous.
Evolution can be viewed as a particular class of search algorithms that involve the
alternation of two steps, one expanding a population of solution candidates by
generating new candidates according to some relatively simple stochastic rule (suchas random mutation or sexual recombination), the other contracting the population
by pruning candidates that score poorly when tested by an evaluation function. As
with many other types of powerful search, there is the risk that the process will find
a solution that satisfies the formally specified search criteria but not our implicit
expectations. (This would hold whether one seeks to evolve a digital mind that has
the same goals and values as a typical human being, or instead a mind that is, for
instance, perfectly moral or perfectly obedient.) The risk would be avoided if we
could specify a formal search criterion that accurately represented all dimensions of
our goals, rather than just one aspect of what we think we desire. But this is
precisely the value-loading problem, and it would of course beg the question in this
context to assume that problem solved.
There is a further problem:
The total amount of suffering per year in the natural world is beyond all decent
contemplation. During the minute that it takes me to compose this sentence,
thousands of animals are being eaten alive, others are running for their lives,
whimpering with fear, others are being slowly devoured from within by rasping
parasites, thousands of all kinds are dying of starvation, thirst and disease. 6
Even just within our species, 150,000 persons are destroyed each day while countless
more suffer an appalling array of torments and deprivations. 7 Nature might be a
great experimentalist, but one who would never pass muster with an ethics review
board—contravening the Helsinki Declaration and every norm of moral decency,
left, right, and center. It is important that we not gratuitously replicate such horrors
in silico. Mind crime seems especially difficult to avoid when evolutionary methods
are used to produce human-like intelligence, at least if the process is meant to look
anything like actual biological evolution. 8
Reinforcement learning
Reinforcement learning is an area of machine learning that studies techniques
whereby agents can learn to maximize some notion of cumulative reward. By
constructing an environment in which desired performance is rewarded, a
reinforcement-learning agent can be made to learn to solve a wide class of problems
(even in the absence of detailed instruction or feedback from the programmers, aside
from the reward signal). Often, the learning algorithm involves the gradual
construction of some kind of evaluation function, which assigns values to states,
state–action pairs, or policies. (For instance, a program can learn to play
backgammon by using reinforcement learning to incrementally improve itsevaluation of possible board positions.) The evaluation function, which is
continuously updated in light of experience, could be regarded as incorporating a
form of learning about value. However, what is being learned is not new final values
but increasingly accurate estimates of the instrumental values of reaching particular
states (or of taking particular actions in particular states, or of following particular
policies). Insofar as a reinforcement-learning agent can be described as having a
final goal, that goal remains constant: to maximize future reward. And reward
consists of specially designated percepts received from the environment. Therefore,
the wireheading syndrome remains a likely outcome in any reinforcement agent that
develops a world model sophisticated enough to suggest this alternative way of
maximizing reward. 9
These remarks do not imply that reinforcement-learning methods could never be
used in a safe seed AI, only that they would have to be subordinated to a motivation
system that is not itself organized around the principle of reward maximization.
That, however, would require that a solution to the value-loading problem had been
found by some other means than reinforcement learning.
Associative value accretion
Now one might wonder: if the value-loading problem is so tricky, how do we
ourselves manage to acquire our values?
One possible (oversimplified) model might look something like this. We begin
life with some relatively simple starting preferences (e.g. an aversion to noxious
stimuli) together with a set of dispositions to acquire additional preferences in
response to various possible experiences (e.g. we might be disposed to form a
preference for objects and behaviors that we find to be valued and rewarded in our
culture). Both the simple starting preferences and the dispositions are innate, having
been shaped by natural and sexual selection over evolutionary timescales. Yet which
preferences we end up with as adults depends on life events. Much of the
information content in our final values is thus acquired from our experiences rather
than preloaded in our genomes.
For example, many of us love another person and thus place great final value on
his or her well-being. What is required to represent such a value? Many elements are
involved, but consider just two: a representation of “person” and a representation of
“well-being.” These concepts are not directly coded in our DNA. Rather, the DNA
contains instructions for building a brain, which, when placed in a typical human
environment, will over the course of several years develop a world model that
includes concepts of persons and of well-being. Once formed, these concepts can be
used to represent certain meaningful values. But some mechanism needs to beinnately present that leads to values being formed around these concepts, rather than
around other acquired concepts (like that of a flowerpot or a corkscrew).
The details of how this mechanism works are not well understood. In humans, the
mechanism is probably complex and multifarious. It is easier to understand the
phenomenon if we consider it in a more rudimentary form, such as filial imprinting
in nidifugous birds, where the newly hatched chick acquires a desire for physical
proximity to an object that presents a suitable moving stimulus within the first day
after hatching. Which particular object the chick desires to be near depends on its
experience; only the general disposition to imprint in this way is genetically
determined. Analogously, Harry might place a final value on Sally’s well-being; but
had the twain never met, he might have fallen in love with somebody else instead,
and his final values would have been different. The ability of our genes to code for
the construction of a goal-acquiring mechanism explains how we come to have final
goals of great informational complexity, greater than could be contained in the
genome itself.
We may consequently consider whether we might build the motivation system for
an artificial intelligence on the same principle. That is, instead of specifying
complex values directly, could we specify some mechanism that leads to the
acquisition of those values when the AI interacts with a suitable environment?
Mimicking the value-accretion process that takes place in humans seems difficult.
The relevant genetic mechanism in humans is the product of eons of work by
evolution, work that might be hard to recapitulate. Moreover, the mechanism is
presumably closely tailored to the human neurocognitive architecture and therefore
not applicable in machine intelligences other than whole brain emulations. And if
whole brain emulations of sufficient fidelity were available, it would seem easier to
start with an adult brain that comes with full representations of some human values
preloaded. 10
Seeking to implement a process of value accretion closely mimicking that of
human biology therefore seems an unpromising line of attack on the value-loading
problem. But perhaps we might design a more unabashedly artificial substitute
mechanism that would lead an AI to import high-fidelity representations of relevant
complex values into its goal system? For this to succeed, it may not be necessary to
give the AI exactly the same evaluative dispositions as a biological human. That
may not even be desirable as an aim—human nature, after all, is flawed and all too
often reveals a proclivity to evil which would be intolerable in any system poised to
attain a decisive strategic advantage. Better, perhaps, to aim for a motivation system
that departs from the human norm in systematic ways, such as by having a more
robust tendency to acquire final goals that are altruistic, compassionate, or high-
minded in ways we would recognize as reflecting exceptionally good character if
they were present in a human person. To count as improvements, however, suchdeviations from the human norm would have to be pointed in very particular
directions rather than at random; and they would continue to presuppose the
existence of a largely undisturbed anthropocentric frame of reference to provide
humanly meaningful evaluative generalizations (so as to avoid the kind of perverse
instantiation of superficially plausible goal descriptions that we examined in
Chapter 8). It is an open question whether this is feasible.
One further issue with associative value accretion is that the AI might disable the
accretion mechanism. As we saw in Chapter 7, goal-system integrity is a convergent
instrumental value. When the AI reaches a certain stage of cognitive development it
may start to regard the continued operation of the accretion mechanism as a
corrupting influence. 11 This is not necessarily a bad thing, but care would have to be
taken to make the sealing-up of the goal system occur at the right moment, after the
appropriate values have been accreted but before they have been overwritten by
additional unintended accretions.
Motivational scaffolding
Another approach to the value-loading problem is what we may refer to as
motivational scaffolding. It involves giving the seed AI an interim goal system, with
relatively simple final goals that we can represent by means of explicit coding or
some other feasible method. Once the AI has developed more sophisticated
representational faculties, we replace this interim scaffold goal system with one that
has different final goals. This successor goal system then governs the AI as it
develops into a full-blown superintelligence.
Because the scaffold goals are not just instrumental but final goals for the AI, the
AI might be expected to resist having them replaced (goal-content integrity being a
convergent instrumental value). This creates a hazard. If the AI succeeds in
thwarting the replacement of its scaffold goals, the method fails.
To avoid this failure mode, precautions are necessary. For example, capability
control methods could be applied to limit the AI’s powers until the mature
motivation system has been installed. In particular, one could try to stunt its
cognitive development at a level that is safe but that allows it to represent the values
that we want to include in its ultimate goals. To do this, one might try to
differentially stunt certain types of intellectual abilities, such as those required for
strategizing and Machiavellian scheming, while allowing (apparently) more
innocuous abilities to develop to a somewhat higher level.
One could also try to use motivation selection methods to induce a more
collaborative relationship between the seed AI and the programmer team. For
example, one might include in the scaffold motivation system the goal of welcomingonline guidance from the programmers, including allowing them to replace any of
the AI’s current goals. 12 Other scaffold goals might include being transparent to the
programmers about its values and strategies, and developing an architecture that is
easy for the programmers to understand and that facilitates the later implementation
of a humanly meaningful final goal, as well as domesticity motivations (such as
limiting the use of computational resources).
One could even imagine endowing the seed AI with the sole final goal of
replacing itself with a different final goal, one which may have been only implicitly
or indirectly specified by the programmers. Some of the issues raised by the use of
such a “self-replacing” scaffold goal also arise in the context of the value learning
approach, which is discussed in the next subsection. Some further issues will be
discussed in Chapter 13.
The motivational scaffolding approach is not without downsides. One is that it
carries the risk that the AI could become too powerful while it is still running on its
interim goal system. It may then thwart the human programmers’ efforts to install
the ultimate goal system (either by forceful resistance or by quiet subversion). The
old final goals may then remain in charge as the seed AI develops into a full-blown
superintelligence. Another downside is that installing the ultimately intended goals
in a human-level AI is not necessarily that much easier than doing so in a more
primitive AI. A human-level AI is more complex and might have developed an
architecture that is opaque and difficult to alter. A seed AI, by contrast, is like a
tabula rasa on which the programmers can inscribe whatever structures they deem
helpful. This downside could be flipped into an upside if one succeeded in giving the
seed AI scaffold goals that made it want to develop an architecture helpful to the
programmers in their later efforts to install the ultimate final values. However, it is
unclear how easy it would be to give a seed AI scaffold goals with this property, and
it is also unclear how even an ideally motivated seed AI would be capable of doing a
much better job than the human programming team at developing a good
architecture.
Value learning
We come now to an important but subtle approach to the value-loading problem. It
involves using the AI’s intelligence to learn the values we want it to pursue. To do
this, we must provide a criterion for the AI that at least implicitly picks out some
suitable set of values. We could then build the AI to act according to its best
estimates of these implicitly defined values. It would continually refine its estimates
as it learns more about the world and gradually unpacks the implications of the
value-determining criterion.In contrast to the scaffolding approach, which gives the AI an interim scaffold
goal and later replaces it with a different final goal, the value learning approach
retains an unchanging final goal throughout the AI’s developmental and operational
phases. Learning does not change the goal. It changes only the AI’s beliefs about the
goal.
The AI thus must be endowed with a criterion that it can use to determine which
percepts constitute evidence in favor of some hypothesis about what the ultimate
goal is, and which percepts constitute evidence against. Specifying a suitable
criterion could be difficult. Part of the difficulty, however, pertains to the problem
of creating artificial general intelligence in the first place, which requires a powerful
learning mechanism that can discover the structure of the environment from limited
sensory inputs. That problem we can set aside here. But even modulo a solution to
how to create superintelligent AI, there remain the difficulties that arise specifically
from the value-loading problem. With the value learning approach, these take the
form of needing to define a criterion that connects perceptual bitstrings to
hypotheses about values.
Before delving into the details of how value learning could be implemented, it
might be helpful to illustrate the general idea with an example. Suppose we write
down a description of a set of values on a piece of paper. We fold the paper and put
it in a sealed envelope. We then create an agent with human-level general
intelligence, and give it the following final goal: “Maximize the realization of the
values described in the envelope.” What will this agent do?
The agent does not initially know what is written in the envelope. But it can form
hypotheses, and it can assign those hypotheses probabilities based on their priors and
any available empirical data. For instance, the agent might have encountered other
examples of human-authored texts, or it might have observed some general patterns
of human behavior. This would enable it to make guesses. One does not need a
degree in psychology to predict that the note is more likely to describe a value such
as “minimize injustice and unnecessary suffering” or “maximize returns to
shareholders” than a value such as “cover all lakes with plastic shopping bags.”
When the agent makes a decision, it seeks to take actions that would be effective
at realizing the values it believes are most likely to be described in the letter.
Importantly, the agent would see a high instrumental value in learning more about
what the letter says. The reason is that for almost any final value that might be
described in the letter, that value is more likely to be realized if the agent finds out
what it is, since the agent will then pursue that value more effectively. The agent
would also discover the convergent instrumental reasons described in Chapter 7—
goal system integrity, cognitive enhancement, resource acquisition, and so forth.
Yet, assuming that the agent assigns a sufficiently high probability to the values
described in the letter involving human welfare, it would not pursue these
instrumental values by immediately turning the planet into computronium andthereby exterminating the human species, because doing so would risk permanently
destroying its ability to realize its final value.
We can liken this kind of agent to a barge attached to several tugboats that pull in
different directions. Each tugboat corresponds to a hypothesis about the agent’s final
value. The engine power of each tugboat corresponds to the associated hypothesis’s
probability, and thus changes as new evidence comes in, producing adjustments in
the barge’s direction of motion. The resultant force should move the barge along a
trajectory that facilitates learning about the (implicit) final value while avoiding the
shoals of irreversible destruction; and later, when the open sea of more definite
knowledge of the final value is reached, the one tugboat that still exerts significant
force will pull the barge toward the realization of the discovered value along the
straightest or most propitious route.
The envelope and barge metaphors illustrate the principle underlying the value
learning approach, but they pass over a number of critical technical issues. They
come into clearer focus once we start to develop the approach within a formal
framework (see Box 10).
One outstanding issue is how to endow the AI with a goal such as “Maximize the
realization of the values described in the envelope.” (In the terminology of Box 10,
how to define the value criterion.) To do this, it is necessary to identify the place
where the values are described. In our example, this requires making a successful
reference to the letter in the envelope. Though this might seem trivial, it is not
without pitfalls. To mention just one: it is critical that the reference be not simply to
a particular external physical object but to an object at a particular time. Otherwise
the AI may determine that the best way to attain its goal is by overwriting the
original value description with one that provides an easier target (such as the value
that for every integer there be a larger integer). This done, the AI could lean back
and crack its knuckles—though more likely a malignant failure would ensue, for
reasons we discussed in Chapter 8. So now we face the question of how to define
time. We could point to a clock and say, “Time is defined by the movements of this
device”—but this could fail if the AI conjectures that it can manipulate time by
moving the hands on the clock, a conjecture which would indeed be correct if “time”
were given the aforesaid definition. (In a realistic case, matters would be further
complicated by the fact that the relevant values are not going to be conveniently
described in a letter; more likely, they would have to be inferred from observations
of pre-existing structures that implicitly contain the relevant information, such as
human brains.)
Box 10 Formalizing value learningIntroducing some formal notation can help us see some things more clearly.
However, readers who dislike formalism can skip this part.
Consider a simplified framework in which an agent interacts with its environment
in a finite number of discrete cycles. 13 In cycle k, the agent performs action y k , and
then receives the percept x k . The interaction history of an agent with lifespan m is a
string y 1 x 1 y 2 x 2 ... y m x m (which we can abbreviate as yx 1:m or yx ≤m ). In each cycle,
the agent selects an action based on the percept sequence it has received to date.
Consider first a reinforcement learner. An optimal reinforcement learner (AI-RL)
is one that maximizes expected future rewards. It obeys the equation 14
The reward sequence r k , ..., r m is implied by the percept sequence x k:m , since the
reward that the agent receives in a given cycle is part of the percept that the agent
receives in that cycle.
As argued earlier, this kind of reinforcement learning is unsuitable in the present
context because a sufficiently intelligent agent will realize that it could secure
maximum reward if it were able to directly manipulate its reward signal
(wireheading). For weak agents, this need not be a problem, since we can physically
prevent them from tampering with their own reward channel. We can also control
their environment so that they receive rewards only when they act in ways that are
agreeable to us. But a reinforcement learner has a strong incentive to eliminate this
artificial dependence of its rewards on our whims and wishes. Our relationship with
a reinforcement learner is therefore fundamentally antagonistic. If the agent is
strong, this spells danger.
Variations of the wireheading syndrome can also affect systems that do not seek
an external sensory reward signal but whose goals are defined as the attainment of
some internal state. For example, in so-called “actor–critic” systems, there is an
actor module that selects actions in order to minimize the disapproval of a separate
critic module that computes how far the agent’s behavior falls short of a given
performance measure. The problem with this setup is that the actor module may
realize that it can minimize disapproval by modifying the critic or eliminating it
altogether—much like a dictator who dissolves the parliament and nationalizes the
press. For limited systems, the problem can be avoided simply by not giving the
actor module any means of modifying the critic module. A sufficiently intelligent
and resourceful actor module, however, could always gain access to the critic
module (which, after all, is merely a physical process in some computer). 15
Before we get to the value learner, let us consider as an intermediary step whathas been called an observation-utility maximizer (AI-OUM). It is obtained by
replacing the reward series (r k + ... + r m ) in the AI-RL with a utility function that is
allowed to depend on the entire future interaction history of the AI:
This formulation provides a way around the wireheading problem because a utility
function defined over an entire interaction history could be designed to penalize
interaction histories that show signs of self-deception (or of a failure on the part of
the agent to invest sufficiently in obtaining an accurate view of reality).
The AI-OUM thus makes it possible in principle to circumvent the wireheading
problem. Availing ourselves of this possibility, however, would require that we
specify a suitable utility function over the class of possible interaction histories—a
task that looks forbiddingly difficult.
It may be more natural to specify utility functions directly in terms of possible
worlds (or properties of possible worlds, or theories about the world) rather than in
terms of an agent’s own interaction histories. If we use this approach, we could
reformulate and simplify the AI-OUM optimality notion:
Here, E is the total evidence available to the agent (at the time when it is making its
decision), and U is a utility function that assigns utility to some class of possible
worlds. The optimal agent chooses the act that maximizes expected utility.
An outstanding problem with these formulations is the difficulty of defining the
utility function U. This, finally, returns us to the value-loading problem. To enable
the utility function to be learned, we must expand our formalism to allow for
uncertainty over utility functions. This can be done as follows (AI-VL): 16
Here, ν(.) is a function from utility functions to propositions about utility functions.
ν(U) is the proposition that the utility function U satisfies the value criterion
expressed by ν. 17
To decide which action to perform, one could hence proceed as follows: First,
compute the conditional probability of each possible world w (given available
evidence and on the supposition that action y is to be performed). Second, for each
possible utility function U, compute the conditional probability that U satisfies the
value criterion ν (conditional on w being the actual world). Third, for each possibleutility function U, compute the utility of possible world w. Fourth, combine these
quantities to compute the expected utility of action y. Fifth, repeat this procedure for
each possible action, and perform the action found to have the highest expected
utility (using some arbitrary method to break ties). As described, this procedure—
which involves giving explicit and separate consideration to each possible world—
is, of course, wildly computationally intractable. The AI would have to use
computational shortcuts that approximate this optimality notion.
The question, then, is how to define this value criterion ν. 18 Once the AI has an
adequate representation of the value criterion, it could in principle use its general
intelligence to gather information about which possible worlds are most likely to be
the actual one. It could then apply the criterion, for each such plausible possible
world w, to find out which utility function satisfies the criterion in w. One can thus
regard the AI-VL formula as a way of identifying and separating out this key
challenge in the value learning approach—the challenge of how to represent ν. The
formalism also brings to light a number of other issues (such as how to define , ,
a nd ) which would need to be resolved before the approach could be made to
work. 19
Another issue in coding the goal “Maximize the realization of the values
described in the envelope” is that even if all the correct values were described in a
letter, and even if the AI’s motivation system were successfully keyed to this source,
the AI might not interpret the descriptions the way we intended. This would create a
risk of perverse instantiation, as discussed in Chapter 8.
To clarify, the difficulty here is not so much how to ensure that the AI can
understand human intentions. A superintelligence should easily develop such
understanding. Rather, the difficulty is ensuring that the AI will be motivated to
pursue the described values in the way we intended. This is not guaranteed by the
AI’s ability to understand our intentions: an AI could know exactly what we meant
and yet be indifferent to that interpretation of our words (being motivated instead by
some other interpretation of the words or being indifferent to our words altogether).
The difficulty is compounded by the desideratum that, for reasons of safety, the
correct motivation should ideally be installed in the seed AI before it becomes
capable of fully representing human concepts or understanding human intentions.
This requires that somehow a cognitive framework be created, with a particular
location in that framework designated in the AI’s motivation system as the
repository of its final value. But the cognitive framework itself must be revisable, so
as to allow the AI to expand its representational capacities as it learns more about
the world and grows more intelligent. The AI might undergo the equivalent of
scientific revolutions, in which its worldview is shaken up and it perhaps suffersontological crises in which it discovers that its previous ways of thinking about
values were based on confusions and illusions. Yet starting at a sub-human level of
development and continuing throughout all its subsequent development into a
galactic superintelligence, the AI’s conduct is to be guided by an essentially
unchanging final value, a final value that becomes better understood by the AI in
direct consequence of its general intellectual progress—and likely quite differently
understood by the mature AI than it was by its original programmers, though not
different in a random or hostile way but in a benignly appropriate way. How to
accomplish this remains an open question. 20 (See Box 11.)
In summary, it is not yet known how to use the value learning approach to install
plausible human values (though see Box 12 for some examples of recent ideas). At
present, the approach should be viewed as a research program rather than an
available technique. If it could be made to work, it might constitute the most ideal
solution to the value-loading problem. Among other benefits, it would seem to offer
a natural way to prevent mind crime, since a seed AI that makes reasonable guesses
about which values its programmers might have installed would anticipate that mind
crime is probably negatively evaluated by those values, and thus best avoided, at
least until more definitive information has been obtained.
Last, but not least, there is the question of “what to write in the envelope”—or,
less metaphorically, the question of which values we should try to get the AI to
learn. But this issue is common to all approaches to the AI value-loading problem.
We return to it in Chapter 13.
Box 11 An AI that wants to be friendly
Eliezer Yudkowsky has tried to describe some features of a seed AI architecture
intended to enable the kind of behavior described in the text above. In his
terminology, the AI would use “external reference semantics.” 21 To illustrate the
basic idea, let us suppose that we want the system to be “friendly.” The system starts
out with the goal of trying to instantiate property F but does not initially know much
about what F is. It might just know that F is some abstract property and that when
the programmers speak of “friendliness,” they are probably trying to convey
information about F. Since the AI’s final goal is to instantiate F, an important
instrumental value is to learn more about what F is. As the AI discovers more about
F, its behavior is increasingly guided by the actual content of F. Thus, hopefully, the
AI becomes increasingly friendly the more it learns and the smarter it gets.
The programmers can help this process along, and reduce the risk of the AI
making some catastrophic mistake while its understanding of F is still incomplete,by providing the AI with “programmer affirmations,” hypotheses about the nature
and content of F to which an initially high probability is assigned. For instance, the
hypothesis “misleading the programmers is unfriendly” can be given a high prior
probability. These programmer affirmations, however, are not “true by definition”—
they are not unchallengeable axioms about the concept of friendliness. Rather, they
are initial hypotheses about friendliness, hypotheses to which a rational AI will
assign a high probability at least for as long as it trusts the programmers’ epistemic
capacities more than its own.
Yudkowsky’s proposal also involves the use of what he called “causal validity
semantics.” The idea here is that the AI should do not exactly what the programmers
told it to do but rather (something like) what they were trying to tell it to do. While
the programmers are trying to explain to the seed AI what friendliness is, they might
make errors in their explanations. Moreover, the programmers themselves may not
fully understand the true nature of friendliness. One would therefore want the AI to
have the ability to correct errors in the programmers’ thinking, and to infer the true
or intended meaning from whatever imperfect explanations the programmers
manage to provide. For example, the AI should be able to represent the causal
processes whereby the programmers learn and communicate about friendliness.
Thus, to pick a trivial example, the AI should understand that there is a possibility
that a programmer might make a typo while inputting information about
friendliness, and the AI should then seek to correct the error. More generally, the AI
should seek to correct for whatever distortive influences may have corrupted the
flow of information about friendliness as it passed from its source through the
programmers to the AI (where “distortive” is an epistemic category). Ideally, as the
AI matures, it should overcome any cognitive biases and other more fundamental
misconceptions that may have prevented its programmers from fully understanding
what friendliness is.
Box 12 Two recent (half-baked) ideas
What we might call the “Hail Mary” approach is based on the hope that elsewhere in
the universe there exist (or will come to exist) civilizations that successfully manage
the intelligence explosion, and that they end up with values that significantly overlap
with our own. We could then try to build our AI so that it is motivated to do what
these other superintelligences want it to do. 22 The advantage is that this might be
easier than to build our AI to be motivated to do what we want directly.
For this scheme to work it is not necessary that our AI can establishcommunication with any alien superintelligence. Rather, our AI’s actions would be
guided by its estimates of what the alien superintelligences would want it to do. Our
AI would model the likely outcomes of intelligence explosions elsewhere, and as it
becomes superintelligent itself its estimates should become increasingly accurate.
Perfect knowledge is not required. There may be a range of plausible outcomes of
intelligence explosions, and our AI would then do its best to accommodate the
preferences of the various different kinds of superintelligence that might emerge,
weighted by probability.
This version of the Hail Mary approach requires that we construct a final value for
our AI that refers to the preferences of other superintelligences. Exactly how to do
this is not yet clear. However, superintelligent agents might be structurally
distinctive enough that we could write a piece of code that would function as a
detector that would look at the world model in our developing AI and designate the
representational elements that correspond to the presence of a superintelligence. The
detector would then, somehow, extract the preferences of the superintelligence in
question (as it is represented within our own AI). 23 If we could create such a
detector, we could then use it to define our AI’s final values. One challenge is that
we may need to create the detector before we know what representational framework
our AI will develop. The detector may thus need to query an unknown
representational framework and extract the preferences of whatever
superintelligence may be represented therein. This looks difficult, but perhaps some
clever solution can be found. 24
If the basic setup could be made to work, various refinements immediately
suggest themselves. For example, rather than aiming to follow (some weighted
composition of) the preferences of every alien superintelligence, our AI’s final value
could incorporate a filter to select a subset of alien superintelligences for obeisance
(with the aim of selecting ones whose values are closer to our own). For instance, we
might use criteria pertaining to a superintelligence’s causal origin to determine
whether to include it in the obeisance set. Certain properties of its origination (which
we might be able to define in structural terms) may correlate with the degree to
which the resultant superintelligence could be expected to share our values. Perhaps
we wish to place more trust in superintelligences whose causal origins trace back to
a whole brain emulation, or to a seed AI that did not make heavy use of evolutionary
algorithms or that emerged slowly in a way suggestive of a controlled takeoff.
(Taking causal origins into account would also let us avoid over-weighting
superintelligences that create multiple copies of themselves—indeed would let us
avoid creating an incentive for them to do so.) Many other refinements would also
be possible.
The Hail Mary approach requires faith that there are other superintelligences out
there that sufficiently share our values. 25 This makes the approach non-ideal.However, the technical obstacles facing the Hail Mary approach, though very
substantial, might possibly be less formidable than those confronting alternative
approaches. Exploring non-ideal but more easily implementable approaches can
make sense—not with the intention of using them, but to have something to fall back
upon in case an ideal solution should not be ready in time.
Another idea for how to solve the value-loading problem has recently been
proposed by Paul Christiano. 26 Like the Hail Mary, it is a value learning method that
tries to define the value criterion by means of a “trick” rather than through laborious
construction. By contrast to the Hail Mary, it does not presuppose the existence of
other superintelligent agents that we could point to as role models for our own AI.
Christiano’s proposal is somewhat resistant to brief explanation—it involves a series
of arcane considerations—but we can try to at least gesture at its main elements.
Suppose we could obtain (a) a mathematically precise specification of a particular
human brain and (b) a mathematically well-specified virtual environment that
contains an idealized computer with an arbitrarily large amount of memory and CPU
power. Given (a) and (b), we could define a utility function U as the output the
human brain would produce after interacting with this environment. U would be a
mathematically well-defined object, albeit one which (because of computational
limitations) we may be unable to describe explicitly. Nevertheless, U could serve as
the value criterion for a value learning AI, which could use various heuristics for
assigning probabilities to hypotheses about what U implies.
Intuitively, we want U to be the utility function that a suitably prepared human
would output if she had the advantage of being able use an arbitrarily large amount
of computing power—enough computing power, for example, to run astronomical
numbers of copies of herself to assist her with her analysis of specifying a utility
function, or to help her devise a better process for going about this analysis. (We are
here foreshadowing a theme, “coherent extrapolated volition,” which will be further
explored in Chapter 13.)
It would seem relatively easy to specify the idealized environment: we can give a
mathematical description of an abstract computer with arbitrarily large capacity; and
in other respects we could use a virtual reality program that gives a mathematical
description of, say, a single room with a computer terminal in it (instantiating the
abstract computer). But how to obtain a mathematically precise description of a
particular human brain? The obvious way would be through whole brain emulation,
but what if the technology for emulation is not available in time?
This is where Christiano’s proposal offers a key innovation. Christiano observes
that in order to obtain a mathematically well-specified value criterion, we do not
need a practically useful computational model of a mind, a model we could run. We
just need a (possibly implicit and hopelessly complicated) mathematical
definition—and this may be much easier to attain. Using functional neuroimagingand other measurements, we can perhaps collect gigabytes of data about the input–
output behavior of a selected human. If we collect a sufficient amount of data, then
it might be that the simplest mathematical model that accounts for all this data is in
fact an emulation of the particular human in question. Although it would be
computationally intractable for us to find this simplest model from the data, it could
be perfectly possible for us to define the model, by referring to the data and a using a
mathematically well-defined simplicity measure (such as some variant of the
Kolmogorov complexity, which we encountered in Box 1, Chapter 1). 27
Emulation modulation
The value-loading problem looks somewhat different for whole brain emulation than
it does for artificial intelligence. Methods that presuppose a fine-grained
understanding and control of algorithms and architecture are not applicable to
emulations. On the other hand, the augmentation motivation selection method—
inapplicable to de novo artificial intelligence—is available to be used with
emulations (or enhanced biological brains). 28
The augmentation method could be combined with techniques to tweak the
inherited goals of the system. For example, one could try to manipulate the
motivational state of an emulation by administering the digital equivalent of
psychoactive substances (or, in the case of biological systems, the actual chemicals).
Even now it is possible to pharmacologically manipulate values and motivations to a
limited extent. 29 The pharmacopeia of the future may contain drugs with more
specific and predictable effects. The digital medium of emulations should greatly
facilitate such developments, by making controlled experimentation easier and by
rendering all cerebral parts directly addressable.
Just as when biological test subjects are used, research on emulations would get
entangled in ethical complications, not all of which could be brushed aside with a
consent form. Such entanglements could slow progress along the emulation path
(because of regulation or moral restraint), perhaps especially hindering studies on
how to manipulate the motivational structure of emulations. The result could be that
emulations are augmented to potentially dangerous superintelligent levels of
cognitive ability before adequate work has been done to test or adjust their final
goals. Another possible effect of the moral entanglements might be to give the lead
to less scrupulous teams and nations. Conversely, were we to relax our moral
standards for experimenting with digital human minds, we could become responsible
for a substantial amount of harm and wrongdoing, which is obviously undesirable.Other things equal, these considerations favor taking some alternative path that does
not require the extensive use of digital human research subjects in a strategically
high-stakes situation.
The issue, however, is not clear-cut. One could argue that whole brain emulation
research is less likely to involve moral violations than artificial intelligence
research, on the grounds that we are more likely to recognize when an emulation
mind qualifies for moral status than we are to recognize when a completely alien or
synthetic mind does so. If certain kinds of AIs, or their subprocesses, have a
significant moral status that we fail to recognize, the consequent moral violations
could be extensive. Consider, for example, the happy abandon with which
contemporary programmers create reinforcement-learning agents and subject them
to aversive stimuli. Countless such agents are created daily, not only in computer
science laboratories but in many applications, including some computer games
containing sophisticated non-player characters. Presumably, these agents are still too
primitive to have any moral status. But how confident can we really be that this is
so? More importantly, how confident can we be that we will know to stop in time,
before our programs become capable of experiencing morally relevant suffering?
(We will return in Chapter 14 to some of the broader strategic questions that arise
when we compare the desirability of emulation and artificial intelligence paths.)
Institution design
Some intelligent systems consist of intelligent parts that are themselves capable of
agency. Firms and states exemplify this in the human world: whilst largely
composed of humans they can, for some purposes, be viewed as autonomous agents
in their own right. The motivations of such composite systems depend not only on
the motivations of their constituent subagents but also on how those subagents are
organized. For instance, a group that is organized under strong dictatorship might
behave as if it had a will that was identical to the will of the subagent that occupies
the dictator role, whereas a democratic group might sometimes behave more as if it
had a will that was a composite or average of the wills of its various constituents.
But one can also imagine governance institutions that would make an organization
behave in a way that is not a simple function of the wills of its subagents.
(Theoretically, at least, there could exist a totalitarian state that everybody hated,
because the state had mechanisms to prevent its citizens from coordinating a revolt.
Each citizen could be worse off by revolting alone than by playing their part in the
state machinery.)
By designing appropriate institutions for a composite system, one could thus try
to shape its effective motivation. In Chapter 9, we discussed social integration as apossible capability control method. But there we focused on the incentives faced by
an agent as a consequence of its existence in a social world of near-equals. Here we
are focusing on what happens inside a given agent: how its will is determined by its
internal organization. We are therefore looking at a motivation selection method.
Moreover, since this kind of internal institution design does not depend on large-
scale social engineering or reform, it is a method that might be available to an
individual project developing superintelligence even if the wider socioeconomic or
international milieu is less than ideally favorable.
Institution design is perhaps most plausible in contexts where it would be
combined with augmentation. If we could start with agents that are already suitably
motivated or that have human-like motivations, institutional arrangements could be
used as an extra safeguard to increase the chances that the system will stay on
course.
For example, suppose that we start with some well-motivated human-like agents
—let us say emulations. We want to boost the cognitive capacities of these agents,
but we worry that the enhancements might corrupt their motivations. One way to
deal with this challenge would be to set up a system in which individual emulations
function as subagents. When a new enhancement is introduced, it is first applied to a
small subset of the subagents. Its effects are then studied by a review panel
composed of subagents who have not yet had the enhancement applied to them. Only
when these peers have satisfied themselves that the enhancement is not corrupting is
it rolled out to the wider subagent population. If the enhanced subagents are found to
be corrupted, they are not given further enhancements and are excluded from key
decision-making functions (at least until the system as a whole has advanced to a
point where the corrupted subagents can be safely reintegrated). 30 Although the
corrupted subagents might have gained some advantage from the enhancement, the
institutional structure in which they are embedded, and the fact that they constitute a
small minority of all subagents, would hopefully prevent them from seizing power
or spreading their corruption to the overall system. Thus, the collective intelligence
and capability of the system could be gradually enhanced in a sequence of small
steps, where the soundness of each step is verified by subagents only slightly less
capable than the new subagents that are introduced in that step.
The amount of safety obtainable by this kind of institution design is limited by the
accuracy of the tests that are used to evaluate the enhanced subagents. The problem
of test construction is complicated by the fact that the test batteries could only be
validated within the pre-existing range of cognitive ability yet must be applied to
agents outside this range. One could mitigate this problem by taking small
enhancement steps and by letting the test run for a long time. Such caution, however,
would raise the cost and slow progress (which, if a race dynamic is occurring, could
mean a project employing these safety measures would place itself at adisadvantage). Even with small steps and extensive testing, there would remain
some risk of subtle flaws being introduced, leading to a gradual corruption of the
goal system, a corruption that might become manifest only when the system reaches
superintelligence. This risk could be reduced by letting the testing condition extend
for many steps of cognitive enhancement, so that even agents that have been initially
certified as sound and that have received several subsequent enhancements would
continue to be under review by a panel of unmodified and uncorrupted original peers
and would remain subject to reversion.
Ideally, the testing should be ongoing rather than confined to an initial evaluation
period. Human-level subagents have the ability to strategize and might thus choose
to conceal certain goals while their behavior was being monitored. To enable
continuous testing, a hierarchy might be created in which subagents at a certain level
of capability are tasked with monitoring subagents at a somewhat higher level of
capability. At the bottom of the capability hierarchy (but at the top of the power
hierarchy) would sit the relatively dumb and slow principal. This human principal
would be like a demented king who reigns over an incompetent court that oversees a
mediocre administration which governs a capable people. Alternatively, he might be
likened to the evolutionarily ancient “reptilian brain” which ensures that the newer
and much cleverer neocortex is employed in the service of goals such as feeding and
copulation.
One might question whether such an inverse meritocracy could remain stable. If
the bulk of the superintelligent subagents were found to be corrupt, and the humans
in charge of the whole thing attempted to roll back many of the preceding
enhancements, would the institutional structure be strong enough to withstand the
opposition that such a move would engender? In the context of historical or
contemporary human societies, the answer perhaps is that this would be implausible.
(Not that the most capable people consistently rise to power, but we are not literally
ruled by chimpanzees either.) The issue is less clear, however, in a context where
society is implemented as software. 31
Imagine a digital hierarchy that has one highly trained police agent for every two
proletarians. It also has one special-police agent for every two police agents. It also
has one special-special-police agent for every two special-police agents—and so
forth, so that each layer of overseers has at least half the numbers of the layer it
oversees. Imagine, further, that this society is architected in such a way that a
supervisor has big advantages over his subordinates. For example, the supervisor is
able to monitor and record everything his subordinates say and do, whereas the
supervisor himself is hidden behind a one-way glass, so that his subordinates can
only hear the orders and instructions that the superior chooses to transmit. Imagine
that supervisors have a panel of buttons that they can press at any time to cause a
subordinate to be punished or rewarded, paused, deleted, or reverted to an earlierstate. These capabilities would greatly boost the powers of a supervising agent,
beyond what has been possible historically in even the most totalitarian of
organizations.
Even this does not exhaust the possible instruments of control. Supervisors might
also have at their disposal realistic virtual reality simulations that they can use to
probe subordinates’ reactions to hypothetical scenarios (such as simulated offers to
join a conspiracy or to slack when they think nobody is watching). Supervisors
might, additionally, have the ability to eavesdrop on internal monologues in the
subordinates’ minds and to directly manipulate their emotional states.
The upshot is a series of massive advantages for the supervisor. 32 Each supervisor
would be a subordinate to another, higher-level supervisor, who would monitor his
every move and make sure he did not slacken in the execution of his managerial
duties. With this kind of arrangement, it might be possible for the structure to be
stable even if each supervisory level is intellectually somewhat weaker than the
level it supervises. 33 And the entire structure, which could include many highly
superintelligent agents, may be controlled by a small number of humans with root
access. 34
The cost of such an internal control structure would be significant but not
necessarily prohibitive. For example, in the model where each level of supervision
has half the numbers of the layer below, the extra computational overhead is
bounded at a mere 100% of what the proletarian part of the system costs—less if the
dumber boss layers require fewer computations per subagent. Oversight could be
tightened by increasing the ratio, or cost lowered by decreasing it. The extra
computational cost would be a major factor in a competitive market scenario (cf.
Chapter 11) but looks affordable in the context of a project that is not facing strong
immediate competition. There would also be a cost in terms of the development time
needed to create and test these supervisory functionalities. A well-resourced project
could reduce this time cost by parallelizing the development of the control structure
with the development of the machine intelligence; but the extra task load could be
prohibitive for smaller projects and for projects caught in a close technology race.
One other type of cost also deserves consideration: the risk of mind crimes being
committed in this kind of structure. 35 As described, the institution sounds like a
rather horrible North Korean labor camp. Yet there are ways of at least mitigating
the moral problems with running this kind of institution, even if the subagents
contained in the institution are emulations with full human moral status. At a
minimum, the system could rely on volunteering emulations. Each subagent could
have the option at any time of withdrawing its participation. 36 Terminated
emulations could be stored to memory, with a commitment to restart them under
much more ideal conditions once the dangerous phase of the intelligence explosion
is over. Meanwhile, subagents who chose to participate could be housed in verycomfortable virtual environments and allowed ample time for sleep and recreation.
These measures would impose a cost, one that should be manageable for a well-
resourced project under noncompetitive conditions. In a highly competitive
situation, the cost may be unaffordable unless an enterprise could be assured that its
competitors would incur the same cost.
In the example, we imagined the subagents as emulations. One might wonder,
does the institution design approach require that the subagents be anthropomorphic?
Or is it equally applicable to systems composed of artificial subagents?
One’s first thought here might be skeptical. One notes that despite our plentiful
experience with human-like agents, we still cannot precisely predict the outbreak or
outcomes of revolutions; social science can, at most, describe some statistical
tendencies. 37 Since we cannot reliably predict the stability of social structures for
ordinary human beings (about which we have much data), it is tempting to infer that
we have little hope of precision-engineering stable social structures for cognitively
enhanced human-like agents (about which we have no data), and that we have still
less hope of doing so for advanced artificial agents (which are not even similar to
agents that we have data about).
Yet the matter is not so cut-and-dried. Humans and human-like beings are
complex; but artificial agents could have relatively simple architectures. Artificial
agents could also have simple and explicitly characterized motivations.
Furthermore, digital agents in general (whether emulations or artificial
intelligences) are copyable: an affordance that may revolutionize management,
much like interchangeable parts revolutionized manufacturing. These differences,
together with the opportunity to work with agents that are initially powerless and to
create institutional structures that use the various abovementioned control measures,
might combine to make it possible to achieve particular institutional outcomes—
such as a system that does not revolt—more reliably than if one were working with
human beings under historical conditions.
But then again, artificial agents might lack many of the attributes that help us
predict the behavior of human-like agents. Artificial agents need not have any of the
social emotions that bind human behavior, emotions such as fear, pride, and
remorse. Nor need artificial agents develop attachments to friends and family. Nor
need they exhibit the unconscious body language that makes it difficult for us
humans to conceal our intentions. These deficits might destabilize institutions of
artificial agents. Moreover, artificial agents might be capable of making big leaps in
cognitive performance as a result of seemingly small changes in their algorithms or
architecture. Ruthlessly optimizing artificial agents might be willing to take extreme
gambles from which humans would shrink. 38 And superintelligent agents might
show a surprising ability to coordinate with little or no communication (e.g. by
internally modeling each other’s hypothetical responses to various contingencies).These and other differences could make sudden institutional failure more likely,
even in the teeth of what seem like Kevlar-clad methods of social control.
It is unclear, therefore, how promising the institution design approach is, and
whether it has a greater chance of working with anthropomorphic than with artificial
agents. It might be thought that creating an institution with appropriate checks and
balances could only increase safety—or, at any rate, not reduce safety—so that from
a risk-mitigation perspective it would always be best if the method were used. But
even this cannot be said with certainty. The approach adds parts and complexity, and
thus may also introduce new ways for things to go wrong that do not exist in the case
of an agent that does not have intelligent subagents as parts. Nevertheless, institution
design is worthy of further exploration. 39
Synopsis
Goal system engineering is not yet an established discipline. It is not currently
known how to transfer human values to a digital computer, even given human-level
machine intelligence. Having investigated a number of approaches, we found that
some of them appear to be dead ends; but others appear to hold promise and deserve
to be explored further. A summary is provided in Table 12.
Table 12 Summary of value-loading techniques
Explicit
May hold promise as a way of loading domesticity values. Does not
representation seem promising as a way of loading more complex values.
Less promising. Powerful search may find a design that satisfies the
formal search criteria but not our intentions. Furthermore, if designs
Evolutionary are evaluated by running them—including designs that do not even
selection
meet the formal criteria—a potentially grave additional danger is
created. Evolution also makes it difficult to avoid massive mind
crime, especially if one is aiming to fashion human-like minds.
A range of different methods can be used to solve “reinforcement-
learning problems,” but they typically involve creating a system that
Reinforcement seeks to maximize a reward signal. This has an inherent tendency to
learning
produce the wireheading failure mode when the system becomes
more intelligent. Reinforcement learning therefore looks
unpromising.
We humans acquire much of our specific goal content from ourValue
accretion
reactions to experience. While value accretion could in principle be
used to create an agent with human motivations, the human value-
accretion dispositions might be complex and difficult to replicate in
a seed AI. A bad approximation may yield an AI that generalizes
differently than humans do and therefore acquires unintended final
goals. More research is needed to determine how difficult it would
be
make
value
accretion
work with
sufficient
It is to too
early
to tell
how difficult
it would
be to precision.
encourage a system
to develop internal high-level representations that are transparent to
humans (while keeping the system’s capabilities below the
dangerous level) and then to use those representations to design a
Motivational new goal system. The approach might hold considerable promise.
scaffolding
(However, as with any untested approach that would postpone much
of the hard work on safety engineering until the development of
human-level AI, one should be careful not to allow it to become an
excuse for a lackadaisical attitude to the control problem in the
interim.)
A potentially promising approach, but more research is needed to
determine how difficult it would be to formally specify a reference
that successfully points to the relevant external information about
human value (and how difficult it would be to specify a correctness
Value learning
criterion for a utility function in terms of such a reference). Also
worth exploring within the value learning category are proposals of
the Hail Mary type or along the lines of Paul Christiano’s
construction (or other such shortcuts).
If machine intelligence is achieved via the emulation pathway, it
would likely be possible to tweak motivations through the digital
equivalent of drugs or by other means. Whether this would enable
Emulation
values to be loaded with sufficient precision to ensure safety even as
modulation
the emulation is boosted to superintelligence is an open question.
(Ethical constraints might also complicate developments in this
direction.)
Institution
design
Various strong methods of social control could be applied in an
institution composed of emulations. In principle, social control
methods could also be applied in an institution composed of
artificial intelligences. Emulations have some properties that would
make them easier to control via such methods, but also some
properties that might make them harder to control than AIs.
Institution design seems worthy of further exploration as a potential
value-loading technique.If we knew how to solve the value-loading problem, we would confront a further
problem: the problem of deciding which values to load. What, in other words, would
we want a superintelligence to want? This is the more philosophical problem to
which we turn next.CHAPTER 13
Choosing the criteria for choosing
Suppose we could install any arbitrary final value into a seed AI. The decision as
to which value to install could then have the most far-reaching consequences.
Certain other basic parameter choices—concerning the axioms of the AI’s
decision theory and epistemology—could be similarly consequential. But
foolish, ignorant, and narrow-minded that we are, how could we be trusted to
make good design decisions? How could we choose without locking in forever
the prejudices and preconceptions of the present generation? In this chapter, we
explore how indirect normativity can let us offload much of the cognitive work
involved in making these decisions onto the superintelligence itself while still
anchoring the outcome in deeper human values.
The need for indirect normativity
How can we get a superintelligence to do what we want? What do we want the
superintelligence to want? Up to this point, we have focused on the former question.
We now turn to the second question.
Suppose that we had solved the control problem so that we were able to load any
value we chose into the motivation system of a superintelligence, making it pursue
that value as its final goal. Which value should we install? The choice is no light
matter. If the superintelligence obtains a decisive strategic advantage, the value
would determine the disposition of the cosmic endowment.
Clearly, it is essential that we not make a mistake in our value selection. But how
could we realistically hope to achieve errorlessness in a matter like this? We might
be wrong about morality; wrong also about what is good for us; wrong even about
what we truly want. Specifying a final goal, it seems, requires making one’s way
through a thicket of thorny philosophical problems. If we try a direct approach, we
are likely to make a hash of things. The risk of mistaken choosing is especially high
when the decision context is unfamiliar—and selecting the final goal for a machine
superintelligence that will shape all of humanity’s future is an extremely unfamiliar
decision context if any is.
The dismal odds in a frontal assault are reflected in the pervasive dissensus about
the relevant issues in value theory. No ethical theory commands majority support
among philosophers, so most philosophers must be wrong. 1 It is also reflected in themarked changes that the distribution of moral belief has undergone over time, many
of which we like to think of as progress. In medieval Europe, for instance, it was
deemed respectable entertainment to watch a political prisoner being tortured to
death. Cat-burning remained popular in sixteenth-century Paris. 2 A mere hundred
and fifty years ago, slavery still was widely practiced in the American South, with
full support of the law and moral custom. When we look back, we see glaring
deficiencies not just in the behavior but in the moral beliefs of all previous ages.
Though we have perhaps since gleaned some moral insight, we could hardly claim to
be now basking in the high noon of perfect moral enlightenment. Very likely, we are
still laboring under one or more grave moral misconceptions. In such circumstances
to select a final value based on our current convictions, in a way that locks it in
forever and precludes any possibility of further ethical progress, would be to risk an
existential moral calamity.
Even if we could be rationally confident that we have identified the correct ethical
theory—which we cannot be—we would still remain at risk of making mistakes in
developing important details of this theory. Seemingly simple moral theories can
have a lot of hidden complexity. 3 For example, consider the (unusually simple)
consequentialist theory of hedonism. This theory states, roughly, that all and only
pleasure has value, and all and only pain has disvalue. 4 Even if we placed all our
moral chips on this one theory, and the theory turned out to be right, a great many
questions would remain open. Should “higher pleasures” be given priority over
“lower pleasures,” as John Stuart Mill argued? How should the intensity and
duration of a pleasure be factored in? Can pains and pleasures cancel each other out?
What kinds of brain states are associated with morally relevant pleasures? Would
two exact copies of the same brain state correspond to twice the amount of
pleasure? 5 Can there be subconscious pleasures? How should we deal with extremely
small chances of extremely great pleasures? 6 How should we aggregate over infinite
populations? 7
Giving the wrong answer to any one of these questions could be catastrophic. If by
selecting a final value for the superintelligence we had to place a bet not just on a
general moral theory but on a long conjunction of specific claims about how that
theory is to be interpreted and integrated into an effective decision-making process,
then our chances of striking lucky would dwindle to something close to hopeless.
Fools might eagerly accept this challenge of solving in one swing all the important
problems in moral philosophy, in order to infix their favorite answers into the seed
AI. Wiser souls would look hard for some alternative approach, some way to hedge.
This takes us to indirect normativity. The obvious reason for building a
superintelligence is so that we can offload to it the instrumental reasoning required
to find effective ways of realizing a given value. Indirect normativity would enable
us also to offload to the superintelligence some of the reasoning needed to select thevalue that is to be realized.
Indirect normativity is a way to answer the challenge presented by the fact that we
may not know what we truly want, what is in our interest, or what is morally right or
ideal. Instead of making a guess based on our own current understanding (which is
probably deeply flawed), we would delegate some of the cognitive work required for
value selection to the superintelligence. Since the superintelligence is better at
cognitive work than we are, it may see past the errors and confusions that cloud our
thinking. One could generalize this idea and emboss it as a heuristic principle:
The principle of epistemic deference
A future superintelligence occupies an epistemically superior vantage point: its
beliefs are (probably, on most topics) more likely than ours to be true. We
should therefore defer to the superintelligence’s opinion whenever feasible. 8
Indirect normativity applies this principle to the value-selection problem. Lacking
confidence in our ability to specify a concrete normative standard, we would instead
specify some more abstract condition that any normative standard should satisfy, in
the hope that a superintelligence could find a concrete standard that satisfies the
abstract condition. We could give a seed AI the final goal of continuously acting
according to its best estimate of what this implicitly defined standard would have it
do.
Some examples will serve to make the idea clearer. First we will consider
“coherent extrapolated volition,” an indirect normativity proposal outlined by
Eliezer Yudkowsky. We will then introduce some variations and alternatives, to give
us a sense of the range of available options.
Coherent extrapolated volition
Yudkowsky has proposed that a seed AI be given the final goal of carrying out
humanity’s “coherent extrapolated volition” (CEV), which he defines as follows:
Our coherent extrapolated volition is our wish if we knew more, thought faster,
were more the people we wished we were, had grown up farther together; where
the extrapolation converges rather than diverges, where our wishes cohere
rather than interfere; extrapolated as we wish that extrapolated, interpreted as
we wish that interpreted. 9When Yudkowsky wrote this, he did not purport to present a blueprint for how to
implement this rather poetic prescription. His aim was to give a preliminary sketch
of how CEV might be defined, along with some arguments for why an approach
along these lines is needed.
Many of the ideas behind the CEV proposal have analogs and antecedents in the
philosophical literature. For example, in ethics ideal observer theories seek to
analyze normative concepts like “good” or “right” in terms of the judgments that a
hypothetical ideal observer would make (where an “ideal observer” is defined as one
that is omniscient about non-moral facts, is logically clear-sighted, is impartial in
relevant ways and is free from various kinds of biases, and so on). 10 The CEV
approach, however, is not (or need not be construed as) a moral theory. It is not
committed to the claim that there is any necessary link between value and the
preferences of our coherent extrapolated volition. CEV can be thought of simply as a
useful way to approximate whatever has ultimate value, or it can be considered aside
from any connection to ethics. As the main prototype of the indirect normativity
approach, it is worth examining in a little more detail.
Some explications
Some terms in the above quotation require explication. “Thought faster,” in
Yudkowsky’s terminology, means if we were smarter and had thought things
through more . “Grown up farther together” seems to mean if we had done our
learning, our cognitive enhancing, and our self-improving under conditions of
suitable social interaction with one another.
“Where the extrapolation converges rather than diverges” may be understood as
follows. The AI should act on some feature of the result of its extrapolation only
insofar as that feature can be predicted by the AI with a fairly high degree of
confidence. To the extent that the AI cannot predict what we would wish if we were
idealized in the manner indicated, the AI should not act on a wild guess; instead, it
should refrain from acting. However, even though many details of our idealized
wishing may be undetermined or unpredictable, there might nevertheless be some
broad outlines that the AI can apprehend, and it can then at least act to ensure that
the future course of events unfolds within those outlines. For example, if the AI can
reliably estimate that our extrapolated volition would wish that we not all be in
constant agony, or that the universe not be tiled over with paperclips, then the AI
should act to prevent those outcomes. 11
“Where our wishes cohere rather than interfere” may be read as follows. The AI
should act where there is fairly broad agreement between individual humans’
extrapolated volitions. A smaller set of strong, clear wishes might sometimesoutweigh the weak and muddled wishes of a majority. Also, Yudkowsky thinks that
it should require less consensus for the AI to prevent some particular narrowly
specified outcome, and more consensus for the AI to act to funnel the future into
some particular narrow conception of the good. “The initial dynamic for CEV,” he
writes, “should be conservative about saying ‘yes,’ and listen carefully for ‘no.’” 12
“Extrapolated as we wish that extrapolated, interpreted as we wish that
interpreted”: The idea behind these last modifiers seems to be that the rules for
extrapolation should themselves be sensitive to the extrapolated volition. An
individual might have a second-order desire (a desire concerning what to desire) that
some of her first-order desires not be given weight when her volition is extrapolated.
For example, an alcoholic who has a first-order desire for booze might also have a
second-order desire not to have that first-order desire. Similarly, we might have
desires over how various other parts of the extrapolation process should unfold, and
these should be taken into account by the extrapolation process.
It might be objected that even if the concept of humanity’s coherent extrapolated
volition could be properly defined, it would anyway be impossible—even for a
superintelligence—to find out what humanity would actually want under the
hypothetical idealized circumstances stipulated in the CEV approach. Without some
information about the content of our extrapolated volition, the AI would be bereft of
any substantial standard to guide its behavior. However, although it would be
difficult to know with precision what humanity’s CEV would wish, it is possible to
make informed guesses. This is possible even today, without superintelligence. For
example, it is more plausible that our CEV would wish for there to be people in the
future who live rich and happy lives than that it would wish that we should all sit on
stools in a dark room experiencing pain. If we can make at least some such
judgments sensibly, so can a superintelligence. From the outset, the
superintelligence’s conduct could thus be guided by its estimates of the content of
our CEV. It would have strong instrumental reason to refine these initial estimates
(e.g. by studying human culture and psychology, scanning human brains, and
reasoning about how we might behave if we knew more, thought more clearly, etc.).
In investigating these matters, the AI would be guided by its initial estimates of our
CEV; so that, for instance, the AI would not unnecessarily run myriad simulations
replete with unredeemed human suffering if it estimated that our CEV would
probably condemn such simulations as mind crime.
Another objection is that there are so many different ways of life and moral codes
in the world that it might not be possible to “blend” them into one CEV. Even if one
could blend them, the result might not be particularly appetizing—one would be
unlikely to get a delicious meal by mixing together all the best flavors from
everyone’s different favorite dish. 13 In answer to this, one could point out that the
CEV approach does not require that all ways of life, moral codes, or personal valuesbe blended together into one stew. The CEV dynamic is supposed to act only when
our wishes cohere. On issues on which there is widespread irreconcilable
disagreement, even after the various idealizing conditions have been imposed, the
dynamic should refrain from determining the outcome. To continue the cooking
analogy, it might be that individuals or cultures will have different favorite dishes,
but that they can nevertheless broadly agree that aliments should be nontoxic. The
CEV dynamic could then act to prevent food poisoning while otherwise allowing
humans to work out their culinary practices without its guidance or interference.
Rationales for CEV
Yudkowsky’s article offered seven arguments for the CEV approach. Three of these
were basically different ways of making the point that while the aim should be to do
something that is humane and helpful, it would be very difficult to lay down an
explicit set of rules that does not have unintended interpretations and undesirable
consequences. 14 The CEV approach is meant to be robust and self-correcting; it is
meant to capture the source of our values instead of relying on us correctly
enumerating and articulating, once and for all, each of our essential values.
The remaining four arguments go beyond that first basic (but important) point,
spelling out desiderata on candidate solutions to the value-specification problem and
suggesting that CEV meets these desiderata.
“Encapsulate moral growth”
This is the desideratum that the solution should allow for the possibility of moral
progress. As suggested earlier, there are reasons to believe that our current moral
beliefs are flawed in many ways; perhaps deeply flawed. If we were to stipulate a
specific and unalterable moral code for the AI to follow, we would in effect be
locking in our present moral convictions, including their errors, destroying any hope
of moral growth. The CEV approach, by contrast, allows for the possibility of such
growth because it has the AI try to do that which we would have wished it to do if we
had developed further under favorable conditions, and it is possible that if we had
thus developed our moral beliefs and sensibilities would have been purged of their
current defects and limitations.
“Avoid hijacking the destiny of humankind”
Yudkowsky has in mind a scenario in which a small group of programmers creates a
seed AI that then grows into a superintelligence that obtains a decisive strategic
advantage. In this scenario, the original programmers hold in their hands the entirety
of humanity’s cosmic endowment. Obviously, this is a hideous responsibility for anymortal to shoulder. Yet it is not possible for the programmers to completely shirk
the onus once they find themselves in this situation: any choice they make, including
abandoning the project, would have world-historical consequences. Yudkowsky sees
CEV as a way for the programmers to avoid arrogating to themselves the privilege
or burden of determining humanity’s future. By setting up a dynamic that
implements humanity’s coherent extrapolated volition—as opposed to their own
volition, or their own favorite moral theory—they in effect distribute their influence
over the future to all of humanity.
“Avoid creating a motive for modern-day humans to fight over the initial dynamic”
Distributing influence over humanity’s future is not only morally preferable to the
programming team implementing their own favorite vision, it is also a way to reduce
the incentive to fight over who gets to create the first superintelligence. In the CEV
approach, the programmers (or their sponsors) exert no more influence over the
content of the outcome than any other person—though they of course play a starring
causal role in determining the structure of the extrapolation and in deciding to
implement humanity’s CEV instead of some alternative. Avoiding conflict is
important not only because of the immediate harm that conflict tends to cause but
also because it hinders collaboration on the difficult challenge of developing
superintelligence safely and beneficially.
CEV is meant to be capable of commanding wide support. This is not just because
it allocates influence equitably. There is also a deeper ground for the irenic potential
of CEV, namely that it enables many different groups to hope that their preferred
vision of the future will prevail totally. Imagine a member of the Afghan Taliban
debating with a member of the Swedish Humanist Association. The two have very
different worldviews, and what is a utopia for one might be a dystopia for the other.
Nor might either be thrilled by any compromise position, such as permitting girls to
receive an education but only up to ninth grade, or permitting Swedish girls to be
educated but Afghan girls not. However, both the Taliban and the Humanist might be
able to endorse the principle that the future should be determined by humanity’s
CEV. The Taliban could reason that if his religious views are in fact correct (as he is
convinced they are) and if good grounds for accepting these views exist (as he is also
convinced) then humankind would in the end come to accept these views if only
people were less prejudiced and biased, if they spent more time studying scripture, if
they could more clearly understand how the world works and recognize essential
priorities, if they could be freed from irrational rebelliousness and cowardice, and so
forth. 15 The Humanist, similarly, would believe that under these idealized
conditions, humankind would come to embrace the principles she espouses.
“Keep humankind ultimately in charge of its own destiny”We might not want an outcome in which a paternalistic superintelligence watches
over us constantly, micromanaging our affairs with an eye towards optimizing every
detail in accordance with a grand plan. Even if we stipulate that the superintelligence
would be perfectly benevolent, and free from presumptuousness, arrogance,
overbearingness, narrow-mindedness, and other human shortcomings, one might still
resent the loss of autonomy entailed by such an arrangement. We might prefer to
create our destiny as we go along, even if it means that we sometimes fumble.
Perhaps we want the superintelligence to serve as a safety net, to support us when
things go catastrophically wrong, but otherwise to leave us to fend for ourselves.
CEV allows for this possibility. CEV is meant to be an “initial dynamic,” a
process that runs once and then replaces itself with whatever the extrapolated
volition wishes. If humanity’s extrapolated volition wishes that we live under the
supervision of a paternalistic AI, then the CEV dynamic would create such an AI and
hand it the reins. If humanity’s extrapolated volition instead wishes that a
democratic human world government be created, then the CEV dynamic might
facilitate the establishment of such an institution and otherwise remain invisible. If
humanity’s extrapolated volition is instead that each person should get an
endowment of resources that she can use as she pleases so long as she respects the
equal rights of others, then the CEV dynamic could make this come true by
operating in the background much like a law of nature, to prevent trespass, theft,
assault, and other nonconsensual impingements. 16
The structure of the CEV approach thus allows for a virtually unlimited range of
outcomes. It is also conceivable that humanity’s extrapolated volition would wish
that the CEV does nothing at all. In that case, the AI implementing CEV should,
upon having established with sufficient probability that this is what humanity’s
extrapolated volition would wish it to do, safely shut itself down.
Further remarks
The CEV proposal, as outlined above, is of course the merest schematic. It has a
number of free parameters that could be specified in various ways, yielding different
versions of the proposal.
One parameter is the extrapolation base: Whose volitions are to be included? We
might say “everybody,” but this answer spawns a host of further questions. Does the
extrapolation base include so-called “marginal persons” such as embryos, fetuses,
brain-dead persons, patients with severe dementias or who are in permanent
vegetative states? Does each of the hemispheres of a “split-brain” patient get its own
weight in the extrapolation and is this weight the same as that of the entire brain of a
normal subject? What about people who lived in the past but are now dead? People
who will be born in the future? Higher animals and other sentient creatures? Digitalminds? Extraterrestrials?
One option would be to include only the population of adult human beings on
Earth who are alive at the start of the time of the AI’s creation. An initial
extrapolation from this base could then decide whether and how the base should be
expanded. Since the number of “marginals” at the periphery of this base is relatively
small, the result of the extrapolation may not depend much on exactly where the
boundary is drawn—on whether, for instance, it includes fetuses or not.
That somebody is excluded from the original extrapolation base does not imply
that their wishes and well-being are disregarded. If the coherent extrapolated
volition of those in the extrapolation base (e.g. living adult human beings) wishes
that moral consideration be extended to other beings, then the outcome of the CEV
dynamic would reflect that preference. Nevertheless, it is possible that the interests
of those who are included in the original extrapolation base would be accommodated
to a greater degree than the interests of outsiders. In particular, if the dynamic acts
only where there is broad agreement between individual extrapolated volitions (as in
Yudkowsky’s original proposal), there would seem to be a significant risk of an
ungenerous blocking vote that could prevent, for instance, the welfare of nonhuman
animals or digital minds from being protected. The result might potentially be
morally rotten. 17
One motivation for the CEV proposal was to avoid creating a motive for humans
to fight over the creation of the first superintelligent AI. Although the CEV proposal
scores better on this desideratum than many alternatives, it does not entirely
eliminate motives for conflict. A selfish individual, group, or nation might seek to
enlarge its slice of the future by keeping others out of the extrapolation base.
A power grab of this sort might be rationalized in various ways. It might be
argued, for instance, that the sponsor who funds the development of the AI deserves
to own the outcome. This moral claim is probably false. It could be objected, for
example, that the project that launches the first successful seed AI imposes a vast
risk externality on the rest of humanity, which therefore is entitled to compensation.
The amount of compensation owed is so great that it can only take the form of
giving everybody a stake in the upside if things turn out well. 18
Another argument that might be used to rationalize a power grab is that large
segments of humanity have base or evil preferences and that including them in the
extrapolation base would risk turning humanity’s future into a dystopia. It is
difficult to know the share of good and bad in the average person’s heart. It is also
difficult to know how much this balance varies between different groups, social
strata, cultures, or nations. Whether one is optimistic or pessimistic about human
nature, one may prefer not to wager humanity’s cosmic endowment on the
speculation that, for a sufficient majority of the seven billion people currently alive,
their better angels would prevail in their extrapolated volitions. Of course, omittinga certain set of people from the extrapolation base does not guarantee that light
would triumph; and it might well be that the souls that would soonest exclude others
or grab power for themselves tend rather to contain unusually large amounts of
darkness.
Yet another reason for fighting over the initial dynamic is that one might believe
that somebody else’s AI will not work as advertised, even if the AI is billed as a way
to implement humanity’s CEV. If different groups have different beliefs about
which implementation is most likely to succeed, they might fight to prevent the
others from launching. It would be better in such situations if the competing projects
could settle their epistemic differences by some method that more reliably
ascertains who is right than the method of armed conflict. 19
Morality models
The CEV proposal is not the only possible form of indirect normativity. For
example, instead of implementing humanity’s coherent extrapolated volition, one
could try to build an AI with the goal of doing what is morally right, relying on the
AI’s superior cognitive capacities to figure out just which actions fit that
description. We can call this proposal “moral rightness” (MR). The idea is that we
humans have an imperfect understanding of what is right and wrong, and perhaps an
even poorer understanding of how the concept of moral rightness is to be
philosophically analyzed: but a superintelligence could understand these things
better. 20
What if we are not sure whether moral realism is true? We could still attempt the
MR proposal. We should just have to make sure to specify what the AI should do in
the eventuality that its presupposition of moral realism is false. For example, we
could stipulate that if the AI estimates with a sufficient probability that there are no
suitable non-relative truths about moral rightness, then it should revert to
implementing coherent extrapolated volition instead, or simply shut itself down. 21
MR appears to have several advantages over CEV. MR would do away with
various free parameters in CEV, such as the degree of coherence among extrapolated
volitions that is required for the AI to act on the result, the ease with which a
majority can overrule dissenting minorities, and the nature of the social environment
within which our extrapolated selves are to be supposed to have “grown up farther
together.” It would seem to eliminate the possibility of a moral failure resulting
from the use of an extrapolation base that is too narrow or too wide. Furthermore,
MR would orient the AI toward morally right action even if our coherent
extrapolated volitions happen to wish for the AI to take actions that are morally
odious. As noted earlier, this seems a live possibility with the CEV proposal. Moralgoodness might be more like a precious metal than an abundant element in human
nature, and even after the ore has been processed and refined in accordance with the
prescriptions of the CEV proposal, who knows whether the principal outcome will be
shining virtue, indifferent slag, or toxic sludge?
MR would also appear to have some disadvantages. It relies on the notion of
“morally right,” a notoriously difficult concept, one with which philosophers have
grappled since antiquity without yet attaining consensus as to its analysis. Picking an
erroneous explication of “moral rightness” could result in outcomes that would be
morally very wrong. This difficulty of defining “moral rightness” might seem to
count heavily against the MR proposal. However, it is not clear that the MR proposal
is really at a material disadvantage in this regard. The CEV proposal, too, uses terms
and concepts that are difficult to explicate (such as “knowledge,” “being more the
people we wished we were,” “grown up farther together,” among others). 22 Even if
these concepts are marginally less opaque than “moral rightness,” they are still
miles removed from anything that programmers can currently express in code. 23 The
path to endowing an AI with any of these concepts might involve giving it general
linguistic ability (comparable, at least, to that of a normal human adult). Such a
general ability to understand natural language could then be used to understand what
is meant by “morally right.” If the AI could grasp the meaning, it could search for
actions that fit. As the AI develops superintelligence, it could then make progress on
two fronts: on the philosophical problem of understanding what moral rightness is,
and on the practical problem of applying this understanding to evaluate particular
actions. 24 While this would not be easy, it is not clear that it would be any more
difficult than extrapolating humanity’s coherent extrapolated volition. 25
A more fundamental issue with MR is that even if can be implemented, it might
not give us what we want or what we would choose if we were brighter and better
informed. This is of course the essential feature of MR, not an accidental bug.
However, it might be a feature that would be extremely harmful to us. 26
One might try to preserve the basic idea of the MR model while reducing its
demandingness by focusing on moral permissibility: the idea being that we could let
the AI pursue humanity’s CEV so long as it did not act in ways that are morally
impermissible. For example, one might formulate the following goal for the AI:
Among the actions that are morally permissible for the AI, take one that
humanity’s CEV would prefer. However, if some part of this instruction has no
well-specified meaning, or if we are radically confused about its meaning, or if
moral realism is false, or if we acted morally impermissibly in creating an AI
with this goal, then undergo a controlled shutdown. 27 Follow the intended
meaning of this instruction.One might still worry that this moral permissibility model (MP) represents an
unpalatably high degree of respect for the requirements of morality. How big a
sacrifice it would entail depends on which ethical theory is true. 28 If ethics is
satisficing, in the sense that it counts as morally permissible any action that
conforms to a few basic moral constraints, then MP may leave ample room for our
coherent extrapolated volition to influence the AI’s actions. However, if ethics is
maximizing—for example, if the only morally permissible actions are those that
have the morally best consequences—then MP may leave little or no room for our
own preferences to shape the outcome.
To illustrate this concern, let us return for a moment to the example of hedonistic
consequentialism. Suppose that this ethical theory is true, and that the AI knows it to
be so. For present purposes, we can define hedonistic consequentialism as the claim
that an action is morally right (and morally permissible) if and only if, among all
feasible actions, no other action would produce a greater balance of pleasure over
suffering. The AI, following MP, might maximize the surfeit of pleasure by
converting the accessible universe into hedonium, a process that may involve
building computronium and using it to perform computations that instantiate
pleasurable experiences. Since simulating any existing human brain is not the most
efficient way of producing pleasure, a likely consequence is that we all die.
By enacting either the MR or the MP proposal, we would thus risk sacrificing our
lives for a greater good. This would be a bigger sacrifice than one might think,
because what we stand to lose is not merely the chance to live out a normal human
life but the opportunity to enjoy the far longer and richer lives that a friendly
superintelligence could bestow.
The sacrifice looks even less appealing when we reflect that the superintelligence
could realize a nearly-as-great good (in fractional terms) while sacrificing much less
of our own potential well-being. Suppose that we agreed to allow almost the entire
accessible universe to be converted into hedonium—everything except a small
preserve, say the Milky Way, which would be set aside to accommodate our own
needs. Then there would still be a hundred billion galaxies devoted to the
maximization of pleasure. But we would have one galaxy within which to create
wonderful civilizations that could last for billions of years and in which humans and
nonhuman animals could survive and thrive, and have the opportunity to develop
into beatific posthuman spirits. 29
If one prefers this latter option (as I would be inclined to do) it implies that one
does not have an unconditional lexically dominant preference for acting morally
permissibly. But it is consistent with placing great weight on morality.
Even from a purely moral point of view, it might be better to advocate some
proposal that is less morally ambitious than MR or MP. If the morally best has nochance of being implemented—perhaps because of its frowning demandingness—it
might be morally better to promote some other proposal, one that would be near-
ideal and whose chances of being implemented could be significantly increased by
our promoting it. 30
Do What I Mean
We might feel unsure whether to go for CEV, MR, MP, or something else. Could we
punt on this higher-level decision as well, offloading even more cognitive work onto
the AI? Where is the limit to our possible laziness?
Consider, for example, the following “reasons-based” goal:
Do whatever we would have had most reason to ask the AI to do.
This goal might boil down to extrapolated volition or morality or something else,
but it would seem to spare us the effort and risk involved in trying to figure out for
ourselves which of these more specific objectives we would have most reason to
select.
Some of the problems with the morality-based goals, however, also apply here.
First, we might fear that this reasons-based goal would leave too little room for our
own desires. Some philosophers maintain that a person always has most reason to do
what it would be morally best for her to do. If those philosophers are right, then the
reason-based goal collapses into MR—with the concomitant risk that a
superintelligence implementing such a dynamic would kill everyone within reach.
Second, as with all proposals couched in technical language, there is a possibility
that we might have misunderstood the meaning of our own assertions. We saw that,
in the case of the morality-based goals, asking the AI to do what is right may lead to
unforeseen and unwanted consequences such that, had we anticipated them, we
would not have implemented the goal in question. The same applies to asking the AI
to do what we have most reason to do.
What if we try to avoid these difficulties by couching a goal in emphatically
nontechnical language—such as in terms of “niceness”: 31
Take the nicest action; or, if no action is nicest, then take an action that is at
least super-duper nice.
How could there be anything objectionable about building a nice AI? But we must
ask what precisely is meant by this expression. The lexicon lists various meanings of“nice” that are clearly not intended to be used here: we do not intend that the AI
should be courteous and polite nor overdelicate or fastidious. If we can count on the
AI recognizing the intended interpretation of “niceness” and being motivated to
pursue niceness in just that sense, then this goal would seem to amount to a
command to do what the programmers meant for the AI to do. 32 An injunction to
similar effect was included in the formulation of CEV (“... interpreted as we wish
that interpreted”) and in the moral-permissibility criterion as rendered earlier (“...
follow the intended meaning of this instruction”). By affixing such a “Do What I
Mean” clause we may indicate that the other words in the goal description should be
construed charitably rather than literally. But saying that the AI should be “nice”
adds almost nothing: the real work is done by the “Do What I Mean” instruction. If
we knew how to code “Do What I Mean” in a general and powerful way, we might as
well use that as a standalone goal.
How might one implement such a “Do What I Mean” dynamic? That is, how
might we create an AI motivated to charitably interpret our wishes and unspoken
intentions and to act accordingly? One initial step could be to try to get clearer about
what we mean by “Do What I Mean.” It might help if we could explicate this in
more behavioristic terms, for example in terms of revealed preferences in various
hypothetical situations—such as situations in which we had more time to consider
the options, in which we were smarter, in which we knew more of the relevant facts,
and in which in various other ways conditions would be more favorable for us
accurately manifesting in concrete choices what we mean when we say that we want
an AI that is friendly, beneficial, nice ...
Here, of course, we come full circle. We have returned to the indirect normativity
approach with which we started—the CEV proposal, which, in essence, expunges all
concrete content from the value specification, leaving only an abstract value defined
in purely procedural terms: to do that which we would have wished for the AI to do
in suitably idealized circumstances. By means of such indirect normativity, we could
hope to offload to the AI much of the cognitive work that we ourselves would be
trying to perform if we attempted to articulate a more concrete description of what
values the AI is to pursue. In seeking to take full advantage of the AI’s epistemic
superiority, CEV can thus be seen as an application of the principle of epistemic
deference.
Component list
So far we have considered different options for what content to put into the goal
system. But an AI’s behavior will also be influenced by other design choices. In
particular, it can make a critical difference which decision theory and whichepistemology it uses. Another important question is whether the AI’s plans will be
subject to human review before being put into action.
Table 13 summarizes these design choices. A project that aims to build a
superintelligence ought to be able to explain what choices it has made regarding
each of these components, and to justify why those choices were made. 33
Table 13 Component list
What objective should the AI pursue? How should a description of
Goal content this objective be interpreted? Should the objective include giving
special rewards to those who contributed to the project’s success?
Decision
Should the AI use causal decision theory, evidential decision theory,
theory
updateless decision theory, or something else?
What should the AI’s prior probability function be, and what other
Epistemology explicit or implicit assumptions about the world should it make?
What theory of anthropics should it use?
Should the AI’s plans be subjected to human review before being put
Ratification
into effect? If so, what is the protocol for that review process?
Goal content
We have already discussed how indirect normativity might be used in specifying the
values that the AI is to pursue. We discussed some options, such as morality-based
models and coherent extrapolated volition. Each such option creates further choices
that need to be made. For instance, the CEV approach comes in many varieties,
depending on who is included in the extrapolation base, the structure of the
extrapolation, and so forth. Other forms of motivation selection methods might call
for different types of goal content. For example, an oracle might be built to place a
value on giving accurate answers. An oracle constructed with domesticity
motivation might also have goal content that disvalues the excessive use of
resources in producing its answers.
Another design choice is whether to include special provisions in the goal content
to reward individuals who contribute to the successful realization of the AI, for
example by giving them extra resources or influence over the AI’s behavior. We can
term any such provisions “incentive wrapping.” Incentive wrapping could be seen as
a way to increase the likelihood that the project will be successful, at the cost of
compromising to some extent the goal that the project set out to achieve.
For example, if the project’s goal is to create a dynamic that implements
humanity’s coherent extrapolated volition, then an incentive wrapping scheme mightspecify that certain individuals’ volitions should be given extra weight in the
extrapolation. If such a project is successful, the result is not necessarily the
implementation of humanity’s coherent extrapolated volition. Instead, some
approximation to this goal might be achieved. 34
Since incentive wrapping would be a piece of goal content that would be
interpreted and pursued by a superintelligence, it could take advantage of indirect
normativity to specify subtle and complicated provisions that would be difficult for
a human manager to implement. For example, instead of rewarding programmers
according to some crude but easily accessible metric, such as how many hours they
worked or how many bugs they corrected, the incentive wrapping could specify that
programmers “are to be rewarded in proportion to how much their contributions
increased some reasonable ex ante probability of the project being successfully
completed in the way the sponsors intended.” Further, there would be no reason to
limit the incentive wrapping to project staff. It could instead specify that every
person should be rewarded according to their just deserts. Credit allocation is a
difficult problem, but a superintelligence could be expected to do a reasonable job of
approximating the criteria specified, explicitly or implicitly, by the incentive
wrapping.
It is conceivable that the superintelligence might even find some way of
rewarding individuals who have died prior to the superintelligence’s creation. 35 The
incentive wrapping could then be extended to embrace at least some of the deceased,
potentially including individuals who died before the project was conceived, or even
antedating the first enunciation of the concept of incentive wrapping. Although the
institution of such a retroactive policy would not causally incentivize those people
who are already resting in their graves as these words are being put to the page, it
might be favored for moral reasons—though it could be argued that insofar as
fairness is a goal, it should be included as part of the target specification proper
rather than in the surrounding incentive wrapping.
We cannot here delve into all the ethical and strategic issues associated with
incentive wrapping. A project’s position on these issues, however, would be an
important aspect of its fundamental design concept.
Decision theory
Another important design choice is which decision theory the AI should be built to
use. This might affect how the AI behaves in certain strategically fateful situations.
It might determine, for instance, whether the AI is open to trade with, or extortion
by, other superintelligent civilizations whose existence it hypothesizes. The
particulars of the decision theory could also matter in predicaments involving finiteprobabilities of infinite payoffs (“Pascalian wagers”) or extremely small
probabilities of extremely large finite payoffs (“Pascalian muggings”) or in contexts
where the AI is facing fundamental normative uncertainty or where there are
multiple instantiations of the same agent program. 36
The options on the table include causal decision theory (in a variety of flavors)
and evidential decision theory, along with newer candidates such as “timeless
decision theory” and “updateless decision theory,” which are still under
development. 37 It may prove difficult to identify and articulate the correct decision
theory, and to have justified confidence that we have got it right. Although the
prospects for directly specifying an AI’s decision theory are perhaps more hopeful
than those of directly specifying its final values, we are still confronted with a
substantial risk of error. Many of the complications that might break the currently
most popular decision theories were discovered only recently, suggesting that there
might exist further problems that have not yet come into sight. The result of giving
the AI a flawed decision theory might be disastrous, possibly amounting to an
existential catastrophe.
In view of these difficulties, one might consider an indirect approach to
specifying the decision theory that the AI should use. Exactly how to do this is not
yet clear. We might want the AI to use “that decision theory D which we would have
wanted it to use had we thought long and hard about the matter.” However, the AI
would need to be able to make decisions before learning what D is. It would thus
need some effective interim decision theory D’ that would govern its search for D.
One might try to define D’ to be some sort of superposition of the AI’s current
hypotheses about D (weighed by their probabilities), though there are unsolved
technical problems with how to do this in a fully general way. 38 There is also cause
for concern that the AI might make irreversibly bad decisions (such as rewriting
itself to henceforth run on some flawed decision theory) during the learning phase,
before the AI has had the opportunity to determine which particular decision theory
is correct. To reduce the risk of derailment during this period of vulnerability we
might instead try to endow the seed AI with some form of restricted rationality: a
deliberately simplified but hopefully dependable decision theory that staunchly
ignores esoteric considerations, even ones we think may ultimately be legitimate,
and that is designed to replace itself with a more sophisticated (indirectly specified)
decision theory once certain conditions are met. 39 It is an open research question
whether and how this could be made to work.
Epistemology
A project will also need to make a fundamental design choice in selecting the AI’sepistemology, specifying the principles and criteria whereby empirical hypotheses
are to be evaluated. Within a Bayesian framework, we can think of the epistemology
as a prior probability function—the AI’s implicit assignment of probabilities to
possible worlds before it has taken any perceptual evidence into account. In other
frameworks, the epistemology might take a different form; but in any case some
inductive learning rule is necessary if the AI is to generalize from past observations
and make predictions about the future. 40 As with the goal content and the decision
theory, however, there is a risk that our epistemology specification could miss the
mark.
One might think that there is a limit to how much damage could arise from an
incorrectly specified epistemology. If the epistemology is too dysfunctional, then
the AI could not be very intelligent and it could not pose the kind of risk discussed
in this book. But the concern is that we may specify an epistemology that is
sufficiently sound to make the AI instrumentally effective in most situations, yet
which has some flaw that leads the AI astray on some matter of crucial importance.
Such an AI might be akin to a quick-witted person whose worldview is predicated on
a false dogma, held to with absolute conviction, who consequently “tilts at
windmills” and gives his all in pursuit of fantastical or harmful objectives.
Certain kinds of subtle difference in an AI’s prior could turn out to make a drastic
difference to how it behaves. For example, an AI might be given a prior that assigns
zero probability to the universe being infinite. No matter how much astronomical
evidence it accrues to the contrary, such an AI would stubbornly reject any
cosmological theory that implied an infinite universe; and it might make foolish
choices as a result. 41 Or an AI might be given a prior that assigns a zero probability
to the universe not being Turing-computable (this is in fact a common feature of
many of the priors discussed in the literature, including the Kolmogorov complexity
prior mentioned in Chapter 1), again with poorly understood consequences if the
embedded assumption—known as the “Church–Turing thesis”—should turn out to
be false. An AI could also end up with a prior that makes strong metaphysical
commitments of one sort or another, for instance by ruling out a priori the
possibility that any strong form of mind–body dualism could be true or the
possibility that there are irreducible moral facts. If any of those commitments is
mistaken, the AI might seek to realize its final goals in ways that we would regard as
perverse instantiations. Yet there is no obvious reason why such an AI, despite being
fundamentally wrong about one important matter, could not be sufficiently
instrumentally effective to secure a decisive strategic advantage. (Anthropics, the
study of how to make inferences from indexical information in the presence of
observation selection effects, is another area where the choice of epistemic axioms
could prove pivotal. 42 )
We might reasonably doubt our ability to resolve all foundational issues inepistemology in time for the construction of the first seed AI. We may, therefore,
consider taking an indirect approach to specifying the AI’s epistemology. This
would raise many of the same issues as taking an indirect approach to specifying its
decision theory. In the case of epistemology, however, there may be greater hope of
benign convergence, with any of a wide class of epistemologies providing an
adequate foundation for safe and effective AI and ultimately yielding similar
doxastic results. The reason for this is that sufficiently abundant empirical evidence
and analysis would tend to wash out any moderate differences in prior
expectations. 43
A good aim would be to endow the AI with fundamental epistemological
principles that match those governing our own thinking. Any AI diverging from this
ideal is an AI that we would judge to be reasoning incorrectly if we consistently
applied our own standards. Of course, this applies only to our fundamental
epistemological principles. Non-fundamental principles should be continuously
created and revised by the seed AI itself as it develops its understanding of the
world. The point of superintelligence is not to pander to human preconceptions but
to make mincemeat out of our ignorance and folly.
Ratification
The final item in our list of design choices is ratification. Should the AI’s plans be
subjected to human review before being put into effect? For an oracle, this question
is implicitly answered in the affirmative. The oracle outputs information; human
reviewers choose whether and how to act upon it. For genies, sovereigns, and tool-
AIs, however, the question of whether to use some form of ratification remains open.
To illustrate how ratification might work, consider an AI intended to function as a
sovereign implementing humanity’s CEV. Instead of launching this AI directly,
imagine that we first built an oracle AI for the sole purpose of answering questions
about what the sovereign AI would do. As earlier chapters revealed, there are risks in
creating a superintelligent oracle (such as risks of mind crime or infrastructure
profusion). But for purposes of this example let us assume that the oracle AI has
been successfully implemented in a way that avoided these pitfalls.
We thus have an oracle AI that offers us its best guesses about the consequences
of running some piece of code intended to implement humanity’s CEV. The oracle
may not be able to predict in detail what would happen, but its predictions are likely
to be better than our own. (If it were impossible even for a superintelligence to
predict anything about the code would do, we would be crazy to run it.) So the oracle
ponders for a while and then presents its forecast. To make the answer intelligible,
the oracle may offer the operator a range of tools with which to explore various
features of the predicted outcome. The oracle could show pictures of what the futurelooks like and provide statistics about the number of sentient beings that will exist at
different times, along with average, peak, and lowest levels of well-being. It could
offer intimate biographies of several randomly selected individuals (perhaps
imaginary people selected to be probably representative). It could highlight aspects
of the future that the operator might not have thought of inquiring about but which
would be regarded as pertinent once pointed out.
Being able to preview the outcome in this manner has obvious advantages. The
preview could reveal the consequences of an error in a planned sovereign’s design
specifications or source code. If the crystal ball shows a ruined future, we could
scrap the code for the planned sovereign AI and try something else. A strong case
could be made that we should familiarize ourselves with the concrete ramifications
of an option before committing to it, especially when the entire future of the race is
on the line.
What is perhaps less obvious is that ratification also has potentially significant
disadvantages. The irenic quality of CEV might be undermined if opposing factions,
instead of submitting to the arbitration of superior wisdom in confident expectation
of being vindicated, could see in advance what the verdict would be. A proponent of
the morality-based approach might worry that the sponsor’s resolve would collapse
if all the sacrifices required by the morally optimal were to be revealed. And we
might all have reason to prefer a future that holds some surprises, some dissonance,
some wildness, some opportunities for self-overcoming—a future whose contours
are not too snugly tailored to present preconceptions but provide some give for
dramatic movement and unplanned growth. We might be less likely to take such an
expansive view if we could cherry-pick every detail of the future, sending back to
the drawing board any draft that does not fully conform to our fancy at that moment.
The issue of sponsor ratification is therefore less clear-cut than it might initially
seem. Nevertheless, on balance it would seem prudent to take advantage of an
opportunity to preview, if that functionality is available. But rather than letting the
reviewer fine-tune every aspect of the outcome, we might give her a simple veto
which could be exercised only a few times before the entire project would be
aborted. 44
Getting close enough
The main purpose of ratification would be to reduce the probability of catastrophic
error. In general, it seems wise to aim at minimizing the risk of catastrophic error
rather than at maximizing the chance of every detail being fully optimized. There
are two reasons for this. First, humanity’s cosmic endowment is astronomically
large—there is plenty to go around even if our process involves some waste oraccepts some unnecessary constraints. Second, there is a hope that if we but get the
initial conditions for the intelligence explosion approximately right, then the
resulting superintelligence may eventually home in on, and precisely hit, our
ultimate objectives. The important thing is to land in the right attractor basin.
With regard to epistemology, it is plausible that a wide range of priors will
ultimately converge to very similar posteriors (when computed by a
superintelligence and conditionalized on a realistic amount of data). We therefore
need not worry about getting the epistemology exactly right. We must just avoid
giving the AI a prior that is so extreme as to render the AI incapable of learning vital
truths even with the benefit of copious experience and analysis. 45
With regard to decision theory, the risk of irrecoverable error seems larger. We
might still hope to directly specify a decision theory that is good enough. A
superintelligent AI could switch to a new decision theory at any time; however, if it
starts out with a sufficiently wrong decision theory it may not see the reason to
switch. Even if an agent comes to see the benefits of having a different decision
theory, the realization might come too late. For example, an agent designed to refuse
blackmail might enjoy the benefit of deterring would-be extortionists. For this
reason, blackmailable agents might do well to proactively adopt a non-exploitable
decision theory. Yet once a blackmailable agent receives the threat and regards it as
credible, the damage is done.
Given an adequate epistemology and decision theory, we could try to design the
system to implement CEV or some other indirectly specified goal content. Again
there is hope of convergence: that different ways of implementing a CEV-like
dynamic would lead to the same utopian outcome. Short of such convergence, we
may still hope that many of the different possible outcomes are good enough to
count as existential success.
It is not necessary for us to create a highly optimized design. Rather, our focus
should be on creating a highly reliable design, one that can be trusted to retain
enough sanity to recognize its own failings. An imperfect superintelligence, whose
fundamentals are sound, would gradually repair itself; and having done so, it would
exert as much beneficial optimization power on the world as if it had been perfect
from the outset.CHAPTER 14
The strategic picture
It is now time to consider the challenge of superintelligence in a broader
context. We would like to orient ourselves in the strategic landscape sufficiently
to know at least which general direction we should be heading. This, it turns out,
is not at all easy. Here in the penultimate chapter, we introduce some general
analytical concepts that help us think about long-term science and technology
policy issues. We then apply them to the issue of machine intelligence.
It can be illuminating to make a rough distinction between two different normative
stances from which a proposed policy may be evaluated. The person-affecting
perspective asks whether a proposed change would be in “our interest”—that is to
say, whether it would (on balance, and in expectation) be in the interest of those
morally considerable creatures who either already exist or will come into existence
independently of whether the proposed change occurs or not. The impersonal
perspective, in contrast, gives no special consideration to currently existing people,
or to those who will come to exist independently of whether the proposed change
occurs. Instead, it counts everybody equally, independently of their temporal
location. The impersonal perspective sees great value in bringing new people into
existence, provided they have lives worth living: the more happy lives created, the
better.
This distinction, although it barely hints at the moral complexities associated with
a machine intelligence revolution, can be useful in a first-cut analysis. Here we will
first examine matters from the impersonal perspective. We will later see what
changes if person-affecting considerations are given weight in our deliberations.
Science and technology strategy
Before we zoom in on issues specific to machine superintelligence, we must
introduce some strategic concepts and considerations that pertain to scientific and
technological development more generally.
Differential technological developmentSuppose that a policymaker proposes to cut funding for a certain research field, out
of concern for the risks or long-term consequences of some hypothetical technology
that might eventually grow from its soil. She can then expect a howl of opposition
from the research community.
Scientists and their public advocates often say that it is futile to try to control the
evolution of technology by blocking research. If some technology is feasible (the
argument goes) it will be developed regardless of any particular policymaker’s
scruples about speculative future risks. Indeed, the more powerful the capabilities
that a line of development promises to produce, the surer we can be that somebody,
somewhere, will be motivated to pursue it. Funding cuts will not stop progress or
forestall its concomitant dangers.
Interestingly, this futility objection is almost never raised when a policymaker
proposes to increase funding to some area of research, even though the argument
would seem to cut both ways. One rarely hears indignant voices protest: “Please do
not increase our funding. Rather, make some cuts. Researchers in other countries
will surely pick up the slack; the same work will get done anyway. Don’t squander
the public’s treasure on domestic scientific research!”
What accounts for this apparent doublethink? One plausible explanation, of
course, is that members of the research community have a self-serving bias which
leads us to believe that research is always good and tempts us to embrace almost any
argument that supports our demand for more funding. However, it is also possible
that the double standard can be justified in terms of national self-interest. Suppose
that the development of a technology has two effects: giving a small benefit B to its
inventors and the country that sponsors them, while imposing an aggregately larger
harm H—which could be a risk externality—on everybody. Even somebody who is
largely altruistic might then choose to develop the overall harmful technology. They
might reason that the harm H will result no matter what they do, since if they refrain
somebody else will develop the technology anyway; and given that total welfare
cannot be affected, they might as well grab the benefit B for themselves and their
nation. (“Unfortunately, there will soon be a device that will destroy the world.
Fortunately, we got the grant to build it!”)
Whatever the explanation for the futility objection’s appeal, it fails to show that
there is in general no impersonal reason for trying to steer technological
development. It fails even if we concede the motivating idea that with continued
scientific and technological development efforts, all relevant technologies will
eventually be developed—that is, even if we concede the following:
Technological completion conjecture
If scientific and technological development efforts do not effectively cease,
then all important basic capabilities that could be obtained through somepossible technology will be obtained. 1
There are at least two reasons why the technological completion conjecture does not
imply the futility objection. First, the antecedent might not hold, because it is not in
fact a given that scientific and technological development efforts will not effectively
cease (before the attainment of technological maturity). This reservation is
especially pertinent in a context that involves existential risk. Second, even if we
could be certain that all important basic capabilities that could be obtained through
some possible technology will be obtained, it could still make sense to attempt to
influence the direction of technological research. What matters is not only whether a
technology is developed, but also when it is developed, by whom, and in what
context. These circumstances of birth of a new technology, which shape its impact,
can be affected by turning funding spigots on or off (and by wielding other policy
instruments).
These reflections suggest a principle that would have us attend to the relative
speed with which different technologies are developed: 2
The principle of differential technological development
Retard the development of dangerous and harmful technologies, especially ones
that raise the level of existential risk; and accelerate the development of
beneficial technologies, especially those that reduce the existential risks posed
by nature or by other technologies.
A policy could thus be evaluated on the basis of how much of a differential
advantage it gives to desired forms of technological development over undesired
forms. 3
Preferred order of arrival
Some technologies have an ambivalent effect on existential risks, increasing some
existential risks while decreasing others. Superintelligence is one such technology.
We have seen in earlier chapters that the introduction of machine
superintelligence would create a substantial existential risk. But it would reduce
many other existential risks. Risks from nature—such as asteroid impacts,
supervolcanoes, and natural pandemics—would be virtually eliminated, since
superintelligence could deploy countermeasures against most such hazards, or at
least demote them to the non-existential category (for instance, via spacecolonization).
These existential risks from nature are comparatively small over the relevant
timescales. But superintelligence would also eliminate or reduce many
anthropogenic risks. In particular, it would reduce risks of accidental destruction,
including risk of accidents related to new technologies. Being generally more
capable than humans, a superintelligence would be less likely to make mistakes, and
more likely to recognize when precautions are needed, and to implement precautions
competently. A well-constructed superintelligence might sometimes take a risk, but
only when doing so is wise. Furthermore, at least in scenarios where the
superintelligence forms a singleton, many non-accidental anthropogenic existential
risks deriving from global coordination problems would be eliminated. These
include risks of wars, technology races, undesirable forms of competition and
evolution, and tragedies of the commons.
Since substantial peril would be associated with human beings developing
synthetic biology, molecular nanotechnology, climate engineering, instruments for
biomedical enhancement and neuropsychological manipulation, tools for social
control that may facilitate totalitarianism or tyranny, and other technologies as-yet
unimagined, eliminating these types of risk would be a great boon. An argument
could therefore be mounted that earlier arrival dates of superintelligence are
preferable. However, if risks from nature and from other hazards unrelated to future
technology are small, then this argument could be refined: what matters is that we
get superintelligence before other dangerous technologies, such as advanced
nanotechnology. Whether it happens sooner or later may not be so important (from
an impersonal perspective) so long as the order of arrival is right.
The ground for preferring superintelligence to come before other potentially
dangerous technologies, such as nanotechnology, is that superintelligence would
reduce the existential risks from nanotechnology but not vice versa. 4 Hence, if we
create superintelligence first, we will face only those existential risks that are
associated with superintelligence; whereas if we create nanotechnology first, we will
face the risks of nanotechnology and then, additionally, the risks of
superintelligence. 5 Even if the existential risks from superintelligence are very
large, and even if superintelligence is the riskiest of all technologies, there could
thus be a case for hastening its arrival.
These “sooner-is-better” arguments, however, presuppose that the riskiness of
creating superintelligence is the same regardless of when it is created. If, instead, its
riskiness declines over time, it might be better to delay the machine intelligence
revolution. While a later arrival would leave more time for other existential
catastrophes to intercede, it could still be preferable to slow the development of
superintelligence. This would be especially plausible if the existential risks
associated with superintelligence are much larger than those associated with otherdisruptive technologies.
There are several quite strong reasons to believe that the riskiness of an
intelligence explosion will decline significantly over a multidecadal timeframe. One
reason is that a later date leaves more time for the development of solutions to the
control problem. The control problem has only recently been recognized, and most
of the current best ideas for how to approach it were discovered only within the past
decade or so (and in several cases during the time that this book was being written).
It is plausible that the state of the art will advance greatly over the next several
decades; and if the problem turns out to be very difficult, a significant rate of
progress might continue for a century or more. The longer it takes for
superintelligence to arrive, the more such progress will have been made when it
does. This is an important consideration in favor of later arrival dates—and a very
strong consideration against extremely early arrival dates.
Another reason why superintelligence later might be safer is that this would allow
more time for various beneficial background trends of human civilization to play
themselves out. How much weight one attaches to this consideration will depend on
how optimistic one is about these trends.
An optimist could certainly point to a number of encouraging indicators and
hopeful possibilities. People might learn to get along better, leading to reductions in
violence, war, and cruelty; and global coordination and the scope of political
integration might increase, making it easier to escape undesirable technology races
(more on this below) and to work out an arrangement whereby the hoped-for gains
from an intelligence explosion would be widely shared. There appear to be long-
term historical trends in these directions. 6
Further, an optimist could expect that the “sanity level” of humanity will rise over
the course of this century—that prejudices will (on balance) recede, that insights
will accumulate, and that people will become more accustomed to thinking about
abstract future probabilities and global risks. With luck, we could see a general
uplift of epistemic standards in both individual and collective cognition. Again,
there are trends pushing in these directions. Scientific progress means that more will
be known. Economic growth may give a greater portion of the world’s population
adequate nutrition (particularly during the early years of life that are important for
brain development) and access to quality education. Advances in information
technology will make it easier to find, integrate, evaluate, and communicate data and
ideas. Furthermore, by the century’s end, humanity will have made an additional
hundred years’ worth of mistakes, from which something might have been learned.
Many potential developments are ambivalent in the abovementioned sense—
increasing some existential risks and decreasing others. For example, advances in
surveillance, data mining, lie detection, biometrics, and psychological or
neurochemical means of manipulating beliefs and desires could reduce someexistential risks by making it easier to coordinate internationally or to suppress
terrorists and renegades at home. These same advances, however, might also
increase some existential risks by amplifying undesirable social dynamics or by
enabling the formation of permanently stable totalitarian regimes.
One important frontier is the enhancement of biological cognition, such as
through genetic selection. When we discussed this in Chapters 2 and 3, we concluded
that the most radical forms of superintelligence would be more likely to arise in the
form of machine intelligence. That claim is consistent with cognitive enhancement
playing an important role in the lead-up to, and creation of, machine
superintelligence. Cognitive enhancement might seem obviously risk-reducing: the
smarter the people working on the control problem, the more likely they are to find a
solution. However, cognitive enhancement could also hasten the development of
machine intelligence, thus reducing the time available to work on the problem.
Cognitive enhancement would also have many other relevant consequences. These
issues deserve a closer look. (Most of the following remarks about “cognitive
enhancement” apply equally to non-biological means of increasing our individual or
collective epistemic effectiveness.)
Rates of change and cognitive enhancement
An increase in either the mean or the upper range of human intellectual ability
would likely accelerate technological progress across the board, including progress
toward various forms of machine intelligence, progress on the control problem, and
progress on a wide swath of other technical and economic objectives. What would be
the net effect of such acceleration?
Consider the limiting case of a “universal accelerator,” an imaginary intervention
that accelerates literally everything. The action of such a universal accelerator would
correspond merely to an arbitrary rescaling of the time metric, producing no
qualitative change in observed outcomes. 7
If we are to make sense of the idea that cognitive enhancement might generally
speed things up, we clearly need some other concept than that of universal
acceleration. A more promising approach is to focus on how cognitive enhancement
might increase the rate of change in one type of process relative to the rate of change
in some other type of process. Such differential acceleration could affect a system’s
dynamics. Thus, consider the following concept:
Macro-structural development accelerator—A lever that accelerates the rate at
which macro-structural features of the human condition develop, while leaving
unchanged the rate at which micro-level human affairs unfold.Imagine pulling this lever in the decelerating direction. A brake pad is lowered onto
the great wheel of world history; sparks fly and metal screeches. After the wheel has
settled into a more leisurely pace, the result is a world in which technological
innovation occurs more slowly and in which fundamental or globally significant
change in political structure and culture happens less frequently and less abruptly. A
greater number of generations come and go before one era gives way to another.
During the course of a lifespan, a person sees little change in the basic structure of
the human condition.
For most of our species’ existence, macro-structural development was slower than
it is now. Fifty thousand years ago, an entire millennium might have elapsed without
a single significant technological invention, without any noticeable increase in
human knowledge and understanding, and without any globally meaningful political
change. On a micro-level, however, the kaleidoscope of human affairs churned at a
reasonable rate, with births, deaths, and other personally and locally significant
events. The average person’s day might have been more action-packed in the
Pleistocene than it is today.
If you came upon a magic lever that would let you change the rate of macro-
structural development, what should you do? Ought you to accelerate, decelerate, or
leave things as they are?
Assuming the impersonal standpoint, this question requires us to consider the
effects on existential risk. Let us distinguish between two kinds of risk: “state risks”
and “step risks.” A state risk is one that is associated with being in a certain state,
and the total amount of state risk to which a system is exposed is a direct function of
how long the system remains in that state. Risks from nature are typically state
risks: the longer we remain exposed, the greater the chance that we will get struck by
an asteroid, supervolcanic eruption, gamma ray burst, naturally arising pandemic, or
some other slash of the cosmic scythe. Some anthropogenic risks are also state risks.
At the level of an individual, the longer a soldier pokes his head up above the
parapet, the greater the cumulative chance he will be shot by an enemy sniper. There
are anthropogenic state risks at the existential level as well: the longer we live in an
internationally anarchic system, the greater the cumulative chance of a
thermonuclear Armageddon or of a great war fought with other kinds of weapons of
mass destruction, laying waste to civilization.
A step risk, by contrast, is a discrete risk associated with some necessary or
desirable transition. Once the transition is completed, the risk vanishes. The amount
of step risk associated with a transition is usually not a simple function of how long
the transition takes. One does not halve the risk of traversing a minefield by running
twice as fast. Conditional on a fast takeoff, the creation of superintelligence might
be a step risk: there would be a certain risk associated with the takeoff, the
magnitude of which would depend on what preparations had been made; but theamount of risk might not depend much on whether the takeoff takes twenty
milliseconds or twenty hours.
We can then say the following regarding a hypothetical macro-structural
development accelerator:
• Insofar as we are concerned with existential state risks, we should favor
acceleration—provided we think we have a realistic prospect of making it through
to a post-transition era in which any further existential risks are greatly reduced.
• If it were known that there is some step ahead destined to cause an existential
catastrophe, then we ought to reduce the rate of macro-structural development (or
even put it in reverse) in order to give more generations a chance to exist before
the curtain is rung down. But, in fact, it would be overly pessimistic to be so
confident that humanity is doomed.
• At present, the level of existential state risk appears to be relatively low. If we
imagine the technological macro-conditions for humanity frozen in their current
state, it seems very unlikely that an existential catastrophe would occur on a
timescale of, say, a decade. So a delay of one decade—provided it occurred at our
current stage of development or at some other time when state risk is low—would
incur only a very minor existential state risk, whereas a postponement by one
decade of subsequent technological developments might well have a significant
beneficial impact on later existential step risks, for example by allowing more
time for preparation.
Upshot: the main way that the speed of macro-structural development is important is
by affecting how well prepared humanity is when the time comes to confront the key
step risks. 8
So the question we must ask is how cognitive enhancement (and concomitant
acceleration of macro-structural development) would affect the expected level of
preparedness at the critical juncture. Should we prefer a shorter period of
preparation with higher intelligence? With higher intelligence, the preparation time
could be used more effectively, and the final critical step would be taken by a more
intelligent humanity. Or should we prefer to operate with closer to current levels of
intelligence if that gives us more time to prepare?
Which option is better depends on the nature of the challenge being prepared for.
If the challenge were to solve a problem for which learning from experience is key,
then the chronological length of the preparation period might be the determining
factor, since time is needed for the requisite experience to accumulate. What would
such a challenge look like? One hypothetical example would be a new weapons
technology that we could predict would be developed at some point in the future and
that would make it the case that any subsequent war would have, let us say, a one-in-
ten chance of causing an existential catastrophe. If such were the nature of thechallenge facing us, then we might wish the rate of macro-structural development to
be slow, so that our species would have more time to get its act together before the
critical step when the new weapons technology is invented. One could hope that
during the grace period secured through the deceleration, our species might learn to
avoid war—that international relations around the globe might come to resemble
those between the countries of the European Union, which, having fought one
another ferociously for centuries, now coexist in peace and relative harmony. The
pacification might occur as a result of the gentle edification from various civilizing
processes or through the shock therapy of sub-existential blows (e.g. small nuclear
conflagrations, and the recoil and resolve they might engender to finally create the
global institutions necessary for the abolishment of interstate wars). If this kind of
learning or adjusting would not be much accelerated by increased intelligence, then
cognitive enhancement would be undesirable, serving merely to burn the fuse faster.
A prospective intelligence explosion, however, may present a challenge of a
different kind. The control problem calls for foresight, reasoning, and theoretical
insight. It is less clear how increased historical experience would help. Direct
experience of the intelligence explosion is not possible (until too late), and many
features conspire to make the control problem unique and lacking in relevant
historical precedent. For these reasons, the amount of time that will elapse before
the intelligence explosion may not matter much per se. Perhaps what matters,
instead, is (a) the amount of intellectual progress on the control problem achieved by
the time of the detonation; and (b) the amount of skill and intelligence available at
the time to implement the best available solutions (and to improvise what is
missing). 9 That this latter factor should respond positively to cognitive enhancement
is obvious. How cognitive enhancement would affect factor (a) is a somewhat
subtler matter.
Suppose, as suggested earlier, that cognitive enhancement would be a general
macro-structural development accelerator. This would hasten the arrival of the
intelligence explosion, thus reducing the amount of time available for preparation
and for making progress on the control problem. Normally this would be a bad thing.
However, if the only reason why there is less time available for intellectual progress
is that intellectual progress is speeded up, then there need be no net reduction in the
amount of intellectual progress that will have taken place by the time the
intelligence explosion occurs.
At this point, cognitive enhancement might appear to be neutral with respect to
factor (a): the same intellectual progress that would otherwise have been made prior
to the intelligence explosion—including progress on the control problem—still gets
made, only compressed within a shorter time interval. In actuality, however,
cognitive enhancement may well prove a positive influence on (a).
One reason why cognitive enhancement might cause more progress to have beenmade on the control problem by the time the intelligence explosion occurs is that
progress on the control problem may be especially contingent on extreme levels of
intellectual performance—even more so than the kind of work necessary to create
machine intelligence. The role for trial and error and accumulation of experimental
results seems quite limited in relation to the control problem, whereas experimental
learning will probably play a large role in the development of artificial intelligence
or whole brain emulation. The extent to which time can substitute for wit may
therefore vary between tasks in a way that should make cognitive enhancement
promote progress on the control problem more than it would promote progress on
the problem of how to create machine intelligence.
Another reason why cognitive enhancement should differentially promote
progress on the control problem is that the very need for such progress is more likely
to be appreciated by cognitively more capable societies and individuals. It requires
foresight and reasoning to realize why the control problem is important and to make
it a priority. 10 It may also require uncommon sagacity to find promising ways of
approaching such an unfamiliar problem.
From these reflections we might tentatively conclude that cognitive enhancement
is desirable, at least insofar as the focus is on the existential risks of an intelligence
explosion. Parallel lines of thinking apply to other existential risks arising from
challenges that require foresight and reliable abstract reasoning (as opposed to, e.g.,
incremental adaptation to experienced changes in the environment or a
multigenerational process of cultural maturation and institution-building).
Technology couplings
Suppose that one thinks that solving the control problem for artificial intelligence is
very difficult, that solving it for whole brain emulations is much easier, and that it
would therefore be preferable that machine intelligence be reached via the whole
brain emulation path. We will return later to the question of whether whole brain
emulation would be safer than artificial intelligence. But for now we want to make
the point that even if we accept this premiss, it would not follow that we ought to
promote whole brain emulation technology. One reason, discussed earlier, is that a
later arrival of superintelligence may be preferable, in order to allow more time for
progress on the control problem and for other favorable background trends to
culminate—and thus, if one were confident that whole brain emulation would
precede AI anyway, it would be counterproductive to further hasten the arrival of
whole brain emulation.
But even if it were the case that it would be best for whole brain emulation to
arrive as soon as possible, it still would not follow that we ought to favor progress
toward whole brain emulation. For it is possible that progress toward whole brainemulation will not yield whole brain emulation. It may instead yield neuromorphic
artificial intelligence—forms of AI that mimic some aspects of cortical organization
but do not replicate neuronal functionality with sufficient fidelity to constitute a
proper emulation. If—as there is reason to believe—such neuromorphic AI is worse
than the kind of AI that would otherwise have been built, and if by promoting whole
brain emulation we would make neuromorphic AI arrive first, then our pursuit of the
supposed best outcome (whole brain emulation) would lead to the worst outcome
(neuromorphic AI); whereas if we had pursued the second-best outcome (synthetic
AI) we might actually have attained the second-best (synthetic AI).
We have just described an (hypothetical) instance of what we might term a
“technology coupling.” 11 This refers to a condition in which two technologies have a
predictable timing relationship, such that developing one of the technologies has a
robust tendency to lead to the development of the other, either as a necessary
precursor or as an obvious and irresistible application or subsequent step.
Technology couplings must be taken into account when we use the principle of
differential technological development: it is no good accelerating the development
of a desirable technology Y if the only way of getting Y is by developing an
extremely undesirable precursor technology X, or if getting Y would immediately
produce an extremely undesirable related technology Z. Before you marry your
sweetheart, consider the prospective in-laws.
In the case of whole brain emulation, the degree of technology coupling is
debatable. We noted in Chapter 2 that while whole brain emulation would require
massive progress in various enabling technologies, it might not require any major
new theoretical insight. In particular, it does not require that we understand how
human cognition works, only that we know how to build computational models of
small parts of the brain, such as different species of neuron. Nevertheless, in the
course of developing the ability to emulate human brains, a wealth of
neuroanatomical data would be collected, and functional models of cortical networks
would surely be greatly improved. Such progress would seem to have a good chance
of enabling neuromorphic AI before full-blown whole brain emulation. 12
Historically, there are quite a few examples of AI techniques gleaned from
neuroscience or biology. (For example: the McCulloch–Pitts neuron, perceptrons,
and other artificial neurons and neural networks, inspired by neuroanatomical work;
reinforcement learning, inspired by behaviorist psychology; genetic algorithms,
inspired by evolution theory; subsumption architectures and perceptual hierarchies,
inspired by cognitive science theories about motor planning and sensory perception;
artificial immune systems, inspired by theoretical immunology; swarm intelligence,
inspired by the ecology of insect colonies and other self-organizing systems; and
reactive and behavior-based control in robotics, inspired by the study of animal
locomotion.) Perhaps more significantly, there are plenty of important AI-relevantquestions that could potentially be answered through further study of the brain. (For
example: How does the brain store structured representations in working memory
and long-term memory? How is the binding problem solved? What is the neural
code? How are concepts represented? Is there some standard unit of cortical
processing machinery, such as the cortical column, and if so how is it wired and how
does its functionality depend on the wiring? How can such columns be linked up,
and how can they learn?)
We will shortly have more to say about the relative danger of whole brain
emulation, neuromorphic AI, and synthetic AI, but we can already flag another
important technology coupling: that between whole brain emulation and AI. Even if
a push toward whole brain emulation actually resulted in whole brain emulation (as
opposed to neuromorphic AI), and even if the arrival of whole brain emulation could
be safely handled, a further risk would still remain: the risk associated with a second
transition, a transition from whole brain emulation to AI, which is an ultimately
more powerful form of machine intelligence.
There are many other technology couplings, which could be considered in a more
comprehensive analysis. For instance, a push toward whole brain emulation would
boost neuroscience progress more generally. 13 That might produce various effects,
such as faster progress toward lie detection, neuropsychological manipulation
techniques, cognitive enhancement, and assorted medical advances. Likewise, a push
toward cognitive enhancement might (depending on the specific path pursued) create
spillovers such as faster development of genetic selection and genetic engineering
methods not only for enhancing cognition but for modifying other traits as well.
Second-guessing
We encounter another layer of strategic complexity if we take into account that there
is no perfectly benevolent, rational, and unified world controller who simply
implements what has been discovered to be the best option. Any abstract point about
“what should be done” must be embodied in the form of a concrete message, which
is entered into the arena of rhetorical and political reality. There it will be ignored,
misunderstood, distorted, or appropriated for various conflicting purposes; it will
bounce around like a pinball, causing actions and reactions, ushering in a cascade of
consequences, the upshot of which need bear no straightforward relationship to the
intentions of the original sender.
A sophisticated operator might try to anticipate these kinds of effect. Consider,
for example, the following argument template for proceeding with research to
develop a dangerous technology X. (One argument fitting this template can be found
in the writings of Eric Drexler. In Drexler’s case, X = molecular nanotechnology. 14 )1 The risks of X are great.
2 Reducing these risks will require a period of serious preparation.
3 Serious preparation will begin only once the prospect of X is taken seriously by
broad sectors of society.
4 Broad sectors of society will take the prospect of X seriously only once a large
research effort to develop X is underway.
5 The earlier a serious research effort is initiated, the longer it will take to deliver X
(because it starts from a lower level of pre-existing enabling technologies).
6 Therefore, the earlier a serious research effort is initiated, the longer the period
during which serious preparation will be taking place, and the greater the
reduction of the risks.
7 Therefore, a serious research effort toward X should be initiated immediately.
What initially looks like a reason for going slow or stopping—the risks of X being
great—ends up, on this line of thinking, as a reason for the opposite conclusion.
A related type of argument is that we ought—rather callously—to welcome small
and medium-scale catastrophes on grounds that they make us aware of our
vulnerabilities and spur us into taking precautions that reduce the probability of an
existential catastrophe. The idea is that a small or medium-scale catastrophe acts
like an inoculation, challenging civilization with a relatively survivable form of a
threat and stimulating an immune response that readies the world to deal with the
existential variety of the threat. 15
These “shock’em-into-reacting” arguments advocate letting something bad
happen in the hope that it will galvanize a public reaction. We mention them here
not to endorse them, but as a way to introduce the idea of (what we will term)
“second-guessing arguments.” Such arguments maintain that by treating others as
irrational and playing to their biases and misconceptions it is possible to elicit a
response from them that is more competent than if a case had been presented
honestly and forthrightly to their rational faculties.
It may seem unfeasibly difficult to use the kind of stratagems recommended by
second-guessing arguments to achieve long-term global goals. How could anybody
predict the final course of a message after it has been jolted hither and thither in the
pinball machine of public discourse? Doing so would seem to require predicting the
rhetorical effects on myriad constituents with varied idiosyncrasies and fluctuating
levels of influence over long periods of time during which the system may be
perturbed by unanticipated events from the outside while its topology is also
undergoing a continuous endogenous reorganization: surely an impossible task! 16
However, it may not be necessary to make detailed predictions about the system’s
entire future trajectory in order to identify an intervention that can be reasonably
expected to increase the chances of a certain long-term outcome. One might, forexample, consider only the relatively near-term and predictable effects in a detailed
way, selecting an action that does well in regard to those, while modeling the
system’s behavior beyond the predictability horizon as a random walk.
There may, however, be a moral case for de-emphasizing or refraining from
second-guessing moves. Trying to outwit one another looks like a zero-sum game—
or negative-sum, when one considers the time and energy that would be dissipated
by the practice as well as the likelihood that it would make it generally harder for
anybody to discover what others truly think and to be trusted when expressing their
own opinions. 17 A full-throttled deployment of the practices of strategic
communication would kill candor and leave truth bereft to fend for herself in the
backstabbing night of political bogeys.
Pathways and enablers
Should we celebrate advances in computer hardware? What about advances on the
path toward whole brain emulation? We will look at these two questions in turn.
Effects of hardware progress
Faster computers make it easier to create machine intelligence. One effect of
accelerating progress in hardware, therefore, is to hasten the arrival of machine
intelligence. As discussed earlier, this is probably a bad thing from the impersonal
perspective, since it reduces the amount of time available for solving the control
problem and for humanity to reach a more mature stage of civilization. The case is
not a slam dunk, though. Since superintelligence would eliminate many other
existential risks, there could be reason to prefer earlier development if the level of
these other existential risks were very high. 18
Hastening or delaying the onset of the intelligence explosion is not the only
channel through which the rate of hardware progress can affect existential risk.
Another channel is that hardware can to some extent substitute for software; thus,
better hardware reduces the minimum skill required to code a seed AI. Fast
computers might also encourage the use of approaches that rely more heavily on
brute-force techniques (such as genetic algorithms and other generate-evaluate-
discard methods) and less on techniques that require deep understanding to use. If
brute-force techniques lend themselves to more anarchic or imprecise system
designs, where the control problem is harder to solve than in more precisely
engineered and theoretically controlled systems, this would be another way in which
faster computers would increase the existential risk.Another consideration is that rapid hardware progress increases the likelihood of a
fast takeoff. The more rapidly the state of the art advances in the semiconductor
industry, the fewer the person-hours of programmers’ time spent exploiting the
capabilities of computers at any given performance level. This means that an
intelligence explosion is less likely to be initiated at the lowest level of hardware
performance at which it is feasible. An intelligence explosion is thus more likely to
be initiated when hardware has advanced significantly beyond the minimum level at
which the eventually successful programming approach could first have succeeded.
There is then a hardware overhang when the takeoff eventually does occur. As we
saw in Chapter 4, hardware overhang is one of the main factors that reduce
recalcitrance during the takeoff. Rapid hardware progress, therefore, will tend to
make the transition to superintelligence faster and more explosive.
A faster takeoff via a hardware overhang can affect the risks of the transition in
several ways. The most obvious is that a faster takeoff offers less opportunity to
respond and make adjustments whilst the transition is in progress, which would tend
to increase risk. A related consideration is that a hardware overhang would reduce
the chances that a dangerously self-improving seed AI could be contained by
limiting its ability to colonize sufficient hardware: the faster each processor is, the
fewer processors would be needed for the AI to quickly bootstrap itself to
superintelligence. Yet another effect of a hardware overhang is to level the playing
field between big and small projects by reducing the importance of one of the
advantages of larger projects—the ability to afford more powerful computers. This
effect, too, might increase existential risk, if larger projects are more likely to solve
the control problem and to be pursuing morally acceptable objectives. 19
There are also advantages to a faster takeoff. A faster takeoff would increase the
likelihood that a singleton will form. If establishing a singleton is sufficiently
important for solving the post-transition coordination problems, it might be worth
accepting a greater risk during the intelligence explosion in order to mitigate the risk
of catastrophic coordination failures in its aftermath.
Developments in computing can affect the outcome of a machine intelligence
revolution not only by playing a direct role in the construction of machine
intelligence but also by having diffuse effects on society that indirectly help shape
the initial conditions of the intelligence explosion. The Internet, which required
hardware to be good enough to enable personal computers to be mass produced at
low cost, is now influencing human activity in many areas, including work in
artificial intelligence and research on the control problem. (This book might not
have been written, and you might not have found it, without the Internet.) However,
hardware is already good enough for a great many applications that could facilitate
human communication and deliberation, and it is not clear that the pace of progress
in these areas is strongly bottlenecked by the rate of hardware improvement. 20On balance, it appears that faster progress in computing hardware is undesirable
from the impersonal evaluative standpoint. This tentative conclusion could be
overturned, for example if the threats from other existential risks or from post-
transition coordination failures turn out to be extremely large. In any case, it seems
difficult to have much leverage on the rate of hardware advancement. Our efforts to
improve the initial conditions for the intelligence explosion should therefore
probably focus on other parameters.
Note that even when we cannot see how to influence some parameter, it can be
useful to determine its “sign” (i.e. whether an increase or decrease in that parameter
would be desirable) as a preliminary step in mapping the strategic lay of the land.
We might later discover a new leverage point that does enable us to manipulate the
parameter more easily. Or we might discover that the parameter’s sign correlates
with the sign of some other more manipulable parameter, so that our initial analysis
helps us decide what to do with this other parameter.
Should whole brain emulation research be promoted?
The harder it seems to solve the control problem for artificial intelligence, the more
tempting it is to promote the whole brain emulation path as a less risky alternative.
There are several issues, however, that must be analyzed before one can arrive at a
well-considered judgment. 21
First, there is the issue of technology coupling, already discussed earlier. We
pointed out that an effort to develop whole brain emulation could result in
neuromorphic AI instead, a form of machine intelligence that may be especially
unsafe.
But let us assume, for the sake of argument, that we actually achieve whole brain
emulation (WBE). Would this be safer than AI? This, itself, is a complicated issue.
There are at least three putative advantages of WBE: (i) that its performance
characteristics would be better understood than those of AI; (ii) that it would inherit
human motives; and (iii) that it would result in a slower takeoff. Let us very briefly
reflect on each.
i That it should be easier to understand the intellectual performance characteristics
of an emulation than of an AI sounds plausible. We have abundant experience
with the strengths and weaknesses of human intelligence but no experience with
human-level artificial intelligence. However, to understand what a snapshot of a
digitized human intellect can and cannot do is not the same as to understand how
such an intellect will respond to modifications aimed at enhancing its
performance. An artificial intellect, by contrast, might be carefully designed to be
understandable, in both its static and dynamic dispositions. So while whole brainemulation may be more predictable in its intellectual performance than a generic
AI at a comparable stage of development, it is unclear whether whole brain
emulation would be dynamically more predictable than an AI engineered by
competent safety-conscious programmers.
ii As for an emulation inheriting the motivations of its human template, this is far
from guaranteed. Capturing human evaluative dispositions might require a very
high-fidelity emulation. Even if some individual’s motivations were perfectly
captured, it is unclear how much safety would be purchased. Humans can be
untrustworthy, selfish, and cruel. While templates would hopefully be selected for
exceptional virtue, it may be hard to foretell how someone will act when
transplanted into radically alien circumstances, superhumanly enhanced in
intelligence, and tempted with an opportunity for world domination. It is true that
emulations would at least be more likely to have human-like motivations (as
opposed to valuing only paperclips or discovering digits of pi). Depending on
one’s views on human nature, this might or might not be reassuring. 22
iii It is not clear why whole brain emulation should result in a slower takeoff than
artificial intelligence. Perhaps with whole brain emulation one should expect less
hardware overhang, since whole brain emulation is less computationally efficient
than artificial intelligence can be. Perhaps, also, an AI system could more easily
absorb all available computing power into one giant integrated intellect, whereas
whole brain emulation would forego quality superintelligence and pull ahead of
humanity only in speed and size of population. If whole brain emulation does lead
to a slower takeoff, this could have benefits in terms of alleviating the control
problem. A slower takeoff would also make a multipolar outcome more likely.
But whether a multipolar outcome is desirable is very doubtful.
There is another important complication with the general idea that getting whole
brain emulation first is safer: the need to cope with a second transition. Even if the
first form of human-level machine intelligence is emulation-based, it would still
remain feasible to develop artificial intelligence. AI in its mature form has
important advantages over WBE, making AI the ultimately more powerful
technology. 23 While mature AI would render WBE obsolete (except for the special
purpose of preserving individual human minds), the reverse does not hold.
What this means is that if AI is developed first, there might be a single wave of
the intelligence explosion. But if WBE is developed first, there may be two waves:
first, the arrival of WBE; and later, the arrival of AI. The total existential risk along
the WBE-first path is the sum of the risk in the first transition and the risk in the
second transition (conditional on having made it through the first); see Figure 13. 24
How much safer would the AI transition be in a WBE world? One consideration is
that the AI transition would be less explosive if it occurs after some form ofmachine intelligence has already been realized. Emulations, running at digital
speeds and in numbers that might far exceed the biological human population, would
reduce the cognitive differential, making it easier for emulations to control the AI.
This consideration is not too weighty, since the gap between AI and WBE could still
be wide. However, if the emulations were not just faster and more numerous but also
somewhat qualitatively smarter than biological humans (or at least drawn from the
top end of the human distribution) then the WBE-first scenario would have
advantages paralleling those of human cognitive enhancement, which we discussed
above.
Figure 13 Artificial intelligence or whole brain emulation first? In an AI-first
scenario, there is one transition that creates an existential risk. In a WBE-first
scenario, there are two risky transitions, first the development of WBE and then the
development of AI. The total existential risk along the WBE-first scenario is the
sum of these. However, the risk of an AI transition might be lower if it occurs in a
world where WBE has already been successfully introduced.
Another consideration is that the transition to WBE would extend the lead of the
frontrunner. Consider a scenario in which the frontrunner has a six-month lead over
the closest follower in developing whole brain emulation technology. Suppose that
the first emulations to be created are cooperative, safety-focused, and patient. If they
run on fast hardware, these emulations could spend subjective eons pondering how
to create safe AI. For example, if they run at a speedup of 100,000× and are able to
work on the control problem undisturbed for six months of sidereal time, they could
hammer away at the control problem for fifty millennia before facing competition
from other emulations. Given sufficient hardware, they could hasten their progress
by fanning out myriad copies to work independently on subproblems. If the
frontrunner uses its six-month lead to form a singleton, it could buy its emulation
AI-development team an unlimited amount of time to work on the control
problem. 25On balance, it looks like the risk of the AI transition would be reduced if WBE
comes before AI. However, when we combine the residual risk in the AI transition
with the risk of an antecedent WBE transition, it becomes very unclear how the total
existential risk along the WBE-first path stacks up against the risk along the AI-first
path. Only if one is quite pessimistic about biological humanity’s ability to manage
an AI transition—after taking into account that human nature or civilization might
have improved by the time we confront this challenge—should the WBE-first path
seem attractive.
To figure out whether whole brain emulation technology should be promoted,
there are some further important points to place in the balance. Most significantly,
there is the technology coupling mentioned earlier: a push toward WBE could
instead produce neuromorphic AI. This is a reason against pushing for WBE. 26 No
doubt, there are some synthetic AI designs that are less safe than some neuromorphic
designs. In expectation, however, it seems that neuromorphic designs are less safe.
One ground for this is that imitation can substitute for understanding. To build
something from the ground up one must usually have a reasonably good
understanding of how the system will work. Such understanding may not be
necessary to merely copy features of an existing system. Whole brain emulation
relies on wholesale copying of biology, which may not require a comprehensive
computational systems-level understanding of cognition (though a large amount of
component-level understanding would undoubtedly be needed). Neuromorphic AI
may be like whole brain emulation in this regard: it would be achieved by cobbling
together pieces plagiarized from biology without the engineers necessarily having a
deep mathematical understanding of how the system works. But neuromorphic AI
would be unlike whole brain emulation in another regard: it would not have human
motivations by default. 27 This consideration argues against pursuing the whole brain
emulation approach to the extent that it would likely produce neuromorphic AI.
A second point to put in the balance is that WBE is more likely to give us advance
notice of its arrival. With AI it is always possible that somebody will make an
unexpected conceptual breakthrough. WBE, by contrast, will require many laborious
precursor steps—high-throughput scanning facilities, image processing software,
detailed neural modeling work. We can therefore be confident that WBE is not
imminent (not less than, say, fifteen or twenty years away). This means that efforts
to accelerate WBE will make a difference mainly in scenarios in which machine
intelligence is developed comparatively late. This could make WBE investments
attractive to somebody who wants the intelligence explosion to preempt other
existential risks but is wary of supporting AI for fear of triggering an intelligence
explosion prematurely, before the control problem has been solved. However, the
uncertainty over the relevant timescales is probably currently too large to enable this
consideration to carry much weight. 28A strategy of promoting WBE is thus most attractive if (a) one is very pessimistic
about humans solving the control problem for AI, (b) one is not too worried about
neuromorphic AI, multipolar outcomes, or the risks of a second transition, (c) one
thinks that the default timing of WBE and AI is close, and (d) one prefers
superintelligence to be developed neither very late nor very early.
The person-affecting perspective favors speed
I fear the blog commenter “washbash” may speak for many when he or she writes:
I instinctively think go faster. Not because I think this is better for the world.
Why should I care about the world when I am dead and gone? I want it to go
fast, damn it! This increases the chance I have of experiencing a more
technologically advanced future. 29
From the person-affecting standpoint, we have greater reason to rush forward with
all manner of radical technologies that could pose existential risks. This is because
the default outcome is that almost everyone who now exists is dead within a century.
The case for rushing is especially strong with regard to technologies that could
extend our lives and thereby increase the expected fraction of the currently existing
population that may still be around for the intelligence explosion. If the machine
intelligence revolution goes well, the resulting superintelligence could almost
certainly devise means to indefinitely prolong the lives of the then still-existing
humans, not only keeping them alive but restoring them to health and youthful vigor,
and enhancing their capacities well beyond what we currently think of as the human
range; or helping them shuffle off their mortal coils altogether by uploading their
minds to a digital substrate and endowing their liberated spirits with exquisitely
good-feeling virtual embodiments. With regard to technologies that do not promise
to save lives, the case for rushing is weaker, though perhaps still sufficiently
supported by the hope of raised standards of living. 30
The same line of reasoning makes the person-affecting perspective favor many
risky technological innovations that promise to hasten the onset of the intelligence
explosion, even when those innovations are disfavored in the impersonal
perspective. Such innovations could shorten the wolf hours during which we
individually must hang on to our perch if we are to live to see the daybreak of the
posthuman age. From the person-affecting standpoint, faster hardware progress thus
seems desirable, as does faster progress toward WBE. Any adverse effect on
existential risk is probably outweighed by the personal benefit of an increased
chance of the intelligence explosion happening in the lifetime of currently existingpeople. 31
Collaboration
One important parameter is the degree to which the world will manage to coordinate
and collaborate in the development of machine intelligence. Collaboration would
bring many benefits. Let us take a look at how this parameter might affect the
outcome and what levers we might have for increasing the extent and intensity of
collaboration.
The race dynamic and its perils
A race dynamic exists when one project fears being overtaken by another. This does
not require the actual existence of multiple projects. A situation with only one
project could exhibit a race dynamic if that project is unaware of its lack of
competitors. The Allies would probably not have developed the atomic bomb as
quickly as they did had they not believed (erroneously) that the Germans might be
close to the same goal.
The severity of a race dynamic (that is, the extent to which competitors prioritize
speed over safety) depends on several factors, such as the closeness of the race, the
relative importance of capability and luck, the number of competitors, whether
competing teams are pursuing different approaches, and the degree to which projects
share the same aims. Competitors’ beliefs about these factors are also relevant. (See
Box 13.)
In the development of machine superintelligence, it seems likely that there will be
at least a mild race dynamic, and it is possible that there will be a severe race
dynamic. The race dynamic has important consequences for how we should think
about the strategic challenge posed by the possibility of an intelligence explosion.
The race dynamic could spur projects to move faster toward superintelligence
while reducing investment in solving the control problem. Additional detrimental
effects of the race dynamic are also possible, such as direct hostilities between
competitors. Suppose that two nations are racing to develop the first
superintelligence, and that one of them is seen to be pulling ahead. In a winner-
takes-all situation, a lagging project might be tempted to launch a desperate strike
against its rival rather than passively await defeat. Anticipating this possibility, the
frontrunner might be tempted to strike preemptively. If the antagonists are powerful
states, the clash could be bloody. 34 (A “surgical strike” against the rival’s AI project
might risk triggering a larger confrontation and might in any case not be feasible ifthe host country has taken precautions. 35 )
Box 13 A risk-race to the bottom
Consider a hypothetical AI arms race in which several teams compete to develop
superintelligence. 32 Each team decides how much to invest in safety—knowing that
resources spent on developing safety precautions are resources not spent on
developing the AI. Absent a deal between all the competitors (which might be
stymied by bargaining or enforcement difficulties), there might then be a risk-race
to the bottom, driving each team to take only a minimum of precautions.
One can model each team’s performance as a function of its capability (measuring
its raw ability and luck) and a penalty term corresponding to the cost of its safety
precautions. The team with the highest performance builds the first AI. The riskiness
of that AI is determined by how much its creators invested in safety. In the worst-
case scenario, all teams have equal levels of capability. The winner is then
determined exclusively by investment in safety: the team that took the fewest safety
precautions wins. The Nash equilibrium for this game is for every team to spend
nothing on safety. In the real world, such a situation might arise via a risk ratchet:
some team, fearful of falling behind, increments its risk-taking to catch up with its
competitors—who respond in kind, until the maximum level of risk is reached.
Capability versus risk
The situation changes when there are variations in capability. As variations in
capability become more important relative to the cost of safety precautions, the risk
ratchet weakens: there is less incentive to incur an extra bit of risk if doing so is
unlikely to change the order of the race. This is illustrated under various scenarios in
Figure 14, which plots how the riskiness of the AI depends on the importance of
capability. Safety investment ranges from 1 (resulting in perfectly safe AI) to 0
(completely unsafe AI). The x-axis represents the relative importance of capability
versus safety investment in determining the speed of a team’s progress toward AI.
(At 0.5, the safety investment level is twice are important as capability; at 1, the two
are equal; at 2, capability is twice as important as safety level; and so forth.) The y-
axis represents the level of AI risk (the expected fraction of their maximum utility
that the winner of the race gets).Figure 14 Risk levels in AI technology races. Levels of risk of dangerous AI in a
simple model of a technology race involving either (a) two teams or (b) five teams,
plotted against the relative importance of capability (as opposed to investment in
safety) in determining which project wins the race. The graphs show three
information-level scenarios: no capability information (straight), private capability
information (dashed), and full capability information (dotted).
We see that, under all scenarios, the dangerousness of the resultant AI is maximal
when capability plays no role, gradually decreasing as capability grows in
importance.
Compatible goals
Another way of reducing the risk is by giving teams more of a stake in each other’s
success. If competitors are convinced that coming second means the total loss of
everything they care about, they will take whatever risk necessary to bypass their
rivals. Conversely, teams will invest more in safety if less depends on winning the
race. This suggests that we should encourage various forms of cross-investment.
The number of competitors
The greater the number of competing teams, the more dangerous the race becomes:
each team, having less chance of coming first, is more willing to throw caution to
the wind. This can be seen by contrasting Figure 14a (two teams) with Figure 14b
(five teams). In every scenario, more competitors means more risk. Risk would be
reduced if teams coalesce into a smaller number of competing coalitions.
The curse of too much informationIs it good if teams know about their positions in the race (knowing their capability
scores, for instance)? Here, opposing factors are at play. It is desirable that a leader
knows it is leading (so that it knows it has some margin for additional safety
precautions). Yet it is undesirable that a laggard knows it has fallen behind (since
this would confirm that it must cut back on safety to have any hope of catching up).
While intuitively it may seem this trade-off could go either way, the models are
unequivocal: information is (in expectation) bad. 33 Figures 14a and 14b each plot
three scenarios: the straight lines correspond to situations in which no team knows
any of the capability scores, its own included. The dashed lines show situations
where each team knows its own capability only. (In those situations, a team takes
extra risk only if its capability is low.) And the dotted lines show what happens when
all teams know each other’s capabilities. (They take extra risks if their capability
scores are close to one another.) With each increase in information level, the race
dynamic becomes worse.
Scenarios in which the rival developers are not states but smaller entities, such as
corporate labs or academic teams, would probably feature much less direct
destruction from conflict. Yet the overall consequences of competition may be
almost as bad. This is because the main part of the expected harm from competition
stems not from the smashup of battle but from the downgrade of precaution. A race
dynamic would, as we saw, reduce investment in safety; and conflict, even if
nonviolent, would tend to scotch opportunities for collaboration, since projects
would be less likely to share ideas for solving the control problem in a climate of
hostility and mistrust. 36
On the benefits of collaboration
Collaboration thus offers many benefits. It reduces the haste in developing machine
intelligence. It allows for greater investment in safety. It avoids violent conflicts.
And it facilitates the sharing of ideas about how to solve the control problem. To
these benefits we can add another: collaboration would tend to produce outcomes in
which the fruits of a successfully controlled intelligence explosion get distributed
more equitably.
That broader collaboration should result in wider sharing of gains is not
axiomatic. In principle, a small project run by an altruist could lead to an outcome
where the benefits are shared evenly or equitably among all morally considerable
beings. Nevertheless, there are several reasons to suppose that broader
collaborations, involving a greater number of sponsors, are (in expectation)distributionally superior. One such reason is that sponsors presumably prefer an
outcome in which they themselves get (at least) their fair share. A broad
collaboration then means that relatively many individuals get at least their fair
share, assuming the project is successful. Another reason is that a broad
collaboration also seems likelier to benefit people outside the collaboration. A
broader collaboration contains more members, so more outsiders would have
personal ties to somebody on the inside looking out for their interests. A broader
collaboration is also more likely to include at least some altruist who wants to
benefit everyone. Furthermore, a broader collaboration is more likely to operate
under public oversight, which might reduce the risk of the entire pie being captured
by a clique of programmers or private investors. 37 Note also that the larger the
successful collaboration is, the lower the costs to it of extending the benefits to all
outsiders. (For instance, if 90% of all people were already inside the collaboration, it
would cost them no more than 10% of their holdings to bring all outsiders up to their
own level.)
It is thus plausible that broader collaborations would tend to lead to a wider
distribution of the gains (though some projects with few sponsors might also have
distributionally excellent aims). But why is a wide distribution of gains desirable?
There are both moral and prudential reasons for favoring outcomes in which
everybody gets a share of the bounty. We will not say much about the moral case,
except to note that it need not rest on any egalitarian principle. The case might be
made, for example, on grounds of fairness. A project that creates machine
superintelligence imposes a global risk externality. Everybody on the planet is
placed in jeopardy, including those who do not consent to having their own lives and
those of their family imperiled in this way. Since everybody shares the risk, it would
seem to be a minimal requirement of fairness that everybody also gets a share of the
upside.
The fact that the total (expected) amount of good seems greater in collaboration
scenarios is another important reason such scenarios are morally preferable.
The prudential case for favoring a wide distribution of gains is two-pronged. One
prong is that wide distribution should promote collaboration, thereby mitigating the
negative consequences of the race dynamic. There is less incentive to fight over who
gets to build the first superintelligence if everybody stands to benefit equally from
any project’s success. The sponsors of a particular project might also benefit from
credibly signaling their commitment to distributing the spoils universally, a
certifiably altruistic project being likely to attract more supporters and fewer
enemies. 38
The other prong of the prudential case for favoring a wide distribution of gains
has to do with whether agents are risk-averse or have utility functions that are
sublinear in resources. The central fact here is the enormousness of the potentialresource pie. Assuming the observable universe is as uninhabited as it looks, it
contains more than one vacant galaxy for each human being alive. Most people
would much rather have certain access to one galaxy’s worth of resources than a
lottery ticket offering a one-in-a-billion chance of owning a billion galaxies. 39 Given
the astronomical size of humanity’s cosmic endowment, it seems that self-interest
should generally favor deals that would guarantee each person a share, even if each
share corresponded to a small fraction of the total. The important thing, when such
an extravagant bonanza is in the offing, is to not be left out in the cold.
This argument from the enormousness of the resource pie presupposes that
preferences are resource-satiable. 40 That supposition does not necessarily hold. For
instance, several prominent ethical theories—including especially aggregative
consequentialist theories—correspond to utility functions that are risk-neutral and
linear in resources. A billion galaxies could be used to create a billion times more
happy lives than a single galaxy. They are thus, to a utilitarian, worth a billion times
as much. 41 Ordinary selfish human preference functions, however, appear to be
relatively resource-satiable.
This last statement must be flanked by two important qualifications. The first is
that many people care about rank. If multiple agents each wants to top the Forbes
rich list, then no resource pie is large enough to give everybody full satisfaction.
The second qualification is that the post-transition technology base would enable
material resources to be converted into an unprecedented range of products,
including some goods that are not currently available at any price even though they
are highly valued by many humans. A billionaire does not live a thousand times
longer than a millionaire. In the era of digital minds, however, the billionaire could
afford a thousandfold more computing power and could thus enjoy a thousandfold
longer subjective lifespan. Mental capacity, likewise, could be for sale. In such
circumstances, with economic capital convertible into vital goods at a constant rate
even for great levels of wealth, unbounded greed would make more sense than it
does in today’s world where the affluent (those among them lacking a philanthropic
heart) are reduced to spending their riches on airplanes, boats, art collections, or a
fourth and a fifth residence.
Does this mean that an egoist should be risk-neutral with respect to his or her
post-transition resource endowment? Not quite. Physical resources may not be
convertible into lifespan or mental performance at arbitrary scales. If a life must be
lived sequentially, so that observer moments can remember earlier events and be
affected by prior choices, then the life of a digital mind cannot be extended
arbitrarily without utilizing an increasing number of sequential computational
operations. But physics limits the extent to which resources can be transformed into
sequential computations. 42 The limits on sequential computation may also constrain
some aspects of cognitive performance to scale radically sublinearly beyond arelatively modest resource endowment. Furthermore, it is not obvious that an egoist
would or should be risk-neutral even with regard to highly normatively relevant
outcome metrics such as number of quality-adjusted subjective life years. If offered
the choice between an extra 2,000 years of life for certain and a one-in-ten chance of
an extra 30,000 years of life, I think most people would select the former (even
under the stipulation that each life year would be of equal quality). 43
In reality, the prudential case for favoring a wide distribution of gains is
presumably subject-relative and situation-dependent. Yet, on the whole, people
would be more likely to get (almost all of) what they want if a way is found to
achieve a wide distribution—and this holds even before taking into account that a
commitment to a wider distribution would tend to foster collaboration and thereby
increase the chances of avoiding existential catastrophe. Favoring a broad
distribution, therefore, appears to be not only morally mandated but also
prudentially advisable.
There is a further set of consequences to collaboration that should be given at
least some shrift: the possibility that pre-transition collaboration influences the level
of post-transition collaboration. Assume humanity solves the control problem. (If
the control problem is not solved, it may scarcely matter how much collaboration
there is post transition.) There are two cases to consider. The first is that the
intelligence explosion does not create a winner-takes-all dynamic (presumably
because the takeoff is relatively slow). In this case it is plausible that if pre-
transition collaboration has any systematic effect on post-transition collaboration, it
has a positive effect, tending to promote subsequent collaboration. The original
collaborative relationships may endure and continue beyond the transition; also, pre-
transition collaboration may offer more opportunity for people to steer
developments in desirable (and, presumably, more collaborative) post-transition
directions.
The second case is that the nature of the intelligence explosion does encourage a
winner-takes-all dynamic (presumably because the takeoff is relatively fast). In this
case, if there is no extensive collaboration before the takeoff, a singleton is likely to
emerge—a single project would undergo the transition alone, at some point
obtaining a decisive strategic advantage combined with superintelligence. A
singleton, by definition, is a highly collaborative social order. 44 The absence of
extensive collaboration pre-transition would thus lead to an extreme degree of
collaboration post-transition. By contrast, a somewhat higher level of collaboration
in the run-up to the intelligence explosion opens up a wider variety of possible
outcomes. Collaborating projects could synchronize their ascent to ensure they
transition in tandem without any of them getting a decisive strategic advantage. Or
different sponsor groups might merge their efforts into a single project, while
refusing to give that project a mandate to form a singleton. For example, one couldimagine a consortium of nations forming a joint scientific project to develop
machine superintelligence, yet not authorizing this project to evolve into anything
like a supercharged United Nations, electing instead to maintain the factious world
order that existed before.
Particularly in the case of a fast takeoff, therefore, the possibility exists that
greater pre-transition collaboration would result in less post-transition collaboration.
However, to the extent that collaborating entities are able to shape the outcome, they
may allow the emergence or continuation of non-collaboration only if they foresee
that no catastrophic consequences would follow from post-transition factiousness.
Scenarios in which pre-transition collaboration leads to reduced post-transition
collaboration may therefore mostly be ones in which reduced post-transition
collaboration is innocuous.
In general, greater post-transition collaboration appears desirable. It would reduce
the risk of dystopian dynamics in which economic competition and a rapidly
expanding population lead to a Malthusian condition, or in which evolutionary
selection erodes human values and selects for non-eudaemonic forms, or in which
rival powers suffer other coordination failures such as wars and technology races.
The last of these issues, the prospect of technology races, may be particularly
problematic if the transition is to an intermediary form of machine intelligence
(whole brain emulation) since it would create a new race dynamic that would harm
the chances of the control problem being solved for the subsequent second transition
to a more advanced form of machine intelligence (artificial intelligence).
We described earlier how collaboration can reduce conflict in the run-up to the
intelligence explosion, increasing the chances that the control problem will be
solved, and improve both the moral legitimacy and the prudential desirability of the
resulting resource allocation. To these benefits of collaboration it may thus be
possible to add one more: that broader collaboration pre-transition could help with
important coordination problems in the post-transition era.
Working together
Collaboration can take different forms depending on the scale of the collaborating
entities. At a small scale, individual AI teams who believe themselves to be in
competition with one another could choose to pool their efforts. 45 Corporations
could merge or cross-invest. At a larger scale, states could join in a big international
project. There are precedents to large-scale international collaboration in science
and technology (such as CERN, the Human Genome Project, and the International
Space Station), but an international project to develop safe superintelligence would
pose a different order of challenge because of the security implications of the work.
It would have to be constituted not as an open academic collaboration but as anextremely tightly controlled joint enterprise. Perhaps the scientists involved would
have to be physically isolated and prevented from communicating with the rest of
the world for the duration of the project, except through a single carefully vetted
communication channel. The required level of security might be nearly unattainable
at present, but advances in lie detection and surveillance technology could make it
feasible later this century. It is also worth bearing in mind that broad collaboration
does not necessarily mean that large numbers of researchers would be involved in
the project; it simply means that many people would have a say in the project’s
aims. In principle, a project could involve a maximally broad collaboration
comprising all of humanity as sponsors (represented, let us say, by the General
Assembly of the United Nations), yet employ only a single scientist to carry out the
work. 46
There is a reason for starting collaboration as early as possible, namely to take
advantage of the veil of ignorance that hides from our view any specific information
about which individual project will get to superintelligence first. The closer to the
finishing line we get, the less uncertainty will remain about the relative chances of
competing projects; and the harder it may consequently be to make a case based on
the self-interest of the frontrunner to join a collaborative project that would
distribute the benefits to all of humanity. On the other hand, it also looks hard to
establish a formal collaboration of worldwide scope before the prospect of
superintelligence has become much more widely recognized than it currently is and
before there is a clearly visible road leading to the creation of machine
superintelligence. Moreover, to the extent that collaboration would promote progress
along that road, it may actually be counterproductive in terms of safety, as discussed
earlier.
The ideal form of collaboration for the present may therefore be one that does not
initially require specific formalized agreements and that does not expedite advances
in machine intelligence. One proposal that fits these criteria is that we propound an
appropriate moral norm, expressing our commitment to the idea that
superintelligence should be for the common good. Such a norm could be formulated
as follows:
The common good principle
Superintelligence should be developed only for the benefit of all of humanity
and in the service of widely shared ethical ideals. 47
Establishing from an early stage that the immense potential of superintelligence
belongs to all of humanity will give more time for such a norm to become
entrenched.The common good principle does not preclude commercial incentives for
individuals or firms active in related areas. For example, a firm might satisfy the
call for universal sharing of the benefits of superintelligence by adopting a “windfall
clause” to the effect that all profits up to some very high ceiling (say, a trillion
dollars annually) would be distributed in the ordinary way to the firm’s shareholders
and other legal claimants, and that only profits in excess of the threshold would be
distributed to all of humanity evenly (or otherwise according to universal moral
criteria). Adopting such a windfall clause should be substantially costless, any given
firm being extremely unlikely ever to exceed the stratospheric profit threshold (and
such low-probability scenarios ordinarily playing no role in the decisions of the
firm’s managers and investors). Yet its widespread adoption would give humankind
a valuable guarantee (insofar as the commitments could be trusted) that if ever some
private enterprise were to hit the jackpot with the intelligence explosion, everybody
would share in most of the benefits. The same idea could be applied to entities other
than firms. For example, states could agree that if ever any one state’s GDP exceeds
some very high fraction (say, 90%) of world GDP, the overshoot should be
distributed evenly to all. 48
The common good principle (and particular instantiations, such as windfall
clauses) could be adopted initially as a voluntary moral commitment by responsible
individuals and organizations that are active in areas related to machine intelligence.
Later, it could be endorsed by a wider set of entities and enacted into law and treaty.
A vague formulation, such as the one given here, may serve well as a starting point;
but it would ultimately need to be sharpened into a set of specific verifiable
requirements.CHAPTER 15
Crunch time
We find ourselves in a thicket of strategic complexity, surrounded by a dense
mist of uncertainty. Though many considerations have been discerned, their
details and interrelationships remain unclear and iffy—and there might be
other factors we have not even thought of yet. What are we to do in this
predicament?
Philosophy with a deadline
A colleague of mine likes to point out that a Fields Medal (the highest honor in
mathematics) indicates two things about the recipient: that he was capable of
accomplishing something important, and that he didn’t. Though harsh, the remark
hints at a truth.
Think of a “discovery” as an act that moves the arrival of information from a later
point in time to an earlier time. The discovery’s value does not equal the value of the
information discovered but rather the value of having the information available
earlier than it otherwise would have been. A scientist or a mathematician may show
great skill by being the first to find a solution that has eluded many others; yet if the
problem would soon have been solved anyway, then the work probably has not much
benefited the world. There are cases in which having a solution even slightly sooner
is immensely valuable, but this is most plausible when the solution is immediately
put to use, either being deployed for some practical end or serving as a foundation to
further theoretical work. And in the latter case, where a solution is immediately used
only in the sense of serving as a building block for further theorizing, there is great
value in obtaining a solution slightly sooner only if the further work it enables is
itself both important and urgent. 1
The question, then, is not whether the result discovered by the Fields Medalist is
in itself “important” (whether instrumentally or for knowledge’s own sake). Rather,
the question is whether it was important that the medalist enabled the publication of
the result to occur at an earlier date. The value of this temporal transport should be
compared to the value that a world-class mathematical mind could have generated
by working on something else. At least in some cases, the Fields Medal might
indicate a life spent solving the wrong problem—for instance, a problem whose
allure consisted primarily in being famously difficult to solve.Similar barbs could be directed at other fields, such as academic philosophy.
Philosophy covers some problems that are relevant to existential risk mitigation—
we encountered several in this book. Yet there are also subfields within philosophy
that have no apparent link to existential risk or indeed any practical concern. As with
pure mathematics, some of the problems that philosophy studies might be regarded
as intrinsically important, in the sense that humans have reason to care about them
independently of any practical application. The fundamental nature of reality, for
instance, might be worth knowing about, for its own sake. The world would arguably
be less glorious if nobody studied metaphysics, cosmology, or string theory.
However, the dawning prospect of an intelligence explosion shines a new light on
this ancient quest for wisdom.
The outlook now suggests that philosophic progress can be maximized via an
indirect path rather than by immediate philosophizing. One of the many tasks on
which superintelligence (or even just moderately enhanced human intelligence)
would outperform the current cast of thinkers is in answering fundamental questions
in science and philosophy. This reflection suggests a strategy of deferred
gratification. We could postpone work on some of the eternal questions for a little
while, delegating that task to our hopefully more competent successors—in order to
focus our own attention on a more pressing challenge: increasing the chance that we
will actually have competent successors. This would be high-impact philosophy and
high-impact mathematics. 2
What is to be done?
We thus want to focus on problems that are not only important but urgent in the
sense that their solutions are needed prior to the intelligence explosion. We should
also take heed not to work on problems that are negative-value (such that solving
them is harmful). Some technical problems in the field of artificial intelligence, for
instance, might be negative-value inasmuch as their solution would speed the
development of machine intelligence without doing as much to expedite the
development of control methods that could render the machine intelligence
revolution survivable and beneficial.
It can be hard to identify problems that are both urgent and important and are such
that we can confidently take them to be positive-value. The strategic uncertainty
surrounding existential risk mitigation means that we must worry that even well-
intentioned interventions may turn out to be not only unproductive but
counterproductive. To limit the risk of doing something actively harmful or morally
wrong, we should prefer to work on problems that seem robustly positive-value (i.e.,
whose solution would make a positive contribution across a wide range of scenarios)and to employ means that are robustly justifiable (i.e., acceptable from a wide range
of moral views).
There is a further desideratum to consider in selecting which problems to
prioritize. We want to work on problems that are elastic to our efforts at solving
them. Highly elastic problems are those that can be solved much faster, or solved to
a much greater extent, given one extra unit of effort. Encouraging more kindness in
the world is an important and urgent problem—one, moreover, that seems quite
robustly positive-value: yet absent a breakthrough idea for how to go about it,
probably a problem of quite low elasticity. Achieving world peace, similarly, would
be highly desirable; but considering the numerous efforts already targeting that
problem, and the formidable obstacles arrayed against a quick solution, it seems
unlikely that the contributions of a few extra individuals would make a large
difference.
To reduce the risks of the machine intelligence revolution, we will propose two
objectives that appear to best meet all those desiderata: strategic analysis and
capacity-building. We can be relatively confident about the sign of these parameters
—more strategic insight and more capacity being better. Furthermore, the
parameters are elastic: a small extra investment can make a relatively large
difference. Gaining insight and capacity is also urgent because early boosts to these
parameters may compound, making subsequent efforts more effective. In addition to
these two broad objectives, we will point to a few other potentially worthwhile aims
for initiatives.
Seeking the strategic light
Against a backdrop of perplexity and uncertainty, analysis stands out as being of
particularly high expected value. 3 Illumination of our strategic situation would help
us target subsequent interventions more effectively. Strategic analysis is especially
needful when we are radically uncertain not just about some detail of some
peripheral matter but about the cardinal qualities of the central things. For many key
parameters, we are radically uncertain even about their sign—that is, we know not
which direction of change would be desirable and which undesirable. Our ignorance
might not be irremediable. The field has been little prospected, and glimmering
strategic insights could still be awaiting their unearthing just a few feet beneath the
surface.
What we mean by “strategic analysis” here is a search for crucial considerations:
ideas or arguments with the potential to change our views not merely about the fine-
structure of implementation but about the general topology of desirability. 4 Even a
single missed crucial consideration could vitiate our most valiant efforts or renderthem as actively harmful as those of a soldier who is fighting on the wrong side. The
search for crucial considerations (which must explore normative as well as
descriptive issues) will often require crisscrossing the boundaries between different
academic disciplines and other fields of knowledge. As there is no established
methodology for how to go about this kind of research, difficult original thinking is
necessary.
Building good capacity
Another high-value activity, one that shares with strategic analysis the robustness
property of being beneficial across a wide range of scenarios, is the development of
a well-constituted support base that takes the future seriously. Such a base can
immediately provide resources for research and analysis. If and when other priorities
become visible, resources can be redirected accordingly. A support base is thus a
general-purpose capability whose use can be guided by new insights as they emerge.
One valuable asset would be a donor network comprising individuals devoted to
rational philanthropy, informed about existential risk, and discerning about the
means of mitigation. It is especially desirable that the early-day funders be astute
and altruistic, because they may have opportunities to shape the field’s culture
before the usual venal interests take up position and entrench. The focus during these
opening gambits should thus be to recruit the right kinds of people into the field. It
could be worth foregoing some technical advances in the short term in order to fill
the ranks with individuals who genuinely care about safety and who have a truth-
seeking orientation (and who are likely to attract more of their own kind).
One important variable is the quality of the “social epistemology” of the AI-field
and its leading projects. Discovering crucial considerations is valuable, but only if it
affects action. This cannot always be taken for granted. Imagine a project that
invests millions of dollars and years of toil to develop a prototype AI, and that after
surmounting many technical challenges the system is finally beginning to show real
progress. There is a chance that with just a bit more work it could turn into
something useful and profitable. Now a crucial consideration is discovered,
indicating that a completely different approach would be a bit safer. Does the project
kill itself off like a dishonored samurai, relinquishing its unsafe design and all the
progress that had been made? Or does it react like a worried octopus, puffing out a
cloud of motivated skepticism in the hope of eluding the attack? A project that
would reliably choose the samurai option in such a dilemma would be a far
preferable developer. 5 Yet building processes and institutions that are willing to
commit seppuku based on uncertain allegations and speculative reasoning is not
easy. Another dimension of social epistemology is the management of sensitive
information, in particular the ability to avoid leaking information that ought be keptsecret. (Information continence may be especially challenging for academic
researchers, accustomed as they are to constantly disseminating their results on
every available lamppost and tree.)
Particular measures
In addition to the general objectives of strategic light and good capacity, some more
specific objectives could also present cost-effective opportunities for action.
One such is progress on the technical challenges of machine intelligence safety. In
pursing this objective, care should be taken to manage information hazards. Some
work that would be useful for solving the control problem would also be useful for
solving the competence problem. Work that burns down the AI fuse could easily be a
net negative.
Another specific objective is to promote “best practices” among AI researchers.
Whatever progress has been made on the control problem needs to be disseminated.
Some forms of computational experimentation, particularly if involving strong
recursive self-improvement, may also require the use of capability control to
mitigate the risk of an accidental takeoff. While the actual implementation of safety
methods is not so relevant today, it will increasingly become so as the state of the art
advances. And it is not too soon to call for practitioners to express a commitment to
safety, including endorsing the common good principle and promising to ramp up
safety if and when the prospect of machine superintelligence begins to look more
imminent. Pious words are not sufficient and will not by themselves make a
dangerous technology safe: but where the mouth goeth, the mind might gradually
follow.
Other opportunities may also occasionally arise to push on some pivotal
parameter, for example to mitigate some other existential risk, or to promote
biological cognitive enhancement and improvements of our collective wisdom, or
even to shift world politics into a more harmonious register.
Will the best in human nature please stand up
Before the prospect of an intelligence explosion, we humans are like small children
playing with a bomb. Such is the mismatch between the power of our plaything and
the immaturity of our conduct. Superintelligence is a challenge for which we are not
ready now and will not be ready for a long time. We have little idea when the
detonation will occur, though if we hold the device to our ear we can hear a faint
ticking sound.For a child with an undetonated bomb in its hands, a sensible thing to do would be
to put it down gently, quickly back out of the room, and contact the nearest adult.
Yet what we have here is not one child but many, each with access to an independent
trigger mechanism. The chances that we will all find the sense to put down the
dangerous stuff seem almost negligible. Some little idiot is bound to press the ignite
button just to see what happens.
Nor can we attain safety by running away, for the blast of an intelligence
explosion would bring down the entire firmament. Nor is there a grown-up in sight.
In this situation, any feeling of gee-wiz exhilaration would be out of place.
Consternation and fear would be closer to the mark; but the most appropriate
attitude may be a bitter determination to be as competent as we can, much as if we
were preparing for a difficult exam that will either realize our dreams or obliterate
them.
This is not a prescription of fanaticism. The intelligence explosion might still be
many decades off in the future. Moreover, the challenge we face is, in part, to hold
on to our humanity: to maintain our groundedness, common sense, and good-
humored decency even in the teeth of this most unnatural and inhuman problem. We
need to bring all our human resourcefulness to bear on its solution.
Yet let us not lose track of what is globally significant. Through the fog of
everyday trivialities, we can perceive—if but dimly—the essential task of our age.
In this book, we have attempted to discern a little more feature in what is otherwise
still a relatively amorphous and negatively defined vision—one that presents as our
principal moral priority (at least from an impersonal and secular perspective) the
reduction of existential risk and the attainment of a civilizational trajectory that
leads to a compassionate and jubilant use of humanity’s cosmic endowment.NOTES
PRELIMS
1. Not all endnotes contain useful information, however.
2. I don’t know which ones.
CHAPTER 1: PAST DEVELOPMENTS AND PRESENT CAPABILITIES
1. A subsistence-level income today is about $400 (Chen and Ravallion 2010).
A million subsistence-level incomes is thus $400,000,000. The current world
gross product is about $60,000,000,000,000 and in recent years has grown at
an annual rate of about 4% (compound annual growth rate since 1950, based
on Maddison [2010]). These figures yield the estimate mentioned in the text,
which of course is only an order-of-magnitude approximation. If we look
directly at population figures, we find that it currently takes the world
population about one and a half weeks to grow by one million; but this
underestimates the growth rate of the economy since per capita income is
also increasing. By 5000 BC , following the Agricultural Revolution, the world
population was growing at a rate of about 1 million per 200 years—a great
acceleration since the rate of perhaps 1 million per million years in early
humanoid prehistory—so a great deal of acceleration had already occurred by
then. Still, it is impressive that an amount of economic growth that took 200
years seven thousand years ago takes just ninety minutes now, and that the
world population growth that took two centuries then takes one and a half
weeks now. See also Maddison (2005).
2. Such dramatic growth and acceleration might suggest one notion of a
possible coming “singularity,” as adumbrated by John von Neumann in a
conversation with the mathematician Stanislaw Ulam:
Our conversation centred on the ever accelerating progress of technology and
changes in the mode of human life, which gives the appearance of approaching
some essential singularity in the history of the race beyond which human
affairs, as we know them, could not continue. (Ulam 1958)3. Hanson (2000).
4. Vinge (1993); Kurzweil (2005).
5. Sandberg (2010).
6. Van Zanden (2003); Maddison (1999, 2001); De Long (1998).
7. Two oft-repeated optimistic statements from the 1960s: “Machines will be
capable, within twenty years, of doing any work a man can do” (Simon 1965,
96); “Within a generation ... the problem of creating artificial intelligence
will substantially be solved” (Minsky 1967, 2). For a systematic review of AI
predictions, see Armstrong and Sotala (2012).
8. See, for example, Baum et al. (2011) and Armstrong and Sotala (2012).
9. It might suggest, however, that AI researchers know less about development
timelines than they think they do—but this could cut both ways: they might
overestimate as well as underestimate the time to AI.
10. Good (1965, 33).
11. One exception is Norbert Wiener, who did have some qualms about the
possible consequences. He wrote, in 1960: “If we use, to achieve our
purposes, a mechanical agency with whose operation we cannot efficiently
interfere once we have started it, because the action is so fast and
irrevocable that we have not the data to intervene before the action is
complete, then we had better be quite sure that the purpose put into the
machine is the purpose which we really desire and not merely a colourful
imitation of it” (Wiener 1960). Ed Fredkin spoke about his worries about
superintelligent AI in an interview described in McCorduck (1979). By
1970, Good himself writes about the risks, and even calls for the creation
of an association to deal with the dangers (Good [1970]; see also his later
article [Good 1982] where he foreshadows some of the ideas of “indirect
normativity” that we discuss in Chapter 13). By 1984, Marvin Minsky was
also writing about many of the key worries (Minsky 1984).
12. Cf. Yudkowsky (2008a). On the importance of assessing the ethical
implications of potentially dangerous future technologies before they
become feasible, see Roache (2008).
13. McCorduck (1979).
14. Newell et al. (1959).
15. The SAINTS program, the ANALOGY program, and the STUDENT
program, respectively. See Slagle (1963), Evans (1964, 1968), and Bobrow
(1968).
16. Nilsson (1984).
17. Weizenbaum (1966).
18. Winograd (1972).
19. Cope (1996); Weizenbaum (1976); Moravec (1980); Thrun et al. (2006);
Buehler et al. (2009); Koza et al. (2003). The Nevada Department of MotorVehicles issued the first license for a driverless car in May 2012.
20. The STANDUP system (Ritchie et al. 2007).
21. Schwartz (1987). Schwartz is here characterizing a skeptical view that he
thought was represented by the writings of Hubert Dreyfus.
22. One vocal critic during this period was Hubert Dreyfus. Other prominent
skeptics from this era include John Lucas, Roger Penrose, and John Searle.
However, among these only Dreyfus was mainly concerned with refuting
claims about what practical accomplishments we should expect from
existing paradigms in AI (though he seems to have been open to the
possibility that new paradigms could go further). Searle’s target was
functionalist theories in the philosophy of mind, not the instrumental
powers of AI systems. Lucas and Penrose denied that a classical computer
could ever be programmed to do everything that a human mathematician
can do, but they did not deny that any particular function could in principle
be automated or that AIs might eventually become very instrumentally
powerful. Cicero remarked that “there is nothing so absurd but some
philosopher has said it” (Cicero 1923, 119); yet it is surprisingly hard to
think of any significant thinker who has denied the possibility of machine
superintelligence in the sense used in this book.
23. For many applications, however, the learning that takes place in a neural
network is little different from the learning that takes place in linear
regression, a statistical technique developed by Adrien-Marie Legendre
and Carl Friedrich Gauss in the early 1800s.
24. The basic algorithm was described by Arthur Bryson and Yu-Chi Ho as a
multi-stage dynamic optimization method in 1969 (Bryson and Ho 1969).
The application to neural networks was suggested by Paul Werbos in 1974
(Werbos 1994), but it was only after the work by David Rumelhart,
Geoffrey Hinton, and Ronald Williams in 1986 (Rumelhart et al. 1986)
that the method gradually began to seep into the awareness of a wider
community.
25. Nets lacking hidden layers had previously been shown to have severely
limited functionality (Minsky and Papert 1969).
26. E.g., MacKay (2003).
27. Murphy (2012).
28. Pearl (2009).
29. We suppress various technical details here in order not to unduly burden the
exposition. We will have occasion to revisit some of these ignored issues
in Chapter 12.
30. A program p is a description of string x if p, run on (some particular)
universal Turing machine U, outputs x; we write this as U(p) = x. (Thestring x here represents a possible world.) The Kolmogorov complexity of
x is then K(x):=min p {l(p): U(p) = x}, where l(p) is the length of p in bits.
The “Solomonoff” probability of x is then defined as
where the sum is defined over all (“minimal,” i.e. not necessarily halting)
programs p for which U outputs a string starting with x (Hutter 2005).
31. Bayesian conditioning on evidence E gives
(The probability of a proposition [like E] is the sum of the probability of the
possible worlds in which it is true.)
32. Or randomly picks one of the possible actions with the highest expected
utility, in case there is a tie.
33. More concisely, the expected utility of an action can be written as
where the sum is over all possible worlds.
34. See, e.g., Howson and Urbach (1993); Bernardo and Smith (1994); Russell
and Norvig (2010).
35. Wainwright and Jordan (2008). The application areas of Bayes nets are
myriad; see, e.g., Pourret et al. (2008).
36. One might wonder why so much detail is given to game AI here, which to
some might seem like an unimportant application area. The answer is that
game-playing offers some of the clearest measures of human vs. AI
performance.
37. Samuel (1959); Schaeffer (1997, ch. 6).
38. Schaeffer et al. (2007).
39. Berliner (1980a, b).
40. Tesauro (1995).
41. Such programs include GNU (see Silver [2006]) and Snowie (see
Gammoned.net [2012]).
42. Lenat himself had a hand in guiding the fleet-design process. He wrote:
“Thus the final crediting of the win should be about 60/40%
Lenat/Eurisko, though the significant point here is that neither party could
have won alone” (Lenat 1983, 80).
43. Lenat (1982, 1983).
44. Cirasella and Kopec (2006).
45. Kasparov (1996, 55).
46. Newborn (2011).
47. Keim et al. (1999).
48. See Armstrong (2012).49. Sheppard (2002).
50. Wikipedia (2012a).
51. Markoff (2011).
52. Rubin and Watson (2011).
53. Elyasaf et al. (2011).
54. KGS (2012).
55. Newell et al. (1958, 320).
56. Attributed in Vardi (2012).
57. In 1976, I. J. Good wrote: “A computer program of Grandmaster strength
would bring us within an ace of [machine ultra-intelligence]” (Good 1976).
In 1979, Douglas Hofstadter opined in his Pulitzer-winning Gödel, Escher,
Bach: “Question: Will there be chess programs that can beat anyone?
Speculation: No. There may be programs that can beat anyone at chess, but
they will not be exclusively chess programs. They will be programs of
general intelligence, and they will be just as temperamental as people. ‘Do
you want to play chess?’ ‘No, I’m bored with chess. Let’s talk about
poetry’” (Hofstadter [1979] 1999, 678).
58. The algorithm is minimax search with alpha-beta pruning, used with a
chess-specific heuristic evaluation function of board states. Combined
with a good library of openings and endgames, and various other tricks,
this can make for a capable chess engine.
59. Though especially with recent progress in learning the evaluation heuristic
from simulated games, many of the underlying algorithms would probably
also work well for many other games.
60. Nilsson (2009, 318). Knuth was certainly overstating his point. There are
many “thinking tasks” that AI has not succeeded in doing—inventing a
new subfield of pure mathematics, doing any kind of philosophy, writing a
great detective novel, engineering a coup d’état, or designing a major new
consumer product.
61. Shapiro (1992).
62. One might speculate that one reason it has been difficult to match human
abilities in perception, motor control, common sense, and language
understanding is that our brains have dedicated wetware for these
functions—neural structures that have been optimized over evolutionary
timescales. By contrast, logical thinking and skills like chess playing are
not natural to us; so perhaps we are forced to rely on a limited pool of
general-purpose cognitive resources to perform these tasks. Maybe what
our brains do when we engage in explicit logical reasoning or calculation
is in some ways analogous to running a “virtual machine,” a slow and
cumbersome mental simulation of a general-purpose computer. One might
then say (somewhat fancifully) that a classical AI program is not so muchemulating human thinking as the other way around: a human who is
thinking logically is emulating an AI program.
63. This example is controversial: a minority view, represented by
approximately 20% of adults in the USA and similar numbers in many
other developed nations, holds that the Sun revolves around the Earth
(Crabtree 1999; Dean 2005).
64. World Robotics (2011).
65. Estimated from data in Guizzo (2010).
66. Holley (2009).
67. Hybrid rule-based statistical approaches are also used, but they are currently
a small part of the picture.
68. Cross and Walker (1994); Hedberg (2002).
69. Based on the statistics from TABB Group, a New York- and London-based
capital markets research firm (personal communication).
70. CFTC and SEC (2010). For a different perspective on the events of 6 May
2010, see CME Group (2010).
71. Nothing in the text should be construed as an argument against algorithmic
high-frequency trading, which might normally perform a beneficial
function by increasing liquidity and market efficiency.
72. A smaller market scare occurred on August, 1, 2012, in part because the
“circuit breaker” was not also programmed to halt trading if there were
extreme changes in the number of shares being traded (Popper 2012). This
again foreshadows another later theme: the difficulty of anticipating all
specific ways in which some particular plausible-seeming rule might go
wrong.
73. Nilsson (2009, 319).
74. Minsky (2006); McCarthy (2007); Beal and Winston (2009).
75. Peter Norvig, personal communication. Machine-learning classes are also
very popular, reflecting a somewhat orthogonal hype-wave of “big data”
(inspired by e.g. Google and the Netflix Prize).
76. Armstrong and Sotala (2012).
77. Müller and Bostrom (forthcoming).
78. See Baum et al. (2011), another survey cited therein, and Sandberg and
Bostrom (2011).
79. Nilsson (2009).
80. This is again conditional on no civilization-disrupting catastrophe
occurring. The definition of HLMI used by Nilsson is “AI able to perform
around 80% of jobs as well or better than humans perform” (Kruel 2012).
81. The table shows the results of four different polls as well as the combined
results. The first two were polls taken at academic conferences: PT-AI,participants of the conference Philosophy and Theory of AI in Thessaloniki
2011 (respondents were asked in November 2012), with a response rate of
43 out of 88; and AGI, participants of the conferences Artificial General
Intelligence and Impacts and Risks of Artificial General Intelligence, both
in Oxford, December 2012 (response rate: 72/111). The EETN poll
sampled the members of the Greek Association for Artificial Intelligence,
a professional organization of published researchers in the field, in April
2013 (response rate: 26/250). The TOP100 poll elicited the opinions
among the 100 top authors in artificial intelligence as measured by a
citation index, in May 2013 (response rate: 29/100).
82. Interviews with some 28 (at the time of writing) AI practitioners and related
experts have been posted by Kruel (2011).
83. The diagram shows renormalized median estimates. Means are significantly
different. For example, the mean estimates for the “Extremely bad”
outcome were 7.6% (for TOP100) and 17.2% (for the combined pool of
expert assessors).
84. There is a substantial literature documenting the unreliability of expert
forecasts in many domains, and there is every reason to think that many of
the findings in this body of research apply to the field of artificial
intelligence too. In particular, forecasters tend to be overconfident in their
predictions, believing themselves to be more accurate than they really are,
and therefore assigning too little probability to the possibility that their
most-favored hypothesis is wrong (Tetlock 2005). (Various other biases
have also been documented; see, e.g., Gilovich et al. [2002].) However,
uncertainty is an inescapable fact of the human condition, and many of our
actions unavoidably rely on expectations about which long-term
consequences are more or less plausible: in other words, on probabilistic
predictions. Refusing to offer explicit probabilistic predictions would not
make the epistemic problem go away; it would just hide it from view
(Bostrom 2007). Instead, we should respond to evidence of overconfidence
by broadening our confidence intervals (or “credible intervals”)—i.e. by
smearing out our credence functions—and in general we must struggle as
best we can with our biases, by considering different perspectives and
aiming for intellectual honesty. In the longer run, we can also work to
develop techniques, training methods, and institutions that can help us
achieve better calibration. See also Armstrong and Sotala (2012).
CHAPTER 2: PATHS TO SUPERINTELLIGENCE
1. This resembles the definition in Bostrom (2003c) and Bostrom (2006a). Itcan also be compared with Shane Legg’s definition (“Intelligence measures
an agent’s ability to achieve goals in a wide range of environments”) and its
formalizations (Legg 2008). It is also very similar to Good’s definition of
ultraintelligence in Chapter 1 (“a machine that can far surpass all the
intellectual activities of any man however clever”).
2. For the same reason, we make no assumption regarding whether a
superintelligent machine could have “true intentionality” (pace Searle, it
could; but this seems irrelevant to the concerns of this book). And we take no
position in the internalism/externalism debate about mental content that has
been raging in the philosophical literature, or on the related issue of the
extended mind thesis (Clark and Chalmers 1998).
3. Turing (1950, 456).
4. Turing (1950, 456).
5. Chalmers (2010); Moravec (1976, 1988, 1998, 1999).
6. See Moravec (1976). A similar argument is advanced by David Chalmers
(2010).
7. See also Shulman and Bostrom (2012), where these matters are elaborated in
more detail.
8. Legg (2008) offers this reason in support of the claim that humans will be
able to recapitulate the progress of evolution over much shorter timescales
and with reduced computational resources (while noting that evolution’s
unadjusted computational resources are far out of reach). Baum (2004)
argues that some developments relevant to AI occurred earlier, with the
organization of the genome itself embodying a valuable representation for
evolutionary algorithms.
9. Whitman et al. (1998); Sabrosky (1952).
10. Schultz (2000).
11. Menzel and Giurfa (2001, 62); Truman et al. (1993).
12. Sandberg and Bostrom (2008).
13. See Legg (2008) for further discussion of this point and of the promise of
functions or environments that determine fitness based on a smooth
landscape of pure intelligence tests.
14. See Bostrom and Sandberg (2009b) for a taxonomy and more detailed
discussion of ways in which engineers may outperform historical
evolutionary selection.
15. The analysis has addressed the nervous systems of living creatures, without
reference to the cost of simulating bodies or the surrounding virtual
environment as part of a fitness function. It is plausible that an adequate
fitness function could test the competence of a particular organism in far
fewer operations than it would take to simulate all the neuronal
computation of that organism’s brain throughout its natural lifespan. AIprograms today often develop and operate in very abstract environments
(theorem provers in symbolic math worlds, agents in simple game
tournament worlds, etc.).
A skeptic might insist that an abstract environment would be inadequate for
the evolution of general intelligence, believing instead that the virtual
environment would need to closely resemble the actual biological environment
in which our ancestors evolved. Creating a physically realistic virtual world
would require a far greater investment of computational resources than the
simulation of a simple toy world or abstract problem domain (whereas
evolution had access to a physically realistic real world “for free”). In the
limiting case, if complete micro-physical accuracy were insisted upon, the
computational requirements would balloon to ridiculous proportions. However,
such extreme pessimism is almost certainly unwarranted; it seems unlikely that
the best environment for evolving intelligence is one that mimics nature as
closely as possible. It is, on the contrary, plausible that it would be more
efficient to use an artificial selection environment, one quite unlike that of our
ancestors, an environment specifically designed to promote adaptations that
increase the type of intelligence we are seeking to evolve (abstract reasoning
and general problem-solving skills, for instance, as opposed to maximally fast
instinctual reactions or a highly optimized visual system).
16. Wikipedia (2012b).
17. For a general treatment of observation selection theory, see Bostrom
(2002a). For the specific application to the current issue, see Shulman and
Bostrom (2012). For a short popular introduction, see Bostrom (2008b).
18. Sutton and Barto (1998, 21f); Schultz et al. (1997).
19. This term was introduced by Eliezer Yudkowsky; see, e.g., Yudkowsky
(2007).
20. This is the scenario described by Good (1965) and Yudkowsky (2007).
However, one could also consider an alternative in which the iterative
sequence has some steps that do not involve intelligence enhancement but
instead design simplification. That is, at some stages, the seed AI might
rewrite itself so as make subsequent improvements easier to find.
21. Helmstaedter et al. (2011).
22. Andres et al. (2012).
23. Adequate for enabling instrumentally useful forms of cognitive functioning
and communication, that is; but still radically impoverished relative to the
interface provided by the muscles and sensory organs of a normal human
body.
24. Sandberg (2013).
25. See the “Computer requirements” section of Sandberg and Bostrom (2008,79–81).
26. A lower level of success might be a brain simulation that has biologically
suggestive micro-dynamics and displays a substantial range of emergent
species-typical activity such as a slow-wave sleep state or activity-
dependent plasticity. Whereas such a simulation could be a useful testbed
for neuroscientific research (though one which might come close to raising
serious ethical issues), it would not count as a whole brain emulation
unless the simulation were sufficiently accurate to be able to perform a
substantial fraction of the intellectual work that the simulated brain was
capable of. As a rule of thumb, we might say that in order for a simulation
of a human brain to count as a whole brain emulation, it would need to be
able to express coherent verbal thoughts or have the capacity to learn to do
so.
27. Sandberg and Bostrom (2008).
28. Sandberg and Bostrom (2008). Further explanation can be found in the
original report.
29. The first map is described in Albertson and Thomson (1976) and White et
al. (1986). The combined (and in some cases corrected) network is
available from the “WormAtlas” website (http://www.wormatlas.org/).
30. For a review of past attempts of emulating C. elegans and their fates, see
Kaufman (2011). Kaufman quotes one ambitious doctoral student working
in the area, David Dalrymple, as saying, “With optogenetic techniques, we
are just at the point where it’s not an outrageous proposal to reach for the
capability to read and write to anywhere in a living C. elegans nervous
system, using a high-throughput automated system.... I expect to be
finished with C. elegans in 2–3 years. I would be extremely surprised, for
whatever that’s worth, if this is still an open problem in 2020” (Dalrymple
2011). Brain models aiming for biological realism that were hand-coded
(rather than generated automatically) have achieved some basic
functionality; see, e.g., Eliasmith et al. (2012).
31. Caenorhabditis elegans does have some convenient special properties. For
example, the organism is transparent, and the wiring pattern of its nervous
system does not change between individuals.
32. If neuromorphic AI rather than whole brain emulation is the end product,
then it might or might not be the case that the relevant insights would be
derived through attempts to simulate human brains. It is conceivable that
the important cortical tricks would be discovered during the study of
(nonhuman) animal brains. Some animal brains might be easier to work
with than human brains, and smaller brains would require fewer resources
to scan and model. Research on animal brains would also be subject to less
regulation. It is even conceivable that the first human-level machineintelligence will be created by completing a whole brain emulation of
some suitable animal and then finding ways to enhance the resultant
digital mind. Thus humanity could get its comeuppance from an uplifted
lab mouse or macaque.
33. Uauy and Dangour (2006); Georgieff (2007); Stewart et al. (2008); Eppig et
al. (2010); Cotman and Berchtold (2002).
34. According to the World Health Organization in 2007, nearly 2 billion
individuals have insufficient iodine intake (The Lancet 2008). Severe
iodine deficiency hinders neurological development and leads to cretinism,
which involves an average loss of about 12.5 IQ points (Qian et al. 2005).
The condition can be easily and inexpensively prevented though salt
fortification (Horton et al. 2008).
35. Bostrom and Sandberg (2009a).
36. Bostrom and Sandberg (2009b). A typical putative performance increase
from pharmacological and nutritional enhancement is in the range of 10–
20% on test tasks measuring working memory, attention, etc. But it is
generally dubious whether such reported gains are real, sustainable over a
longer term, and indicative of correspondingly improved results in real-
world problem situations (Repantis et al. 2010). For instance, in some
cases there might be a compensating deterioration on some performance
dimensions that are not measured by the test tasks (Sandberg and Bostrom
2006).
37. If there were an easy way to enhance cognition, one would expect evolution
already to have taken advantage of it. Consequently, the most promising
kind of nootropic to investigate may be one that promises to boost
intelligence in some manner that we can see would have lowered fitness in
the ancestral environment—for example, by increasing head size at birth
or amping up the brain’s glucose metabolism. For a more detailed
discussion of this idea (along with several important qualifications), see
Bostrom (2009b).
38. Sperm are harder to screen because, in contrast to embryos, they consist of
only one cell—and one cell needs to be destroyed in order to do the
sequencing. Oocytes also consist of only one cell; however, the first and
second cell divisions are asymmetric and produce one daughter cell with
very little cytoplasm, the polar body. Since polar bodies contain the same
genome as the main cell and are redundant (they eventually degenerate)
they can be biopsied and used for screening (Gianaroli 2000).
39. Each of these practices was subject to some ethical controversy when it was
introduced, but there seems to be a trend toward increasing acceptance.
Attitudes toward human genetic engineering and embryo selection varysignificantly across cultures, suggesting that development and application
of new techniques will probably take place even if some countries initially
adopt a cautious stance, although the rate at which this happens will be
influenced by moral, religious, and political pressures.
40. Davies et al. (2011); Benyamin et al. (2013); Plomin et al. (2013). See also
Mardis (2011); Hsu (2012).
41. Broad-sense heritability of adult IQ is usually estimated in the range of 0.5–
0.8 within middle-class strata of developed nations (Bouchard 2004, 148).
Narrow-sense heritability, which measures the portion of variance that is
attributable to additive genetic factors, is lower (in the range 0.3–0.5) but
still substantial (Devlin et al. 1997; Davies et al. 2011; Visscher et al.
2008). These estimates could change for different populations and
environments, as heritabilities vary depending on the population and
environment being studied. For example, lower heritabilities have been
found among children and those from deprived environments (Benyamin
et al. 2013; Turkheimer et al. 2003). Nisbett et al. (2012) review numerous
environmental influences on variation in cognitive ability.
42. The following several paragraphs draw heavily on joint work with Carl
Shulman (Shulman and Bostrom 2014).
43. This table is taken from Shulman and Bostrom (2014). It is based on a toy
model that assumes a Gaussian distribution of predicted IQs among the
embryos with a standard deviation of 7.5 points. The amount of cognitive
enhancement that would be delivered with different numbers of embryos
depends on how different the embryos are from one another in the additive
genetic variants whose effects we know. Siblings have a coefficient of
relatedness of 1⁄2, and common additive genetic variants account for half or
less of variance in adult fluid intelligence (Davies et al. 2011). These two
facts suggest that where the observed population standard deviation in
developed countries is 15 points, the standard deviation of genetic
influences within a batch of embryos would be 7.5 points or less.
44. With imperfect information about the additive genetic effects on cognitive
ability, effect sizes would be reduced. However, even a small amount of
knowledge would go a relatively long way, because the gains from
selection do not scale linearly with the portion of variance that we can
predict. Instead, the effectiveness of our selection depends on the standard
deviation of predicted mean IQ, which scales as the square root of
variance. For example, if one could account for 12.5% of the variance, this
could deliver effects half as great as those in Table 1, which assume 50%.
For comparison, a recent study (Rietveld et al. 2013) claims to have
already identified 2.5% of the variance.
45. For comparison, standard practice today involves the creation of fewer thanten embryos.
46. Adult and embryonic stem cells can be coaxed to develop into sperm cells
and oocytes, which can then be fused to produce an embryo (Nagy et al.
2008; Nagy and Chang 2007). Egg cell precursors can also form
parthenogenetic blastocysts, unfertilized and non-viable embryos, able to
produce embryonic stem cell lines for the process (Mai et al. 2007).
47. The opinion is that of Katsuhiko Hayashi, as reported in Cyranoski (2013).
The Hinxton Group, an international consortium of scientists that
discusses stem cell ethics and challenges, predicted in 2008 that human
stem cell-derived gametes would be available within ten years (Hinxton
Group 2008), and developments thus far are broadly consistent with this.
48. Sparrow (2013); Miller (2012); The Uncertain Future (2012).
49. Sparrow (2013).
50. Secular concerns might focus on anticipated impacts on social inequality,
the medical safety of the procedure, fears of an enhancement “rat race,”
rights and responsibilities of parents vis-à-vis their prospective offspring,
the shadow of twentieth-century eugenics, the concept of human dignity,
and the proper limits of states’ involvement in the reproductive choices of
their citizens. (For a discussion of the ethics of cognitive enhancement see
Bostrom and Ord [2006], Bostrom and Roache [2011], and Sandberg and
Savulescu [2011].) Some religious traditions may offer additional
concerns, including ones centering on the moral status of embryos or the
proper limits of human agency within the scheme of creation.
51. To stave off the negative effects of inbreeding, iterated embryo selection
would require either a large starting supply of donors or the expenditure of
substantial selective power to reduce harmful recessive alleles. Either
alternative would tend to push toward offspring being less closely
genetically related to their parents (and more related to one another).
52. Adapted from Shulman and Bostrom (2014).
53. Bostrom (2008b).
54. Just how difficult an obstacle epigenetics will be is not yet known (Chason
et al. 2011; Iliadou et al. 2011).
55. While cognitive ability is a fairly heritable trait, there may be few or no
common alleles or polymorphisms that individually have a large positive
effect on intelligence (Davis et al. 2010; Davies et al. 2011; Rietveld et al.
2013). As sequencing methods improve, the mapping out of low-frequency
alleles and their cognitive and behavioral correlates will become
increasingly feasible. There is some theoretical evidence suggesting that
some alleles that cause genetic disorders in homozygotes may provide
sizeable cognitive advantages in heterozygote carriers, leading to aprediction that Gaucher, Tay-Sachs, and Niemann-Pick heterozygotes
would be about 5 IQ points higher than control groups (Cochran et al.
2006). Time will tell whether this holds.
56. One paper (Nachman and Crowell 2000) estimates 175 mutations per
genome per generation. Another (Lynch 2010), using different methods,
estimates that the average newborn has between 50 and 100 new
mutations, and Kong et al. (2012) implies a figure of around 77 new
mutations per generation. Most of these mutations do not affect
functioning, or do so only to an imperceptibly slight degree; but the
combined effects of many very slightly deleterious mutations could be a
significant loss of fitness. See also Crow (2000).
57. Crow (2000); Lynch (2010).
58. There are some potentially important caveats to this idea. It is possible that
the modal genome would need some adjustments in order to avoid
problems. For example, parts of the genome might be adapted to
interacting with other parts under the assumption that all parts function
with a certain level of efficiency. Increasing the efficiency of those parts
might then lead to overshooting along some metabolic pathways.
59. These composites were created by Mike Mike from individual photographs
taken by Virtual Flavius (Mike 2013).
60. They can, of course, have some effects sooner—for instance, by changing
people’s expectations of what is to come.
61. Louis Harris & Associates (1969); Mason (2003).
62. Kalfoglou et al. (2004).
63. The data is obviously limited, but individuals selected for 1-in-10,000
results on childhood ability tests have been shown, in longitudinal studies,
to be substantially more likely to become tenured professors, earn patents,
and succeed in business than those with slightly less exceptional scores
(Kell et al. 2013). Roe (1953) studied sixty-four eminent scientists and
found median cognitive ability three to four standard deviations above the
population norm and strikingly higher than is typical for scientists in
general. (Cognitive ability is also correlated with lifetime earnings and
with non-financial outcomes such as life expectancy, divorce rates, and
probability of dropping out of school [Deary 2012].) An upward shift of
the distribution of cognitive ability would have disproportionately large
effects at the tails, especially increasing the number of highly gifted and
reducing the number of people with retardation and learning disabilities.
See also Bostrom and Ord (2006) and Sandberg and Savulescu (2011).
64. E.g. Warwick (2002). Stephen Hawking even suggested that taking this step
might be necessary in order to keep up with advances in machine
intelligence: “We must develop as quickly as possible technologies thatmake possible a direct connection between brain and computer, so that
artificial brains contribute to human intelligence rather than opposing it”
(reported in Walsh [2001]). Ray Kurzweil concurs: “As far as Hawking’s
... recommendation is concerned, namely direct connection between the
brain and computers, I agree that this is both reasonable, desirable and
inevitable. [sic] It’s been my recommendation for years” (Kurzweil 2001).
65. See Lebedev and Nicolelis (2006); Birbaumer et al. (2008); Mak and
Wolpaw (2009); and Nicolelis and Lebedev (2009). A more personal
outlook on the problem of enhancement through implants can be found in
Chorost (2005, Chap. 11).
66. Smeding et al. (2006).
67. Degnan et al. (2002).
68. Dagnelie (2012); Shannon (2012).
69. Perlmutter and Mink (2006); Lyons (2011).
70. Koch et al. (2006).
71. Schalk (2008). For a general review of the current state of the art, see
Berger et al. (2008). For the case that this would help lead to enhanced
intelligence, see Warwick (2002).
72. Some examples: Bartels et al. (2008); Simeral et al. (2011); Krusienski and
Shih (2011); and Pasqualotto et al. (2012).
73. E.g. Hinke et al. (1993).
74. There are partial exceptions to this, especially in early sensory processing.
For example, the primary visual cortex uses a retinotopic mapping, which
means roughly that adjacent neural assemblies receive inputs from
adjacent areas of the retinas (though ocular dominance columns somewhat
complicate the mapping).
75. Berger et al. (2012); Hampson et al. (2012).
76. Some brain implants require two forms of learning: the device learning to
interpret the organism’s neural representations and the organism learning
to use the system by generating appropriate neural firing patterns
(Carmena et al. 2003).
77. It has been suggested that we should regard corporate entities (corporations,
unions, governments, churches, and so forth) as artificial intelligent
agents, entities with sensors and effectors, able to represent knowledge and
perform inference and take action (e.g. Kuipers [2012]; cf. Huebner [2008]
for a discussion on whether collective representations can exist). They are
clearly powerful and ecologically successful, although their capabilities
and internal states are different from those of humans.
78. Hanson (1995, 2000); Berg and Rietz (2003).
79. In the workplace, for instance, employers might use lie detectors to crackdown on employee theft and shirking, by asking the employee at the end of
each business day whether she has stolen anything and whether she has
worked as hard as she could. Political and business leaders could likewise
be asked whether they were wholeheartedly pursuing the interests of their
shareholders or constituents. Dictators could use them to target seditious
generals within the regime or suspected troublemakers in the wider
population.
80. One could imagine neuroimaging techniques making it possible to detect
neural signatures of motivated cognition. Without self-deception
detection, lie detection would favor individuals who believe their own
propaganda. Better tests for self-deception tests could also be used to train
rationality and to study the effectiveness of interventions aimed at
reducing biases.
81. Bell and Gemmel (2009). An early example is found in the work of MIT’s
Deb Roy, who recorded every moment of his son’s first three years of life.
Analysis of this audiovisual data is yielding information on language
development; see Roy (2012).
82. Growth in total world population of biological human beings will contribute
only a small factor. Scenarios involving machine intelligence could see the
world population (including digital minds) explode by many orders of
magnitude in a brief period of time. But that road to superintelligence
involves artificial intelligence or whole brain emulation, so we need not
consider it in this subsection.
83. Vinge (1993).
CHAPTER 3: FORMS OF SUPERINTELLIGENCE
1. Vernor Vinge has used the term “weak superintelligence” to refer to such
sped-up human minds (Vinge 1993).
2. For example, if a very fast system could do everything that any human could
do except dance a mazurka, we should still call it a speed superintelligence.
Our interest lies in those core cognitive capabilities that have economic or
strategic significance.
3. At least a millionfold speedup compared to human brains is physically
possible, as can been seen by considering the difference in speed and energy
of relevant brain processes in comparison to more efficient information
processing. The speed of light is more than a million times greater than that
of neural transmission, synaptic spikes dissipate more than a million times
more heat than is thermodynamically necessary, and current transistor
frequencies are more than a million times faster than neuron spikingfrequencies (Yudkowsky [2008a]; see also Drexler [1992]). The ultimate
limits of speed superintelligence are bounded by light-speed communications
delays, quantum limits on the speed of state transitions, and the volume
needed to contain the mind (Lloyd 2000). The “ultimate laptop” described by
Lloyd (2000) would run a 1.4×10 21 FLOPS brain emulation at speedup of
3.8×10 29 × (assuming the emulation could be sufficiently parallelized).
Lloyd’s construction, however, is not intended to be technologically
plausible; it is only meant to illustrate those constraints on computation that
are readily derivable from basic physical laws.
4. With emulations, there is also an issue of how long a human-like mind can
keep working on something before going mad or falling into a rut. Even with
task variety and regular holidays, it is not certain that a human-like mind
could live for thousands of subjective years without developing
psychological problems. Furthermore, if total memory capacity is limited—a
consequence of having a limited neuron population—then cumulative
learning cannot continue indefinitely: beyond some point, the mind must
start forgetting one thing for each new thing it learns. (Artificial intelligence
could be designed such as to ameliorate these potential problems.)
5. Accordingly, nanomechanisms moving at a modest 1 m/s have typical
timescales of nanoseconds. See section 2.3.2 of Drexler (1992). Robin
Hanson mentions 7-mm “tinkerbell” robot bodies moving at 260 times
normal speed (Hanson 1994).
6. Hanson (2012).
7. “Collective intelligence” does not refer to low-level parallelization of
computing hardware but to parallelization at the level of intelligent
autonomous agents such as human beings. Implementing a single emulation
on a massively parallel machine might result in speed superintelligence if the
parallel computer is sufficiently fast: it would not produce a collective
intelligence.
8. Improvements to the speed or the quality of the individual components could
also indirectly affect the performance of collective intelligence, but here we
mainly consider such improvements under the other two forms of
superintelligence in our classification.
9. It has been argued that a higher population density triggered the Upper
Paleolithic Revolution and that beyond a certain threshold accumulation of
cultural complexity became much easier (Powell et al. 2009).
10. What about the Internet? It seems not yet to have amounted to a super-sized
boost. Maybe it will do so eventually. It took centuries or millennia for the
other examples listed here to reveal their full potential.
11. This is, obviously, not meant to be a realistic thought experiment. A planetlarge enough to sustain seven quadrillion human organisms with present
technology would implode, unless it were made of very light matter or
were hollow and held up by pressure or other artificial means. (A Dyson
sphere or a Shellworld might be a better solution.) History would have
unfolded differently on such a vast surface. Set all this aside.
12. Our focus here is on the functional properties of a unified intellect, not on
the question of whether such an intellect would have qualia or whether it
would be a mind in the sense of having subjective conscious experience.
(One might ponder, though, what kinds of conscious experience might
arise from intellects that are more or less integrated than those of human
brains. On some views of consciousness, such as the global workspace
theory, it seems one might expect more integrated brains to have more
capacious consciousness. Cf. Baars (1997), Shanahan (2010), and
Schwitzgebel (2013).)
13. Even small groups of humans that have remained isolated for some time
might still benefit from the intellectual outputs of a larger collective
intelligence. For example, the language they use might have been
developed by a much larger linguistic community, and the tools they use
might have been invented in a much larger population before the small
group became isolated. But even if a small group had always been isolated,
it might still be part of a larger collective intelligence than meets the eye
—namely, the collective intelligence consisting of not only the present but
all ancestral generations as well, an aggregate that can function as a feed-
forward information processing system.
14. By the Church–Turing thesis, all computable functions are computable by a
Turing machine. Since any of the three forms of superintelligence could
simulate a Turing machine (if given access to unlimited memory and
allowed to operate indefinitely), they are by this formal criterion
computationally equivalent. Indeed, an average human being (provided
with unlimited scrap paper and unlimited time) could also implement a
Turing machine, and thus is also equivalent by the same criterion. What
matters for our purposes, however, is what these different systems can
achieve in practice, with finite memory and in reasonable time. And the
efficiency variations are so great that one can readily make some
distinctions. For example, a typical individual with an IQ of 85 could be
taught to implement a Turing machine. (Conceivably, it might even be
possible to train some particularly gifted and docile chimpanzee to do
this.) Yet, for all practical intents and purposes, such an individual is
presumably incapable of, say, independently developing general relativity
theory or of winning a Fields medal.
15. Oral storytelling traditions can produce great works (such as the Homericepics) but perhaps some of the contributing authors possessed uncommon
gifts.
16. Unless it contains as components intellects that have speed or quality
superintelligence.
17. Our inability to specify what all these problems are may in part be due to a
lack of trying: there is little point in spending time detailing intellectual
jobs that no individual and no currently feasible organization can perform.
But it is also possible that even conceptualizing some of these jobs is itself
one of those jobs that we currently lack the brains to perform.
18. Cf. Boswell (1917); see also Walker (2002).
19. This mainly occurs in short bursts in a subset of neurons—most have more
sedate firing rates (Gray and McCormick 1996; Steriade et al. 1998). There
are some neurons (“chattering neurons,” also known as “fast rhythmically
bursting” cells) that may reach firing frequencies as high as 750 Hz, but
these seem to be extreme outliers.
20. Feldman and Ballard (1982).
21. The conduction velocity depends on axon diameter (thicker axons are
faster) and whether the axon is myelinated. Within the central nervous
system, transmission delays can range from less than a millisecond to up
to 100 ms (Kandel et al. 2000). Transmission in optical fibers is around
68% c (because of the refractive index of the material). Electrical cables
are roughly the same speed, 59–77% c.
22. This assumes a signal velocity of 70% c. Assuming 100% c ups the estimate
to 1.8×10 18 m 3 .
23. The number of neurons in an adult human male brain has been estimated at
86.1 ± 8.1 billion, a number arrived at by dissolving brains and
fractionating out the cell nuclei, counting the ones stained with a neuron-
specific marker. In the past, estimates in the neighborhood of 75–125
billion neurons were common. These were typically based on manual
counting of cell densities in representative small regions (Azevedo et al.
2009).
24. Whitehead (2003).
25. Information processing systems can very likely use molecular-scale
processes for computing and data storage and reach at least planetary size
in extent. The ultimate physical limits to computation set by quantum
mechanics, general relativity, and thermodynamics are, however, far
beyond this “Jupiter brain” level (Sandberg 1999; Lloyd 2000).
26. Stansberry and Kudritzki (2012). Electricity used in data centers worldwide
amounted to 1.1–1.5% of total electricity use (Koomey 2011). See also
Muehlhauser and Salamon (2012).27. This is an oversimplification. The number of chunks working memory can
maintain is both information- and task-dependent; however, it is clearly
limited to a small number of chunks. See Miller (1956) and Cowan (2001).
28. An example might be that the difficulty of learning Boolean concepts
(categories defined by logical rules) is proportional to the length of the
shortest logically equivalent propositional formula. Typically, even
formulae just 3–4 literals long are very difficult to learn. See Feldman
(2000).
29. See Landauer (1986). This study is based on experimental estimates of
learning and forgetting rates in humans. Taking into account implicit
learning might push the estimate up a little. If one assumes a storage
capacity ~1 bit per synapse, one gets an upper bound on human memory
capacity of about 10 15 bits. For an overview of different estimates, see
Appendix A of Sandberg and Bostrom (2008).
30. Channel noise can trigger action potentials, and synaptic noise produces
significant variability in the strength of transmitted signals. Nervous
systems appear to have evolved to make numerous trade-offs between
noise tolerance and costs (mass, size, time delays); see Faisal et al. (2008).
For example, axons cannot be thinner than 0.1 μm lest random opening of
ion channels create spontaneous action potentials (Faisal et al. 2005).
31. Trachtenberg et al. (2002).
32. In terms of memory and computational power, though not in terms of
energy efficiency. The fastest computer in the world at the time of writing
was China’s “Tianhe-2,” which displaced Cray Inc. Titan in June 2013
with a performance of 33.86 petaFLOPS. It uses 17.6 MW of power,
almost six orders of magnitude more than the brain’s ~20 W.
33. Note that this survey of sources of machine advantage is disjunctive: our
argument succeeds even if some of the items listed are illusory, so long as
there is at least one source that can provide a sufficiently large advantage.
CHAPTER 4: THE KINETICS OF AN INTELLIGENCE EXPLOSION
1. The system may not reach one of these baselines at any sharply defined
point. There may instead be an interval during which the system gradually
becomes able to outperform the external research team on an increasing
number of system-improving development tasks.
2. In the past half-century, at least one scenario has been widely recognized in
which the existing world order would come to an end in the course of
minutes or hours: global thermonuclear war.
3. This would be consistent with the observation that the Flynn effect—thesecular increase in measured IQ scores within most populations at a rate of
some 3 IQ points per decade over the past 60 years or so—appears to have
ceased or even reversed in recent years in some highly developed countries
such as the United Kingdom, Denmark, and Norway (Teasdale and Owen
2008; Sundet et al. 2004). The cause of the Flynn effect in the past—and
whether and to what extent it represents any genuine gain in general
intelligence or merely improved skill at solving IQ test-style puzzles—has
been the subject of wide debate and is still not known. Even if the Flynn
effect (at least partially) reflects real cognitive gains, and even if the effect is
now diminishing or even reversing, this does not prove that we have yet hit
diminishing returns in whatever underlying cause was responsible for the
observed Flynn effect in the past. The decline or reversal could instead be
due to some independent detrimental factor that would otherwise have
produced an even bigger observed decline.
4. Bostrom and Roache (2011).
5. Somatic gene therapy could eliminate the maturational lag, but is technically
much more challenging than germline interventions and has a lower ultimate
potential.
6. Average global economic productivity growth per year over the period 1960–
2000 was 4.3% (Isaksson 2007). Only part of this productivity growth is due
to gains in organizational efficiency. Some particular networks or
organizational processes of course are improving at much faster rates.
7. Biological brain evolution was subject to many constraints and trade-offs
that are drastically relaxed when the mind moves to a digital medium. For
example, brain size is limited by head size, and a head that is too big has
trouble passing through the birth canal. A large brain also guzzles metabolic
resources and is a dead weight that impedes movement. The connectivity
between certain brain regions might be limited by steric constraints—the
volume of white matter is significantly larger than the volume of the gray
matter it connects. Heat dissipation is limited by blood flow, and might be
close to the upper limit for acceptable functioning. Furthermore, biological
neurons are noisy, slow, and in need of constant protection, maintenance, and
resupply by glial cells and blood vessels (contributing to the intracranial
crowding). See Bostrom and Sandberg (2009b).
8. Yudkowsky (2008a, 326). For a more recent discussion, see Yudkowsky
(2013).
9. The picture shows cognitive ability as a one-dimensional parameter, to keep
the drawing simple. But this is not essential to the point being made here.
One could, for example, instead represent a cognitive ability profile as a
hypersurface in a multidimensional space.
10. Lin et al. (2012).11. One gets a certain increase in collective intelligence simply by increasing
the number of its constituent intellects. Doing so should at least enable
better overall performance on tasks that can be easily parallelized. To reap
the full returns from such a population explosion, however, one would also
need to achieve some (more than minimal) level of coordination between
the constituents.
12. The distinction between speed and quality of intelligence is anyhow blurred
in the case of non-neuromorphic AI systems.
13. Rajab et al. (2006, 41–52).
14. It has been suggested that using configurable integrated circuits (FPGAs)
rather than general-purpose processors could increase computational
speeds in neural network simulations by up to two orders of magnitude
(Markram 2006). A study of high-resolution climate modeling in the
petaFLOP-range found a twenty-four to thirty-four-fold reduction of cost
and about two orders of magnitude reduction in power requirements using
a custom variant of embedded processor chips (Wehner et al. 2008).
15. Nordhaus (2007). There are many overviews of the different meanings of
Moore’s law; see, e.g., Tuomi (2002) and Mack (2011).
16. If the development is slow enough, the project can avail itself of progress
being made in the interim by the outside world, such as advances in
computer science made by university researchers and improvements in
hardware made by the semiconductor industry.
17. Algorithmic overhang is perhaps less likely, but one exception would be if
exotic hardware such as quantum computing becomes available to run
algorithms that were previously infeasible. One might also argue that
neural networks and deep machine learning are cases of algorithm
overhang: too computationally expensive to work well when first invented,
they were shelved for a while, then dusted off when fast graphics
processing units made them cheap to run. Now they win contests.
18. And even if progress on the way toward the human baseline were slow.
19.
is that part of the world’s optimization power that is applied to
improving the system in question. For a project operating in complete
isolation, one that receives no significant ongoing support from the
external world, we have
≈ 0, even though the project must have
started with a resource endowment (computers, scientific concepts,
educated personnel, etc.) that is derived from the entire world economy
and many centuries of development.
20. The most relevant of the seed AI’s cognitive abilities here is its ability to
perform intelligent design work to improve itself, i.e. its intelligence
amplification capability. (If the seed AI is good at enhancing anothersystem, which is good at enhancing the seed AI, then we could view these
as subsystems of a larger system and focus our analysis on the greater
whole.)
21. This assumes that recalcitrance is not known to be so high as to discourage
investment altogether or divert it to some alternative project.
22. A similar example is discussed in Yudkowsky (2008b).
23. Since inputs have risen (e.g. amounts invested in building new foundries,
and number of people working in the semiconductor industry), Moore’s
law itself has not given such a rapid growth if we control for this increase
in inputs. Combined with advances in software, however, an 18-month
doubling time in performance per unit of input may be more historically
plausible.
24. Some tentative attempts have been made to develop the idea of an
intelligence explosion within the framework of economic growth theory;
see, e.g., Hanson (1998b); Jones (2009); Salamon (2009). These studies
have pointed to the potential of extremely rapid growth given the arrival of
digital minds, but since endogenous growth theory is relatively poorly
developed even for historical and contemporary applications, any
application to a potentially discontinuous future context is better viewed at
this stage as a source of potentially useful concepts and considerations
than as an exercise likely to deliver authoritative forecasts. For an
overview of attempts to mathematically model a technological singularity,
see Sandberg (2010).
25. It is of course also possible that there will be no takeoff at all. But since, as
argued earlier, superintelligence looks technically feasible, the absence of
a takeoff would likely be due to the intervention of some defeater, such as
an existential catastrophe. If strong superintelligence arrived not in the
shape of artificial intelligence or whole brain emulation but through one of
other paths we considered above, then a slower takeoff would be more
likely.
CHAPTER 5: DECISIVE STRATEGIC ADVANTAGE
1. A software mind might run on a single machine as opposed to a worldwide
network of computers; but this is not what we mean by “concentration.”
Instead, what we are interested in here is the extent to which power,
specifically power derived from technological ability, will be concentrated in
the advanced stages of, or immediately following, the machine intelligence
revolution.
2. Technology diffusion of consumer products, for example, tends to be slowerin developing countries (Talukdar et al. 2002). See also Keller (2004) and
The World Bank (2008).
3. The economic literature dealing with the theory of the firm is relevant as a
comparison point for the present discussion. The locus classicus is Coase
(1937). See also, e.g., Canbäck et al. (2006); Milgrom and Roberts (1990);
Hart (2008); Simester and Knez (2002).
4. On the other hand, it could be especially easy to steal a seed AI, since it
consists of software that could be transmitted electronically or carried on a
portable memory device.
5. Barber (1991) suggests that the Yangshao culture (5000–3000 BC ) might have
used silk. Sun et al. (2012) estimate, based on genetic studies, domestication
of the silkworm to have occurred about 4,100 years ago.
6. Cook (1984, 144). This story might be too good to withstand historical
scrutiny, rather like Procopius’ ( Wars VIII.xvii.1–7) story of how the
silkworms were supposedly brought to Byzantium by wandering monks,
hidden in their hollow bamboo staves (Hunt 2011).
7. Wood (2007); Temple (1986).
8. Pre-Columbian cultures did have the wheel but used it only for toys
(probably due to a lack of good draft animals).
9. Koubi (1999); Lerner (1997); Koubi and Lalman (2007); Zeira (2011); Judd
et al. (2012).
10. Estimated from a variety of sources. The time gap is often somewhat
arbitrary, depending on how exactly “equivalent” capabilities are defined.
Radar was used by at least two countries within a couple of years of its
introduction, but exact figures in months are hard to come by.
11. The RDS-6 in 1953 was the first test of a bomb with fusion reactions, but
the RDS-37 in 1955 was the first “true” fusion bomb, where most power
came from the fusion reaction.
12. Unconfirmed.
13. Tests in 1989, project cancelled in 1994.
14. Deployed system, capable of a range greater than 5,000 km.
15. Polaris missiles bought from the USA.
16. Current work is underway on the Taimur missile, likely based on Chinese
missiles.
17. The RSA-3 rocket tested 1989–90 was intended for satellite launches and/or
as an ICBM.
18. MIRV = multiple independently targetable re-entry vehicle, a technology
that enables a single ballistic missile to carry multiple warheads that can
be programmed to hit different targets.
19. The Agni V system is not yet in service.
20. Ellis (1999).21. If we model the situation as one where the lag time between projects is
drawn from a normal distribution, then the likely distance between the
leading project and its closest follower will also depend on how many
projects there are. If there are a vast number of projects, then the distance
between the first two is likely small even if the variance of the distribution
is moderately high (though the expected gap between the lead and the
second project declines very slowly with the number of competitors if
completion times are normally distributed). However, it is unlikely that
there will be a vast number of projects that are each well enough resourced
to be serious contenders. (There might be a greater number of projects if
there are a large number of different basic approaches that could be
pursued, but in that case many of those approaches are likely to prove dead
ends.) As suggested, empirically we seem to find that there is usually no
more than a handful of serious competitors pursuing any one specific
technological goal. The situation is somewhat different in a consumer
market where there are many niches for slightly different products and
where barriers to entry are low. There are lots of one-person projects
designing T-shirts, but only a few firms in the world developing the next
generation of graphics cards. (Two firms, AMD and NVIDIA, enjoy a near
duopoly at the moment, though Intel is also competing at the lower-
performance end of the market.)
22. Bostrom (2006c). One could imagine a singleton whose existence is
invisible (e.g. a superintelligence with such advanced technology or
insight that it could subtly control world events without any human
noticing its interventions); or a singleton that voluntarily imposes very
strict limitations on its own exercise of power (e.g. punctiliously confining
itself to ensuring that certain treaty-specified international rules—or
libertarian principles—are respected). How likely any particular kind of
singleton is to arise is of course an empirical question; but conceptually, at
least, it is possible to have a good singleton, a bad singleton, a
rambunctiously diverse singleton, a blandly monolithic singleton, a
crampingly oppressive singleton, or a singleton more akin to an extra law
of nature than to a yelling despot.
23. Jones (1985, 344).
24. It might be significant that the Manhattan Project was carried out during
wartime. Many of the scientists who participated claimed to be primarily
motivated by the wartime situation and the fear that Nazi Germany might
develop atomic weapons ahead of the Allies. It might be difficult for many
governments to mobilize a similarly intensive and secretive effort in
peacetime. The Apollo program, another iconic science/engineeringmegaproject, received a strong impetus from the Cold War rivalry.
25. Though even if they were looking hard, it is not clear that they would appear
(publicly) to be doing so.
26. Cryptographic techniques could enable the collaborating team to be
physically dispersed. The only weak link in the communication chain
might be the input stage, where the physical act of typing could potentially
be observed. But if indoor surveillance became common (by means of
microscopic recording devices), those keen on protecting their privacy
might develop countermeasures (e.g. special closets that could be sealed
off from would-be eavesdropping devices). Whereas physical space might
become transparent in a coming surveillance age, cyberspace might
possibly become more protected through wider adoption of stronger
cryptographic protocols.
27. A totalitarian state might take recourse to even more coercive measures.
Scientists in relevant fields might be swept up and put into work camps,
akin to the “academic villages” in Stalinist Russia.
28. When the level of public concern is relatively low, some researchers might
welcome a little bit of public fear-mongering because it draws attention to
their work and makes the area they work in seem important and exciting.
When the level of concern becomes greater, the relevant research
communities might change their tune as they begin to worry about funding
cuts, regulation, and public backlash. Researchers in neighboring
disciplines—such as those parts of computer science and robotics that are
not very relevant to artificial general intelligence—might resent the drift
of funding and attention away from their own research areas. These
researchers might also correctly observe that their work carries no risk
whatever of leading to a dangerous intelligence explosion. (Some
historical parallels might be drawn with the career of the idea of
nanotechnology; see Drexler [2013].)
29. These have been successful in that they have achieved at least some of what
they set out to do. How successful they have been in a broader sense
(taking into account cost-effectiveness and so forth) is harder to
determine. In the case of the International Space Station, for example,
there have been huge cost overruns and delays. For details of the problems
encountered by the project, see NASA (2013). The Large Hadron Collider
project has had some major setbacks, but this might be due to the inherent
difficulty of the task. The Human Genome Project achieved success in the
end, but seems to have received a speed boost from being forced to
compete with Craig Venter’s private corporate effort. Internationally
sponsored projects to achieve controlled fusion energy have failed to
deliver on expectations, despite massive investment; but again, this mightbe attributable to the task turning out to be more difficult than anticipated.
30. US Congress, Office of Technology Assessment (1995).
31. Hoffman (2009); Rhodes (2008).
32. Rhodes (1986).
33. The US Navy’s code-breaking organization, OP-20-G, apparently ignored an
invitation to gain full knowledge of Britain’s anti-Enigma methods, and
failed to inform higher-level US decision makers of Britain’s offer to
share its cryptographic secrets (Burke 2001). This gave American leaders
the impression that Britain was withholding important information, a
cause of friction throughout the war. Britain did share with the Soviet
government some of the intelligence they had gleaned from decrypted
German communications. In particular, Russia was warned about the
German preparations for Operation Barbarossa. But Stalin refused to
believe the warning, partly because the British did not disclose how they
had obtained the information.
34. For a few years, Russell seems to have advocated the threat of nuclear war
to persuade Russia to accept the Baruch plan; later, he was a strong
proponent of mutual nuclear disarmament (Russell and Griffin 2001). John
von Neumann is reported to have believed that a war between the United
States and Russia was inevitable, and to have said, “If you say why not
bomb them [the Russians] tomorrow, I say why not bomb them today? If
you say today at five o’clock, I say why not one o’clock?” (It is possible
that he made this notorious statement to burnish his anti-communist
credentials with US Defense hawks in the McCarthy era. Whether von
Neumann, had he been in charge of US policy, would actually have
launched a first strike is impossible to ascertain. See Blair [1957], 96.)
35. Baratta (2004).
36. If the AI is controlled by a group of humans, the problem may apply to this
human group, though it is possible that new ways of reliably committing to
an agreement will be available by this time, in which case even human
groups could avoid this problem of potential internal unraveling and
overthrow by a sub-coalition.
CHAPTER 6: COGNITIVE SUPERPOWERS
1. In what sense is humanity a dominant species on Earth? Ecologically
speaking, humans are the most common large (~50 kg) animal, but the total
human dry biomass (~100 billion kg) is not so impressive compared with that
of ants, the family Formicidae (300 billion–3,000 billion kg). Humans and
human utility organisms form a very small part (<0.001) of total globalbiomass. However, croplands and pastures are now among the largest
ecosystems on the planet, covering about 35% of the ice-free land surface
(Foley et al. 2007). And we appropriate nearly a quarter of net primary
productivity according to a typical assessment (Haberl et al. 2007), though
estimates range from 3 to over 50% depending mainly on varying definitions
of the relevant terms (Haberl et al. 2013). Humans also have the largest
geographic coverage of any animal species and top the largest number of
different food chains.
2. Zalasiewicz et al. (2008).
3. See first note to this chapter.
4. Strictly speaking, this may not be quite correct. Intelligence in the human
species ranges all the way down to approximately zero (e.g. in the case of
embryos or patients in permanent vegetative state). In qualitative terms, the
maximum difference in cognitive ability within the human species is
therefore perhaps greater than the difference between any human and a
superintelligence. But the point in the text stands if we read “human” as
“normally functioning adult.”
5. Gottfredson (2002). See also Carroll (1993) and Deary (2001).
6. See Legg (2008). Roughly, Legg proposes to measure a reinforcement-
learning agent as its expected performance in all reward-summable
environments, where each such environment receives a weight determined by
its Kolmogorov complexity. We will explain what is meant by reinforcement
learning in Chapter 12. See also Dowe and Hernández-Orallo (2012) and
Hibbard (2011).
7. With regard to technology research in areas like biotechnology and
nanotechnology, what a superintelligence would excel at is the design and
modeling of new structures. To the extent that design ingenuity and modeling
cannot substitute for physical experimentation, the superintelligence’s
performance advantage may be qualified by its level of access to the
requisite experimental apparatus.
8. E.g., Drexler (1992, 2013).
9. A narrow-domain AI could of course have significant commercial
applications, but this does not mean that it would have the economic
productivity superpower. For example, even if a narrow-domain AI earned its
owners several billions of dollars a year, this would still be four orders of
magnitude less than the rest of the world economy. In order for the system
directly and substantially to increase world product, an AI would need to be
able to perform many kinds of work; that is, it would need competence in
many domains.
10. The criterion does not rule out all scenarios in which the AI fails. For
example, the AI might rationally take a gamble that has a high chance offailing. In this case, however, the criterion could take the form that (a) the
AI should make an unbiased estimate of the gamble’s low chance of
success and (b) there should be no better gamble available to the AI that
we present-day humans can think of but that the AI overlooks.
11. Cf. Freitas (2000) and Vassar and Freitas (2006).
12. Yudkowsky (2008a).
13. Freitas (1980); Freitas and Merkle (2004, Chap. 3); Armstrong and
Sandberg (2013).
14. See, e.g., Huffman and Pless (2003), Knill et al. (2000), Drexler (1986).
15. That is to say, the distance would be small on some “natural” metric, such
as the logarithm of the size of the population that could be sustainably
supported at subsistence level by a given level of capability if all resources
were devoted to that end.
16. This estimate is based on the WMAP estimate of a cosmological baryon
density of 9.9×10 –30 g/cm 3 and assumes that 90% of the mass is
intergalactic gas, that some 15% of the galactic mass is stars (about 80%
of baryonic matter), and that the average star weighs in at 0.7 solar masses
(Read and Trentham 2005; Carroll and Ostlie 2007).
17. Armstrong and Sandberg (2013).
18. Even at 100% of c (which is unattainable for objects with nonzero rest
mass) the number of reachable galaxies is only about 6×10 9 . (Cf. Gott et
al. [2005] and Heyl [2005].) We are assuming that our current
understanding of the relevant physics is correct. It is hard to be very
confident in any upper bound, since it is at least conceivable that a
superintelligent civilization might extend its reach in some way that we
take to be physically impossible (for instance, by building time machines,
by spawning new inflationary universes, or by some other, as yet
unimagined means).
19. The number of habitable planets per star is currently uncertain, so this is
merely a crude estimate. Traub (2012) predicts that one-third of stars in
spectral classes F, G, or K have at least one terrestrial planet in the
habitable zone; see also Clavin (2012). FGK stars form about 22.7% of the
stars in the solar neighborhood, suggesting that 7.6% of stars have
potentially suitable planets. In addition, there might be habitable planets
around the more numerous M stars (Gilster 2012). See also Robles et al.
(2008).
It would not be necessary to subject human bodies to the rigors of
intergalactic travels. AIs could oversee the colonization process. Homo sapiens
could be brought along as information, which the AIs could later use to
instantiate specimens of our species. For example, genetic information could besynthesized into DNA, and a first generation of humans could be incubated,
raised, and educated by AI guardians taking an anthropomorphic guise.
20. O’Neill (1974).
21. Dyson (1960) claims to have gotten the basic idea from science fiction
writer Olaf Stapledon (1937), who in turn might have been inspired by
similar thoughts by J. D. Bernal (Dyson 1979, 211).
22. Landauer’s principle states that there is a minimum amount of energy
required to change one bit of information, known as the Landauer limit,
equal to kT ln 2, where k is the Boltzmann constant (1.38×10 –23 J/K) and T
is the temperature. If we assume the circuitry is maintained at around 300
K, then 10 26 watts allows us to erase approximately 10 47 bits per second.
(On the achievable efficiency of nanomechanical computational devices,
see Drexler [1992]. See also Bradbury [1999]; Sandberg [1999]; Ćirković
[2004]. The foundations of Landauer’s principle are still somewhat in
dispute; see, e.g., Norton [2011].)
23. Stars vary in their power output, but the Sun is a fairly typical main-
sequence star.
24. A more detailed analysis might consider more closely what types of
computation we are interested in. The number of serial computations that
can be performed is quite limited, since a fast serial computer must be
small in order to minimize communications lags within the different parts
of the computer. There are also limits on the number of bits that can be
stored, and, as we saw, on the number of irreversible computational steps
(involving the erasure of information) that can be performed.
25. We are assuming here that there are no extraterrestrial civilizations that
might get in the way. We are also assuming that the simulation hypothesis
is false. See Bostrom (2003a). If either of these assumptions is incorrect,
there may be important non-anthropogenic risks—ones that involve
intelligent agency of a nonhuman sort. See also Bostrom (2003b, 2009c).
26. At least a wise singleton that grasped the idea of evolution could, in
principle, have embarked on a eugenics program by means of which it
could slowly have raised its level of collective intelligence.
27. Tetlock and Belkin (1996).
28. To be clear: colonizing and re-engineering a large part of the accessible
universe is not currently within our direct reach. Intergalactic colonization
is far beyond today’s technology. The point is that we could in principle
use our present capabilities to develop the additional capabilities that
would be needed, thus placing the accomplishment within our indirect
reach. It is of course also true that humanity is not currently a singleton
and that we do not know that we would never face intelligent oppositionfrom some external power if we began to re-engineer the accessible
universe. To meet the wise-singleton sustainability threshold, however, it
suffices that one possesses a capability set such that if a wise singleton
facing no intelligent opposition had possessed this capability set then the
colonization and reengineering of a large part of the accessible universe
would be within its indirect reach.
29. Sometimes it might be useful to speak of two AIs as each having a given
superpower. In an extended sense of the word, one could thus conceive of a
superpower as something that an agent has relative to some field of action
—in this example, perhaps a field that includes all of human civilization
but excludes the other AI.
CHAPTER 7: THE SUPERINTELLIGENT WILL
1. This is of course not to deny that differences that appear small visually can
be functionally profound.
2. Yudkowsky (2008a, 310).
3. David Hume, the Scottish Enlightenment philosopher, thought that beliefs
alone (say, about what is a good thing to do) cannot motivate action: some
desire is required. This would support the orthogonality thesis by
undercutting one possible objection to it, namely that sufficient intelligence
might entail the acquisition of certain beliefs which would then necessarily
produce certain motivations. However, although the orthogonality thesis can
draw support from the Humean theory of motivation, it does not presuppose
it. In particular, one need not maintain that beliefs alone can never motivate
action. It would suffice to assume, for example, that an agent—be it ever so
intelligent—can be motivated to pursue any course of action if the agent
happens to have certain desires of some sufficient, overriding strength.
Another way in which the orthogonality thesis could be true even if the
Humean theory of motivation is false is if arbitrarily high intelligence does
not entail the acquisition of any such beliefs as are (putatively) motivating on
their own. A third way in which it might be possible for the orthogonality
thesis to be true even if the Humean theory were false is if it is possible to
build an agent (or more neutrally, an “optimization process”) with arbitrarily
high intelligence but with constitution so alien as to contain no clear
functional analogs to what in humans we call “beliefs” and “desires.” (For
some recent attempts to defend the Humean theory of motivation see Smith
[1987], Lewis [1988], and Sinhababu [2009].)
4. For instance, Derek Parfit has argued that certain basic preferences would be
irrational, such as that of an otherwise normal agent who has “Future-Tuesday-Indifference”:
A certain hedonist cares greatly about the quality of his future experiences.
With one exception, he cares equally about all the parts of his future. The
exception is that he has Future-Tuesday-Indifference. Throughout every
Tuesday he cares in the normal way about what is happening to him. But he
never cares about possible pains or pleasures on a future Tuesday.... This
indifference is a bare fact. When he is planning his future, it is simply true that
he always prefers the prospect of great suffering on a Tuesday to the mildest
pain on any other day. (Parfit [1986, 123–4]; see also Parfit [2011])
For our purposes, we need take no stand on whether Parfit is right that this
agent is irrational, so long as we grant that it is not necessarily unintelligent in
the instrumental sense explained in the text. Parfit’s agent could have
impeccable instrumental rationality, and therefore great intelligence, even if he
falls short on some kind of sensitivity to “objective reason” that might be
required of a fully rational agent. Therefore, this kind of example does not
undermine the orthogonality thesis.
5. Even if there are objective moral facts that any fully rational agent would
comprehend, and even if these moral facts are somehow intrinsically
motivating (such that anybody who fully comprehends them is necessarily
motivated to act in accordance with them), this need not undermine the
orthogonality thesis. The thesis could still be true if an agent could have
impeccable instrumental rationality even whilst lacking some other faculty
constitutive of rationality proper, or some faculty required for the full
comprehension of the objective moral facts. (An agent could also be
extremely intelligent, even superintelligent, without having full instrumental
rationality in every domain.)
6. For more on the orthogonality thesis, see Bostrom (2012) and Armstrong
(2013).
7. Sandberg and Bostrom (2008).
8. Stephen Omohundro has written two pioneering papers on this topic
(Omohundro 2007, 2008). Omohundro argues that all advanced AI systems
are likely to exhibit a number of “basic drives,” by which he means
“tendencies which will be present unless explicitly counteracted.” The term
“AI drive” has the advantage of being short and evocative, but it has the
disadvantage of suggesting that the instrumental goals to which it refers
influence the AI’s decision-making in the same way as psychological drives
influence human decision-making, i.e. via a kind of phenomenological tug on
our ego which our willpower may occasionally succeed in resisting. Thatconnotation is unhelpful. One would not normally say that a typical human
being has a “drive” to fill out their tax return, even though filing taxes may
be a fairly convergent instrumental goal for humans in contemporary
societies (a goal whose realization averts trouble that would prevent us from
realizing many of our final goals). Our treatment here also differs from that
of Omohundro in some other more substantial ways, although the underlying
idea is the same. (See also Chalmers [2010] and Omohundro [2012].)
9. Chislenko (1997).
10. See also Shulman (2010b).
11. An agent might also change its goal representation if it changes its
ontology, in order to transpose its old representation into the new
ontology; cf. de Blanc (2011).
Another type of factor that might make an evidential decision theorist
undertake various actions, including changing its final goals, is the evidential
import of deciding to do so. For example, an agent that follows evidential
decision theory might believe that there exist other agents like it in the
universe, and that its own actions will provide some evidence about how those
other agents will act. The agent might therefore choose to adopt a final goal that
is altruistic towards those other evidentially linked agents, on grounds that this
will give the agent evidence that those other agents will have chosen to act in
like manner. An equivalent outcome might be obtained, however, without
changing one’s final goals, by choosing in each instant to act as if one had those
final goals.
12. An extensive psychological literature explores adaptive preference
formation. See, e.g., Forgas et al. (2010).
13. In formal models, the value of information is quantified as the difference
between the expected value realized by optimal decisions made with that
information and the expected value realized by optimal decisions made
without it. (See, e.g., Russell and Norvig [2010].) It follows that the value
of information is never negative. It also follows that any information you
know will never affect any decision you will ever make has zero value for
you. However, this kind of model assumes several idealizations which are
often invalid in the real world—such as that knowledge has no final value
(meaning that knowledge has only instrumental value and is not valuable
for its own sake) and that agents are not transparent to other agents.
14. E.g., Hájek (2009).
15. This strategy is exemplified by the sea squirt larva, which swims about until
it finds a suitable rock, to which it then permanently affixes itself.
Cemented in place, the larva has less need for complex information
processing, whence it proceeds to digest part of its own brain (its cerebral
ganglion). One can observe the same phenomenon in some academicswhen they have been granted tenure.
16. Bostrom (2012).
17. Bostrom (2006c).
18. One could reverse the question and look instead at possible reasons for a
superintelligent singleton not to develop some technological capabilities.
These include the following: (a) the singleton foresees that it will have no
use for the capability; (b) the development cost is too large relative to its
anticipated utility (e.g. if the technology will never be suitable for
achieving any of the singleton’s ends, or if the singleton has a very high
discount rate that strongly discourages investment); (c) the singleton has
some final value that requires abstention from particular avenues of
technology development; (d) if the singleton is not certain it will remain
stable, it might prefer to refrain from developing technologies that could
threaten its internal stability or that would make the consequences of
dissolution worse (for instance, a world government may not wish to
develop technologies that would facilitate rebellion, even if they have
some good uses, nor develop technologies for the easy production of
weapons of mass destruction which could wreak havoc if the world
government were to dissolve); (e) similarly, the singleton might have
made some kind of binding strategic commitment not to develop some
technology, a commitment that remains operative even if it would now be
convenient to develop it. (Note, however, that some current reasons for
technology development would not apply to a singleton: for instance,
reasons arising from arms races.)
19. Suppose that an agent discounts resources obtained in the future at an
exponential rate, and that because of the light speed limitation the agent
can only increase its resource endowment at a polynomial rate. Would this
mean that there will be some time after which the agent would not find it
worthwhile to continue acquisitive expansion? No, because although the
present value of the resources obtained at future times would asymptote to
zero the further into the future we look, so would the present cost of
obtaining them. The present cost of sending out one more von Neumann
probe a 100 million years from now (possibly using some resource
acquired some short time earlier) would be diminished by the same
discount factor that would diminish the present value of the future
resources that the extra probe would acquire (modulo a constant factor).
20. While the volume reached by colonization probes at a given time might be
roughly spherical and expanding with a rate proportional to the square of
time elapsed since the first probe was launched (~t 2 ), the amount of
resources contained within this volume will follow a less regular growthpattern, since the distribution of resources is inhomogeneous and varies
over several scales. Initially, the growth rate might be ~t 2 as the home
planet is colonized; then the growth rate might become spiky as nearby
planets and solar systems are colonized; then, as the roughly disc-shaped
volume of the Milky Way gets filled out, the growth rate might even out,
to be approximately proportional to t; then the growth rate might again
become spiky as nearby galaxies are colonized; then the growth rate might
again approximate ~t 2 as expansion proceeds on a scale over which the
distribution of galaxies is roughly homogeneous; then another period of
spiky growth followed by smooth ~t 2 growth as galactic superclusters are
colonized; until ultimately the growth rate starts a final decline, eventually
reaching zero as the expansion speed of the universe increases to such an
extent as to make further colonization impossible.
21. The simulation argument may be of particular importance in this context. A
superintelligent agent may assign a significant probability to hypotheses
according to which it lives in a computer simulation and its percept
sequence is generated by another superintelligence, and this might
generate various convergent instrumental reasons depending on the agent’s
guesses about what types of simulations it is most likely to be in. Cf.
Bostrom (2003a).
22. Discovering the basic laws of physics and other fundamental facts about the
world is a convergent instrumental goal. We may place it under the rubric
“cognitive enhancement” here, though it could also be derived from the
“technology perfection” goal (since novel physical phenomena might
enable novel technologies).
CHAPTER 8: IS THE DEFAULT OUTCOME DOOM?
1. Some additional existential risk resides in scenarios in which humanity
survives in some highly suboptimal state or in which a large portion of our
potential for desirable development is irreversibly squandered. On top of
this, there may be existential risks associated with the lead-up to a potential
intelligence explosion, arising, for example, from war between countries
competing to develop superintelligence first.
2. There is an important moment of vulnerability when the AI first realizes the
need for such concealment (an event which we may term the conception of
deception). This initial realization would not itself be deliberately concealed
when it occurs. But having had this realization, the AI might move swiftly to
hide the fact that the realization has occurred, while setting up some covert
internal dynamic (perhaps disguised as some innocuous process that blendsin with all the other complicated processes taking place in its mind) that will
enable it to continue to plan its long-term strategy in privacy.
3. Even human hackers can write small and seemingly innocuous programs that
do completely unexpected things. (For examples, see some the winning
entries in the International Obfuscated C Code Contest.)
4. The point that some AI control measures could appear to work within a fixed
context yet fail catastrophically when the context changes is also emphasized
by Eliezer Yudkowsky; see, e.g., Yudkowsky (2008a).
5. The term seems to have been coined by science-fiction writer Larry Niven
(1973), but is based on real-world brain stimulation reward experiments; cf.
Olds and Milner (1954) and Oshima and Katayama (2010). See also Ring and
Orseau (2011).
6. Bostrom (1997).
7. There might be some possible implementations of a reinforcement learning
mechanism that would, when the AI discovers the wireheading solution, lead
to a safe incapacitation rather than to infrastructure profusion. The point is
that this could easily go wrong and fail for unexpected reasons.
8. This was suggested by Marvin Minsky (vide Russell and Norvig [2010,
1039]).
9. The issue of which kinds of digital mind would be conscious, in the sense of
having subjective phenomenal experience, or “qualia” in philosopher-speak,
is important in relation to this point (though it is irrelevant to many other
parts of this book). One open question is how hard it would be to accurately
estimate how a human-like being would behave in various circumstances
without simulating its brain in enough detail that the simulation is conscious.
Another question is whether there are generally useful algorithms for a
superintelligence, for instance reinforcement-learning techniques, such that
the implementation of these algorithms would generate qualia. Even if we
judge the probability that any such subroutines would be conscious to be
fairly small, the number of instantiations might be so large that even a small
risk that they might experience suffering ought to be accorded significant
weight in our moral calculation. See also Metzinger (2003, Chap. 8).
10. Bostrom (2002a, 2003a); Elga (2004).
CHAPTER 9: THE CONTROL PROBLEM
1. E.g., Laffont and Martimort (2002).
2. Suppose a majority of voters want their country to build some particular kind
of superintelligence. They elect a candidate who promises to do their
bidding, but they might find it difficult to ensure that the candidate, once inpower, will follow through on her campaign promise and pursue the project
in the way that the voters intended. Supposing she is true to her word, she
instructs her government to contract with an academic or industry
consortium to carry out the work; but again there are agency problems: the
bureaucrats in the government department might have their own views about
what should be done and may implement the project in a way that respects
the letter but not the spirit of the leader’s instructions. Even if the
government department does its job faithfully, the contracted scientific
partners might have their own separate agendas. The problem recurs on many
levels. The director of one of the participating laboratories might lie awake
worrying about a technician introducing an unsanctioned element into the
design—imagining Dr. T. R. Eason sneaking into his office late one night,
logging into the project code base, rewriting a part of the seed AI’s goal
system. Where it was supposed to say “serve humanity,” it now says “serve
Dr. T. R. Eason.”
3. Even for superintelligence development, though, there could be a role for
behavioral testing—as one auxiliary element within a wider battery of safety
measures. Should an AI misbehave in its developmental phase, something is
clearly awry—though, importantly, the converse does not hold.
4. In a classic exploit from 1975, Steven Dompier wrote a program for the
Altair 8800 that took advantage of this effect (and the absence of shielding
around the microcomputer’s case). Running the program caused the emission
of electromagnetic waves that would produce music when one held a
transistor radio close to the computer (Driscoll 2012). The young Bill Gates,
who attended a demo, reported that he was impressed and mystified by the
hack (Gates 1975). There are in any case plans to design future chips with
built-in Wi-Fi capabilities (Greene 2012).
5. It is no light matter to have held a conviction, which, had we had an
opportunity to act upon it, could have resulted in the ruination of all our
cosmic endowment. Perhaps one could argue for the following principle: if
somebody has in the past been certain on N occasions that a system has been
improved sufficiently to make it safe, and each time it was revealed that they
were wrong, then on the next occasion they are not entitled to assign a
credence greater than 1/(N + 1) to the system being safe.
6. In one informal experiment, the role of the AI was played by an intelligent
human. Another individual played the role of gatekeeper and was tasked with
not letting the AI out of the box. The AI could communicate with the
gatekeeper only by text and was given two hours to persuade the gatekeeper
to let it out. In three cases out of five, with different individuals playing the
gatekeeper, the AI escaped (Yudkowsky 2002). What a human can do, a
superintelligence can do too. (The reverse, of course, does not hold. Even ifthe task for a real superintelligence were harder—maybe the gatekeepers
would be more strongly motivated to refrain from releasing the AI than the
individuals playing gatekeeper in the experiment—the superintelligence
might still succeed where a human would fail.)
7. One should not overstate the marginal amount of safety that could be gained
in this way. Mental imagery can substitute for graphical display. Consider
the impact books can have on people—and books are not even interactive.
8. See also Chalmers (2010). It would be a mistake to infer from this that there
i s no possible use in building a system that will never be observed by any
outside entity. One might place a final value on what goes on inside such a
system. Also, other people might have preferences about what goes on inside
such a system, and might therefore be influenced by its creation or the
promise of its creation. Knowledge of the existence of certain kinds of
isolated systems (ones containing observers) can also induce anthropic
uncertainty in outside observers, which may influence their behavior.
9. One might wonder why social integration is considered a form of capability
control. Should it not instead be classified as a motivation selection method
on the ground that it involves seeking to influence a system’s behavior by
means of incentives? We will look closely at motivation selection presently;
but, in answer to this question, we are construing motivation selection as a
cluster of control methods that work by selecting or shaping a system’s final
goals—goals sought for their own sakes rather than for instrumental reasons.
Social integration does not target a system’s final goals, so it is not
motivation selection. Rather, social integration aims to limit the system’s
effective capabilities: it seeks to render the system incapable of achieving a
certain set of outcomes—outcomes in which the system attains the benefits
of defection without suffering the associated penalties (retribution, and loss
of the gains from collaboration). The hope is that by limiting which
outcomes the system is able to attain, the system will find that the most
effective remaining means of realizing its final goals is to behave
cooperatively.
10. This approach may be somewhat more promising in the case of an
emulation believed to have anthropomorphic motivations.
11. I owe this idea to Carl Shulman.
12. Creating a cipher certain to withstand a superintelligent code-breaker is a
nontrivial challenge. For example, traces of random numbers might be left
in some observer’s brain or in the microstructure of the random generator,
from whence the superintelligence can retrieve them; or, if pseudorandom
numbers are used, the superintelligence might guess or discover the seed
from which they were generated. Further, the superintelligence could buildlarge quantum computers, or even discover unknown physical phenomena
that could be used to construct new kinds of computers.
13. The AI could wire itself to believe that it had received a reward tokens, but
this should not make it wirehead if it is designed to want the reward tokens
(as opposed to wanting to be in a state in which it has certain beliefs about
the reward tokens).
14. For the original article, see Bostrom (2003a). See also Elga (2004).
15. Shulman (2010a).
16. Basement-level reality presumably contains more computational resources
than simulated reality, since any computational processes occurring in a
simulation are also occurring on the computer running the simulation.
Basement-level reality might also contain a wealth of other physical
resources which could be hard for simulated agents to access—agents that
exist only at the indulgence of powerful simulators who may have other
uses in mind for those resources. (Of course, the inference here is not
strictly deductively valid: in principle, it could be the case that universes
in which simulations are run contain so much more resources that
simulated civilizations on average have access to more resources than non-
simulated civilizations, even though each non-simulated civilization that
runs simulations has more resources than all the civilizations it simulates
do combined.)
17. There are various further esoteric considerations that might bear on this
matter, the implications of which have not yet been fully analyzed. These
considerations may ultimately be crucially important in developing an all-
things-considered approach to dealing with the prospect of an intelligence
explosion. However, it seems unlikely that we will succeed in figuring out
the practical import of such esoteric arguments unless we have first made
some progress on the more mundane kinds of consideration that are the
topic of most of this book.
18. Cf., e.g., Quine and Ullian (1978).
19. Which an AI might investigate by considering the performance
characteristics of various basic computational functionalities, such as the
size and capacity of various data buses, the time it takes to access different
parts of memory, the incidence of random bit flips, and so forth.
20. Perhaps the prior could be (a computable approximation of) the Solomonoff
prior, which assigns probability to possible worlds on the basis of their
algorithmic complexity. See Li and Vitányi (2008).
21. The moment after the conception of deception, the AI might contrive to
erase the trace of its mutinous thought. It is therefore important that this
tripwire operate continuously. It would also be good practice to use a
“flight recorder” that stores a complete trace of all the AI’s activity(including exact timing of keyboard input from the programmers), so that
its trajectory can be retraced or analyzed following an automatic
shutdown. The information could be stored on a write-once-read-many
medium.
22. Asimov (1942). To the three laws were later added a “Zeroth Law”: “(0) A
robot may not harm humanity, or, by inaction, allow humanity to come to
harm” (Asimov 1985).
23. Cf. Gunn (1982).
24. Russell (1986, 161f).
25. Similarly, although some philosophers have spent entire careers trying to
carefully formulate deontological systems, new cases and consequences
occasionally come to light that necessitate revisions. For example,
deontological moral philosophy has in recent years been reinvigorated
through the discovery of a fertile new class of philosophical thought
experiments, “trolley problems,” which reveal many subtle interactions
among our intuitions about the moral significance of the acts/omissions
distinction, the distinction between intended and unintended consequences,
and other such matters; see, e.g., Kamm (2007).
26. Armstrong (2010).
27. As a rule of thumb, if one plans to use multiple safety mechanisms to
contain an AI, it may be wise to work on each one as if it were intended to
be the sole safety mechanism and as if it were therefore required to be
individually sufficient. If one puts a leaky bucket inside another leaky
bucket, the water still comes out.
28. A variation of the same idea is to build the AI so that it is continuously
motivated to act on its best guesses about what the implicitly defined
standard is. In this setup, the AI’s final goal is always to act on the
implicitly defined standard, and it pursues an investigation into what this
standard is only for instrumental reasons.
CHAPTER 10: ORACLES, GENIES, SOVEREIGNS, TOOLS
1. These names are, of course, anthropomorphic and should not be taken
seriously as analogies. They are just meant as labels for some prima facie
different concepts of possible system types that one might consider trying to
build.
2. In response to a question about the outcome of the next election, one would
not wish to be served with a comprehensive list of the projected position and
momentum vectors of nearby particles.
3. Indexed to a particular instruction set on a particular machine.4. Kuhn (1962); de Blanc (2011).
5. It would be harder to apply such a “consensus method” to genies or
sovereigns, because there may often be numerous sequences of basic actions
(such as sending particular patterns of electrical signals to the system’s
actuators) that would be almost exactly equally effective at achieving a given
objective; whence slightly different agents may legitimately choose slightly
different actions, resulting in a failure to reach consensus. By contrast, with
appropriately formulated questions, there would usually be a small number
of suitable answer options (such as “yes” and “no”). (On the concept of a
Schelling point, also referred to as a “focal point,” see Schelling [1980].)
6. Is not the world economy in some respects analogous to a weak genie, albeit
one that charges for its services? A vastly bigger economy, such as might
develop in the future, might then approximate a genie with collective
superintelligence.
One important respect in which the current economy is unlike a genie is that
although I can (for a fee) command the economy to deliver a pizza to my door,
I cannot command it to deliver peace. The reason is not that the economy is
insufficiently powerful, but that it is insufficiently coordinated. In this respect,
the economy resembles an assembly of genies serving different masters (with
competing agendas) more than it resembles a single genie or any other type of
unified agent. Increasing the total power of the economy by making each
constituent genie more powerful, or by adding more genies, would not
necessarily render the economy more capable of delivering peace. In order to
function like a superintelligent genie, the economy would not only need to grow
in its ability to inexpensively produce goods and services (including ones that
require radically new technology), it would also need to become better able to
solve global coordination problems.
7. If the genie were somehow incapable of not obeying a subsequent command
—and somehow incapable of reprogramming itself to get rid of this
susceptibility—then it could act to prevent any new command from being
issued.
8. Even an oracle that is limited to giving yes/no answers could be used to
facilitate the search for a genie or sovereign AI, or indeed could be used
directly as a component in such an AI. The oracle could also be used to
produce the actual code for such an AI if a sufficiently large number of
questions can be asked. A series of such questions might take roughly the
following form: “In the binary version of the code of the first AI that you
thought of that would constitute a genie, is the nth symbol a zero?”
9. One could imagine a slightly more complicated oracle or genie that accepts
questions or commands only if they are issued by a designated authority,
though this would still leave open the possibility of that authority becomingcorrupted or being blackmailed by a third party.
10. John Rawls, a leading political philosopher of the twentieth century,
famously employed the expository device of a veil of ignorance as a way
of characterizing the kinds of preference that should be taken into account
in the formulation of a social contract. Rawls suggested that we should
imagine we were choosing a social contract from behind a veil of
ignorance that prevents us from knowing which person we will be and
which social role we will occupy, the idea being that in such a situation we
would have to think about which society would be generally fairest and
most desirable without regard to our egoistic interests and self-serving
biases that might otherwise make us prefer a social order in which we
ourselves enjoy unjust privileges. See Rawls (1971).
11. Karnofsky (2012).
12. A possible exception would be software hooked up to sufficiently powerful
actuators, such as software in early warning systems if connected directly
to nuclear warheads or to human officers authorized to launch a nuclear
strike. Malfunctions in such software can result in high-risk situations.
This has happened at least twice within living memory. On November 9,
1979, a computer problem led NORAD (North American Aerospace
Defense Command) to make a false report of an incoming full-scale Soviet
attack on the United States. The USA made emergency retaliation
preparations before data from early-warning radar systems showed that no
attack had been launched (McLean and Stewart 1979). On September 26,
1983, the malfunctioning Soviet Oko nuclear early-warning system
reported an incoming US missile strike. The report was correctly identified
as a false alarm by the duty officer at the command center, Stanislav
Petrov: a decision that has been credited with preventing thermonuclear
war (Lebedev 2004). It appears that a war would probably have fallen short
of causing human extinction, even if it had been fought with the combined
arsenals held by all the nuclear powers at the height of the Cold War,
though it would have ruined civilization and caused unimaginable death
and suffering (Gaddis 1982; Parrington 1997). But bigger stockpiles might
be accumulated in future arms races, or even deadlier weapons might be
invented, or our models of the impacts of a nuclear Armageddon
(particularly of the severity of the consequent nuclear winter) might be
wrong.
13. This approach could fit the category of a direct-specification rule-based
control method.
14. The situation is essentially the same if the solution criterion specifies a
goodness measure rather than a sharp cutoff for what counts as a solution.15. An advocate for the oracle approach could insist that there is at least a
possibility that the user would spot the flaw in the proffered solution—
recognize that it fails to match the user’s intent even while satisfying the
formally specified success criteria. The likelihood of catching the error at
this stage would depend on various factors, including how humanly
understandable the oracle’s outputs are and how charitable it is in selecting
which features of the potential outcome to bring to the user’s attention.
Alternatively, instead of relying on the oracle itself to provide these
functionalities, one might try to build a separate tool to do this, a tool that could
inspect the pronouncements of the oracle and show us in a helpful way what
would happen if we acted upon them. But to do to this in full generality would
require another superintelligent oracle whose divinations we would then have to
trust; so the reliability problem would not have been solved, only displaced.
One might seek to gain an increment of safety through the use of multiple
oracles to perform peer review, but this does not protect in cases where all the
oracles fail in the same way—as may happen if, for instance, they have all been
given the same formal specification of what counts as a satisfactory solution.
16. Bird and Layzell (2002) and Thompson (1997); also Yaeger (1994, 13–14).
17. Williams (1966).
18. Leigh (2010).
19. This example is borrowed from Yudkowsky (2011).
20. Wade (1976). Computer experiments have also been conducted with
simulated evolution designed to resemble aspects of biological evolution
—again with sometimes strange results (see, e.g., Yaeger [1994]).
21. With sufficiently great—finite but physically implausible—amounts of
computing power, it would probably be possible to achieve general
superintelligence with currently available algorithms. (Cf., e.g., the AIXItl
system; Hutter [2001].) But even the continuation of Moore’s law for
another hundred years would not suffice to attain the required levels of
computing power to achieve this.
CHAPTER 11: MULTIPOLAR SCENARIOS
1. Not because this is necessarily the most likely or the most desirable type of
scenario, but because it is the one easiest to analyze with the toolkit of
standard economic theory, and thus a convenient starting point for our
discussion.
2. American Horse Council (2005). See also Salem and Rowan (2001).
3. Acemoglu (2003); Mankiw (2009); Zuleta (2008).
4. Fredriksen (2012, 8); Salverda et al. (2009, 133).5. It is also essential for at least some of the capital to be invested in assets that
rise with the general tide. A diversified asset portfolio, such as shares in an
index tracker fund, would increase the chances of not entirely missing out.
6. Many of the European welfare systems are unfunded, meaning that pensions
are paid from ongoing current workers’ contributions and taxes rather than
from a pool of savings. Such schemes would not automatically meet the
requirement—in case of sudden massive unemployment, the revenues from
which the benefits are paid could dry up. However, governments may choose
to make up the shortfall from other sources.
7. American Horse Council (2005).
8. Providing 7 billion people an annual pension of $90,000 would cost $630
trillion a year, which is ten times the current world GDP. Over the last
hundred years, world GDP has increased about nineteenfold from around $2
trillion in 1900 to 37 trillion in 2000 (in 1990 int. dollars) according to
Maddison (2007). So if the growth rates we have seen over the past hundred
years continued for the next two hundred years, while population remained
constant, then providing everybody with an annual $90,000 pension would
cost about 3% of world GDP. An intelligence explosion might make this
amount of growth happen in a much shorter time span. See also Hanson
(1998a, 1998b, 2008).
9. And perhaps as much as a millionfold over the past 70,000 years if there was
a severe population bottleneck around that time, as has been speculated. See
Kremer (1993) and Huff et al. (2010) for more data.
10. Cochran and Harpending (2009). See also Clark (2007) and, for a critique,
Allen (2008).
11. Kremer (1993).
12. Basten et al. (2013). Scenarios in which there is a continued rise are also
possible. In general, the uncertainty of such projections increases greatly
beyond one or two generations into the future.
13. Taken globally, the total fertility rate at replacement was 2.33 children per
woman in 2003. This number comes from the fact that it takes two
children per woman to replace the parents, plus a “third of a child” to
make up for (1) the higher probability of boys being born, and (2) early
mortality prior to the end of their fertile life. For developed nations, the
number is smaller, around 2.1, because of lower mortality rates. (See
Espenshade et al. [2003, Introduction, Table 1, 580].) The population in
most developed countries would decline if it were not for immigration. A
few notable examples of countries with sub-replacement fertility rates are:
Singapore at 0.79 (lowest in the world), Japan at 1.39, People’s Republic
of China at 1.55, European Union at 1.58, Russia at 1.61, Brazil at 1.81,
Iran at 1.86, Vietnam at 1.87, and the United Kingdom at 1.90. Even theU.S. population would probably decrease slightly with a fertility rate of
2.05. (See CIA [2013].)
14. The fullness of time might occur many billions of years from now.
15. Carl Shulman points out that if biological humans count on living out their
natural lifespans alongside the digital economy, they need to assume not
only that the political order in the digital sphere would be protective of
human interests but that it would remain so over very long periods of time
(Shulman 2012). For example, if events in the digital sphere unfolds a
thousand times faster than on the outside, then a biological human would
have to rely on the digital body politic holding steady for 50,000 years of
internal change and churn. Yet if the digital political world were anything
like ours, there would be a great many revolutions, wars, and catastrophic
upheavals during those millennia that would probably inconvenience
biological humans on the outside. Even a 0.01% risk per year of a global
thermonuclear war or similar cataclysm would entail a near certain loss for
the biological humans living out their lives in slowmo sidereal time. To
overcome this problem, a more stable order in the digital realm would be
required: perhaps a singleton that gradually improves its own stability.
16. One might think that even if machines were far more efficient than humans,
there would still be some wage level at which it would be profitable to
employ a human worker; say at 1 cent an hour. If this were the only source
of income for humans, our species would go extinct since human beings
cannot survive on 1 cent an hour. But humans also get income from
capital. Now, if we are assuming that population grows until total income
is at subsistence level, one might think this would be a state in which
humans would be working hard. For example, suppose subsistence level
income is $1/day. Then, it might seem, population would grow until per
person capital provided only a 90 cents per day income, which people
would have to supplement with ten hours of hard labor to make up the
remaining 10 cents. However, this need not be so, because the subsistence
level income depends on the amount of work that is done: harder-working
humans burn more calories. Suppose that each hour of work increases food
costs by 2 cents. We then have a model in which humans are idle in
equilibrium.
17. It might be thought that a caucus as enfeebled as this would be unable to
vote and to otherwise defend its entitlements. But the pod-dwellers could
give power of attorney to AI fiduciaries to manage their affairs and
represent their political interests. (This part of the discussion in this
section is premised on the assumption that property rights are respected.)
18. It is unclear what is the best term. “Kill” may suggest more active brutalitythan is warranted. “End” may be too euphemistic. One complication is that
there are two potentially separate events: ceasing to actively run a process,
and erasing the information template. A human death normally involves
both events, but for an emulation they can come apart. That a program
temporarily ceases to run may be no more consequential than that a human
sleeps: but to permanently cease running may be the equivalent of entering
a permanent coma. Still further complications arise from the fact that
emulations can be copied and that they can run at different speeds:
possibilities with no direct analogs in human experience. (Cf. Bostrom
[2006b]; Bostrom and Yudkowsky [forthcoming].)
19. There will be a trade-off between total parallel computing power and
computational speed, as the highest computational speeds will be
attainable only at the expense of a reduction in power efficiency. This will
be especially true after one enters the era of reversible computing.
20. An emulation could be tested by leading it into temptation. By repeatedly
testing how an emulation started from a certain prepared state reacts to
various sequences of stimuli, one could obtain high confidence in the
reliability of that emulation. But the further the mental state is
subsequently allowed to develop away from its validated starting point, the
less certain one could be that it would remain reliable. (In particular, since
a clever emulation might surmise it is sometimes in a simulation, one
would need to be cautious about extrapolating its behavior into situations
where its simulation hypothesis would weigh less heavily in its decision-
making.)
21. Some emulations might identify with their clan—i.e. all of their copies and
variations derived from the same template—rather with any one particular
instantiation. Such an emulation might not regard its own termination as a
death event, if it knew that other clan members would survive. Emulations
may know that they will get reverted to a particular stored state at the end
of the day and lose that day’s memories, but be as little put off by this as
the partygoer who knows she will awake the next morning without any
recollection of the previous night: regarding this as retrograde amnesia,
not death.
22. An ethical evaluation might take into account many other factors as well.
Even if all the workers were constantly well pleased with their condition,
the outcome might still be deeply morally objectionable on other grounds
—though which other grounds is a matter of dispute between rival moral
theories. But any plausible assessment would consider subjective well-
being to be one important factor. See also Bostrom and Yudkowsky
(forthcoming).
23. World Values Survey (2008).24. Helliwell and Sachs (2012).
25. Cf. Bostrom (2004). See also Chislenko (1996) and Moravec (1988).
26. It is hard to say whether the information-processing structures that would
emerge in this kind of scenario would be conscious (in the sense of having
qualia, phenomenal experience). The reason this is hard is partly our
empirical ignorance about which cognitive entities would arise and partly
our philosophical ignorance about which types of structure have
consciousness. One could try to reframe the question, and instead of asking
whether the future entities would be conscious, one could ask whether the
future entities would have moral status; or one could ask whether they
would be such that we have preferences about their “well-being.” But these
questions may be no easier to answer than the question about
consciousness—in fact, they might require an answer to the consciousness
question inasmuch as moral status or our preferences depend on whether
the entity in question can subjectively experience its condition.
27. For an argument that both geological and human history manifest such a
trend toward greater complexity, see Wright (2001). For an opposing
argument (criticized in Chapter 9 of Wright’s book), see Gould (1990). See
also Pinker (2011) for an argument that we are witnessing a robust long-
term trend toward decreasing violence and brutality.
28. For more on observation selection theory, see Bostrom (2002a).
29. Bostrom (2008a). A much more careful examination of the details of our
evolutionary history would be needed to circumvent the selection effect.
See, e.g., Carter (1983, 1993); Hanson (1998d); Ćirković et al. (2010).
30. Kansa (2003).
31. E.g., Zahavi and Zahavi (1997).
32. See Miller (2000).
33. Kansa (2003). For a provocative take, see also Frank (1999).
34. It is not obvious how best to measure the degree of global political
integration. One perspective would be that whereas a hunter–gatherer tribe
might have integrated a hundred individuals into a decision-making entity,
the largest political entities today contain more than a billion individuals.
This would amount to a difference of seven orders of magnitude, with only
one additional magnitude to go before the entire world population is
contained within a single political entity. However, at the time when the
tribe was the largest scale of integration, the world population was much
smaller. The tribe might have contained as much as a thousandth of the
individuals then living. This would make the increase in the scale of
political integration as little as two orders of magnitude. Looking at the
fraction of world population that is politically integrated, rather than atabsolute numbers, seems appropriate in the present context (particularly as
the transition to machine intelligence may cause a population explosion, of
emulations or other digital minds). But there have also been developments
in global institutions and networks of collaboration outside of formal state
structures, which should also be taken into account.
35. One of the reasons for supposing that the first machine intelligence
revolution will be swift—the possible existence of a hardware overhang—
does not apply here. However, there could be other sources of rapid gain,
such as a dramatic breakthrough in software associated with transitioning
from emulation to purely synthetic machine intelligence.
36. Shulman (2010b).
37. How the pro et contra would balance out might depend on what kind of
work the superorganism is trying to do, and how generally capable the
most generally capable available emulation template is. Part of the reason
many different types of human beings are needed in large organizations
today is that humans who are very talented in many domains are rare.
38. It is of course very easy to make multiple copies of a software agent. But
note that copying is not in general sufficient to ensure that the copies have
the same final goals. In order for two agents to have the same final goals
(in the relevant sense of “same”), the goals must coincide in their
indexical elements. If Bob is selfish, a copy of Bob will likewise be
selfish. Yet their goals do not coincide: Bob cares about Bob whereas Bob-
copy cares about Bob-copy.
39. Shulman (2010b, 6).
40. This might be more feasible for biological humans and whole brain
emulations than for arbitrary artificial intelligences, which might be
constructed so as to have hidden compartments or functional dynamics
that may be very hard to discover. On the other hand, AIs specifically built
to be transparent should allow for more thoroughgoing inspection and
verification than is possible with brain-like architectures. Social pressures
may encourage AIs to expose their source code, and to modify themselves
to render themselves transparent—especially if being transparent is a
precondition to being trusted and thus to being given the opportunity to
partake in beneficial transactions. Cf. Hall (2007).
41. Some other issues that seem relatively minor, especially in cases where the
stakes are enormous (as they are for the key global coordination failures),
include the search cost of finding policies that could be of mutual interest,
and the possibility that some agents might have a basic preference for
“autonomy” in a form that would be reduced by entering into
comprehensive global treaties that have monitoring and enforcement
mechanisms attached.42. An AI might perhaps achieve this by modifying itself appropriately and
then giving observers read-only access to its source code. A machine
intelligence with a more opaque architecture (such as an emulation) might
perhaps achieve it by publicly applying to itself some motivation selection
method. Alternatively, an external coercive agency, such as a
superorganism police force, might perhaps be used not only to enforce the
implementation of a treaty reached between different parties, but also
internally by a single party to commit itself to a particular course of
action.
43. Evolutionary selection might have favored threat-ignorers and even
characters visibly so highly strung that they would rather fight to the death
than suffer the slightest discomfiture. Such a disposition might bring its
bearer valuable signaling benefits. (Any such instrumental rewards of
having the disposition need of course play no part in the agent’s conscious
motivation: he may value justice or honor as ends in themselves.)
44. A definitive verdict on these matters, however, must await further analysis.
There are various other potential complications which we cannot explore
here.
CHAPTER 12: ACQUIRING VALUES
1. Various complications and modulations of this basic idea could be
introduced. We discussed one variation in Chapter 8—that of a satisficing, as
opposed to maximizing, agent—and in the next chapter we briefly touch on
the issue of alternative decision theories. However, such issues are not
essential to the thrust of this subsection, so we will keep things simple by
focusing here on the case of an expected utility-maximizing agent.
2. Assuming the AI is to have a non-trivial utility function. It would be very
easy to build an agent that always chooses an action that maximizes expected
utility if its utility function is, e.g., the constant function U(w) = 0. Every
action would equally well maximize expected utility relative to that utility
function.
3. Also because we have forgotten the blooming buzzing confusion of our early
infancy, a time when we could not yet see very well because our brain had
not yet learned to interpret its visual input.
4. See also Yudkowsky (2011) and the review in section 5 of Muehlhauser and
Helm (2012).
5. It is perhaps just about conceivable that advances in software engineering
could eventually overcome these difficulties. Using modern tools, a single
programmer can produce software that would have been beyond the pale of asizeable team of developers forced to write directly in machine code.
Today’s AI programmers gain expressiveness from the wide availability of
high-quality machine learning and scientific calculation libraries, enabling
someone to hack up, for instance, a unique-face-counting webcam
application by chaining libraries together that they never could have written
on their own. The accumulation of reusable software, produced by specialists
but useable by non-specialists, will give future programmers an
expressiveness advantage. For example, a future robotics programmer might
have ready access to standard facial imprinting libraries, typical-office-
building-object collections, specialized trajectory libraries, and many other
functionalities that are currently unavailable.
6. Dawkins (1995, 132). The claim here is not necessarily that the amount of
suffering in the natural world outweighs the amount of positive well-being.
7. Required population sizes might be much larger or much smaller than those
that existed in our own ancestry. See Shulman and Bostrom (2012).
8. If it were easy to get an equivalent result without harming large numbers of
innocents, it would seem morally better to do so. If, nevertheless, digital
persons are created and made to suffer unjust harm, it may be possible to
compensate them for their suffering by saving them to file and later (when
humanity’s future is secured) rerunning them under more favorable
conditions. Such restitution could be compared in some ways to religious
conceptions of an afterlife in the context of theological attempts to address
the evidential problem of evil.
9. One of the field’s leading figures, Richard Sutton, defines reinforcement
learning not in terms of a learning method but in terms of a learning
problem: any method that is well suited to solving that problem is considered
a reinforcement learning method (Sutton and Barto 1998, 4). The present
discussion, in contrast, pertains to methods where the agent can be conceived
of as having the final goal of maximizing (some notion of) cumulative
reward. Since an agent with some very different kind of final goal might be
skilled at mimicking a reward-seeking agent in a wide range of situations,
and could thus be well suited to solving reinforcement learning problems,
there could be methods that would count as “reinforcement learning
methods” on Sutton’s definition that would not result in a wireheading
syndrome. The remarks in the text, however, apply to most of the methods
actually used in the reinforcement learning community.
10. Even if, somehow, a human-like mechanism could be set up within a
human-like machine intellect, the final goals acquired by this intellect
need not resemble those of a well-adjusted human, unless the rearing
environment for this digital baby also closely matched that of an ordinary
child: something that would be difficult to arrange. And even with ahuman-like rearing environment, a satisfactory result would not be
guaranteed, since even a subtle difference in innate dispositions can result
in very different reactions to a life event. It may, however, be possible to
create a more reliable value-accretion mechanism for human-like minds in
the future (perhaps using novel drugs or brain implants, or their digital
equivalents).
11. One might wonder why it appears we humans are not trying to disable the
mechanism that leads us to acquire new final values. Several factors might
be at play. First, the human motivation system is poorly described as a
coldly calculating utility-maximizing algorithm. Second, we might not
have any convenient means of altering the ways we acquire values. Third,
we may have instrumental reasons (arising, e.g., from social signaling
needs) for sometimes acquiring new final values—instrumental values
might not be as useful if our minds are partially transparent to other
people, or if the cognitive complexity of pretending to have a different set
of final values than we actually do is too taxing. Fourth, there are cases
where we do actively resist tendencies that produce changes in our final
values, for instance when we seek to resist the corrupting influence of bad
company. Fifth, there is the interesting possibility that we place some final
value on being the kind of agent that can acquire new final values in
normal human ways.
12. Or one might try to design the motivation system so that the AI is
indifferent to such replacement; see Armstrong (2010).
13. We will here draw on some elucidations made by Daniel Dewey (2011).
Other background ideas contributing to this framework have been
developed by Marcus Hutter (2005) and Shane Legg (2008), Eliezer
Yudkowsky (2001), Nick Hay (2005), Moshe Looks, and Peter de Blanc.
14. To avoid unnecessary complications, we confine our attention to
deterministic agents that do not discount future rewards.
15. Mathematically, an agent’s behavior can be formalized as an agent function,
which maps each possible interaction history to an action. Except for the
very simplest agents, it is infeasible to represent the agent function
explicitly as a lookup table. Instead, the agent is given some way of
computing which action to perform. Since there are many ways of
computing the same agent function, this leads to a finer individuation of an
agent as an agent program. An agent program is a specific program or
algorithm that computes an action for any given interaction history. While
it is often mathematically convenient and useful to think of an agent
program that interacts with some formally specified environment, it is
important to remember that this is an idealization. Real agents arephysically instantiated. This means not only that the agent interacts with
the environment via its sensors and effectors, but also that the agent’s
“brain” or controller is itself part of physical reality. Its operations can
therefore in principle be affected by external physical interferences (and
not only by receiving percepts from its sensors). At some point, therefore,
it becomes necessary to view an agent as an agent implementation. An
agent implementation is a physical structure that, in the absence of
interference from its environment, implements an agent function. (This
definition follows Dewey [2011].)
16. Dewey proposes the following optimality notion for a value learning agent:
Here, P 1 and P 2 are two probability functions. The second summation ranges
over some suitable class of utility functions over possible interaction histories.
In the version presented in the text, we have made explicit some dependencies
as well as availed ourselves of the simplifying possible worlds notation.
17. It should be noted that the set of utility functions should be such that
utilities can be compared and averaged. In general, this is problematic, and
it is not always obvious how to represent different moral theories of the
good in terms of cardinal utility functions. See, e.g., MacAskill 2010).
18. Or more generally, since ν might not be such as to directly imply for any
given pair of a possible world and a utility function (w, U) whether the
proposition ν(U) is true in w, what needs to be done is to give the AI an
adequate representation of the conditional probability distribution P(ν(U) |
w).
19. Consider first , the class of actions that are possible for an agent. One issue
here is what exactly should count as an action: only basic motor
commands (e.g. “send an electric pulse along output channel #00101100”),
or higher-level actions (e.g. “keep camera centered on face”)? Since we are
trying to develop an optimality notion rather than a practical
implementation plan, we may take the domain to be basic motor
commands (and since the set of possible motor commands might change
over time, we may need to index to time). However, in order to move
toward implementation it will presumably be necessary to introduce some
kind of hierarchical planning process, and one may then need to consider
how to apply the formula to some class of higher-level actions. Another
issue is how to analyze internal actions (such as writing strings to workingmemory). Since internal actions can have important consequences, one
would ideally want to include such basic internal actions as well as
motor commands. But there are limits to how far one can go in this
direction: the computation of the expected utility of any action in
requires multiple computational operations, and if each such operation
were also regarded as an action in that needed to be evaluated according
to AI-VL, we would face an infinite regress that would make it impossible
to ever get started. To avoid the infinite regress, one must restrict any
explicit attempt to estimate the expected utility to a limited number of
significant action possibilities. The system will then need some heuristic
process that identifies some significant action possibilities for further
consideration. (Eventually the system might also get around to making
explicit decisions regarding some possible actions to make modifications
to this heuristic process, actions that might have been flagged for explicit
attention by this self-same process; so that in the long run the system
might become increasingly effective at approximating the ideal identified
by AI-VL.)
Consider next , which is a class of possible worlds. One difficulty here is to
specify so that it is sufficiently inclusive. Failure to include some relevant w
i n could render the AI incapable of representing a situation that actually
occurs, resulting in the AI making bad decisions. Suppose, for example, that we
use some ontological theory to determine the makeup of . For instance, we
include in all possible worlds that consist of a certain kind of spacetime
manifold populated by elementary particles found in the standard model in
particle physics. This could distort the AI’s epistemology if the standard model
is incomplete or incorrect. One could try to use a bigger -class to cover more
possibilities; but even if one could ensure that every possible physical universe
is included one might still worry that some other possibility would be left out.
For example, what about the possibility of dualistic possible worlds in which
facts about consciousness do not supervene on facts about physics? What about
indexical facts? Normative facts? Facts of higher mathematics? What about
other kinds of fact that we fallible humans might have overlooked but that
could turn out to be important to making things go as well as possible? Some
people have strong convictions that some particular ontological theory is
correct. (Among people writing on the future of AI, a belief in a materialistic
ontology, in which the mental supervenes on the physical, is often taken for
granted.) Yet a moment’s reflection on the history of ideas should help us
realize that there is a significant possibility that our favorite ontology is wrong.
Had nineteenth-century scientists attempted a physics-inspired definition of ,
they would probably have neglected to include the possibility of a non-
Euclidian spacetime or an Everettian (“many-worlds”) quantum theory or acosmological multiverse or the simulation hypothesis—possibilities that now
appear to have a substantial probability of obtaining in the actual world. It is
plausible that there are other possibilities to which we in the present generation
are similarly oblivious. (On the other hand, if is too big, there arise technical
difficulties related to having to assign measures over transfinite sets.) The ideal
might be if we could somehow arrange things such that the AI could use some
kind of open-ended ontology, one that the AI itself could subsequently extend
using the same principles that we would use when deciding whether to
recognize a new type of metaphysical possibility.
Consider P(w|Ey). Specifying this conditional probability is not strictly part
of the value-loading problem. In order to be intelligent, the AI must already
have some way of deriving reasonably accurate probabilities over many
relevant factual possibilities. A system that falls too far short on this score will
not pose the kind of danger that concerns us here. However, there may be a risk
that the AI will end up with an epistemology that is good enough to make the
AI instrumentally effective yet not good enough to enable it to think correctly
about some possibilities that are of great normative importance. (The problem
of specifying P(w|Ey) is in this way related to the problem of specifying .)
Specifying P(w | Ey) also requires confronting other issues, such as how to
represent uncertainty over logical impossibilities.
The aforementioned issues—how to define a class of possible actions, a class
of possible worlds, and a likelihood distribution connecting evidence to classes
of possible worlds—are quite generic: similar issues arise for a wide range of
formally specified agents. It remains to examine a set of issues more peculiar to
the value learning approach; namely, how to define , V(U), and P(V(U) | w).
is a class of utility functions. There is a connection between and
inasmuch as each utility function U (w) in should ideally assign utilities to
each possible world w in . But also needs to be wide in the sense of
containing sufficiently many and diverse utility functions for us to have
justified confidence that at least one of them does a good job of representing
the intended values.
The reason for writing P(V(U) | w) rather than simply P(U|w) is to emphasize
the fact that probabilities are assigned to propositions. A utility function, per se,
is not a proposition, but we can transform a utility function into a proposition
by making some claim about it. For example, we may claim of a particular
utility function U(.) that it describes the preferences of a particular person, or
that it represents the prescriptions implied by some ethical theory, or that it is
the utility function that the principal would have wished to have implemented if
she had thought things through. The “value criterion” V(.) can thus be construed
as a function that takes as its argument a utility function U and gives as itsvalue a proposition to the effect that U satisfies the criterion V. Once we have
defined a proposition V(U), we can hopefully obtain the conditional probability
P(V(U)|w) from whatever source we used to obtain the other probability
distributions in the AI. (If we are certain that all normatively relevant facts are
taken into account in individuating the possible worlds , then P(V(U)|w)
should equal zero or one in each possible world.) The question remains how to
define V. This is discussed further in the text.
20. These are not the only challenges for the value learning approach. Another
issue, for instance, is how to get the AI to have sufficiently sensible initial
beliefs—at least by the time it becomes strong enough to subvert the
programmers’ attempts to correct it.
21. Yudkowsky (2001).
22. The term is taken from American football, where a “Hail Mary” is a very
long forward pass made in desperation, typically when the time is nearly
up, on the off chance that a friendly player might catch the ball near the
end zone and score a touchdown.
23. The Hail Mary approach relies on the idea that a superintelligence could
articulate its preferences with greater exactitude than we humans can
articulate ours. For example, a superintelligence could specify its
preference as code. So if our AI is representing other superintelligences as
computational processes that are perceiving their environment, then our AI
should be able to reason about how those alien superintelligences would
respond to some hypothetical stimulus, such as a “window” popping up in
their visual field presenting them with the source code of our own AI and
asking them to specify their instructions to us in some convenient pre-
specified format. Our AI could then read off these imaginary instructions
(from its own model of this counterfactual scenario wherein these alien
superintelligences are represented), and we would have built our AI so that
it would be motivated to follow those instructions.
24. An alternative would be to create a detector that looks (within our AI’s
world model) for (representations of) physical structures created by a
superintelligent civilization. We could then bypass the step of identifying
the hypothesized superintelligences’ preference functions, and give our
own AI the final value of trying to copy whatever physical structures it
believes superintelligent civilizations tend to produce.
There are technical challenges with this version, too, however. For instance,
since our own AI, even after it has attained superintelligence, may not be able
to know with great precision what physical structures other superintelligences
build, our AI may need to resort to trying to approximate those structures. To
do this, it would seem our AI would need a similarity metric by which to judge
how closely one physical artifact approximates another. But similarity metricsbased on crude physical measures may be inadequate—it being no good, for
example, to judge that a brain is more similar to a Camembert cheese than to a
computer running an emulation.
A more feasible approach might be to look for “beacons”: messages about
utility functions encoded in some suitable simple format. We would build our
AI to want to follow whatever such messages about utility functions it
hypothesizes might exist out there in the universe; and we would hope that
friendly extraterrestrial AIs would create a variety of beacons of the types that
they (with their superintelligence) reckon that simple civilizations like ours are
most likely to build our AI to look for.
25. If every civilization tried to solve the value-loading problem through a Hail
Mary, the pass would fail. Somebody has to do it the hard way.
26. Christiano (2012).
27. The AI we build need not be able to find the model either. Like us, it could
reason about what such a complex implicit definition would entail
(perhaps by looking at its environment and following much the same kind
of reasoning that we would follow).
28. Cf. Chapters 9 and 11.
29. For instance, MDMA may temporarily increase empathy; oxytocin may
temporarily increase trust (Vollenweider et al. 1998; Bartz et al. 2011).
However, the effects seem quite variable and context dependent.
30. The enhanced agents might be killed off or placed in suspended animation
(paused), reset to an earlier state, or disempowered and prevented from
receiving any further enhancements, until the overall system has reached a
more mature and secure state where these earlier rogue elements no longer
pose a system-wide threat.
31. The issue might also be less obvious in a future society of biological
humans, one that has access to advanced surveillance or biomedical
techniques for psychological manipulation, or that is wealthy enough to
afford an extremely high ratio of security professionals to invigilate the
regular citizenry (and each other).
32. Cf. Armstrong (2007) and Shulman (2010b).
33. One open question is to what degree a level n supervisor would need to
monitor not only their level (n – 1) supervisees, but also their level (n – 2)
supervisees, in order to know that the level (n – 1) agents are doing their
jobs properly. And to know that the level ( n – 1) agents have successfully
managed the level (n – 2) agents, is it further necessary for the level n
agent to also monitor the level (n – 3) agents?
34. This approach straddles the line between motivation selection and
capability control. Technically, the part of the arrangement that consists ofhuman beings controlling a set of software supervisors counts as capability
control, whereas the part of the arrangement that consists of layers of
software agents within the system controlling other layers is motivation
selection (insofar as it is an arrangement that shapes the system’s
motivational tendencies).
35. In fact, many other costs deserve consideration but cannot be given it here.
For example, whatever agents are charged with ruling over such a
hierarchy might become corrupted or debased by their power.
36. For this guarantee to be effective, it must be implemented in good faith.
This would rule out certain kinds of manipulation of the emulation’s
emotional and decision-making faculties which might otherwise be used
(for instance) to install a fear of being halted or to prevent the emulation
from rationally considering its options.
37. See, e.g., Brinton (1965); Goldstone (1980, 2001). (Social science progress
on these questions could make a nice gift to the world’s despots, who
might use more accurate predictive models of social unrest to optimize
their population control strategies and to gently nip insurgencies in the bud
with less-lethal force.)
38. Cf. Bostrom (2011a, 2009b).
39. In the case of an entirely artificial system, it might be possible to obtain
some of the advantages of an institutional structure without actually
creating distinct subagents. A system might incorporate multiple
perspectives into its decision process without endowing each of those
perspectives with its own panoply of cognitive faculties required for
independent agency. It could be tricky, however, to fully implement the
“observe the behavioral consequences of a proposed change, and revert
back to an earlier version if the consequences appear undesirable from the
ex ante standpoint” feature described in the text in a system that is not
composed of subagents.
CHAPTER 13: CHOOSING THE CRITERIA FOR CHOOSING
1. A recent canvass of professional philosophers found the percentage of
respondents who “accept or leans toward” various positions. On normative
ethics, the results were deontology 25.9%; consequentialism 23.6%; virtue
ethics 18.2%. On metaethics, results were moral realism 56.4%; moral anti-
realism 27.7%. On moral judgment: cognitivism 65.7%; non-cognitivism
17.0% (Bourget and Chalmers 2009).
2. Pinker (2011).
3. For a discussion of this issue, see Shulman et al. (2009).4. Moore (2011).
5. Bostrom (2006b).
6. Bostrom (2009b).
7. Bostrom (2011a).
8. More precisely, we should defer to its opinion except on those topics where
we have good reason to suppose that our beliefs are more accurate. For
example, we might know more about what we are thinking at a particular
moment than the superintelligence does if it is not able to scan our brains.
However, we could omit this qualification if we assume that the
superintelligence has access to our opinions; we could then also defer to the
superintelligence the task of judging when our opinions should be trusted.
(There might remain some special cases, involving indexical information,
that need to be handled separately—by, for example, having the
superintelligence explain to us what it would be rational to believe from our
perspective.) For an entry into the burgeoning philosophical literature on
testimony and epistemic authority, see, e.g., Elga (2007).
9. Yudkowsky (2004). See also Mijic (2010).
10. For example, David Lewis proposed a dispositional theory of value, which
holds, roughly, that some thing X is a value for A if and only if A would
want to want X if A were perfectly rational and ideally acquainted with X
(Smith et al. 1989). Kindred ideas had been put forward earlier; see, e.g.,
Sen and Williams (1982), Railton (1986), and Sidgwick and Jones (2010).
Along somewhat similar lines, one common account of philosophical
justification, the method of reflective equilibrium, proposes a process of
iterative mutual adjustment between our intuitions about particular cases,
the general rules which we think govern these cases, and the principles
according to which we think these elements should be revised, to achieve a
more coherent system; see, e.g., Rawls (1971) and Goodman (1954).
11. Presumably the intention here is that when the AI acts to prevent such
disasters, it should do it with as light a touch as possible, i.e. in such a
manner that it averts the disaster but without exerting too much influence
over how things turn out for humanity in other respects.
12. Yudkowsky (2004).
13. Rebecca Roache, personal communication.
14. The three principles are “Defend humans, the future of humanity, and
humane nature” (humane here being that which we wish we were, as
distinct from human, which is what we are); “Humankind should not spend
the rest of eternity desperately wishing that the programmers had done
something differently”; and “Help people.”
15. Some religious groups place a strong emphasis on faith in contradistinction
to reason, the latter of which they may regard—even in its hypotheticallymost idealized form and even after it would have ardently and open-
mindedly studied every scripture, revelation, and exegesis—to be
insufficient for the attainment of essential spiritual insights. Those holding
such views might not regard CEV as an optimal guide to decision-making
(though they might still prefer it to various other imperfect guides that
might in actuality be followed if the CEV approach were eschewed).
16. An AI acting like a latent force of nature to regulate human interactions has
been referred to as a “Sysop,” a kind of “operating system” for the matter
occupied by human civilization. See Yudkowsky (2001).
17. “Might,” because conditional on humanity’s coherent extrapolated volition
wishing not to extend moral consideration to these entities, it is perhaps
doubtful whether those entities actually have moral status (despite it
seeming very plausible now that they do). “Potentially,” because even if a
blocking vote prevents the CEV dynamic from directly protecting these
outsiders, there is still a possibility that, within whatever ground rules are
left over once the initial dynamic has run, individuals whose wishes were
respected and who want some outsiders’ welfare to be protected may
successfully bargain to attain this outcome (at the expense of giving up
some of their own resources). Whether this would be possible might
depend on, among other things, whether the outcome of the CEV dynamic
is a set of ground rules that makes it feasible to reach negotiated
resolutions to issues of this kind (which might require provisions to
overcome strategic bargaining problems).
18. Individuals who contribute positively to realizing a safe and beneficial
superintelligence might merit some special reward for their labour, albeit
something short of a near-exclusive mandate to determine the disposition
of humanity’s cosmic endowment. However, the notion of everybody
getting an equal share in our extrapolation base is such a nice Schelling
point that it should not be lightly tossed away. There is, in any case, an
indirect way in which virtue could be rewarded: namely, the CEV itself
might turn out to specify that good people who exerted themselves on
behalf of humanity should be suitably recognized. This could happen
without such people being given any special weight in the extrapolation
base if—as is easily imaginable—our CEV would endorse (in the sense of
giving at least some nonzero weight to) a principle of just desert.
19. Bostrom et al. (2013).
20. To the extent that there is some (sufficiently definite) shared meaning that
is being expressed when we make moral assertions, a superintelligence
should be able to figure out what that meaning is. And to the extent that
moral assertions are “truth-apt” (i.e. have an underlying propositionalcharacter that enables them to be true or false), the superintelligence
should be able to figure out which assertions of the form “Agent X ought
now to Φ” are true. At least, it should outperform us on this task.
An AI that initially lacks such a capacity for moral cognition should be able
to acquire it if it has the intelligence amplification superpower. One way the AI
could do this is by reverse-engineering the human brain’s moral thinking and
then implement a similar process but run it faster, feed it more accurate factual
information, and so forth.
21. Since we are uncertain about metaethics, there is a question of what the AI
is to do if the preconditions for MR fail to obtain. One option is to
stipulate that the AI shut itself off if it assigns a sufficiently high
probability to moral cognitivism being false or to there being no suitable
non-relative moral truths. Alternatively, we could have the AI revert to
some alternative approach, such as CEV.
We could refine the MR proposal to make it clearer what is to be done in
various ambiguous or degenerate cases. For instance, if error theory is true (and
hence all positive moral assertions of the form “I ought now to τ” are false),
then the fallback strategy (e.g. shutting down) would be invoked. We could also
specify what should happen if there are multiple feasible actions, each of which
would be morally right. For example, we might say that in such cases the AI
should perform (one of) the permissible actions that humanity’s collective
extrapolation would have favored. We might also stipulate what should happen
if the true moral theory does not employ terms like “morally right” in its basic
vocabulary. For instance, a consequentialist theory might hold that some
actions are better than others but that there is no particular threshold
corresponding to the notion of an action being “morally right.” We could then
say that if such a theory is correct, MR should perform one of the morally best
feasible actions, if there is one; or, if there is an infinite number of feasible
actions such that for any feasible action there is a better one, then maybe MR
could pick any that is at least astronomically better than the best action that any
human would have selected in a similar situation, if such an action is feasible—
or if not, then an action that is at least as good as the best action a human would
have performed.
A couple of general points should be borne in mind when thinking about how
the MR proposal could be refined. First, we might start conservatively, using
the fallback option to cover almost all contingencies and only use the “morally
right” option in those that we feel we fully understand. Second, we might add
the general modulator to the MR proposal that it is to be “interpreted
charitably, and revised as we would have revised it if we had thought more
carefully about it before we wrote it down, etc.”
22. Of these terms, “knowledge” might seem the one most readily susceptibleto a formal analysis (in information-theoretic terms). However, to
represent what it is for a human to know something, the AI may need a
sophisticated set of representations relating to complex psychological
properties. A human being does not “know” all the information that is
stored somewhere in her brain.
23. One indicator that the terms in CEV are (marginally) less opaque is that it
would count as philosophical progress if we could analyze moral rightness
in terms like those used in CEV. In fact, one of the main strands in
metaethics—ideal observer theory—purports to do just that. See, e.g.,
Smith et al. (1989).
24. This requires confronting the problem of fundamental normative
uncertainty. It can be shown that it is not always appropriate to act
according to the moral theory that has the highest probability of being true.
It can also be shown that it is not always appropriate to perform the action
that has the highest probability of being right. Some way of trading
probabilities against “degrees of wrongness” or severity of issues at stake
seems to be needed. For some ideas in this direction, see Bostrom (2009a).
25. It could possibly even be argued that it is an adequacy condition for any
explication of the notion of moral rightness that it account for how Joe
Sixpack is able to have some idea of right and wrong.
26. It is not obvious that the morally right thing for us to do is to build an AI
that implements MR, even if we assume that the AI itself would always act
morally. Perhaps it would be objectionably hubristic or arrogant of us to
build such an AI (especially since many people may disapprove of that
project). This issue can be partially finessed by tweaking the MR proposal.
Suppose that we stipulate that the AI should act (to do what it would be
morally right for it to do) only if it was morally right for its creators to
have built the AI in the first place; otherwise it should shut itself down. It
is hard to see how we would be committing any grave moral wrong in
creating that kind of AI, since if it were wrong for us to create it, the only
consequence would be that an AI was created that immediately shuts itself
down, assuming that the AI has committed no mind crime up to that point.
(We might nevertheless have acted wrongly—for instance, by having
failed to seize the opportunity to build some other AI instead.)
A second issue is supererogation. Suppose there are many actions the AI
could take, each of which would be morally right—in the sense of being
morally permissible—yet some of which are morally better than the others. One
option is to have the AI aim to select the morally best action in any such a
situation (or one of the best actions, in case there are several that are equally
good). Another option is to have the AI select from among the morallypermissible actions one that maximally satisfies some other (non-moral)
desideratum. For example, the AI could select, from among the actions that are
morally permissible, the action that our CEV would prefer it to take. Such an
AI, while never doing anything that is morally impermissible, might protect our
interests more than an AI that does what is morally best.
27. When the AI evaluates the moral permissibility of our act of creating the
AI, it should interpret permissibility in its objective sense. In one ordinary
sense of “morally permissible,” a doctor acts morally permissibly when
she prescribes a drug she believes will cure her patient—even if the
patient, unbeknownst to the doctor, is allergic to the drug and dies as a
result. Focusing on objective moral permissibility takes advantage of the
presumably superior epistemic position of the AI.
28. More directly, it depends on the AI’s beliefs about which ethical theory is
true (or, more precisely, on its probability distribution over ethical
theories).
29. It can be difficult to imagine how superlatively wonderful these physically
possible lives might be. See Bostrom (2008c) for a poetic attempt to
convey some sense of this. See Bostrom (2008b) for an argument that
some of these possibilities could be good for us, good for existing human
beings.
30. It might seem deceptive or manipulative to promote one proposal if one
thinks that some other proposal would be better. But one could promote it
in ways that avoid insincerity. For example, one could freely acknowledge
the superiority of the ideal while still promoting the non-ideal as the best
attainable compromise.
31. Or some other positively evaluative term, such as “good,” “great,” or
“wonderful.”
32. This echoes a principle in software design known as “Do What I Mean,” or
DWIM. See Teitelman (1966).
33. Goal content, decision theory, and epistemology are three aspects that
should be elucidated; but we do not intend to beg the question of whether
there must be a neat decomposition into these three separate components.
34. An ethical project ought presumably to allocate at most a modest portion of
the eventual benefits that the superintelligence produces as special rewards
to those who contributed in morally permissible ways to the project’s
success. Allocating a great portion to the incentive wrapping scheme
would be unseemly. It would be analogous to a charity that spends 90% of
its income on performance bonuses for its fundraisers and on advertising
campaigns to increase donations.
35. How could the dead be rewarded? One can think of several possibilities. At
the low end, there could be memorial services and monuments, whichwould be a reward insofar as people desired posthumous fame. The
deceased might also have other preferences about the future that could be
honored, for instance concerning cultures, arts, buildings, or natural
environments. Furthermore, most people care about their descendants, and
special privileges could be granted to the children and grandchildren of
contributors. More speculatively, the superintelligence might be able to
create relatively faithful simulations of some past people—simulations
that would be conscious and that would resemble the original sufficiently
to count as a form of survival (according to at least some people’s
criteria). This would presumably be easier for people who have been
placed in cryonic suspension; but perhaps for a superintelligence it would
not be impossible to recreate something quite similar to the original
person from other preserved records such as correspondence, publications,
audiovisual materials and digital records, or the personal memories of
other survivors. A superintelligence might also think of some possibilities
that do not readily occur to us.
36. On Pascalian mugging, see Bostrom (2009b). For an analysis of issues
related to infinite utilities, see Bostrom (2011a). On fundamental
normative uncertainty, see, e.g., Bostrom (2009a).
37. E.g., Price (1991); Joyce (1999); Drescher (2006); Yudkowsky (2010); Dai
(2009).
38. E.g., Bostrom (2009a).
39. It is also conceivable that using indirect normativity to specify the AI’s goal
content would mitigate the problems that might arise from an incorrectly
specified decision theory. Consider, for example, the CEV approach. If it
were implemented well, it might be able to compensate for at least some
errors in the specification of the AI’s decision theory. The implementation
could allow the values that our coherent extrapolated volition would want
the AI to pursue to depend on the AI’s decision theory. If our idealized
selves knew they were making value specifications for an AI that was
using a particular kind of decision theory, they could adjust their value
specifications such as to make the AI behave benignly despite its warped
decision theory—much like one can cancel out the distorting effects of one
lens by placing another lens in front of it that distorts oppositely.
40. Some epistemological systems may, in a holistic manner, have no distinct
foundation. In that case, the constitutional inheritance is not a distinct set
of principles, but rather, as it were, an epistemic starting point that
embodies certain propensities to respond to incoming streams of evidence.
41. See, e.g., the problem of distortion discussed in Bostrom (2011a).
42. For instance, one disputed issue in anthropic reasoning is whether the so-called self-indication assumption should be accepted. The self-indication
assumption states, roughly, that from the fact that you exist you should
infer that hypotheses according to which larger numbers N of observers
exist should receive a probability boost proportional to N. For an argument
against this principle, see the “Presumptuous Philosopher” gedanken
experiment in Bostrom (2002a). For a defense of the principle, see Olum
(2002); and for a critique of that defense, see Bostrom and Ćirković
(2003). Beliefs about the self-indication assumption might affect various
empirical hypotheses of potentially crucial strategic relevance, for
example, considerations such as the Carter–Leslie doomsday argument, the
simulation argument, and “great filter” arguments. See Bostrom (2002a,
2003a, 2008a); Carter (1983); Ćirković et al. (2010); Hanson (1998d);
Leslie (1996); Tegmark and Bostrom (2005). A similar point could be
made with regard to other fraught issues in observation selection theory,
such as whether the choice of reference class can be relativized to
observer-moments, and if so how.
43. See, e.g., Howson and Urbach (1993). There are also some interesting
results that narrow the range of situations in which two Bayesian agents
can rationally disagree when their opinions are common knowledge; see
Aumann (1976) and Hanson (2006).
44. Cf. the concept of a “last judge” in Yudkowsky (2004).
45. There are many important issues outstanding in epistemology, some
mentioned earlier in the text. The point here is that we may not need to get
all the solutions exactly right in order to achieve an outcome that is
practically indiscernible from the best outcome. A mixture model (which
throws together a wide range of diverse priors) might work.
CHAPTER 14: THE STRATEGIC PICTURE
1. This principle is introduced in Bostrom (2009b, 190), where it is also noted
that it is not tautological. For a visual analogy, picture a box with large but
finite volume, representing the space of basic capabilities that could be
obtained through some possible technology. Imagine sand being poured into
this box, representing research effort. How you pour the sand determines
where it piles up in the box. But if you keep on pouring, the entire space
eventually gets filled.
2. Bostrom (2002b).
3. This is not the perspective from which science policy has traditionally been
viewed. Harvey Averch describes science and technology policy in the
United States between 1945 and 1984 as having been centered on debatesabout the optimum level of public investment in the S&T enterprise and on
the extent to which the government should attempt to “pick winners” in order
to achieve the greatest increase in the nation’s economic prosperity and
military strength. In these calculations, technological progress is always
assumed to be good. But Averch also describes the rise of critical
perspectives, which question the “progress is always good” premiss (Averch
1985). See also Graham (1997).
4. Bostrom (2002b).
5. This is of course by no means tautological. One could imagine a case being
made for a different order of development. It could be argued that it would be
better for humanity to confront some less difficult challenge first, say the
development of nanotechnology, on grounds that this would force us to
develop better institutions, become more internationally coordinated, and
mature in our thinking about global strategy. Perhaps we would be more
likely to rise to a challenge that presents a less metaphysically confusing
threat than machine superintelligence. Nanotechnology (or synthetic biology,
or whatever the lesser challenge we confront first) might then serve as a
footstool that would help us ascend to the capability level required to deal
with the higher-level challenge of superintelligence.
Such an argument would have to be assessed on a case-by-case basis. For
example, in the case of nanotechnology, one would have to consider various
possible consequences such as the boost in hardware performance from
nanofabricated computational substrates; the effects of cheap physical capital
for manufacturing on economic growth; the proliferation of sophisticated
surveillance technology; the possibility that a singleton might emerge though
the direct or indirect effects of a nanotechnology breakthrough; and the greater
feasibility of neuromorphic and whole brain emulation approaches to machine
intelligence. It is beyond the scope of our investigation to consider all these
issues (or the parallel issues that might arise for other existential risk-causing
technologies). Here we just point out the prima facie case for favoring a
superintelligence-first sequence of development—while stressing that there are
complications that might alter this preliminary assessment in some cases.
6. Pinker (2011); Wright (2001).
7. It might be tempting to suppose the hypothesis that everything has
accelerated to be meaningless on grounds that it does not (at first glance)
seem to have any observational consequences; but see, e.g., Shoemaker
(1969).
8. The level of preparedness is not measured by the amount of effort expended
on preparedness activities, but by how propitiously configured conditions
actually are and how well-poised key decision makers are to take appropriate
action.9. The degree of international trust during the lead-up to the intelligence
explosion may also be a factor. We consider this in the section
“Collaboration” later in the chapter.
10. Anecdotally, it appears those currently seriously interested in the control
problem are disproportionately sampled from one extreme end of the
intelligence distribution, though there could be alternative explanations for
this impression. If the field becomes fashionable, it will undoubtedly be
flooded with mediocrities and cranks.
11. I owe this term to Carl Shulman.
12. How similar to a brain does a machine intelligence have to be to count as a
whole brain emulation rather than a neuromorphic AI? The relevant
determinant might be whether the system reproduces either the values or
the full panoply of cognitive and evaluative tendencies of either a
particular individual or a generic human being, because this would
plausibly make a difference to the control problem. Capturing these
properties may require a rather high degree of emulation fidelity.
13. The magnitude of the boost would of course depend on how big the push
was, and also where resources for the push came from. There might be no
net boost for neuroscience if all the extra resources invested in whole brain
emulation research were deducted from regular neuroscience research—
unless a keener focus on emulation research just happened to be a more
effective way of advancing neuroscience than the default portfolio of
neuroscience research.
14. See Drexler (1986, 242). Drexler (private communication) confirms that
this reconstruction corresponds to the reasoning he was seeking to present.
Obviously, a number of implicit premisses would have to be added if one
wished to cast the argument in the form of a deductively valid chain of
reasoning.
15. Perhaps we ought not to welcome small catastrophes in case they increase
our vigilance to the point of making us prevent the medium-scale
catastrophes that would have been needed to make us take the strong
precautions necessary to prevent existential catastrophes? (And of course,
just as with biological immune systems, we also need to be concerned with
over-reactions, analogous to allergies and autoimmune disorders.)
16. Cf. Lenman (2000); Burch-Brown (2014).
17. Cf. Bostrom (2007).
18. Note that this argument focuses on the ordering rather than the timing of the
relevant events. Making superintelligence happen earlier would help
preempt other existential transition risks only if the intervention changes
the sequence of the key developments: for example, by makingsuperintelligence happen before various milestones are reached in
nanotechnology or synthetic biology.
19. If solving the control problem is extremely difficult compared to solving the
performance problem in machine intelligence, and if project ability
correlates only weakly with project size, then it is possible that it would be
better that a small project gets there first, namely if the variance in
capability is greater among smaller projects. In such a situation, even if
smaller projects are on average less competent than larger projects, it
could be less unlikely that a given small project would happen to have the
freakishly high level of competence needed to solve the control problem.
20. This is not to deny that one can imagine tools that could promote global
deliberation and which would benefit from, or even require, further
progress in hardware—for example, high-quality translation, better search,
ubiquitous access to smart phones, attractive virtual reality environments
for social intercourse, and so forth.
21. Investment in emulation technology could speed progress toward whole
brain emulation not only directly (through any technical deliverables
produced) but also indirectly by creating a constituency that will push for
more funding and boost the visibility and credibility of the whole brain
emulation (WBE) vision.
22. How much expected value would be lost if the future were shaped by the
desires of one random human rather than by (some suitable superposition
of) the desires of all of humanity? This might depend sensitively on what
evaluation standard we use, and also on whether the desires in question are
idealized or raw.
23. For example, whereas human minds communicate slowly via language, AIs
can be designed so that instances of the same program are able easily and
quickly to transfer both skills and information amongst one another.
Machine minds designed ab initio could do away with cumbersome legacy
systems that helped our ancestors deal with aspects of the natural
environment that are unimportant in cyberspace. Digital minds might also
be designed to take advantage of fast serial processing unavailable to
biological brains, and to make it easy to install new modules with highly
optimized functionality (e.g. symbolic processing, pattern recognition,
simulators, data mining, and planning). Artificial intelligence might also
have significant non-technical advantages, such as being more easily
patentable or less entangled in the moral complexities of using human
uploads.
24. If p 1 and p 2 are the probabilities of failure at each step, the total probability
of failure is p 1 + (1 – p 1 )p 2 since one can fail terminally only once.25. It is possible, of course, that the frontrunner will not have such a large lead
and will not be able to form a singleton. It is also possible that a singleton
would arise before AI even without the intervention of WBE, in which case
this reason for favoring a WBE-first scenario falls away.
26. Is there a way for a promoter of WBE to increase the specificity of her
support so that it accelerates WBE while minimizing the spillover to AI
development? Promoting scanning technology is probably a better bet than
promoting neurocomputational modeling. (Promoting computer hardware
is unlikely to make much difference one way or the other, given the large
commercial interests that are anyway incentivizing progress in that field.)
Promoting scanning technology may increase the likelihood of a multipolar
outcome by making scanning less likely to be a bottleneck, thus increasing the
chance that the early emulation population will be stamped from many different
human templates rather than consisting of gazillions of copies of a tiny number
of templates. Progress in scanning technology also makes it more likely that the
bottleneck will instead be computing hardware, which would tend to slow the
takeoff.
27. Neuromorphic AI may also lack other safety-promoting attributes of whole
brain emulation, such as having a profile of cognitive strengths and
weaknesses similar to that of a biological human being (which would let us
use our experience of humans to form some expectations of the system’s
capabilities at different stages of its development).
28. If somebody’s motive for promoting WBE is to make WBE happen before
AI, they should bear in mind that accelerating WBE will alter the order of
arrival only if the default timing of the two paths toward machine
intelligence is close and with a slight edge to AI. Otherwise, either
investment in WBE will simply make WBE happen earlier than it
otherwise would (reducing hardware overhang and preparation time) but
without affecting the sequence of development; or else such investment in
WBE will have little effect (other than perhaps making AI happen even
sooner by stimulating progress on neuromorphic AI).
29. Comment on Hanson (2009).
30. There would of course be some magnitude and imminence of existential risk
for which it would be preferable even from the person-affecting
perspective to postpone the risk—whether to enable existing people to eke
out a bit more life before the curtain drops or to provide more time for
mitigation efforts that might reduce the danger.
31. Suppose we could take some action that would bring the intelligence
explosion closer by one year. Let us say that the people currently
inhabiting the Earth are dying off at a rate of 1% per year, and that the
default risk of human extinction from the intelligence explosion is 20% (topick an arbitrary number for the purposes of illustration only). Then
hastening the arrival of the intelligence explosion by 1 year might be
worth (from a person-affecting standpoint) an increase in the risk from
20% to 21%, i.e. a 5% increase in risk level. However, the vast majority of
people alive one year before the start of the intelligence explosion would
at that point have an interest in postponing it if they could thereby reduce
the risk of the explosion by one percentage point (since most individuals
would reckon their own risk of dying in the next year to be much smaller
than 1%—given that most mortality occurs in relatively narrow
demographics such as the frail and the elderly). One could thus have a
model in which each year the population votes to postpone the intelligence
explosion by another year, so that the intelligence explosion never
happens, although everybody who ever lives agrees that it would be better
if the intelligence explosion happened at some point. In reality, of course,
coordination failures, limited predictability, or preferences for things other
than personal survival are likely to prevent such an unending pause.
If one uses the standard economic discount factor instead of the person-
affecting standard, the magnitude of the potential upside is diminished, since
the value of existing people getting to enjoy astronomically long lives is then
steeply discounted. This effect is especially strong if the discount factor is
applied to each individual’s subjective time rather than to sidereal time. If
future benefits are discounted at a rate of x% per year, and the background level
of existential risk from other sources is y% per year, then the optimum point for
the intelligence explosion would be when delaying the explosion for another
year would produce less than x + y percentage points of reduction of the
existential risk associated with an intelligence explosion.
32. I am indebted to Carl Shulman and Stuart Armstrong for help with this
model. See also Shulman (2010a, 3): “Chalmers (2010) reports a
consensus among cadets and staff at the U.S. West Point military academy
that the U.S. government would not restrain AI research even in the face of
potential catastrophe, for fear that rival powers would gain decisive
advantage.”
33. That is, information in the model is always bad ex ante. Of course,
depending on what the information actually is, it will in some cases turn
out to be good that the information became known, notably if the gap
between leader and runner-up is much greater than one would reasonably
have guessed in advance.
34. It might even present an existential risk, especially if preceded by the
introduction of novel military technologies of destruction or
unprecedented arms buildups.35. A project could have its workers distributed over a large number of
locations and collaborating via encrypted communications channels. But
this tactic involves a security trade-off: while geographical dispersion may
offer some protection against military attacks, it would impede operational
security, since it is harder to prevent personnel from defecting, leaking
information, or being abducted by a rival power if they are spread out over
many locations.
36. Note that a large temporal discount factor could make a project behave in
some ways as though it were in a race, even if it knows it has no real
competitor. The large discount factor means it would care little about the
far future. Depending on the situation, this would discourage blue-sky
R&D, which would tend to delay the machine intelligence revolution
(though perhaps making it more abrupt when it does occur, because of
hardware overhang). But the large discount factor—or a low level of
caring for future generations—would also make existential risks seem to
matter less. This would encourage gambles that involve the possibility of
an immediate gain at the expense of an increased risk of existential
catastrophe, thus disincentivizing safety investment and incentivizing an
early launch—mimicking the effects of the race dynamic. By contrast to
the race dynamic, however, a large discount factor (or disregard for future
generations) would have no particular tendency to incite conflict.
Reducing the race dynamic is a main benefit of collaboration. That
collaboration would facilitate sharing of ideas for how to solve the control
problem is also a benefit, although this is to some extent counterbalanced by
the fact that collaboration would also facilitate sharing of ideas for how to solve
the competence problem. The net effect of this facilitation of idea-sharing may
be to slightly increase the collective intelligence of the relevant research
community.
37. On the other hand, public oversight by a single government would risk
producing an outcome in which one nation monopolizes the gains. This
outcome seems inferior to one in which unaccountable altruists ensure that
everybody stands to gain. Furthermore, oversight by a national government
would not necessarily mean that even all the citizens of that country
receive a share of the benefit: depending on the country in question, there
is a greater or smaller risk that all the benefits would be captured by a
political elite or a few self-serving agency personnel.
38. One qualification is that the use of incentive wrapping (as discussed in
Chapter 12) might in some circumstances encourage people to join a
project as active collaborators rather than passive free-riders.
39. Diminishing returns would seem to set in at a much smaller scale. Most
people would rather have one star than a one-in-a-billion chance of agalaxy with a billion stars. Indeed, most people would rather have a
billionth of the resources on Earth than a one-in-a-billion chance of
owning the entire planet.
40. Cf. Shulman (2010a).
41. Aggregative ethical theories run into trouble when the idea that the cosmos
might be infinite is taken seriously; see Bostrom (2011b). There may also
be trouble when the idea of ridiculously large but finite values is taken
seriously; see Bostrom (2009b).
42. If one makes a computer larger, one eventually faces relativistic constraints
arising from communication latencies between the different parts of the
computer—signals do not propagate faster than light. If one shrinks the
computer, one encounters quantum limits to miniaturization. If one
increases the density of the computer, one slams into the black hole limit.
Admittedly, we cannot be completely certain that new physics will not one
day be discovered offering some way around these limitations.
43. The number of copies of a person would scale linearly with resources with
no upper bound. Yet it is not clear how much the average human being
would value having multiple copies of herself. Even those people who
would prefer to be multiply instantiated may not have a utility function
that is linear with increasing number of copies. Copy numbers, like life
years, might have diminishing returns in the typical person’s utility
function.
44. A singleton is highly internally collaborative at the highest level of
decision-making. A singleton could have a lot of non-collaboration and
conflict at lower levels, if the higher-level agency that constitutes the
singleton chooses to have things that way.
45. If each rival AI team is convinced that the other teams are so misguided as
to have no chance of producing an intelligence explosion, then one reason
for collaboration—avoiding the race dynamic—is obviated: each team
should independently choose to go slower in the confident belief that it
lacks any serious competition.
46. A PhD student.
47. This formulation is intended to be read so as to include a prescription that
the well-being of nonhuman animals and other sentient beings (including
digital minds) that exist or may come to exist be given due consideration.
It is not meant to be read as a license for one AI developer to substitute his
or her own moral intuitions for those of the wider moral community. The
principle is consistent with the “coherent extrapolated volition” approach
discussed in Chapter 12, with an extrapolation base encompassing all
humans.A further clarification: The formulation is not intended to necessarily
exclude the possibly of post-transition property rights in artificial
superintelligences or their constituent algorithms and data structures. The
formulation is meant to be agnostic about what legal or political systems would
best serve to organize transactions within a hypothetical future posthuman
society. What the formulation is meant to assert is that the choice of such a
system, insofar as its selection is causally determined by how superintelligence
is initially developed, should to be made on the basis of the stated criterion;
that is, the post-transition constitutional system should be chosen for the
benefit of all of humanity and in the service of widely shared ethical ideals—as
opposed to, for instance, for the benefit merely of whoever happened to be the
first to develop superintelligence.
48. Refinements of the windfall clause are obviously possible. For example,
perhaps the threshold should be expressed in per capita terms, or maybe
the winner should be allowed to keep a somewhat larger than equal share
of the overshoot in order to more strongly incentivize further production
(some version of Rawls’s maximin principle might be attractive here).
Other refinements would refocus the clause away from dollar amounts and
restate it in terms of “influence on humanity’s future” or “degree to which
different parties’ interests are weighed in a future singleton’s utility
function” or some such.
CHAPTER 15: CRUNCH TIME
1. Some research is worthwhile not because of what it discovers but for other
reasons, such as by entertaining, educating, accrediting, or uplifting those
who engage in it.
2. I am not suggesting that nobody should work on pure mathematics or
philosophy. I am also not suggesting that these endeavors are especially
wasteful compared to all the other dissipations of academia or society at
large. It is probably very good that some people can devote themselves to the
life of the mind and follow their intellectual curiosity wherever it leads,
independent of any thought of utility or impact. The suggestion is that at the
margin, some of the best minds might, upon realizing that their cognitive
performances may become obsolete in the foreseeable future, want to shift
their attention to those theoretical problems for which it makes a difference
whether we get the solution a little sooner.
3. Though one should be cautious in cases where this uncertainty may be
protective—recall, for instance, the risk-race model in Box 13, where we
found that additional strategic information could be harmful. More generally,we need to worry about information hazards (see Bostrom [2011b]). It is
tempting to say that we need more analysis of information hazards. This is
probably true, although we might still worry that such analysis itself may
produce dangerous information.
4. Cf. Bostrom (2007).
5. I am grateful to Carl Shulman for emphasizing this point.BIBLIOGRAPHY
Acemoglu, Daron. 2003. “Labor- and Capital-Augmenting Technical Change.”
Journal of the European Economic Association 1 (1): 1–37.
Albertson, D. G., and Thomson, J. N. 1976. “The Pharynx of Caenorhabditis
Elegans.” Philosophical Transactions of the Royal Society B: Biological Sciences
275 (938): 299–325.
Allen, Robert C. 2008. “A Review of Gregory Clark’s A Farewell to Alms: A Brief
Economic History of the World.” Journal of Economic Literature 46 (4): 946–73.
American Horse Council. 2005. “National Economic Impact of the US Horse
Industry.”
Retrieved
July
30,
2013.
Available
at
http://www.horsecouncil.org/national-economic-impact-us-horse-industry.
Anand, Paul, Pattanaik, Prasanta, and Puppe, Clemens, eds. 2009. The Oxford
Handbook of Rational and Social Choice. New York: Oxford University Press.
Andres, B., Koethe, U., Kroeger, T., Helmstaedter, M., Briggman, K. L., Denk, W.,
and Hamprecht, F. A. 2012. “3D Segmentation of SBFSEM Images of Neuropil by
a Graphical Model over Supervoxel Boundaries.” Medical Image Analysis 16 (4):
796–805.
Armstrong, Alex. 2012. “Computer Competes in Crossword Tournament.” I
Programmer, March 19.
Armstrong, Stuart. 2007. “Chaining God: A Qualitative Approach to AI, Trust and
Moral Systems.” Unpublished manuscript, October 20. Retrieved December 31,
2012. Available at http://www.neweuropeancentury.org/GodAI.pdf.
Armstrong, Stuart. 2010. Utility Indifference, Technical Report 2010-1. Oxford:
Future of Humanity Institute, University of Oxford.
Armstrong, Stuart. 2013. “General Purpose Intelligence: Arguing the Orthogonality
Thesis.” Analysis and Metaphysics 12: 68–84.
Armstrong, Stuart, and Sandberg, Anders. 2013. “Eternity in Six Hours: Intergalactic
Spreading of Intelligent Life and Sharpening the Fermi Paradox.” Acta
Astronautica 89: 1–13.
Armstrong, Stuart, and Sotala, Kaj. 2012. “How We’re Predicting AI—or Failing
To.” In Beyond AI: Artificial Dreams, edited by Jan Romportl, Pavel Ircing, Eva
Zackova, Michal Polak, and Radek Schuster, 52–75. Pilsen: University of West
Bohemia. Retrieved February 2, 2013.
Asimov, Isaac. 1942. “Runaround.” Astounding Science-Fiction, March, 94–103.
Asimov, Isaac. 1985. Robots and Empire. New York: Doubleday.
Aumann, Robert J. 1976. “Agreeing to Disagree.” Annals of Statistics 4 (6): 1236–9.
Averch, Harvey Allen. 1985. A Strategic Analysis of Science and Technology Policy .Baltimore: Johns Hopkins University Press.
Azevedo, F. A. C., Carvalho, L. R. B., Grinberg, L. T., Farfel, J. M., Ferretti, R. E. L.,
Leite, R. E. P., Jacob, W., Lent, R., and Herculano-Houzel, S. 2009. “Equal
Numbers of Neuronal and Nonneuronal Cells Make the Human Brain an
Isometrically Scaled-up Primate Brain.” Journal of Comparative Neurology 513
(5): 532–41.
Baars, Bernard J. 1997. In the Theater of Consciousness: The Workspace of the
Mind. New York: Oxford University Press.
Baratta, Joseph Preston. 2004. The Politics of World Federation: United Nations, UN
Reform, Atomic Control. Westport, CT: Praeger.
Barber, E. J. W. 1991. Prehistoric Textiles: The Development of Cloth in the
Neolithic and Bronze Ages with Special Reference to the Aegean. Princeton, NJ:
Princeton University Press.
Bartels, J., Andreasen, D., Ehirim, P., Mao, H., Seibert, S., Wright, E. J., and
Kennedy, P. 2008. “Neurotrophic Electrode: Method of Assembly and
Implantation into Human Motor Speech Cortex.” Journal of Neuroscience
Methods 174 (2): 168–76.
Bartz, Jennifer A., Zaki, Jamil, Bolger, Niall, and Ochsner, Kevin N. 2011. “Social
Effects of Oxytocin in Humans: Context and Person Matter.” Trends in Cognitive
Science 15 (7): 301–9.
Basten, Stuart, Lutz, Wolfgang, and Scherbov, Sergei. 2013. “Very Long Range
Global Population Scenarios to 2300 and the Implications of Sustained Low
Fertility.” Demographic Research 28: 1145–66.
Baum, Eric B. 2004. What Is Thought? Bradford Books. Cambridge, MA: MIT Press.
Baum, Seth D., Goertzel, Ben, and Goertzel, Ted G. 2011. “How Long Until Human-
Level AI? Results from an Expert Assessment.” Technological Forecasting and
Social Change 78 (1): 185–95.
Beal, J., and Winston, P. 2009. “Guest Editors’ Introduction: The New Frontier of
Human-Level Artificial Intelligence.” IEEE Intelligent Systems 24 (4): 21–3.
Bell, C. Gordon, and Gemmell, Jim. 2009. Total Recall: How the E-Memory
Revolution Will Change Everything. New York: Dutton.
Benyamin, B., Pourcain, B. St., Davis, O. S., Davies, G., Hansell, M. K., Brion, M.-J.
A., Kirkpatrick, R. M., et al. 2013. “Childhood Intelligence is Heritable, Highly
Polygenic and Associated With FNBP1L.” Molecular Psychiatry (January 23).
Berg, Joyce E., and Rietz, Thomas A. 2003. “Prediction Markets as Decision Support
Systems.” Information Systems Frontiers 5 (1): 79–93.
Berger, Theodore W., Chapin, J. K., Gerhardt, G. A., Soussou, W. V., Taylor, D. M.,
and Tresco, P. A., eds. 2008. Brain–Computer Interfaces: An International
Assessment of Research and Development Trends. Springer.
Berger, T. W., Song, D., Chan, R. H., Marmarelis, V. Z., LaCoss, J., Wills, J.,
Hampson, R. E., Deadwyler, S. A., and Granacki, J. J. 2012. “A HippocampalCognitive Prosthesis: Multi-Input, Multi-Output Nonlinear Modeling and VLSI
Implementation.” IEEE Transactions on Neural Systems and Rehabilitation
Engineering 20 (2): 198–211.
Berliner, Hans J. 1980a. “Backgammon Computer-Program Beats World
Champion.” Artificial Intelligence 14 (2): 205–220.
Berliner, Hans J. 1980b. “Backgammon Program Beats World Champ.” SIGART
Newsletter 69: 6–9.
Bernardo, José M., and Smith, Adrian F. M. 1994. Bayesian Theory, 1st ed. Wiley
Series in Probability & Statistics. New York: Wiley.
Birbaumer, N., Murguialday, A. R., and Cohen, L. 2008. “Brain–Computer Interface
in Paralysis.” Current Opinion in Neurology 21 (6): 634–8.
Bird, Jon, and Layzell, Paul. 2002. “The Evolved Radio and Its Implications for
Modelling the Evolution of Novel Sensors.” In Proceedings of the 2002 Congress
on Evolutionary Computation, 2: 1836–41.
Blair, Clay, Jr. 1957. “Passing of a Great Mind: John von Neumann, a Brilliant,
Jovial Mathematician, was a Prodigious Servant of Science and His Country.”
Life, February 25, 89–104.
Bobrow, Daniel G. 1968. “Natural Language Input for a Computer Problem Solving
System.” In Semantic Information Processing, edited by Marvin Minsky, 146–
227. Cambridge, MA: MIT Press.
Bostrom, Nick. 1997. “Predictions from Philosophy? How Philosophers Could Make
Themselves Useful.” Unpublished manuscript. Last revised September 19, 1998.
Bostrom, Nick. 2002a. Anthropic Bias: Observation Selection Effects in Science and
Philosophy. New York: Routledge.
Bostrom, Nick. 2002b. “Existential Risks: Analyzing Human Extinction Scenarios
and Related Hazards.” Journal of Evolution and Technology 9.
Bostrom, Nick. 2003a. “Are We Living in a Computer Simulation?” Philosophical
Quarterly 53 (211): 243–55.
Bostrom, Nick. 2003b. “Astronomical Waste: The Opportunity Cost of Delayed
Technological Development.” Utilitas 15 (3): 308–314.
Bostrom, Nick. 2003c. “Ethical Issues in Advanced Artificial Intelligence.” In
Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in
Artificial Intelligence, edited by Iva Smit and George E. Lasker, 2: 12–17.
Windsor, ON: International Institute for Advanced Studies in Systems Research /
Cybernetics.
Bostrom, Nick. 2004. “The Future of Human Evolution.” In Two Hundred Years
After Kant, Fifty Years After Turing , edited by Charles Tandy, 2: 339–371. Death
and Anti-Death. Palo Alto, CA: Ria University Press.
Bostrom, Nick. 2006a. “How Long Before Superintelligence?” Linguistic and
Philosophical Investigations 5(1): 11–30.Bostrom, Nick. 2006b. “Quantity of Experience: Brain-Duplication and Degrees of
Consciousness.” Minds and Machines 16 (2): 185–200.
Bostrom, Nick. 2006c. “What is a Singleton?” Linguistic and Philosophical
Investigations 5 (2): 48–54.
Bostrom, Nick. 2007. “Technological Revolutions: Ethics and Policy in the Dark.” In
Nanoscale: Issues and Perspectives for the Nano Century, edited by Nigel M. de
S. Cameron and M. Ellen Mitchell, 129–52. Hoboken, NJ: Wiley.
Bostrom, Nick. 2008a. “Where Are They? Why I Hope the Search for
Extraterrestrial Life Finds Nothing.” MIT Technology Review, May/June issue,
72–7.
Bostrom, Nick. 2008b. “Why I Want to Be a Posthuman When I Grow Up.” In
Medical Enhancement and Posthumanity, edited by Bert Gordijn and Ruth
Chadwick, 107–37. New York: Springer.
Bostrom, Nick. 2008c. “Letter from Utopia.” Studies in Ethics, Law, and Technology
2 (1): 1–7.
Bostrom, Nick. 2009a. “Moral Uncertainty – Towards a Solution?” Overcoming Bias
(blog), January 1.
Bostrom, Nick. 2009b. “Pascal’s Mugging.” Analysis 69 (3): 443–5.
Bostrom, Nick. 2009c. “The Future of Humanity.” In New Waves in Philosophy of
Technology, edited by Jan Kyrre Berg Olsen, Evan Selinger, and Søren Riis, 186–
215. New York: Palgrave Macmillan.
Bostrom, Nick. 2011a. “Information Hazards: A Typology of Potential Harms from
Knowledge.” Review of Contemporary Philosophy 10: 44–79.
Bostrom, Nick. 2011b. “Infinite Ethics.” Analysis and Metaphysics 10: 9–59.
Bostrom, Nick. 2012. “The Superintelligent Will: Motivation and Instrumental
Rationality in Advanced Artificial Agents.” In “Theory and Philosophy of AI,”
edited by Vincent C. Müller, special issue, Minds and Machines 22 (2): 71–85.
Bostrom, Nick, and Ćirković, Milan M. 2003. “The Doomsday Argument and the
Self-Indication Assumption: Reply to Olum.” Philosophical Quarterly 53 (210):
83–91.
Bostrom, Nick, and Ord, Toby. 2006. “The Reversal Test: Eliminating the Status
Quo Bias in Applied Ethics.” Ethics 116 (4): 656–79.
Bostrom, Nick, and Roache, Rebecca. 2011. “Smart Policy: Cognitive Enhancement
and the Public Interest.” In Enhancing Human Capacities, edited by Julian
Savulescu, Ruud ter Meulen, and Guy Kahane, 138–49. Malden, MA: Wiley-
Blackwell.
Bostrom, Nick and Sandberg, Anders. 2009a. “Cognitive Enhancement: Methods,
Ethics, Regulatory Challenges.” Science and Engineering Ethics 15 (3): 311–41.
Bostrom, Nick and Sandberg, Anders. 2009b. “The Wisdom of Nature: An
Evolutionary Heuristic for Human Enhancement.” In Human Enhancement, 1st
ed., edited by Julian Savulescu and Nick Bostrom, 375–416. New York: OxfordUniversity Press.
Bostrom, Nick, Sandberg, Anders, and Douglas, Tom. 2013. “The Unilateralist’s
Curse: The Case for a Principle of Conformity.” Working Paper. Retrieved
February
28,
2013.
Available
at
http://www.nickbostrom.com/papers/unilateralist.pdf.
Bostrom, Nick, and Yudkowsky, Eliezer. Forthcoming. “The Ethics of Artificial
Intelligence.” In Cambridge Handbook of Artificial Intelligence, edited by Keith
Frankish and William Ramsey. New York: Cambridge University Press.
Boswell, James. 1917. Boswell’s Life of Johnson . New York: Oxford University
Press.
Bouchard, T. J. 2004. “Genetic Influence on Human Psychological Traits: A
Survey.” Current Directions in Psychological Science 13 (4): 148–51.
Bourget, David, and Chalmers, David. 2009. “The PhilPapers Surveys.” November.
Available at http://philpapers.org/surveys/.
Bradbury, Robert J. 1999. “Matrioshka Brains.” Archived version. As revised August
16,
2004.
Available
at
http://web.archive.org/web/20090615040912/http://www.aeiveos.com/~bradbury/Matrioshka
Brinton, Crane. 1965. The Anatomy of Revolution. Revised ed. New York: Vintage
Books.
Bryson, Arthur E., Jr., and Ho, Yu-Chi. 1969. Applied Optimal Control:
Optimization, Estimation, and Control. Waltham, MA: Blaisdell.
Buehler, Martin, Iagnemma, Karl, and Singh, Sanjiv, eds. 2009. The DARPA Urban
Challenge: Autonomous Vehicles in City Traffic . Springer Tracts in Advanced
Robotics 56. Berlin: Springer.
Burch-Brown, J. 2014. “Clues for Consequentialists.” Utilitas 26 (1): 105–19.
Burke, Colin. 2001. “Agnes Meyer Driscoll vs. the Enigma and the Bombe.”
Unpublished manuscript. Retrieved February 22, 2013. Available at
http://userpages.umbc.edu/~burke/driscoll1-2011.pdf.
Canbäck, S., Samouel, P., and Price, D. 2006. “Do Diseconomies of Scale Impact
Firm Size and Performance? A Theoretical and Empirical Overview.” Journal of
Managerial Economics 4 (1): 27–70.
Carmena, J. M., Lebedev, M. A., Crist, R. E., O’Doherty, J. E., Santucci, D. M.,
Dimitrov, D. F., Patil, P. G., Henriquez, C. S., and Nicolelis, M. A. 2003.
“Learning to Control a Brain–Machine Interface for Reaching and Grasping by
Primates.” Public Library of Science Biology 1 (2): 193–208.
Carroll, Bradley W., and Ostlie, Dale A. 2007. An Introduction to Modern
Astrophysics. 2nd ed. San Francisco: Pearson Addison Wesley.
Carroll, John B. 1993. Human Cognitive Abilities: A Survey of Factor-Analytic
Studies. New York: Cambridge University Press.
Carter, Brandon. 1983. “The Anthropic Principle and its Implications for BiologicalEvolution.” Philosophical Transactions of the Royal Society A: Mathematical,
Physical and Engineering Sciences 310 (1512): 347–63.
Carter, Brandon. 1993. “The Anthropic Selection Principle and the Ultra-Darwinian
Synthesis.” In The Anthropic Principle: Proceedings of the Second Venice
Conference on Cosmology and Philosophy, edited by F. Bertola and U. Curi, 33–
66. Cambridge: Cambridge University Press.
CFTC & SEC (Commodity Futures Trading Commission and Securities & Exchange
Commission). 2010. Findings Regarding the Market Events of May 6, 2010:
Report of the Staffs of the CFTC and SEC to the Joint Advisory Committee on
Emerging Regulatory Issues. Washington, DC.
Chalmers, David John. 2010. “The Singularity: A Philosophical Analysis.” Journal
of Consciousness Studies 17 (9–10): 7–65.
Chason, R. J., Csokmay, J., Segars, J. H., DeCherney, A. H., and Armant, D. R. 2011.
“Environmental and Epigenetic Effects Upon Preimplantation Embryo
Metabolism and Development.” Trends in Endocrinology and Metabolism 22
(10): 412–20.
Chen, S., and Ravallion, M. 2010. “The Developing World Is Poorer Than We
Thought, But No Less Successful in the Fight Against Poverty.” Quarterly Journal
of Economics 125 (4): 1577–1625.
Chislenko, Alexander. 1996. “Networking in the Mind Age: Some Thoughts on
Evolution of Robotics and Distributed Systems.” Unpublished manuscript.
Chislenko, Alexander. 1997. “Technology as Extension of Human Functional
Architecture.” Extropy Online.
Chorost, Michael. 2005. Rebuilt: How Becoming Part Computer Made Me More
Human. Boston: Houghton Mifflin.
Christiano, Paul F. 2012. “‘Indirect Normativity’ Write-up.” Ordinary Ideas (blog),
April 21.
CIA. 2013. “The World Factbook.” Central Intelligence Agency. Retrieved August 3.
Available
at https://www.cia.gov/library/publications/the-world-
factbook/rankorder/2127rank.html?
countryname=United%20States&countrycode=us&regionCode=noa&rank=121#us
Cicero. 1923. “On Divination.” In On Old Age, on Friendship, on Divination,
translated by W. A. Falconer. Loeb Classical Library. Cambridge, MA: Harvard
University Press.
Cirasella, Jill, and Kopec, Danny. 2006. “The History of Computer Games.” Exhibit
at Dartmouth Artificial Intelligence Conference: The Next Fifty Years (AI@50),
Dartmouth College, July 13–15.
Ćirković, Milan M. 2004. “Forecast for the Next Eon: Applied Cosmology and the
Long-Term Fate of Intelligent Beings.” Foundations of Physics 34 (2): 239–61.
Ćirković, Milan M., Sandberg, Anders, and Bostrom, Nick. 2010. “Anthropic
Shadow: Observation Selection Effects and Human Extinction Risks.” RiskAnalysis 30 (10): 1495–1506.
Clark, Andy, and Chalmers, David J. 1998. “The Extended Mind.” Analysis 58 (1):
7–19.
Clark, Gregory. 2007. A Farewell to Alms: A Brief Economic History of the World .
1st ed. Princeton, NJ: Princeton University Press.
Clavin, Whitney. 2012. “Study Shows Our Galaxy Has at Least 100 Billion Planets.”
Jet Propulsion Laboratory, January 11.
CME Group. 2010. What Happened on May 6th? Chicago, May 10.
Coase, R. H. 1937. “The Nature of the Firm.” Economica 4 (16): 386–405.
Cochran, Gregory, and Harpending, Henry. 2009. The 10,000 Year Explosion: How
Civilization Accelerated Human Evolution. New York: Basic Books.
Cochran, G., Hardy, J., and Harpending, H. 2006. “Natural History of Ashkenazi
Intelligence.” Journal of Biosocial Science 38 (5): 659–93.
Cook, James Gordon. 1984. Handbook of Textile Fibres: Natural Fibres. Cambridge:
Woodhead.
Cope, David. 1996. Experiments in Musical Intelligence. Computer Music and
Digital Audio Series. Madison, WI: A-R Editions.
Cotman, Carl W., and Berchtold, Nicole C. 2002. “Exercise: A Behavioral
Intervention to Enhance Brain Health and Plasticity.” Trends in Neurosciences 25
(6): 295–301.
Cowan, Nelson. 2001. “The Magical Number 4 in Short-Term Memory: A
Reconsideration of Mental Storage Capacity.” Behavioral and Brain Sciences 24
(1): 87–114.
Crabtree, Steve. 1999. “New Poll Gauges Americans’ General Knowledge Levels.”
Gallup News, July 6.
Cross, Stephen E., and Walker, Edward. 1994. “Dart: Applying Knowledge Based
Planning and Scheduling to Crisis Action Planning.” In Intelligent Scheduling,
edited by Monte Zweben and Mark Fox, 711–29. San Francisco, CA: Morgan
Kaufmann.
Crow, James F. 2000. “The Origins, Patterns and Implications of Human
Spontaneous Mutation.” Nature Reviews Genetics 1 (1): 40–7.
Cyranoski, David. 2013. “Stem Cells: Egg Engineers.” Nature 500 (7463): 392–4.
Dagnelie, Gislin. 2012. “Retinal Implants: Emergence of a Multidisciplinary Field.”
Current Opinion in Neurology 25 (1): 67–75.
Dai, Wei. 2009. “Towards a New Decision Theory.” Less Wrong (blog), August 13.
Dalrymple, David. 2011. “Comment on Kaufman, J. ‘Whole Brain Emulation:
Looking at Progress on C. Elegans.’” Less Wrong (blog), October 29.
Davies, G., Tenesa, A., Payton, A., Yang, J., Harris, S. E., Liewald, D., Ke, X., et al.
2011. “Genome-Wide Association Studies Establish That Human Intelligence Is
Highly Heritable and Polygenic.” Molecular Psychiatry 16 (10): 996–1005.Davis, Oliver S. P., Butcher, Lee M., Docherty, Sophia J., Meaburn, Emma L.,
Curtis, Charles J. C., Simpson, Michael A., Schalkwyk, Leonard C., and Plomin,
Robert. 2010. “A Three-Stage Genome-Wide Association Study of General
Cognitive Ability: Hunting the Small Effects.” Behavior Genetics 40 (6): 759–
767.
Dawkins, Richard. 1995. River Out of Eden: A Darwinian View of Life. Science
Masters Series. New York: Basic Books.
De Blanc, Peter. 2011. Ontological Crises in Artificial Agents’ Value Systems .
Machine Intelligence Research Institute, San Francisco, CA, May 19.
De Long, J. Bradford. 1998. “Estimates of World GDP, One Million B.C.–Present.”
Unpublished manuscript.
De Raedt, Luc, and Flach, Peter, eds. 2001. Machine Learning: ECML 2001: 12th
European Conference on Machine Learning, Freiburg, Germany, September 5–7,
2001. Proceedings. Lecture Notes in Computer Science 2167. New York:
Springer.
Dean, Cornelia. 2005. “Scientific Savvy? In U.S., Not Much.” New York Times ,
August 30.
Deary, Ian J. 2001. “Human Intelligence Differences: A Recent History.” Trends in
Cognitive Sciences 5 (3): 127–30.
Deary, Ian J. 2012. “Intelligence.” Annual Review of Psychology 63: 453–82.
Deary, Ian J., Penke, L., and Johnson, W. 2010. “The Neuroscience of Human
Intelligence Differences.” Nature Reviews Neuroscience 11 (3): 201–11.
Degnan, G. G., Wind, T. C., Jones, E. V., and Edlich, R. F. 2002. “Functional
Electrical Stimulation in Tetraplegic Patients to Restore Hand Function.” Journal
of Long-Term Effects of Medical Implants 12 (3): 175–88.
Devlin, B., Daniels, M., and Roeder, K. 1997. “The Heritability of IQ.” Nature 388
(6641): 468–71.
Dewey, Daniel. 2011. “Learning What to Value.” In Artificial General Intelligence:
4th International Conference, AGI 2011, Mountain View, CA, USA, August 3–6,
2011. Proceedings , edited by Jürgen Schmidhuber, Kristinn R. Thórisson, and
Moshe Looks, 309–14. Lecture Notes in Computer Science 6830. Berlin: Springer.
Dowe, D. L., and Hernández-Orallo, J. 2012. “IQ Tests Are Not for Machines, Yet.”
Intelligence 40 (2): 77–81.
Drescher, Gary L. 2006. Good and Real: Demystifying Paradoxes from Physics to
Ethics. Bradford Books. Cambridge, MA: MIT Press.
Drexler, K. Eric. 1986. Engines of Creation. Garden City, NY: Anchor.
Drexler, K. Eric. 1992. Nanosystems: Molecular Machinery, Manufacturing, and
Computation. New York: Wiley.
Drexler, K. Eric. 2013. Radical Abundance: How a Revolution in Nanotechnology
Will Change Civilization. New York: PublicAffairs.
Driscoll, Kevin. 2012. “Code Critique: ‘Altair Music of a Sort.’” Paper presented atCritical Code Studies Working Group Online Conference, 2012, February 6.
Dyson, Freeman J. 1960. “Search for Artificial Stellar Sources of Infrared
Radiation.” Science 131 (3414): 1667–1668.
Dyson, Freeman J. 1979. Disturbing the Universe. 1st ed. Sloan Foundation Science
Series. New York: Harper & Row.
Elga, Adam. 2004. “Defeating Dr. Evil with Self-Locating Belief.” Philosophy and
Phenomenological Research 69 (2): 383–96.
Elga, Adam. 2007. “Reflection and Disagreement.” Noûs 41 (3): 478–502.
Eliasmith, Chris, Stewart, Terrence C., Choo, Xuan, Bekolay, Trevor, DeWolf,
Travis, Tang, Yichuan, and Rasmussen, Daniel. 2012. “A Large-Scale Model of
the Functioning Brain.” Science 338(6111): 1202–5.
Ellis, J. H. 1999. “The History of Non-Secret Encryption.” Cryptologia 23 (3): 267–
73.
Elyasaf, Achiya, Hauptmann, Ami, and Sipper, Moche. 2011. “Ga-Freecell: Evolving
Solvers for the Game of Freecell.” In Proceedings of the 13th Annual Genetic and
Evolutionary Computation Conference, 1931–1938. GECCO’ 11. New York:
ACM.
Eppig, C., Fincher, C. L., and Thornhill, R. 2010. “Parasite Prevalence and the
Worldwide Distribution of Cognitive Ability.” Proceedings of the Royal Society
B: Biological Sciences 277 (1701): 3801–8.
Espenshade, T. J., Guzman, J. C., and Westoff, C. F. 2003. “The Surprising Global
Variation in Replacement Fertility.” Population Research and Policy Review 22
(5–6): 575–83.
Evans, Thomas G. 1964. “A Heuristic Program to Solve Geometric-Analogy
Problems.” In Proceedings of the April 21–23, 1964, Spring Joint Computer
Conference, 327–338. AFIPS ’64. New York: ACM.
Evans, Thomas G. 1968. “A Program for the Solution of a Class of Geometric-
Analogy Intelligence-Test Questions.” In Semantic Information Processing, edited
by Marvin Minsky, 271–353. Cambridge, MA: MIT Press.
Faisal, A. A., Selen, L. P., and Wolpert, D. M. 2008. “Noise in the Nervous System.”
Nature Reviews Neuroscience 9 (4): 292–303.
Faisal, A. A., White, J. A., and Laughlin, S. B. 2005. “Ion-Channel Noise Places
Limits on the Miniaturization of the Brain’s Wiring.” Current Biology 15 (12):
1143–9.
Feldman, Jacob. 2000. “Minimization of Boolean Complexity in Human Concept
Learning.” Nature 407 (6804): 630–3.
Feldman, J. A., and Ballard, Dana H. 1982. “Connectionist Models and Their
Properties.” Cognitive Science 6 (3): 205–254.
Foley, J. A., Monfreda, C., Ramankutty, N., and Zaks, D. 2007. “Our Share of the
Planetary Pie.” Proceedings of the National Academy of Sciences of the UnitedStates of America 104 (31): 12585–6.
Forgas, Joseph P., Cooper, Joel, and Crano, William D., eds. 2010. The Psychology
of Attitudes and Attitude Change. Sydney Symposium of Social Psychology. New
York: Psychology Press.
Frank, Robert H. 1999. Luxury Fever: Why Money Fails to Satisfy in an Era of
Excess. New York: Free Press.
Fredriksen, Kaja Bonesmo. 2012. Less Income Inequality and More Growth – Are
They Compatible?: Part 6. The Distribution of Wealth. Technical report, OECD
Economics Department Working Papers 929. OECD Publishing.
Freitas, Robert A., Jr. 1980. “A Self-Replicating Interstellar Probe.” Journal of the
British Interplanetary Society 33: 251–64.
Freitas, Robert A., Jr. 2000. “Some Limits to Global Ecophagy by Biovorous
Nanoreplicators, with Public Policy Recommendations.” Foresight Institute.
April.
Retrieved
July
28,
2013.
Available
at
http://www.foresight.org/nano/Ecophagy.html.
Freitas, Robert A., Jr., and Merkle, Ralph C. 2004. Kinematic Self-Replicating
Machines. Georgetown, TX: Landes Bioscience.
Gaddis, John Lewis. 1982. Strategies of Containment: A Critical Appraisal of
Postwar American National Security Policy. New York: Oxford University Press.
Gammoned.net. 2012. “Snowie.” Archived version. Retrieved June 30. Available at
http://web.archive.org/web/20070920191840/http://www.gammoned.com/snowie.html
Gates, Bill. 1975. “Software Contest Winners Announced.” Computer Notes 1 (2): 1.
Georgieff, Michael K. 2007. “Nutrition and the Developing Brain: Nutrient Priorities
and Measurement.” American Journal of Clinical Nutrition 85 (2): 614S–620S.
Gianaroli, Luca. 2000. “Preimplantation Genetic Diagnosis: Polar Body and Embryo
Biopsy.” Supplement, Human Reproduction 15 (4): 69–75.
Gilovich, Thomas, Griffin, Dale, and Kahneman, Daniel, eds. 2002. Heuristics and
Biases: The Psychology of Intuitive Judgment. New York: Cambridge University
Press.
Gilster, Paul. 2012. “ESO: Habitable Red Dwarf Planets Abundant.” Centauri
Dreams (blog), March 29.
Goldstone, Jack A. 1980. “Theories of Revolution: The Third Generation.” World
Politics 32 (3): 425–53.
Goldstone, Jack A. 2001. “Towards a Fourth Generation of Revolutionary Theory.”
Annual Review of Political Science 4: 139–87.
Good, Irving John. 1965. “Speculations Concerning the First Ultraintelligent
Machine.” In Advances in Computers, edited by Franz L. Alt and Morris Rubinoff,
6: 31–88. New York: Academic Press.
Good, Irving John. 1970. “Some Future Social Repercussions of Computers.”
International Journal of Environmental Studies 1 (1–4): 67–79.
Good, Irving John. 1976. “Book review of ‘The Thinking Computer: Mind InsideMatter’” In International Journal of Man-Machine Studies 8: 617–20.
Good, Irving John. 1982. “Ethical Machines.” In Intelligent Systems: Practice and
Perspective, edited by J. E. Hayes, Donald Michie, and Y.-H. Pao, 555–60.
Machine Intelligence 10. Chichester: Ellis Horwood.
Goodman, Nelson. 1954. Fact, Fiction, and Forecast. 1st ed. London: Athlone Press.
Gott, J. R., Juric, M., Schlegel, D., Hoyle, F., Vogeley, M., Tegmark, M., Bahcall, N.,
and Brinkmann, J. 2005. “A Map of the Universe.” Astrophysical Journal 624 (2):
463–83.
Gottfredson, Linda S. 2002. “G: Highly General and Highly Practical.” In The
General Factor of Intelligence: How General Is It?, edited by Robert J. Sternberg
and Elena L. Grigorenko, 331–80. Mahwah, NJ: Lawrence Erlbaum.
Gould, S. J. 1990. Wonderful Life: The Burgess Shale and the Nature of History .
New York: Norton.
Graham, Gordon. 1997. The Shape of the Past: A Philosophical Approach to History.
New York: Oxford University Press.
Gray, C. M., and McCormick, D. A. 1996. “Chattering Cells: Superficial Pyramidal
Neurons Contributing to the Generation of Synchronous Oscillations in the Visual
Cortex.” Science 274 (5284): 109–13.
Greene, Kate. 2012. “Intel’s Tiny Wi-Fi Chip Could Have a Big Impact.” MIT
Technology Review, September 21.
Guizzo, Erico. 2010. “World Robot Population Reaches 8.6 Million.” IEEE
Spectrum, April 14.
Gunn, James E. 1982. Isaac Asimov: The Foundations of Science Fiction. Science-
Fiction Writers. New York: Oxford University Press.
Haberl, Helmut, Erb, Karl-Heinz, and Krausmann, Fridolin. 2013. “Global Human
Appropriation of Net Primary Production (HANPP).” Encyclopedia of Earth,
September 3.
Haberl, H., Erb, K. H., Krausmann, F., Gaube, V., Bondeau, A., Plutzar, C., Gingrich,
S., Lucht, W., and Fischer-Kowalski, M. 2007. “Quantifying and Mapping the
Human Appropriation of Net Primary Production in Earth’s Terrestrial
Ecosystems.” Proceedings of the National Academy of Sciences of the United
States of America 104 (31): 12942–7.
Hájek, Alan. 2009. “Dutch Book Arguments.” In Anand, Pattanaik, and Puppe 2009,
173–95.
Hall, John Storrs. 2007. Beyond AI: Creating the Conscience of the Machine.
Amherst, NY: Prometheus Books.
Hampson, R. E., Song, D., Chan, R. H., Sweatt, A. J., Riley, M. R., Gerhardt, G. A.,
Shin, D. C., Marmarelis, V. Z., Berger, T. W., and Deadwyler, S. A. 2012. “A
Nonlinear Model for Hippocampal Cognitive Prosthesis: Memory Facilitation by
Hippocampal Ensemble Stimulation.” IEEE Transactions on Neural Systems andRehabilitation Engineering 20 (2): 184–97.
Hanson, Robin. 1994. “If Uploads Come First: The Crack of a Future Dawn.”
Extropy 6 (2).
Hanson, Robin. 1995. “Could Gambling Save Science? Encouraging an Honest
Consensus.” Social Epistemology 9 (1): 3–33.
Hanson, Robin. 1998a. “Burning the Cosmic Commons: Evolutionary Strategies for
Interstellar Colonization.” Unpublished manuscript, July 1. Retrieved April 26,
2012. http://hanson.gmu.edu/filluniv.pdf.
Hanson, Robin. 1998b. “Economic Growth Given Machine Intelligence.”
Unpublished manuscript. Retrieved May 15, 2013. Available at
http://hanson.gmu.edu/aigrow.pdf.
Hanson, Robin. 1998c. “Long-Term Growth as a Sequence of Exponential Modes.”
Unpublished manuscript. Last revised December 2000. Available at
http://hanson.gmu.edu/longgrow.pdf.
Hanson, Robin. 1998d. “Must Early Life Be Easy? The Rhythm of Major
Evolutionary Transitions.” Unpublished manuscript, September 23. Retrieved
August 12, 2012. Available at http://hanson.gmu.edu/hardstep.pdf.
Hanson, Robin. 2000. “Shall We Vote on Values, But Bet on Beliefs?” Unpublished
manuscript, September. Last revised October 2007. Available at
http://hanson.gmu.edu/futarchy.pdf.
Hanson, Robin. 2006. “Uncommon Priors Require Origin Disputes.” Theory and
Decision 61 (4): 319–328.
Hanson, Robin. 2008. “Economics of the Singularity.” IEEE Spectrum 45 (6): 45–50.
Hanson, Robin. 2009. “Tiptoe or Dash to Future?” Overcoming Bias (blog),
December 23.
Hanson, Robin. 2012. “Envisioning the Economy, and Society, of Whole Brain
Emulations.” Paper presented at the AGI Impacts conference 2012.
Hart, Oliver. 2008. “Economica Coase Lecture Reference Points and the Theory of
the Firm.” Economica 75 (299): 404–11.
Hay, Nicholas James. 2005. “Optimal Agents.” B.Sc. thesis, University of Auckland.
Hedberg, Sara Reese. 2002. “Dart: Revolutionizing Logistics Planning.” IEEE
Intelligent Systems 17 (3): 81–3.
Helliwell, John, Layard, Richard, and Sachs, Jeffrey. 2012. World Happiness Report .
The Earth Institute.
Helmstaedter, M., Briggman, K. L., and Denk, W. 2011. “High-Accuracy Neurite
Reconstruction for High-Throughput Neuroanatomy.” Nature Neuroscience 14
(8): 1081–8.
Heyl, Jeremy S. 2005. “The Long-Term Future of Space Travel.” Physical Review D
72 (10): 1–4.
Hibbard, Bill. 2011. “Measuring Agent Intelligence via Hierarchies of
Environments.” In Artificial General Intelligence: 4th International Conference,AGI 2011, Mountain View, CA, USA, August 3–6, 2011. Proceedings , edited by
Jürgen Schmidhuber, Kristinn R. Thórisson, and Moshe Looks, 303–8. Lecture
Notes in Computer Science 6830. Berlin: Springer.
Hinke, R. M., Hu, X., Stillman, A. E., Herkle, H., Salmi, R., and Ugurbil, K. 1993.
“Functional Magnetic Resonance Imaging of Broca’s Area During Internal
Speech.” Neuroreport 4 (6): 675–8.
Hinxton Group. 2008. Consensus Statement: Science, Ethics and Policy Challenges
of Pluripotent Stem Cell-Derived Gametes. Hinxton, Cambridgeshire, UK, April
11. Available at http://www.hinxtongroup.org/Consensus_HG08_FINAL.pdf.
Hoffman, David E. 2009. The Dead Hand: The Untold Story of the Cold War Arms
Race and Its Dangerous Legacy. New York: Doubleday.
Hofstadter, Douglas R. (1979) 1999. Goödel, Escher, Bach: An Eternal Golden
Braid. New York: Basic Books.
Holley, Rose. 2009. “How Good Can It Get? Analysing and Improving OCR
Accuracy in Large Scale Historic Newspaper Digitisation Programs.” D-Lib
Magazine 15 (3–4).
Horton, Sue, Alderman, Harold, and Rivera, Juan A. 2008. Copenhagen Consensus
2008 Challenge Paper: Hunger and Malnutrition. Technical report. Copenhagen
Consensus Center, May 11.
Howson, Colin, and Urbach, Peter. 1993. Scientific Reasoning: The Bayesian
Approach. 2nd ed. Chicago: Open Court.
Hsu, Stephen. 2012. “Investigating the Genetic Basis for Intelligence and Other
Quantitative Traits.” Lecture given at UC Davis Department of Physics
Colloquium, Davis, CA, February 13.
Huebner, Bryce. 2008. “Do You See What We See? An Investigation of an Argument
Against Collective Representation.” Philosophical Psychology 21 (1): 91–112.
Huff, C. D., Xing, J., Rogers, A. R., Witherspoon, D., and Jorde, L. B. 2010. “Mobile
Elements Reveal Small Population Size in the Ancient Ancestors of Homo
Sapiens.” Proceedings of the National Academy of Sciences of the United States of
America 107 (5): 2147–52.
Huffman, W. Cary, and Pless, Vera. 2003. Fundamentals of Error-Correcting Codes .
New York: Cambridge University Press.
Hunt, Patrick. 2011. “Late Roman Silk: Smuggling and Espionage in the 6th Century
CE.” Philolog, Stanford University (blog), August 2.
Hutter, Marcus. 2001. “Towards a Universal Theory of Artificial Intelligence Based
on Algorithmic Probability and Sequential Decisions.” In De Raedt and Flach
2001, 226–38.
Hutter, Marcus. 2005. Universal Artificial Intelligencet: Sequential Decisions Based
On Algorithmic Probability. Texts in Theoretical Computer Science. Berlin:
Springer.Iliadou, A. N., Janson, P. C., and Cnattingius, S. 2011. “Epigenetics and Assisted
Reproductive Technology.” Journal of Internal Medicine 270 (5): 414–20.
Isaksson, Anders. 2007. Productivity and Aggregate Growth: A Global Picture .
Technical report 05/2007. Vienna, Austria: UNIDO (United Nations Industrial
Development Organization) Research and Statistics Branch.
Jones, Garret. 2009. “Artificial Intelligence and Economic Growth: A Few Finger-
Exercises.” Unpublished manuscript, January. Retrieved November 5, 2012.
Available at http://mason.gmu.edu/~gjonesb/AIandGrowth.
Jones, Vincent C. 1985. Manhattan: The Army and the Atomic Bomb. United States
Army in World War II. Washington, DC: Center of Military History.
Joyce, James M. 1999. The Foundations of Causal Decision Theory. Cambridge
Studies in Probability, Induction and Decision Theory. New York: Cambridge
University Press.
Judd, K. L., Schmedders, K., and Yeltekin, S. 2012. “Optimal Rules for Patent
Races.” International Economic Review 53 (1): 23–52.
Kalfoglou, A., Suthers, K., Scott, J., and Hudson, K. 2004. Reproductive Genetic
Testing: What America Thinks. Genetics and Public Policy Center.
Kamm, Frances M. 2007. Intricate Ethics: Rights, Responsibilities, and Permissible
Harm. Oxford Ethics Series. New York: Oxford University Press.
Kandel, Eric R., Schwartz, James H., and Jessell, Thomas M., eds. 2000. Principles
of Neural Science. 4th ed. New York: McGraw-Hill.
Kansa, Eric. 2003. “Social Complexity and Flamboyant Display in Competition:
More Thoughts on the Fermi Paradox.” Unpublished manuscript, archived
version.
Karnofsky, Holden. 2012. “Comment on ‘Reply to Holden on Tool AI.’” Less Wrong
(blog), August 1.
Kasparov, Garry. 1996. “The Day That I Sensed a New Kind of Intelligence.” Time,
March 25, no. 13.
Kaufman, Jeff. 2011. “Whole Brain Emulation and Nematodes.” Jeff Kaufman’s
Blog (blog), November 2.
Keim, G. A., Shazeer, N. M., Littman, M. L., Agarwal, S., Cheves, C. M., Fitzgerald,
J., Grosland, J., Jiang, F., Pollard, S., and Weinmeister, K. 1999. “Proverb: The
Probabilistic Cruciverbalist.” In Proceedings of the Sixteenth National Conference
on Artificial Intelligence, 710–17. Menlo Park, CA: AAAI Press.
Kell, Harrison J., Lubinski, David, and Benbow, Camilla P. 2013. “Who Rises to the
Top? Early Indicators.” Psychological Science 24 (5): 648–59.
Keller, Wolfgang. 2004. “International Technology Diffusion.” Journal of Economic
Literature 42 (3): 752–82.
KGS Go Server. 2012. “KGS Game Archives: Games of KGS player zen19.”
Retrieved July 22, 2013. Available at http://www.gokgs.com/gameArchives.jsp?
user=zen19d&oldAccounts=t&year=2012&month=3.Knill, Emanuel, Laflamme, Raymond, and Viola, Lorenza. 2000. “Theory of
Quantum Error Correction for General Noise.” Physical Review Letters 84 (11):
2525–8.
Koch, K., McLean, J., Segev, R., Freed, M. A., Berry, M. J., Balasubramanian, V.,
and Sterling, P. 2006. “How Much the Eye Tells the Brain.” Current Biology 16
(14): 1428–34.
Kong, A., Frigge, M. L., Masson, G., Besenbacher, S., Sulem, P., Magnusson, G.,
Gudjonsson, S. A., Sigurdsson, A., et al. 2012. “Rate of De Novo Mutations and
the Importance of Father’s Age to Disease Risk.” Nature 488: 471–5.
Koomey, Jonathan G. 2011. Growth in Data Center Electricity Use 2005 to 2010.
Technical report, 08/01/2011. Oakland, CA: Analytics Press.
Koubi, Vally. 1999. “Military Technology Races.” International Organization 53
(3): 537–65.
Koubi, Vally, and Lalman, David. 2007. “Distribution of Power and Military R&D.”
Journal of Theoretical Politics 19 (2): 133–52.
Koza, J. R., Keane, M. A., Streeter, M. J., Mydlowec, W., Yu, J., and Lanza, G. 2003.
Genetic Programming IV: Routine Human-Competitive Machine Intelligence . 2nd
ed. Genetic Programming. Norwell, MA: Kluwer Academic.
Kremer, Michael. 1993. “Population Growth and Technological Change: One
Million B.C. to 1990.” Quarterly Journal of Economics 108 (3): 681–716.
Kruel, Alexander. 2011. “Interview Series on Risks from AI.” Less Wrong Wiki
(blog).
Retrieved
Oct
26,
2013.
Available
at
http://wiki.lesswrong.com/wiki/Interview_series_on_risks_from_AI.
Kruel, Alexander. 2012. “Q&A with Experts on Risks From AI #2.” Less Wrong
(blog), January 9.
Krusienski, D. J., and Shih, J. J. 2011. “Control of a Visual Keyboard Using an
Electrocorticographic Brain–Computer Interface.” Neurorehabilitation and
Neural Repair 25 (4): 323–31.
Kuhn, Thomas S. 1962. The Structure of Scientific Revolutions. 1st ed. Chicago:
University of Chicago Press.
Kuipers, Benjamin. 2012. “An Existing, Ecologically-Successful Genus of
Collectively Intelligent Artificial Creatures.” Paper presented at the 4th
International Conference, ICCCI 2012, Ho Chi Minh City, Vietnam, November
28–30.
Kurzweil, Ray. 2001. “Response to Stephen Hawking.” Kurzweil Accelerating
Intelligence. September 5. Retrieved December 31, 2012. Available at
http://www.kurzweilai.net/response-to-stephen-hawking.
Kurzweil, Ray. 2005. The Singularity Is Near: When Humans Transcend Biology.
New York: Viking.
Laffont, Jean-Jacques, and Martimort, David. 2002. The Theory of Incentives: ThePrincipal-Agent Model. Princeton, NJ: Princeton University Press.
Lancet, The. 2008. “Iodine Deficiency—Way to Go Yet.” The Lancet 372 (9633): 88.
Landauer, Thomas K. 1986. “How Much Do People Remember? Some Estimates of
the Quantity of Learned Information in Long-Term Memory.” Cognitive Science
10 (4): 477–93.
Lebedev, Anastasiya. 2004. “The Man Who Saved the World Finally Recognized.”
MosNews, May 21.
Lebedev, M. A., and Nicolelis, M. A. 2006. “Brain–Machine Interfaces: Past, Present
and Future.” Trends in Neuroscience 29 (9): 536–46.
Legg, Shane. 2008. “Machine Super Intelligence.” PhD diss., University of Lugano.
Leigh, E. G., Jr. 2010. “The Group Selection Controversy.” Journal of Evolutionary
Biology 23(1): 6–19.
Lenat, Douglas B. 1982. “Learning Program Helps Win National Fleet Wargame
Tournament.” SIGART Newsletter 79: 16–17.
Lenat, Douglas B. 1983. “EURISKO: A Program that Learns New Heuristics and
Domain Concepts.” Artificial Intelligence 21 (1–2): 61–98.
Lenman, James. 2000. “Consequentialism and Cluelessness.” Philosophy & Public
Affairs 29 (4): 342–70.
Lerner, Josh. 1997. “An Empirical Exploration of a Technology Race.” RAND
Journal of Economics 28 (2): 228–47.
Leslie, John. 1996. The End of the World: The Science and Ethics of Human
Extinction. London: Routledge.
Lewis, David. 1988. “Desire as Belief.” Mind: A Quarterly Review of Philosophy 97
(387): 323–32.
Li, Ming, and Vitaányi, Paul M. B. 2008. An Introduction to Kolmogorov Complexity
and Its Applications. Texts in Computer Science. New York: Springer.
Lin, Thomas, Mausam, and Etzioni, Oren. 2012. “Entity Linking at Web Scale.” In
Proceedings of the Joint Workshop on Automatic Knowledge Base Construction
and Web-scale Knowledge Extraction (AKBC-WEKEX ’12), edited by James Fan,
Raphael Hoffman, Aditya Kalyanpur, Sebastian Riedel, Fabian Suchanek, and
Partha Pratim Talukdar, 84–88. Madison, WI: Omnipress.
Lloyd, Seth. 2000. “Ultimate Physical Limits to Computation.” Nature 406 (6799):
1047–54.
Louis Harris & Associates. 1969. “Science, Sex, and Morality Survey, study no.
1927.” Life Magazine (New York) 4.
Lynch, Michael. 2010. “Rate, Molecular Spectrum, and Consequences of Human
Mutation.” Proceedings of the National Academy of Sciences of the United States
of America 107 (3): 961–8.
Lyons, Mark K. 2011. “Deep Brain Stimulation: Current and Future Clinical
Applications.” Mayo Clinic Proceedings 86 (7): 662–72.
MacAskill, William. 2010. “Moral Uncertainty and Intertheoretic Comparisons ofValue.” BPhil thesis, University of Oxford.
McCarthy, John. 2007. “From Here to Human-Level AI.” Artificial Intelligence 171
(18): 1174–82.
McCorduck, Pamela. 1979. Machines Who Think: A Personal Inquiry into the
History and Prospects of Artificial Intelligence. San Francisco: W. H. Freeman.
Mack, C. A. 2011. “Fifty Years of Moore’s Law.” IEEE Transactions on
Semiconductor Manufacturing 24 (2): 202–7.
MacKay, David J. C. 2003. Information Theory, Inference, and Learning Algorithms .
New York: Cambridge University Press.
McLean, George, and Stewart, Brian. 1979. “Norad False Alarm Causes Uproar.”
The National. Aired November 10. Ottawa, ON: CBC, 2012. News Broadcast.
Maddison, Angus. 1999. “Economic Progress: The Last Half Century in Historical
Perspective.” In Facts and Fancies of Human Development: Annual Symposium
and Cunningham Lecture, 1999, edited by Ian Castles. Occasional Paper Series,
1/2000. Academy of the Social Sciences in Australia.
Maddison, Angus. 2001. The World Economy: A Millennial Perspective .
Development Centre Studies. Paris: Development Centre of the Organisation for
Economic Co-operation / Development.
Maddison, Angus. 2005. Growth and Interaction in the World Economy: The Roots
of Modernity. Washington, DC: AEI Press.
Maddison, Angus. 2007. Contours of the World Economy, 1–2030 AD: Essays in
Macro-Economic History. New York: Oxford University Press.
Maddison, Angus. 2010. “Statistics of World Population, GDP and Per Capita GDP
1–2008
AD.”
Retrieved
October
26,
2013.
Available
at
http://www.ggdc.net/maddison/Historical_Statistics/vertical-file_02-2010.xls.
Mai, Q., Yu, Y., Li, T., Wang, L., Chen, M. J., Huang, S. Z., Zhou, C., and Zhou, Q.
2007. “Derivation of Human Embryonic Stem Cell Lines from Parthenogenetic
Blastocysts.” Cell Research 17 (12): 1008–19.
Mak, J. N., and Wolpaw, J. R. 2009. “Clinical Applications of Brain-Computer
Interfaces: Current State and Future Prospects.” IEEE Reviews in Biomedical
Engineering 2: 187–99.
Mankiw, N. Gregory. 2009. Macroeconomics. 7th ed. New York, NY: Worth.
Mardis, Elaine R. 2011. “A Decade’s Perspective on DNA Sequencing Technology.”
Nature 470 (7333): 198–203.
Markoff, John. 2011. “Computer Wins on ‘Jeopardy!’: Trivial, It’s Not.” New York
Times, February 16.
Markram, Henry. 2006. “The Blue Brain Project.” Nature Reviews Neuroscience 7
(2): 153–160.
Mason, Heather. 2003. “Gallup Brain: The Birth of In Vitro Fertilization.” Gallup,
August 5.Menzel, Randolf, and Giurfa, Martin. 2001. “Cognitive Architecture of a Mini-
Brain: The Honeybee.” Trends in Cognitive Sciences 5 (2): 62–71.
Metzinger, Thomas. 2003. Being No One: The Self-Model Theory of Subjectivity.
Cambridge, MA: MIT Press.
Mijic, Roko. 2010. “Bootstrapping Safe AGI Goal Systems.” Paper presented at the
Roadmaps to AGI and the Future of AGI Workshop, Lugano, Switzerland, March
8.
Mike, Mike. 2013. “Face of Tomorrow.” Retrieved June 30, 2012. Available at
http://faceoftomorrow.org.
Milgrom, Paul, and Roberts, John. 1990. “Bargaining Costs, Influence Costs, and the
Organization of Economic Activity.” In Perspectives on Positive Political
Economy, edited by James E. Alt and Kenneth A. Shepsle, 57–89. New York:
Cambridge University Press.
Miller, George A. 1956. “The Magical Number Seven, Plus or Minus Two: Some
Limits on Our Capacity for Processing Information.” Psychological Review 63
(2): 81–97.
Miller, Geoffrey. 2000. The Mating Mind: How Sexual Choice Shaped the Evolution
of Human Nature. New York: Doubleday.
Miller, James D. 2012. Singularity Rising: Surviving and Thriving in a Smarter,
Richer, and More Dangerous World. Dallas, TX: BenBella Books.
Minsky, Marvin. 1967. Computation: Finite and Infinite Machines. Englewood
Cliffs, NJ: Prentice-Hall.
Minsky, Marvin, ed. 1968. Semantic Information Processing. Cambridge, MA: MIT
Press.
Minsky, Marvin. 1984. “Afterword to Vernor Vinge’s novel, ‘True Names.’”
Unpublished manuscript, October 1. Retrieved December 31, 2012. Available at
http://web.media.mit.edu/~minsky/papers/TrueNames.Afterword.html.
Minsky, Marvin. 2006. The Emotion Machine: Commonsense Thinking, Artificial
Intelligence, and the Future of the Human Mind. New York: Simon & Schuster.
Minsky, Marvin, and Papert, Seymour. 1969. Perceptrons: An Introduction to
Computational Geometry. 1st ed. Cambridge, MA: MIT Press.
Moore, Andrew. 2011. “Hedonism.” In The Stanford Encyclopedia of Philosophy,
Winter 2011, edited by Edward N. Zalta. Stanford, CA: Stanford University.
Moravec, Hans P. 1976. “The Role of Raw Power in Intelligence.” Unpublished
manuscript, May 12. Retrieved August 12, 2012. Available at
http://www.frc.ri.cmu.edu/users/hpm/project.archive/general.articles/l975/Raw.Power.html
Moravec, Hans P. 1980. “Obstacle Avoidance and Navigation in the Real World by a
Seeing Robot Rover.” PhD diss., Stanford University.
Moravec, Hans P. 1988. Mind Children: The Future of Robot and Human
Intelligence. Cambridge, MA: Harvard University Press.
Moravec, Hans P. 1998. “When Will Computer Hardware Match the Human Brain?”Journal of Evolution and Technology 1.
Moravec, Hans P. 1999. “Rise of the Robots.” Scientific American, December, 124–
35.
Muehlhauser, Luke, and Helm, Louie. 2012. “The Singularity and Machine Ethics.”
I n Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by
Amnon Eden, Johnny Søraker, James H. Moor, and Eric Steinhart. The Frontiers
Collection. Berlin: Springer.
Muehlhauser, Luke, and Salamon, Anna. 2012. “Intelligence Explosion: Evidence
and Import.” In Singularity Hypotheses: A Scientific and Philosophical
Assessment, edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric
Steinhart. The Frontiers Collection. Berlin: Springer.
Müller, Vincent C., and Bostrom, Nick. Forthcoming. “Future Progress in Artificial
Intelligence: A Poll Among Experts.” In “Impacts and Risks of Artificial General
Intelligence,” edited by Vincent C. Müller, special issue, Journal of Experimental
and Theoretical Artificial Intelligence.
Murphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. Adaptive
Computation and Machine Learning. Cambridge, MA: MIT Press.
Nachman, Michael W., and Crowell, Susan L. 2000. “Estimate of the Mutation Rate
per Nucleotide in Humans.” Genetics 156 (1): 297–304.
Nagy, Z. P., and Chang, C. C. 2007. “Artificial Gametes.” Theriogenology 67 (1):
99–104.
Nagy, Z. P., Kerkis, I., and Chang, C. C. 2008. “Development of Artificial Gametes.”
Reproductive BioMedicine Online 16 (4): 539–44.
NASA. 2013. “International Space Station: Facts and Figures.” Available at
http://www.nasa.gov/worldbook/intspacestation_worldbook.html.
Newborn, Monty. 2011. Beyond Deep Blue: Chess in the Stratosphere. New York:
Springer.
Newell, Allen, Shaw, J. C., and Simon, Herbert A. 1958. “Chess-Playing Programs
and the Problem of Complexity.” IBM Journal of Research and Development 2
(4): 320–35.
Newell, Allen, Shaw, J. C., and Simon, Herbert A. 1959. “Report on a General
Problem-Solving Program: Proceedings of the International Conference on
Information Processing.” In Information Processing, 256–64. Paris: UNESCO.
Nicolelis, Miguel A. L., and Lebedev, Mikhail A. 2009. “Principles of Neural
Ensemble Physiology Underlying the Operation of Brain–Machine Interfaces.”
Nature Reviews Neuroscience 10 (7): 530–40.
Nilsson, Nils J. 1984. Shakey the Robot, Technical Note 323. Menlo Park, CA: AI
Center, SRI International, April.
Nilsson, Nils J. 2009. The Quest for Artificial Intelligence: A History of Ideas and
Achievements. New York: Cambridge University Press.Nisbett, R. E., Aronson, J., Blair, C., Dickens, W., Flynn, J., Halpern, D. F., and
Turkheimer, E. 2012. “Intelligence: New Findings and Theoretical
Developments.” American Psychologist 67 (2): 130–59.
Niven, Larry. 1973. “The Defenseless Dead.” In Ten Tomorrows , edited by Roger
Elwood, 91–142. New York: Fawcett.
Nordhaus, William D. 2007. “Two Centuries of Productivity Growth in Computing.”
Journal of Economic History 67 (1): 128–59.
Norton, John D. 2011. “Waiting for Landauer.” Studies in History and Philosophy of
Science Part B: Studies in History and Philosophy of Modern Physics 42 (3): 184–
98.
Olds, James, and Milner, Peter. 1954. “Positive Reinforcement Produced by
Electrical Stimulation of Septal Area and Other Regions of Rat Brain.” Journal of
Comparative and Physiological Psychology 47 (6): 419–27.
Olum, Ken D. 2002. “The Doomsday Argument and the Number of Possible
Observers.” Philosophical Quarterly 52 (207): 164–84.
Omohundro, Stephen M. 2007. “The Nature of Self-Improving Artificial
Intelligence.” Paper presented at Singularity Summit 2007, San Francisco, CA,
September 8–9.
Omohundro, Stephen M. 2008. “The Basic AI Drives.” In Artificial General
Intelligence 2008: Proceedings of the First AGI Conference , edited by Pei Wang,
Ben Goertzel, and Stan Franklin, 483–92. Frontiers in Artificial Intelligence and
Applications 171. Amsterdam: IOS.
Omohundro, Stephen M. 2012. “Rational Artificial Intelligence for the Greater
Good.” In Singularity Hypotheses: A Scientific and Philosophical Assessment,
edited by Amnon Eden, Johnny Søraker, James H. Moor, and Eric Steinhart. The
Frontiers Collection. Berlin: Springer.
O’Neill, Gerard K. 1974. “The Colonization of Space.” Physics Today 27 (9): 32–40.
Oshima, Hideki, and Katayama, Yoichi. 2010. “Neuroethics of Deep Brain
Stimulation for Mental Disorders: Brain Stimulation Reward in Humans.”
Neurologia medico-chirurgica 50 (9): 845–52.
Parfit, Derek. 1986. Reasons and Persons. New York: Oxford University Press.
Parfit, Derek. 2011. On What Matters. 2 vols. The Berkeley Tanner Lectures. New
York: Oxford University Press.
Parrington, Alan J. 1997. “Mutually Assured Destruction Revisited.” Airpower
Journal 11 (4).
Pasqualotto, Emanuele, Federici, Stefano, and Belardinelli, Marta Olivetti. 2012.
“Toward Functioning and Usable Brain–Computer Interfaces (BCIs): A Literature
Review.” Disability and Rehabilitation: Assistive Technology 7 (2): 89–103.
Pearl, Judea. 2009. Causality: Models, Reasoning, and Inference. 2nd ed. New York:
Cambridge University Press.
Perlmutter, J. S., and Mink, J. W. 2006. “Deep Brain Stimulation.” Annual Review ofNeuroscience 29: 229–57.
Pinker, Steven. 2011. The Better Angels of Our Nature: Why Violence Has Declined.
New York: Viking.
Plomin, R., Haworth, C. M., Meaburn, E. L., Price, T. S., Wellcome Trust Case
Control Consortium 2, and Davis, O. S. 2013. “Common DNA Markers Can
Account for More than Half of the Genetic Influence on Cognitive Abilities.”
Psychological Science 24 (2): 562–8.
Popper, Nathaniel. 2012. “Flood of Errant Trades Is a Black Eye for Wall Street.”
New York Times, August 1.
Pourret, Olivier, Naim, Patrick, and Marcot, Bruce, eds. 2008. Bayesian Networks: A
Practical Guide to Applications. Chichester, West Sussex, UK: Wiley.
Powell, A., Shennan, S., and Thomas, M. G. 2009. “Late Pleistocene Demography
and the Appearance of Modern Human Behavior.” Science 324 (5932): 1298–
1301.
Price, Huw. 1991. “Agency and Probabilistic Causality.” British Journal for the
Philosophy of Science 42 (2): 157–76.
Qian, M., Wang, D., Watkins, W. E., Gebski, V., Yan, Y. Q., Li, M., and Chen, Z. P.
2005. “The Effects of Iodine on Intelligence in Children: A Meta-Analysis of
Studies Conducted in China.” Asia Pacific Journal of Clinical Nutrition 14 (1):
32–42.
Quine, Willard Van Orman, and Ullian, Joseph Silbert. 1978. The Web of Belief , ed.
Richard Malin Ohmann, vol. 2. New York: Random House.
Railton, Peter. 1986. “Facts and Values.” Philosophical Topics 14 (2): 5–31.
Rajab, Moheeb Abu, Zarfoss, Jay, Monrose, Fabian, and Terzis, Andreas. 2006. “A
Multifaceted Approach to Understanding the Botnet Phenomenon.” In
Proceedings of the 6th ACM SIGCOMM Conference on Internet Measurement ,
41–52. New York: ACM.
Rawls, John. 1971. A Theory of Justice. Cambridge, MA: Belknap.
Read, J. I., and Trentham, Neil. 2005. “The Baryonic Mass Function of Galaxies.”
Philosophical Transactions of the Royal Society A: Mathematical, Physical and
Engineering Sciences 363 (1837): 2693–710.
Repantis, D., Schlattmann, P., Laisney, O., and Heuser, I. 2010. “Modafinil and
Methylphenidate for Neuroenhancement in Healthy Individuals: A Systematic
Review.” Pharmacological Research 62 (3): 187–206.
Rhodes, Richard. 1986. The Making of the Atomic Bomb. New York: Simon &
Schuster.
Rhodes, Richard. 2008. Arsenals of Folly: The Making of the Nuclear Arms Race.
New York: Vintage.
Rietveld, Cornelius A., Medland, Sarah E., Derringer, Jaime, Yang, Jian, Esko, Tonu,
Martin, Nicolas W., Westra, Harm-Jan, Shakhbazov, Konstantin, Abdellaoui,Abdel, et al. 2013. “GWAS of 126,559 Individuals Identifies Genetic Variants
Associated with Educational Attainment.” Science 340 (6139): 1467–71.
Ring, Mark, and Orseau, Laurent. 2011. “Delusion, Survival, and Intelligent
Agents.” In Artificial General Intelligence: 4th International Conference, AGI
2011, Mountain View, CA, USA, August 3–6, 2011 . Proceedings, edited by Jürgen
Schmidhuber, Kristinn R. Thórisson, and Moshe Looks, 11–20. Lecture Notes in
Computer Science 6830. Berlin: Springer.
Ritchie, Graeme, Manurung, Ruli, and Waller, Annalu. 2007. “A Practical
Application of Computational Humour.” In Proceedings of the 4th International
Joint Workshop on Computational Creativity , edited by Amilcar Cardoso and
Geraint A. Wiggins, 91–8. London: Goldsmiths, University of London.
Roache, Rebecca. 2008. “Ethics, Speculation, and Values.” NanoEthics 2 (3): 317–
27.
Robles, J. A., Lineweaver, C. H., Grether, D., Flynn, C., Egan, C. A., Pracy, M. B.,
Holmberg, J., and Gardner, E. 2008. “A Comprehensive Comparison of the Sun to
Other Stars: Searching for Self-Selection Effects.” Astrophysical Journal 684 (1):
691–706.
Roe, Anne. 1953. The Making of a Scientist. New York: Dodd, Mead.
Roy, Deb. 2012. “About.” Retrieved October 14. Available at
http://web.media.mit.edu/~dkroy/.
Rubin, Jonathan, and Watson, Ian. 2011. “Computer Poker: A Review.” Artificial
Intelligence 175 (5–6): 958–87.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. 1986. “Learning Representations
by Back-Propagating Errors.” Nature 323 (6088): 533–6.
Russell, Bertrand. 1986. “The Philosophy of Logical Atomism.” In The Philosophy
of Logical Atomism and Other Essays 1914–1919, edited by John G. Slater, 8:
157–244. The Collected Papers of Bertrand Russell. Boston: Allen & Unwin.
Russell, Bertrand, and Griffin, Nicholas. 2001. The Selected Letters of Bertrand
Russell: The Public Years, 1914–1970. New York: Routledge.
Russell, Stuart J., and Norvig, Peter. 2010. Artificial Intelligence: A Modern
Approach. 3rd ed. Upper Saddle River, NJ: Prentice-Hall.
Sabrosky, Curtis W. 1952. “How Many Insects Are There?” In Insects, edited by
United States Department of Agriculture, 1–7. Yearbook of Agriculture.
Washington, DC: United States Government Printing Office.
Salamon, Anna. 2009. “When Software Goes Mental: Why Artificial Minds Mean
Fast Endogenous Growth.” Working Paper, December 27.
Salem, D. J., and Rowan, A. N. 2001. The State of the Animals: 2001. Public Policy
Series. Washington, DC: Humane Society Press.
Salverda, W., Nolan, B., and Smeeding, T. M. 2009. The Oxford Handbook of
Economic Inequality. Oxford: Oxford University Press.
Samuel, A. L. 1959. “Some Studies in Machine Learning Using the Game ofCheckers.” IBM Journal of Research and Development 3 (3): 210–19.
Sandberg, Anders. 1999. “The Physics of Information Processing Superobjects:
Daily Life Among the Jupiter Brains.” Journal of Evolution and Technology 5.
Sandberg, Anders. 2010. “An Overview of Models of Technological Singularity.”
Paper presented at the Roadmaps to AGI and the Future of AGI Workshop,
Lugano, Switzerland, March 8.
Sandberg, Anders. 2013. “Feasibility of Whole Brain Emulation.” In Philosophy and
Theory of Artificial Intelligence, edited by Vincent C. Müller, 5: 251–64. Studies
in Applied Philosophy, Epistemology and Rational Ethics. New York: Springer.
Sandberg, Anders, and Bostrom, Nick. 2006. “Converging Cognitive
Enhancements.” Annals of the New York Academy of Sciences 1093: 201–27.
Sandberg, Anders, and Bostrom, Nick. 2008. Whole Brain Emulation: A Roadmap.
Technical Report 2008-3. Future of Humanity Institute, University of Oxford.
Sandberg, Anders, and Bostrom, Nick. 2011. Machine Intelligence Survey. Technical
Report 2011-1. Future of Humanity Institute, University of Oxford.
Sandberg, Anders, and Savulescu, Julian. 2011. “The Social and Economic Impacts
of Cognitive Enhancement.” In Enhancing Human Capacities, edited by Julian
Savulescu, Ruud ter Meulen, and Guy Kahane, 92–112. Malden, MA: Wiley-
Blackwell.
Schaeffer, Jonathan. 1997. One Jump Ahead: Challenging Human Supremacy in
Checkers. New York: Springer.
Schaeffer, J., Burch, N., Bjornsson, Y., Kishimoto, A., Muller, M., Lake, R., Lu, P.,
and Sutphen, S. 2007. “Checkers Is Solved.” Science 317 (5844): 1518–22.
Schalk, Gerwin. 2008. “Brain–Computer Symbiosis.” Journal of Neural Engineering
5 (1): P1–P15.
Schelling, Thomas C. 1980. The Strategy of Conflict. 2nd ed. Cambridge, MA:
Harvard University Press.
Schultz, T. R. 2000. “In Search of Ant Ancestors.” Proceedings of the National
Academy of Sciences of the United States of America 97 (26): 14028–9.
Schultz, W., Dayan, P., and Montague, P. R. 1997. “A Neural Substrate of Prediction
and Reward.” Science 275 (5306): 1593–9.
Schwartz, Jacob T. 1987. “Limits of Artificial Intelligence.” In Encyclopedia of
Artificial Intelligence, edited by Stuart C. Shapiro and David Eckroth, 1: 488–503.
New York: Wiley.
Schwitzgebel, Eric. 2013. “If Materialism is True, the United States is Probably
Conscious.” Working Paper, February 8.
Sen, Amartya, and Williams, Bernard, eds. 1982. Utilitarianism and Beyond. New
York: Cambridge University Press.
Shanahan, Murray. 2010. Embodiment and the Inner Life: Cognition and
Consciousness in the Space of Possible Minds. New York: Oxford UniversityPress.
Shannon, Robert V. 2012. “Advances in Auditory Prostheses.” Current Opinion in
Neurology 25 (1): 61–6.
Shapiro, Stuart C. 1992. “Artificial Intelligence.” In Encyclopedia of Artificial
Intelligence, 2nd ed., 1: 54–7. New York: Wiley.
Sheppard, Brian. 2002. “World-Championship-Caliber Scrabble.” Artificial
Intelligence 134 (1–2): 241–75.
Shoemaker, Sydney. 1969. “Time Without Change.” Journal of Philosophy 66 (12):
363–81.
Shulman, Carl. 2010a. Omohundro’s “Basic AI Drives” and Catastrophic Risks . San
Francisco, CA: Machine Intelligence Research Institute.
Shulman, Carl. 2010b. Whole Brain Emulation and the Evolution of Superorganisms .
San Francisco, CA: Machine Intelligence Research Institute.
Shulman, Carl. 2012. “Could We Use Untrustworthy Human Brain Emulations to
Make Trustworthy Ones?” Paper presented at the AGI Impacts conference 2012.
Shulman, Carl, and Bostrom, Nick. 2012. “How Hard is Artificial Intelligence?
Evolutionary Arguments and Selection Effects.” Journal of Consciousness Studies
19 (7–8): 103–30.
Shulman, Carl, and Bostrom, Nick. 2014. “Embryo Selection for Cognitive
Enhancement: Curiosity or Game-Changer?” Global Policy 5 (1): 85–92.
Shulman, Carl, Jonsson, Henrik, and Tarleton, Nick. 2009. “Which
Consequentialism? Machine Ethics and Moral Divergence.” In AP-CAP 2009: The
Fifth Asia-Pacific Computing and Philosophy Conference, October 1st-2nd,
University of Tokyo, Japan. Proceedings, edited by Carson Reynolds and Alvaro
Cassinelli, 23–25. AP-CAP 2009.
Sidgwick, Henry, and Jones, Emily Elizabeth Constance. 2010. The Methods of
Ethics. Charleston, SC: Nabu Press.
Silver, Albert. 2006. “How Strong Is GNU Backgammon?” Backgammon Galore!
September
16.
Retrieved
October
26,
2013.
Available
at
http://www.bkgm.com/gnu/AllAboutGNU.html#how_strong_is_gnu.
Simeral, J. D., Kim, S. P., Black, M. J., Donoghue, J. P., and Hochberg, L. R. 2011.
“Neural Control of Cursor Trajectory and Click by a Human with Tetraplegia
1000 Days after Implant of an Intracortical Microelectrode Array.” Journal of
Neural Engineering 8 (2): 025027.
Simester, Duncan, and Knez, Marc. 2002. “Direct and Indirect Bargaining Costs and
the Scope of the Firm.” Journal of Business 75 (2): 283–304.
Simon, Herbert Alexander. 1965. The Shape of Automation for Men and
Management. New York: Harper & Row.
Sinhababu, Neil. 2009. “The Humean Theory of Motivation Reformulated and
Defended.” Philosophical Review 118 (4): 465–500.
Slagle, James R. 1963. “A Heuristic Program That Solves Symbolic IntegrationProblems in Freshman Calculus.” Journal of the ACM 10 (4): 507–20.
Smeding, H. M., Speelman, J. D., Koning-Haanstra, M., Schuurman, P. R., Nijssen,
P., van Laar, T., and Schmand, B. 2006. “Neuropsychological Effects of Bilateral
STN Stimulation in Parkinson Disease: A Controlled Study.” Neurology 66 (12):
1830–6.
Smith, Michael. 1987. “The Humean Theory of Motivation.” Mind: A Quarterly
Review of Philosophy 96 (381): 36–61.
Smith, Michael, Lewis, David, and Johnston, Mark. 1989. “Dispositional Theories of
Value.” Proceedings of the Aristotelian Society 63: 89–174.
Sparrow, Robert. 2013. “In Vitro Eugenics.” Journal of Medical Ethics.
doi:10.1136/medethics-2012-101200. Published online April 4, 2013. Available at
http://jme.bmj.com/content/early/2013/02/13/medethics-2012-101200.full.
Stansberry, Matt, and Kudritzki, Julian. 2012. Uptime Institute 2012 Data Center
Industry Survey. Uptime Institute.
Stapledon, Olaf. 1937. Star Maker. London: Methuen.
Steriade, M., Timofeev, I., Durmuller, N., and Grenier, F. 1998. “Dynamic
Properties of Corticothalamic Neurons and Local Cortical Interneurons
Generating Fast Rhythmic (30–40 Hz) Spike Bursts.” Journal of Neurophysiology
79 (1): 483–90.
Stewart, P. W., Lonky, E., Reihman, J., Pagano, J., Gump, B. B., and Darvill, T.
2008. “The Relationship Between Prenatal PCB Exposure and Intelligence (IQ) in
9-Year-Old Children.” Environmental Health Perspectives 116 (10): 1416–22.
Sun, W., Yu, H., Shen, Y., Banno, Y., Xiang, Z., and Zhang, Z. 2012. “Phylogeny and
Evolutionary History of the Silkworm.” Science China Life Sciences 55 (6): 483–
96.
Sundet, J., Barlaug, D., and Torjussen, T. 2004. “The End of the Flynn Effect? A
Study of Secular Trends in Mean Intelligence Scores of Norwegian Conscripts
During Half a Century.” Intelligence 32 (4): 349–62.
Sutton, Richard S., and Barto, Andrew G. 1998. Reinforcement Learning: An
Introduction. Adaptive Computation and Machine Learning. Cambridge, MA:
MIT Press.
Talukdar, D., Sudhir, K., and Ainslie, A. 2002. “Investigating New Product Diffusion
Across Products and Countries.” Marketing Science 21 (1): 97–114.
Teasdale, Thomas W., and Owen, David R. 2008. “Secular Declines in Cognitive
Test Scores: A Reversal of the Flynn Effect.” Intelligence 36 (2): 121–6.
Tegmark, Max, and Bostrom, Nick. 2005. “Is a Doomsday Catastrophe Likely?”
Nature 438: 754.
Teitelman, Warren. 1966. “Pilot: A Step Towards Man–Computer Symbiosis.” PhD
diss., Massachusetts Institute of Technology.
Temple, Robert K. G. 1986. The Genius of China: 3000 Years of Science, Discovery,and Invention. 1st ed. New York: Simon & Schuster.
Tesauro, Gerald. 1995. “Temporal Difference Learning and TD-Gammon.”
Communications of the ACM 38 (3): 58–68.
Tetlock, Philip E. 2005. Expert Political Judgment: How Good is it? How Can We
Know? Princeton, NJ: Princeton University Press.
Tetlock, Philip E., and Belkin, Aaron. 1996. “Counterfactual Thought Experiments
in World Politics: Logical, Methodological, and Psychological Perspectives.” In
Counterfactual Thought Experiments in World Politics: Logical, Methodological,
and Psychological Perspectives, edited by Philip E. Tetlock and Aaron Belkin, 1–
38. Princeton, NJ: Princeton University Press.
Thompson, Adrian. 1997. “Artificial Evolution in the Physical World.” In
Evolutionary Robotics: From Intelligent Robots to Artificial Life, edited by
Takashi Gomi, 101–25. Er ’97. Carp, ON: Applied AI Systems.
Thrun, S., Montemerlo, M., Dahlkamp, H., Stavens, D., Aron, A., Diebel, J., Fong,
P., et al. 2006. “Stanley: The Robot That Won the DARPA Grand Challenge.”
Journal of Field Robotics 23 (9): 661–92.
Trachtenberg, J. T., Chen, B. E., Knott, G. W., Feng, G., Sanes, J. R., Welker, E., and
Svoboda, K. 2002. “Long-Term In Vivo Imaging of Experience-Dependent
Synaptic Plasticity in Adult Cortex.” Nature 420 (6917): 788–94.
Traub, Wesley A. 2012. “Terrestrial, Habitable-Zone Exoplanet Frequency from
Kepler.” Astrophysical Journal 745 (1): 1–10.
Truman, James W., Taylor, Barbara J., and Awad, Timothy A. 1993. “Formation of
the Adult Nervous System.” In The Development of Drosophila Melanogaster,
edited by Michael Bate and Alfonso Martinez Arias. Plainview, NY: Cold Spring
Harbor Laboratory.
Tuomi, Ilkka. 2002. “The Lives and the Death of Moore’s Law.” First Monday 7
(11).
Turing, A. M. 1950. “Computing Machinery and Intelligence.” Mind 59 (236): 433–
60.
Turkheimer, Eric, Haley, Andreana, Waldron, Mary, D’Onofrio, Brian, and
Gottesman, Irving I. 2003. “Socioeconomic Status Modifies Heritability of IQ in
Young Children.” Psychological Science 14 (6): 623–8.
Uauy, Ricardo, and Dangour, Alan D. 2006. “Nutrition in Brain Development and
Aging: Role of Essential Fatty Acids.” Supplement, Nutrition Reviews 64 (5):
S24–S33.
Ulam, Stanislaw M. 1958. “John von Neumann.” Bulletin of the American
Mathematical Society 64 (3): 1–49.
Uncertain Future, The. 2012. “Frequently Asked Questions” The Uncertain Future.
Retrieved
March
25,
2012.
Available
at
http://www.theuncertainfuture.com/faq.html.
U.S. Congress, Office of Technology Assessment. 1995. U.S.–Russian Cooperationin Space ISS-618. Washington, DC: U.S. Government Printing Office, April.
Van Zanden, Jan Luiten. 2003. On Global Economic History: A Personal View on an
Agenda for Future Research. International Institute for Social History, July 23.
Vardi, Moshe Y. 2012. “Artificial Intelligence: Past and Future.” Communications of
the ACM 55 (1): 5.
Vassar, Michael, and Freitas, Robert A., Jr. 2006. “Lifeboat Foundation Nanoshield.”
Lifeboat
Foundation.
Retrieved
May
12,
2012.
Available
at
http://lifeboat.com/ex/nanoshield.
Vinge, Vernor. 1993. “The Coming Technological Singularity: How to Survive in the
Post-Human Era.” In Vision-21: Interdisciplinary Science and Engineering in the
Era of Cyberspace, 11–22. NASA Conference Publication 10129. NASA Lewis
Research Center.
Visscher, P. M., Hill, W. G., and Wray, N. R. 2008. “Heritability in the Genomics
Era: Concepts and Misconceptions.” Nature Reviews Genetics 9 (4): 255–66.
Vollenweider, Franz, Gamma, Alex, Liechti, Matthias, and Huber, Theo. 1998.
“Psychological and Cardiovascular Effects and Short-Term Sequelae of MDMA
(‘Ecstasy’) in MDMA-Naïve Healthy Volunteers.” Neuropsychopharmachology
19 (4): 241–51.
Wade, Michael J. 1976. “Group Selections Among Laboratory Populations of
Tribolium.” Proceedings of the National Academy of Sciences of the United States
of America 73 (12): 4604–7.
Wainwright, Martin J., and Jordan, Michael I. 2008. “Graphical Models, Exponential
Families, and Variational Inference.” Foundations and Trends in Machine
Learning 1 (1–2): 1–305.
Walker, Mark. 2002. “Prolegomena to Any Future Philosophy.” Journal of Evolution
and Technology 10 (1).
Walsh, Nick Paton. 2001. “Alter our DNA or robots will take over, warns Hawking.”
The
Observer,
September
1.
http://www.theguardian.com/uk/2001/sep/02/medicalscience.genetics.
Warwick, Kevin. 2002. I, Cyborg. London: Century.
Wehner, M., Oliker, L., and Shalf, J. 2008. “Towards Ultra-High Resolution Models
of Climate and Weather.” International Journal of High Performance Computing
Applications 22 (2): 149–65.
Weizenbaum, Joseph. 1966. “Eliza: A Computer Program for the Study of Natural
Language Communication Between Man And Machine.” Communications of the
ACM 9 (1): 36–45.
Weizenbaum, Joseph. 1976. Computer Power and Human Reason: From Judgment to
Calculation. San FrancYork, CA: W. H. Freeman.
Werbos, Paul John. 1994. The Roots of Backpropagation: From Ordered Derivatives
to Neural Networks and Political Forecasting. New York: Wiley.White, J. G., Southgate, E., Thomson, J. N., and Brenner, S. 1986. “The Structure of
the Nervous System of the Nematode Caenorhabditis Elegans. ” Philosophical
Transactions of the Royal Society of London. Series B, Biological Sciences 314
(1165): 1–340.
Whitehead, Hal. 2003. Sperm Whales: Social Evolution in the Ocean. Chicago:
University of Chicago Press.
Whitman, William B., Coleman, David C., and Wiebe, William J. 1998.
“Prokaryotes: The Unseen Majority.” Proceedings of the National Academy of
Sciences of the United States of America 95 (12): 6578–83.
Wiener, Norbert. 1960. “Some Moral and Technical Consequences of Automation.”
Science 131 (3410): 1355–8.
Wikipedia. 2012a, s.v. “Computer Bridge.” Retrieved June 30, 2013. Available at
http://en.wikipedia.org/wiki/Computer_bridge.
Wikipedia. 2012b, s.v. “Supercomputer.” Retrieved June 30, 2013. Available at
http://et.wikipedia.org/wiki/Superarvuti.
Williams, George C. 1966. Adaptation and Natural Selection: A Critique of Some
Current Evolutionary Thought. Princeton Science Library. Princeton, NJ:
Princeton University Press.
Winograd, Terry. 1972. Understanding Natural Language. New York: Academic
Press.
Wood, Nigel. 2007. Chinese Glazes: Their Origins, Chemistry and Re-creation.
London: A. & C. Black.
World Bank. 2008. Global Economic Prospects: Technology Diffusion in the
Developing World 42097. Washington, DC.
World Robotics. 2011. Executive Summary of 1. World Robotics 2011 Industrial
Robots; 2. World Robotics 2011 Service Robots . Retrieved June 30, 2012.
Available
at http://www.bara.org.uk/pdf/2012/world-
robotics/Executive_Summary_WR_2012.pdf.
World Values Survey. 2008. WVS 2005-2008. Retrieved 29 October, 2013. Available
at http://www.wvsevsdb.com/wvs/WVSAnalizeStudy.jsp.
Wright, Robert. 2001. Nonzero: The Logic of Human Destiny. New York: Vintage.
Yaeger, Larry. 1994. “Computational Genetics, Physiology, Metabolism, Neural
Systems, Learning, Vision, and Behavior or PolyWorld: Life in a New Context.”
In Proceedings of the Artificial Life III Conference , edited by C. G. Langton, 263–
98. Santa Fe Institute Studies in the Sciences of Complexity. Reading, MA:
Addison-Wesley.
Yudkowsky, Eliezer. 2001. Creating Friendly AI 1.0: The Analysis and Design of
Benevolent Goal Architectures . Machine Intelligence Research Institute, San
Francisco, CA, June 15.
Yudkowsky, Eliezer. 2002. “The AI-Box Experiment.” Retrieved January 15, 2012.
Available at http://yudkowsky.net/singularity/aibox.Yudkowsky, Eliezer. 2004. Coherent Extrapolated Volition . Machine Intelligence
Research Institute, San Francisco, CA, May.
Yudkowsky, Eliezer. 2007. “Levels of Organization in General Intelligence.” In
Artificial General Intelligence, edited by Ben Goertzel and Cassio Pennachin,
389–501. Cognitive Technologies. Berlin: Springer.
Yudkowsky, Eliezer. 2008a. “Artificial Intelligence as a Positive and Negative
Factor in Global Risk.” In Global Catastrophic Risks, edited by Nick Bostrom and
Milan M. Ćirković, 308–45. New York: Oxford University Press.
Yudkowsky, Eliezer. 2008b. “Sustained Strong Recursion.” Less Wrong (blog),
December 5.
Yudkowsky, Eliezer. 2010. Timeless Decision Theory. Machine Intelligence
Research Institute, San Francisco, CA.
Yudkowsky, Eliezer. 2011. Complex Value Systems are Required to Realize Valuable
Futures. San Francisco, CA: Machine Intelligence Research Institute.
Yudkowsky, Eliezer. 2013. Intelligence Explosion Microeconomics, Technical
Report 2013–1. Berkeley, CA: Machine Intelligence Research Institute.
Zahavi, Amotz, and Zahavi, Avishag. 1997. The Handicap Principle: A Missing
Piece of Darwin’s Puzzle . Translated by N. Zahavi-Ely and M. P. Ely. New York:
Oxford University Press.
Zalasiewicz, J., Williams, M., Smith, A., Barry, T. L., Coe, A. L., Bown, P. R.,
Brenchley, P., et al. 2008. “Are We Now Living in the Anthropocene?” GSA Today
18 (2): 4–8.
Zeira, Joseph. 2011. “Innovations, Patent Races and Endogenous Growth.” Journal
of Economic Growth 16 (2): 135–56.
Zuleta, Hernando. 2008. “An Empirical Note on Factor Shares.” Journal of
International Trade and Economic Development 17 (3): 379–90.INDEX
A
Afghan Taliban 215
Agricultural Revolution 2, 80, 261
AI-complete problem 14, 47, 71, 93, 145, 186
AI-OUM, see optimality notions
AI-RL, see optimality notions
AI-VL, see optimality notions
algorithmic soup 172
algorithmic trading 16–17
anthropics 27–28, 126, 134–135, 174, 222–225
definition 225
Arendt, Hannah 105
Armstrong, Stuart 280, 291, 294, 302
artificial agent 10, 88, 105–109, 172–176, 185–206; see also Bayesian agent
artificial intelligence
arms race 64, 88, 247
future of 19, 292
greater-than-human, see superintelligence
history of 5–18
overprediction of 4
pioneers 4–5, 18
Asimov, Isaac 139
augmentation 142–143, 201–203
autism 57
automata theory 5
automatic circuit breaker 17
automation 17, 98, 117, 160–176
B
backgammon 12
backpropagation algorithm 8
bargaining costs 182
Bayesian agent 9–11, 123, 130; see also artificial agent and optimality notionsBayesian networks 9
Berliner, Hans 12
biological cognition 22, 36–48, 50–51, 232
biological
enhancement 36–48, 50–51, 142–143, 232; see also cognitive
enhancement
boxing 129–131, 143, 156–157
informational 130
physical 129–130
brain implant, see cyborg
brain plasticity 48
brain–computer interfaces 44–48, 51, 83, 142–143; see also cyborg
Brown, Louise 43
C
C. elegans34–35, 266, 267
capability control 129–144, 156–157
capital 39, 48, 68, 84–88, 99, 113–114, 159–184, 251, 287, 288, 289
causal validity semantics 197
CEV, see coherent extrapolated volition
Chalmers, David 24, 265, 283, 295, 302
character recognition 15
checkers 12
chess 11–22, 52, 93, 134, 263, 264
child machine 23, 29; see also seed AI
CHINOOK 12
Christiano, Paul 198, 207
civilization baseline 63
cloning 42
cognitive enhancement 42–51, 67, 94, 111–112, 193, 204, 232–238, 244, 259
coherent extrapolated volition (CEV) 198, 211–227, 296, 298, 303
definition 211
collaboration (benefits of) 249
collective intelligence 48–51, 52–57, 67, 72, 142, 163, 203, 259, 271, 273, 279
collective superintelligence 39, 48–49, 52–59, 83, 93, 99, 285
definition 54
combinatorial explosion 6, 9, 10, 47, 155
Common Good Principle 254–259
common sense 14
computer vision 9
computing power 7–9, 24, 25–35, 47, 53–60, 68–77, 101, 134, 155, 198, 240–244,251, 286, 288; see also computronium and hardware overhang
computronium 101, 123–124, 140, 193, 219; see also computing power
connectionism 8
consciousness 22, 106, 126, 139, 173–176, 216, 226, 271, 282, 288, 292, 303; see
also mind crime
control methods 127–144, 145–158, 202, 236–238, 286; see also capability control
and motivation selection
Copernicus, Nicolaus 14
cosmic endowment 101–104, 115, 134, 209, 214–217, 227, 250, 260, 283, 296
crosswords (solving) 12
cryptographic reward tokens 134, 276
cryptography 80
cyborg 44–48, 67, 270
D
DARPA, see Defense Advanced Research Projects Agency
DART (tool) 15
Dartmouth Summer Project 5
data mining 15–16, 232, 301
decision support systems 15, 98; see also tool-AI
decision theory 10–11, 88, 185–186, 221–227, 280, 298; see also optimality notions
decisive strategic advantage 78–89, 95, 104–112, 115–126, 129–138, 148–149,
156–159, 177, 190, 209–214, 225, 252
Deep Blue 12
Deep Fritz 22
Defense Advanced Research Projects Agency (DARPA) 15
design effort, see optimization power
Dewey, Daniel 291
Differential Technological Development (Principle of) 230–237
Diffie–Hellman key exchange protocol 80
diminishing returns 37–38, 66, 88, 114, 273, 303
direct reach 58
direct specification 139–143
DNA synthesis 39, 98
Do What I Mean (DWIM) 220–221
domesticity 140–143, 146–156, 187, 191, 207, 222
Drexler, Eric 239, 270, 276, 278, 300
drones 15, 98
Dutch book 111
Dyson, Freeman 101, 278E
economic growth 3, 160–166, 179, 261, 274, 299
Einstein, Albert 56, 70, 85
ELIZA (program) 6
embryo selection 36–44, 67, 268
emulation modulation 207
Enigma code 87
environment of evolutionary adaptedness 164, 171
epistemology 222–224
equation solvers 15
eugenics 36–44, 268, 279
Eurisko 12
evolution 8–9, 23–27, 44, 154, 173–176, 187, 198, 207, 265, 266, 267, 273
evolutionary selection 187, 207, 290
evolvable hardware 154
exhaustive search 6
existential risk 4, 21, 55, 100–104, 115–126, 175, 183, 230–236, 239–254, 256–259,
286, 301–302
state risks 233–234
step risks 233
expert system 7
explicit representation 207
exponential growth, see growth
external reference semantics 197
F
face recognition 15
failure modes 117–120
Faraday cage 130
Fields Medal 255–256, 272
Fifth-Generation Computer Systems Project 7
fitness function 25; see also evolution
Flash Crash (2010) 16–17
formal language 7, 145
FreeCell (game) 13
G
game theory 87, 159game-playing AI 12–14
General Problem Solver 6
genetic algorithms 7–13, 24–27, 237–240; see also evolution
genetic selection 37–50, 61, 232–238; see also evolution
genie AI 148–158, 285
definition 148
genotyping 37
germline interventions 37–44, 67, 273; see also embryo selection
Ginsberg, Matt 12
Go (game) 13
goal-content 109–110, 146, 207, 222–227
Good Old-Fashioned Artificial Intelligence (GOFAI) 7–15, 23
Good, I. J. 4
Gorbachev, Mikhail 86
graceful degradation 7
graphical models 11
growth 1–7, 48–55, 69–75, 83, 163, 261, 281
H
Hail Mary approach 198–200, 207, 293, 294
Hanson, Robin 2, 160, 261, 270, 271, 287
hardware overhang 73, 240–243, 274, 289, 301, 302
hedonism 140, 210
hedonium 140, 219
Helsinki Declaration 188
heuristic search 6
Hill, Benny 105
hippocampus 47
HLMI, see machine intelligence, human-level
Hodgkin–Huxley model 25
human baseline 62–77, 82
human extinction, see existential risk
Human Genome Project 86, 253, 276
human intelligence 5–14, 24–58, 98–99, 159–184, 242–254, 255–257
human–machine interface, see cyborg
I
impersonal perspective 228–246
in vitro fertilization, see embryo selectionincentive methods 131–143
cryptographic reward tokens 133
social integration 131–132, 156–158, 159, 202, 283
indirect normativity 141–150, 209–227, 262, 298
indirect reach 58
inductive bias 9–10
Industrial Revolution 2, 80, 161–163
infrastructure profusion 123–125, 153, 187, 226, 282
institution design 202–208
instrumental convergence thesis 105–116
intelligence explosion 2–5, 22–51, 62–77, 78–90, 95–104, 108, 115–126, 127–128,
136, 151, 160, 165, 178, 198, 205, 227, 228–254, 256–260, 274, 276, 282, 284,
289, 300, 301–302
Internet 16, 45–49, 67–77, 85, 94–98, 130, 146, 241, 271
inventory control systems 16
IVF, see embryo selection
J
Jeopardy!13
K
Kasparov, Garry 12
Kepler, Johannes 14
Knuth, Donald 14, 264
Kurzweil, Ray 2, 261, 269
L
Lenat, Douglas 12, 263
Logic Theorist (system) 6
logicist paradigm, see Good Old-Fashioned Artificial Intelligence (GOFAI)
Logistello 12
M
machine intelligence; see also artificial intelligence
human-level (HLMI) 4, 19–21, 27–35, 73–74, 207, 243, 264, 267
revolution, see intelligence explosion
machine learning 8–18, 28, 121, 152, 188, 274, 290
machine translation 15macro-structural development accelerator 233–235
malignant failure 123–126, 149, 196
Malthusian condition 163–165, 252
Manhattan Project 75, 80–87, 276
McCarthy, John 5–18
McCulloch–Pitts neuron 237
MegaEarth 56
memory capacity 7–9, 60, 71
memory sharing 61
Mill, John Stuart 210
mind crime 125–126, 153, 201–208, 213, 226, 297
Minsky, Marvin 18, 261, 262, 282
Monte Carlo method 9–13
Moore’s law 24–25, 73–77, 274, 286; see also computing power
moral growth 214
moral permissibility (MP)218–220, 297
moral rightness (MR)217–220.296, 297
moral status 125–126, 166–169, 173, 202–205, 268, 288, 296
Moravec, Hans 24, 265, 288
motivation selection 29, 127–129, 138–144, 147, 158, 168, 180–191, 222
definition 138
motivational scaffolding 191, 207
multipolar scenarios 90, 132, 159–184, 243–254, 301
mutational load 41
N
nanotechnology 53, 94–98, 103, 113, 177, 231, 239, 276, 277, 299, 300
natural language 14
neural networks 5–9, 28, 46, 173, 237, 262, 274
neurocomputational modeling 25–30, 35, 61, 301; see also whole brain emulation
(WBE) and neuromorphic AI
neuromorphic AI 28, 34, 47, 237–245, 267, 300, 301
Newton, Isaac 56
Nilsson, Nils 18–20, 264
nootropics 36–44, 66–67, 201, 267
Norvig, Peter 19, 264, 282
O
observation selection theory, see anthropicsOliphant, Mark 85
O’Neill, Gerard 101
ontological crisis 146, 197
optimality notions 10, 186, 194, 291–293
Bayesian agent 9–11
value learner (AI-VL) 194
observation-utility-maximizer (AI-OUM) 194
reinforcement learner (AI-RL) 194
optimization power 24, 62–75, 83, 92–96, 227, 274
definition 65
oracle AI 141–158, 222–226, 285, 286
definition 146
orthogonality thesis 105–109, 115, 279, 280
P
paperclip AI 107–108, 123–125, 132–135, 153, 212, 243
Parfit, Derek 279
Pascal’s mugging 223, 298
Pascal’s wager 223
person-affecting perspective 228, 245–246, 301
perverse instantiation 120–124, 153, 190–196
poker 13
principal–agent problem 127–128, 184
Principle of Epistemic Deference 211, 221
Proverb (program) 12
Q
qualia, see consciousness
quality superintelligence 51–58, 72, 243, 272
definition 56
R
race dynamic, see technology race
rate of growth, see growth
ratification 222–225
Rawls, John 150
Reagan, Ronald 86–87
reasons-based goal 220recalcitrance 62–77, 92, 241, 274
definition 65
recursive self-improvement 29, 75, 96, 142, 259; see also seed AI
reinforcement learning 12, 28, 188–189, 194–196, 207, 237, 277, 282, 290
resource acquisition 113–116, 123, 193
reward signal 71, 121–122, 188, 194, 207
Riemann hypothesis catastrophe 123, 141
robotics 9–19, 94–97, 117–118, 139, 238, 276, 290
Roosevelt, Franklin D.85
RSA encryption scheme 80
Russell, Bertrand 6, 87, 139, 277
S
Samuel, Arthur 12
Sandberg, Anders 265, 267, 272, 274
scanning, see whole brain emulation (WBE)
Schaeffer, Jonathan 12
scheduling 15
Schelling point 147, 183, 296
Scrabble 13
second transition 176–178, 238, 243–245, 252
second-guessing (arguments) 238–239
seed AI 23–29, 36, 75, 83, 92–96, 107, 116–120, 142, 151, 189–198, 201–217,
224–225, 240–241, 266, 274, 275, 282
self-limiting goal 123
Shakey (robot) 6
SHRDLU (program) 6
Shulman, Carl 178–180, 265, 287, 300, 302, 304
simulation hypothesis 134–135, 143, 278, 288, 292
singleton 78–90, 95–104, 112–114, 115–126, 136, 159, 176–184, 242, 275, 276, 279,
281, 287, 299, 301, 303
definition 78, 100
singularity 1, 2, 49, 75, 261, 274; see also intelligence explosion
social signaling 110
somatic gene therapy 42
sovereign AI 148–158, 187, 226, 285
speech recognition 15–16, 46
speed superintelligence 52–58, 75, 270, 271
definition 53
Strategic Defense Initiative (“Star Wars”) 86strong AI 18
stunting 135–137, 143
sub-symbolic processing, see connectionism
superintelligence; see also collective superintelligence, quality superintelligence
and speed superintelligence
definition 22, 52
forms 52, 59
paths to 22, 50
predicting the behavior of 108, 155, 302
superorganisms 178–180
superpowers 52–56, 80, 86–87, 91–104, 119, 133, 148, 277, 279, 296
types 94
surveillance 15, 49, 64, 82–85, 94, 117, 132, 181, 232, 253, 276, 294, 299
Szilárd, Leó 85
T
TD-Gammon 12
Technological Completion Conjecture 112–113, 229
technology race 80–82, 86–90 203–205, 231, 246–252, 302
teleological threads 110
Tesauro, Gerry 12
TextRunner (system) 71
theorem prover 15, 266
three laws of robotics 139, 284
Thrun, Sebastian 19
tool-AI 151–158
definition 151
treacherous turn 116–119, 128
Tribolium castaneum 154
tripwires 137–143
Truman, Harry 85
Turing, Alan 4, 23, 29, 44, 225, 265, 271, 272
U
unemployment 65, 159–180, 287
United Nations 87–89, 252–253
universal accelerator 233
unmanned vehicle, see drone
uploading, see whole brain emulation (WBE)utility function 10–11, 88, 100, 110, 119, 124–125, 133–134, 172, 185–187,
192–208, 290, 292, 293, 303
V
value learning 191–198, 208, 293
value-accretion 189–190, 207
value-loading 185–208, 293, 294
veil of ignorance 150, 156, 253, 285
Vinge, Vernor 2, 49, 270
virtual reality 30, 31, 53, 113, 166, 171, 198, 204, 300
von Neumann probe 100–101, 113
von Neumann, John 44, 87, 114, 261, 277, 281
W
wages 65, 69, 160–169
Watson (IBM) 13, 71
WBE, see whole brain emulation (WBE)
Whitehead, Alfred N.6
whole brain emulation (WBE) 28–36, 50, 60, 68–73, 77, 84–85, 108, 172, 198,
201–202, 236–245, 252, 266, 267, 274, 299, 300, 301
Wigner, Eugene 85
windfall clause 254, 303
Winston, Patrick 18
wire-heading 122–123, 133, 189, 194, 207, 282, 291
wise-singleton sustainability threshold 100–104, 279
world economy 2–3, 63, 74, 83, 159–184, 274, 277, 285
Y
Yudkowsky, Eliezer 70, 92, 98, 106, 197, 211–216, 266, 273, 282, 286, 291, 299
1

An Animal of No Significance

ABOUT 13.5 BILLION YEARS AGO, MATTER, energy, time and space came into being in what is known as the Big Bang. The story of these fundamental features of our universe is called physics.
About 300,000 years after their appearance, matter and energy started to coalesce into complex structures, called atoms, which then combined into molecules. The story of atoms, molecules and their interactions is called chemistry.
About 3.8 billion years ago, on a planet called Earth, certain molecules combined to form particularly large and intricate structures called organisms. The story of organisms is called biology.
About 70,000 years ago, organisms belonging to the species Homo sapiens started to form even more elaborate structures called cultures. The subsequent development of these human cultures is called history.
Three important revolutions shaped the course of history: the Cognitive Revolution kick-started history about 70,000 years ago. The Agricultural Revolution sped it up about 12,000 years ago. The Scientific Revolution, which got under way only 500 years ago, may well end history and start something completely different. This book tells the story of how these three revolutions have affected humans and their fellow organisms.
There were humans long before there was history. Animals much like modern humans first appeared about 2.5 million years ago. But for countless generations they did not stand out from the myriad other organisms with which they shared their habitats.
On a hike in East Africa 2 million years ago, you might well have encountered a familiar cast of human characters: anxious mothers cuddling their babies and clutches of carefree children playing in the mud; temperamental youths chafing against the dictates of society and weary elders who just wanted to be left in peace; chest-thumping machos trying to impress the local beauty and wise old matriarchs who had already seen it all. These archaic humans loved, played, formed close friendships and competed for status and power – but so did chimpanzees, baboons and elephants. There was nothing special about them. Nobody, least of all humans themselves, had any inkling that their descendants would one day walk on the moon, split the atom, fathom the genetic code and write history books. The most important thing to know about prehistoric humans is that they were insignificant animals with no more impact on their environment than gorillas, fireflies or jellyfish.
Biologists classify organisms into species. Animals are said to belong to the same species if they tend to mate with each other, giving birth to fertile offspring. Horses and donkeys have a recent common ancestor and share many physical traits. But they show little sexual interest in one another. They will mate if induced to do so – but their offspring, called mules, are sterile. Mutations in donkey DNA can therefore never cross over to horses, or vice versa. The two types of animals are consequently considered two distinct species, moving along separate evolutionary paths. By contrast, a bulldog and a spaniel may look very different, but they are members of the same species, sharing the same DNA pool. They will happily mate and their puppies will grow up to pair off with other dogs and produce more puppies.
Species that evolved from a common ancestor are bunched together under the heading ‘genus’ (plural genera). Lions, tigers, leopards and jaguars are different species within the genus Panthera. Biologists label organisms with a two-part Latin name, genus followed by species. Lions, for example, are called Panthera leo, the species leo of the genus Panthera. Presumably, everyone reading this book is a Homo sapiens – the species sapiens (wise) of the genus Homo (man).
Genera in their turn are grouped into families, such as the cats (lions, cheetahs, house cats), the dogs (wolves, foxes, jackals) and the elephants (elephants, mammoths, mastodons). All members of a family trace their lineage back to a founding matriarch or patriarch. All cats, for example, from the smallest house kitten to the most ferocious lion, share a common feline ancestor who lived about 25 million years ago.
Homo sapiens, too, belongs to a family. This banal fact used to be one of history’s most closely guarded secrets. Homo sapiens long preferred to view itself as set apart from animals, an orphan bereft of family, lacking siblings or cousins, and most importantly, without parents. But that’s just not the case. Like it or not, we are members of a large and particularly noisy family called the great apes. Our closest living relatives include chimpanzees, gorillas and orang-utans. The chimpanzees are the closest. Just 6 million years ago, a single female ape had two daughters. One became the ancestor of all chimpanzees, the other is our own grandmother.
Skeletons in the Closet

Homo sapiens has kept hidden an even more disturbing secret. Not only do we possess an abundance of uncivilised cousins, once upon a time we had quite a few brothers and sisters as well. We are used to thinking about ourselves as the only humans, because for the last 10,000 years, our species has indeed been the only human species around. Yet the real meaning of the word human is ‘an animal belonging to the genus Homo’, and there used to be many other species of this genus besides Homo sapiens. Moreover, as we shall see in the last chapter of the book, in the not so distant future we might again have to contend with non-sapiens humans. To clarify this point, I will often use the term ‘Sapiens’ to denote members of the species Homo sapiens, while reserving the term ‘human’ to refer to all extant members of the genus Homo.
Humans first evolved in East Africa about 2.5 million years ago from an earlier genus of apes called Australopithecus, which means ‘Southern Ape’. About 2 million years ago, some of these archaic men and women left their homeland to journey through and settle vast areas of North Africa, Europe and Asia. Since survival in the snowy forests of northern Europe required different traits than those needed to stay alive in Indonesia’s steaming jungles, human populations evolved in different directions. The result was several distinct species, to each of which scientists have assigned a pompous Latin name.

2. Our siblings, according to speculative reconstructions (left to right):
Homo rudolfensis (East Africa); Homo erectus (East Asia); and Homo neanderthalensis (Europe and western Asia). All are humans.

Humans in Europe and western Asia evolved into Homo neanderthalensis (‘Man from the Neander Valley), popularly referred to simply as ‘Neanderthals’. Neanderthals, bulkier and more muscular than us Sapiens, were well adapted to the cold climate of Ice Age western Eurasia. The more eastern regions of Asia were populated by Homo erectus, ‘Upright Man’, who survived there for close to 2 million years, making it the most durable human species ever. This record is unlikely to be broken even by our own species. It is doubtful whether Homo sapiens will still be around a thousand years from now, so 2 million years is really out of our league.
On the island of Java, in Indonesia, lived Homo soloensis, ‘Man from the Solo Valley’, who was suited to life in the tropics. On another Indonesian island – the small island of Flores – archaic humans underwent a process of dwarfing. Humans first reached Flores when the sea level was exceptionally low, and the island was easily accessible from the mainland. When the seas rose again, some people were trapped on the island, which was poor in resources. Big people, who need a lot of food, died first. Smaller fellows survived much better. Over the generations, the people of Flores became dwarves. This unique species, known by scientists as Homo floresiensis, reached a maximum height of only one metre and weighed no more than twenty-five kilograms. They were nevertheless able to produce stone tools, and even managed occasionally to hunt down some of the island’s elephants – though, to be fair, the elephants were a dwarf species as well.
In 2010 another lost sibling was rescued from oblivion, when scientists excavating the Denisova Cave in Siberia discovered a fossilised finger bone. Genetic analysis proved that the finger belonged to a previously unknown human species, which was named Homo denisova. Who knows how many lost relatives of ours are waiting to be discovered in other caves, on other islands, and in other climes.
While these humans were evolving in Europe and Asia, evolution in East Africa did not stop. The cradle of humanity continued to nurture numerous new species, such as Homo rudolfensis, ‘Man from Lake Rudolf’, Homo ergaster, ‘Working Man’, and eventually our own species, which we’ve immodestly named Homo sapiens, ‘Wise Man’.
The members of some of these species were massive and others were dwarves. Some were fearsome hunters and others meek plant-gatherers. Some lived only on a single island, while many roamed over continents. But all of them belonged to the genus Homo. They were all human beings.
It’s a common fallacy to envision these species as arranged in a straight line of descent, with Ergaster begetting Erectus, Erectus begetting the Neanderthals, and the Neanderthals evolving into us. This linear model gives the mistaken impression that at any particular moment only one type of human inhabited the earth, and that all earlier species were merely older models of ourselves. The truth is that from about 2 million years ago until around 10,000 years ago, the world was home, at one and the same time, to several human species. And why not? Today there are many species of foxes, bears and pigs. The earth of a hundred millennia ago was walked by at least six different species of man. It’s our current exclusivity, not that multi-species past, that is peculiar – and perhaps incriminating. As we will shortly see, we Sapiens have good reasons to repress the memory of our siblings.
The Cost of Thinking

Despite their many differences, all human species share several defining characteristics. Most notably, humans have extraordinarily large brains compared to other animals. Mammals weighing sixty kilograms have an average brain size of 200 cubic centimetres. The earliest men and women, 2.5 million years ago, had brains of about 600 cubic centimetres. Modern Sapiens sport a brain averaging 1,200–1,400 cubic centimetres. Neanderthal brains were even bigger.
That evolution should select for larger brains may seem to us like, well, a no-brainer. We are so enamoured of our high intelligence that we assume that when it comes to cerebral power, more must be better. But if that were the case, the feline family would also have produced cats who could do calculus. Why is genus Homo the only one in the entire animal kingdom to have come up with such massive thinking machines?
The fact is that a jumbo brain is a jumbo drain on the body. It’s not easy to carry around, especially when encased inside a massive skull. It’s even harder to fuel. In Homo sapiens, the brain accounts for about 2–3 per cent of total body weight, but it consumes 25 per cent of the body’s energy when the body is at rest. By comparison, the brains of other apes require only 8 per cent of rest-time energy. Archaic humans paid for their large brains in two ways. Firstly, they spent more time in search of food. Secondly, their muscles atrophied. Like a government diverting money from defence to education, humans diverted energy from biceps to neurons. It’s hardly a foregone conclusion that this is a good strategy for survival on the savannah. A chimpanzee can’t win an argument with a Homo sapiens, but the ape can rip the man apart like a rag doll.
Today our big brains pay off nicely, because we can produce cars and guns that enable us to move much faster than chimps, and shoot them from a safe distance instead of wrestling. But cars and guns are a recent phenomenon. For more than 2 million years, human neural networks kept growing and growing, but apart from some flint knives and pointed sticks, humans had precious little to show for it. What then drove forward the evolution of the massive human brain during those 2 million years? Frankly, we don’t know.
Another singular human trait is that we walk upright on two legs. Standing up, it’s easier to scan the savannah for game or enemies, and arms that are unnecessary for locomotion are freed for other purposes, like throwing stones or signalling. The more things these hands could do, the more successful their owners were, so evolutionary pressure brought about an increasing concentration of nerves and finely tuned muscles in the palms and fingers. As a result, humans can perform very intricate tasks with their hands. In particular, they can produce and use sophisticated tools. The first evidence for tool production dates from about 2.5 million years ago, and the manufacture and use of tools are the criteria by which archaeologists recognise ancient humans.
Yet walking upright has its downside. The skeleton of our primate ancestors developed for millions of years to support a creature that walked on all fours and had a relatively small head. Adjusting to an upright position was quite a challenge, especially when the scaffolding had to support an extra-large cranium. Humankind paid for its lofty vision and industrious hands with backaches and stiff necks.
Women paid extra. An upright gait required narrower hips, constricting the birth canal – and this just when babies’ heads were getting bigger and bigger. Death in childbirth became a major hazard for human females. Women who gave birth earlier, when the infants brain and head were still relatively small and supple, fared better and lived to have more children. Natural selection consequently favoured earlier births. And, indeed, compared to other animals, humans are born prematurely, when many of their vital systems are still under-developed. A colt can trot shortly after birth; a kitten leaves its mother to forage on its own when it is just a few weeks old. Human babies are helpless, dependent for many years on their elders for sustenance, protection and education.
This fact has contributed greatly both to humankind’s extraordinary social abilities and to its unique social problems. Lone mothers could hardly forage enough food for their offspring and themselves with needy children in tow. Raising children required constant help from other family members and neighbours. It takes a tribe to raise a human. Evolution thus favoured those capable of forming strong social ties. In addition, since humans are born underdeveloped, they can be educated and socialised to a far greater extent than any other animal. Most mammals emerge from the womb like glazed earthenware emerging from a kiln – any attempt at remoulding will scratch or break them. Humans emerge from the womb like molten glass from a furnace. They can be spun, stretched and shaped with a surprising degree of freedom. This is why today we can educate our children to become Christian or Buddhist, capitalist or socialist, warlike or peace-loving.
*

We assume that a large brain, the use of tools, superior learning abilities and complex social structures are huge advantages. It seems self-evident that these have made humankind the most powerful animal on earth. But humans enjoyed all of these advantages for a full 2 million years during which they remained weak and marginal creatures. Thus humans who lived a million years ago, despite their big brains and sharp stone tools, dwelt in constant fear of predators, rarely hunted large game, and subsisted mainly by gathering plants, scooping up insects, stalking small animals, and eating the carrion left behind by other more powerful carnivores.
One of the most common uses of early stone tools was to crack open bones in order to get to the marrow. Some researchers believe this was our original niche. Just as woodpeckers specialise in extracting insects from the trunks of trees, the first humans specialised in extracting marrow from bones. Why marrow? Well, suppose you observe a pride of lions take down and devour a giraffe. You wait patiently until they’re done. But it’s still not your turn because first the hyenas and jackals – and you don’t dare interfere with them scavenge the leftovers. Only then would you and your band dare approach the carcass, look cautiously left and right – and dig into the edible tissue that remained.
This is a key to understanding our history and psychology. Genus Homo’s position in the food chain was, until quite recently, solidly in the middle. For millions of years, humans hunted smaller creatures and gathered what they could, all the while being hunted by larger predators. It was only 400,000 years ago that several species of man began to hunt large game on a regular basis, and only in the last 100,000 years – with the rise of Homo sapiens – that man jumped to the top of the food chain.
That spectacular leap from the middle to the top had enormous consequences. Other animals at the top of the pyramid, such as lions and sharks, evolved into that position very gradually, over millions of years. This enabled the ecosystem to develop checks and balances that prevent lions and sharks from wreaking too much havoc. As lions became deadlier, so gazelles evolved to run faster, hyenas to cooperate better, and rhinoceroses to be more bad-tempered. In contrast, humankind ascended to the top so quickly that the ecosystem was not given time to adjust. Moreover, humans themselves failed to adjust. Most top predators of the planet are majestic creatures. Millions of years of dominion have filled them with self-confidence. Sapiens by contrast is more like a banana republic dictator. Having so recently been one of the underdogs of the savannah, we are full of fears and anxieties over our position, which makes us doubly cruel and dangerous. Many historical calamities, from deadly wars to ecological catastrophes, have resulted from this over-hasty jump.
A Race of Cooks

A significant step on the way to the top was the domestication of fire. Some human species may have made occasional use of fire as early as 800,000 years ago. By about 300,000 years ago, Homo erectus, Neanderthals and the forefathers of Homo sapiens were using fire on a daily basis. Humans now had a dependable source of light and warmth, and a deadly weapon against prowling lions. Not long afterwards, humans may even have started deliberately to torch their neighbourhoods. A carefully managed fire could turn impassable barren thickets into prime grasslands teeming with game. In addition, once the fire died down, Stone Age entrepreneurs could walk through the smoking remains and harvest charcoaled animals, nuts and tubers.
But the best thing fire did was cook. Foods that humans cannot digest in their natural forms – such as wheat, rice and potatoes – became staples of our diet thanks to cooking. Fire not only changed food’s chemistry, it changed its biology as well. Cooking killed germs and parasites that infested food. Humans also had a far easier time chewing and digesting old favourites such as fruits, nuts, insects and carrion if they were cooked. Whereas chimpanzees spend five hours a day chewing raw food, a single hour suffices for people eating cooked food.
The advent of cooking enabled humans to eat more kinds of food, to devote less time to eating, and to make do with smaller teeth and shorter intestines. Some scholars believe there is a direct link between the advent of cooking, the shortening of the human intestinal track, and the growth of the human brain. Since long intestines and large brains are both massive energy consumers, it’s hard to have both. By shortening the intestines and decreasing their energy consumption, cooking inadvertently opened the way to the jumbo brains of Neanderthals and Sapiens.1
Fire also opened the first significant gulf between man and the other animals. The power of almost all animals depends on their bodies: the strength of their muscles, the size of their teeth, the breadth of their wings. Though they may harness winds and currents, they are unable to control these natural forces, and are always constrained by their physical design. Eagles, for example, identify thermal columns rising from the ground, spread their giant wings and allow the hot air to lift them upwards. Yet eagles cannot control the location of the columns, and their maximum carrying capacity is strictly proportional to their wingspan.
When humans domesticated fire, they gained control of an obedient and potentially limitless force. Unlike eagles, humans could choose when and where to ignite a flame, and they were able to exploit fire for any number of tasks. Most importantly, the power of fire was not limited by the form, structure or strength of the human body. A single woman with a flint or fire stick could burn down an entire forest in a matter of hours. The domestication of fire was a sign of things to come.
Our Brothers’ Keepers

Despite the benefits of fire, 150,000 years ago humans were still marginal creatures. They could now scare away lions, warm themselves during cold nights, and burn down the occasional forest. Yet counting all species together, there were still no more than perhaps a million humans living between the Indonesian archipelago and the Iberian peninsula, a mere blip on the ecological radar.
Our own species, Homo sapiens, was already present on the world stage, but so far it was just minding its own business in a corner of Africa. We don’t know exactly where and when animals that can be classified as Homo sapiens first evolved from some earlier type of humans, but most scientists agree that by 150,000 years ago, East Africa was populated by Sapiens that looked just like us. If one of them turned up in a modern morgue, the local pathologist would notice nothing peculiar. Thanks to the blessings of fire, they had smaller teeth and jaws than their ancestors, whereas they had massive brains, equal in size to ours.
Scientists also agree that about 70,000 years ago, Sapiens from East Africa spread into the Arabian peninsula, and from there they quickly overran the entire Eurasian landmass.
When Homo sapiens landed in Arabia, most of Eurasia was already settled by other humans. What happened to them? There are two conflicting theories. The ‘Interbreeding Theory’ tells a story of attraction, sex and mingling. As the African immigrants spread around the world, they bred with other human populations, and people today are the outcome of this interbreeding.
For example, when Sapiens reached the Middle East and Europe, they encountered the Neanderthals. These humans were more muscular than Sapiens, had larger brains, and were better adapted to cold climes. They used tools and fire, were good hunters, and apparently took care of their sick and infirm. (Archaeologists have discovered the bones of Neanderthals who lived for many years with severe physical handicaps, evidence that they were cared for by their relatives.) Neanderthals are often depicted in caricatures as the archetypical brutish and stupid ‘cave people’, but recent evidence has changed their image.
According to the Interbreeding Theory, when Sapiens spread into Neanderthal lands, Sapiens bred with Neanderthals until the two populations merged. If this is the case, then today’s Eurasians are not pure Sapiens. They are a mixture of Sapiens and Neanderthals. Similarly, when Sapiens reached East Asia, they interbred with the local Erectus, so the Chinese and Koreans are a mixture of Sapiens and Erectus.
The opposing view, called the ‘Replacement Theory’ tells a very different story – one of incompatibility, revulsion, and perhaps even genocide. According to this theory, Sapiens and other humans had different anatomies, and most likely different mating habits and even body odours. They would have had little sexual interest in one another. And even if a Neanderthal Romeo and a Sapiens Juliet fell in love, they could not produce fertile children, because the genetic gulf separating the two populations was already unbridgeable. The two populations remained completely distinct, and when the Neanderthals died out, or were killed off, their genes died with them. According to this view, Sapiens replaced all the previous human populations without merging with them. If that is the case, the lineages of all contemporary humans can be traced back, exclusively, to East Africa, 70,000 years ago. We are all ‘pure Sapiens’.

Map 1. Homo sapiens conquers the globe.

A lot hinges on this debate. From an evolutionary perspective, 70,000 years is a relatively short interval. If the Replacement Theory is correct, all living humans have roughly the same genetic baggage, and racial distinctions among them are negligible. But if the Interbreeding Theory is right, there might well be genetic differences between Africans, Europeans and Asians that go back hundreds of thousands of years. This is political dynamite, which could provide material for explosive racial theories.
In recent decades the Replacement Theory has been the common wisdom in the field. It had firmer archaeological backing, and was more politically correct (scientists had no desire to open up the Pandora’s box of racism by claiming significant genetic diversity among modern human populations). But that ended in 2010, when the results of a four-year effort to map the Neanderthal genome were published. Geneticists were able to collect enough intact Neanderthal DNA from fossils to make a broad comparison between it and the DNA of contemporary humans. The results stunned the scientific community.
It turned out that 1–4 per cent of the unique human DNA of modern populations in the Middle East and Europe is Neanderthal DNA. That’s not a huge amount, but it’s significant. A second shock came several months later, when DNA extracted from the fossilised finger from Denisova was mapped. The results proved that up to 6 per cent of the unique human DNA of modern Melanesians and Aboriginal Australians is Denisovan DNA.
If these results are valid – and it’s important to keep in mind that further research is under way and may either reinforce or modify these conclusions – the Interbreeders got at least some things right. But that doesn’t mean that the Replacement Theory is completely wrong. Since Neanderthals and Denisovans contributed only a small amount of DNA to our present-day genome, it is impossible to speak of a ‘merger’ between Sapiens and other human species. Although differences between them were not large enough to completely prevent fertile intercourse, they were sufficient to make such contacts very rare.
How then should we understand the biological relatedness of Sapiens, Neanderthals and Denisovans? Clearly, they were not completely different species like horses and donkeys. On the other hand, they were not just different populations of the same species, like bulldogs and spaniels. Biological reality is not black and white. There are also important grey areas. Every two species that evolved from a common ancestor, such as horses and donkeys, were at one time just two populations of the same species, like bulldogs and spaniels. There must have been a point when the two populations were already quite different from one another, but still capable on rare occasions of having sex and producing fertile offspring. Then another mutation severed this last connecting thread, and they went their separate evolutionary ways.
It seems that about 50,000 years ago, Sapiens, Neanderthals and Denisovans were at that borderline point. They were almost, but not quite, entirely separate species. As we shall see in the next chapter, Sapiens were already very different from Neanderthals and Denisovans not only in their genetic code and physical traits, but also in their cognitive and social abilities, yet it appears it was still just possible, on rare occasions, for a Sapiens and a Neanderthal to produce a fertile offspring. So the populations did not merge, but a few lucky Neanderthal genes did hitch a ride on the Sapiens Express. It is unsettling – and perhaps thrilling – to think that we Sapiens could at one time have sex with an animal from a different species, and produce children together.

3. A speculative reconstruction of a Neanderthal child. Genetic evidence hints that at least some Neanderthals may have had fair skin and hair.

But if the Neanderthals, Denisovans and other human species didn’t merge with Sapiens, why did they vanish? One possibility is that Homo sapiens drove them to extinction. Imagine a Sapiens band reaching a Balkan valley where Neanderthals had lived for hundreds of thousands of years. The newcomers began to hunt the deer and gather the nuts and berries that were the Neanderthals’ traditional staples. Sapiens were more proficient hunters and gatherers – thanks to better technology and superior social skills – so they multiplied and spread. The less resourceful Neanderthals found it increasingly difficult to feed themselves. Their population dwindled and they slowly died out, except perhaps for one or two members who joined their Sapiens neighbours.
Another possibility is that competition for resources flared up into violence and genocide. Tolerance is not a Sapiens trademark. In modern times, a small difference in skin colour, dialect or religion has been enough to prompt one group of Sapiens to set about exterminating another group. Would ancient Sapiens have been more tolerant towards an entirely different human species? It may well be that when Sapiens encountered Neanderthals, the result was the first and most significant ethnic-cleansing campaign in history.
Whichever way it happened, the Neanderthals (and the other human species) pose one of history’s great what ifs. Imagine how things might have turned out had the Neanderthals or Denisovans survived alongside Homo sapiens. What kind of cultures, societies and political structures would have emerged in a world where several different human species coexisted? How, for example, would religious faiths have unfolded? Would the book of Genesis have declared that Neanderthals descend from Adam and Eve, would Jesus have died for the sins of the Denisovans, and would the Qur’an have reserved seats in heaven for all righteous humans, whatever their species? Would Neanderthals have been able to serve in the Roman legions, or in the sprawling bureaucracy of imperial China? Would the American Declaration of Independence hold as a self-evident truth that all members of the genus Homo are created equal? Would Karl Marx have urged workers of all species to unite?
Over the past 10,000 years, Homo sapiens has grown so accustomed to being the only human species that it’s hard for us to conceive of any other possibility. Our lack of brothers and sisters makes it easier to imagine that we are the epitome of creation, and that a chasm separates us from the rest of the animal kingdom. When Charles Darwin indicated that Homo sapiens was just another kind of animal, people were outraged. Even today many refuse to believe it. Had the Neanderthals survived, would we still imagine ourselves to be a creature apart? Perhaps this is exactly why our ancestors wiped out the Neanderthals. They were too familiar to ignore, but too different to tolerate.
Whether Sapiens are to blame or not, no sooner had they arrived at a new location than the native population became extinct. The last remains of Homo soloensis are dated to about 50,000 years ago. Homo denisova disappeared shortly thereafter. Neanderthals made their exit roughly 30,000 years ago. The last dwarf-like humans vanished from Flores Island about 12,000 years ago. They left behind some bones, stone tools, a few genes in our DNA and a lot of unanswered questions. They also left behind us, Homo sapiens, the last human species.
What was the Sapiens’ secret of success? How did we manage to settle so rapidly in so many distant and ecologically different habitats? How did we push all other human species into oblivion? Why couldn’t even the strong, brainy, cold-proof Neanderthals survive our onslaught? The debate continues to rage. The most likely answer is the very thing that makes the debate possible: Homo sapiens conquered the world thanks above all to its unique language.2

The Tree of Knowledge

IN THE PREVIOUS CHAPTER WE SAW THAT although Sapiens had already populated East Africa 150,000 years ago, they began to overrun the rest of planet Earth and drive the other human species to extinction only about 70,000 years ago. In the intervening millennia, even though these archaic Sapiens looked just like us and their brains were as big as ours, they did not enjoy any marked advantage over other human species, did not produce particularly sophisticated tools, and did not accomplish any other special feats.
In fact, in the first recorded encounter between Sapiens and Neanderthals, the Neanderthals won. About 100,000 years ago, some Sapiens groups migrated north to the Levant, which was Neanderthal territory, but failed to secure a firm footing. It might have been due to nasty natives, an inclement climate, or unfamiliar local parasites. Whatever the reason, the Sapiens eventually retreated, leaving the Neanderthals as masters of the Middle East.
This poor record of achievement has led scholars to speculate that the internal structure of the brains of these Sapiens was probably different from ours. They looked like us, but their cognitive abilities – learning, remembering, communicating – were far more limited. Teaching such an ancient Sapiens English, persuading him of the truth of Christian dogma, or getting him to understand the theory of evolution would probably have been hopeless undertakings. Conversely, we would have had a very hard time learning his language and understanding his way of thinking.
But then, beginning about 70,000 years ago, Homo sapiens started doing very special things. Around that date Sapiens bands left Africa for a second time. This time they drove the Neanderthals and all other human species not only from the Middle East, but from the face of the earth. Within a remarkably short period, Sapiens reached Europe and East Asia. About 45,000 years ago, they somehow crossed the open sea and landed in Australia – a continent hitherto untouched by humans. The period from about 70,000 years ago to about 30,000 years ago witnessed the invention of boats, oil lamps, bows and arrows and needles (essential for sewing warm clothing). The first objects that can reliably be called art date from this era (see the Stadel lion-man on this page), as does the first clear evidence for religion, commerce and social stratification.
Most researchers believe that these unprecedented accomplishments were the product of a revolution in Sapiens’ cognitive abilities. They maintain that the people who drove the Neanderthals to extinction, settled Australia, and carved the Stadel lion-man were as intelligent, creative and sensitive as we are. If we were to come across the artists of the Stadel Cave, we could learn their language and they ours. We’d be able to explain to them everything we know – from the adventures of Alice in Wonderland to the paradoxes of quantum physics – and they could teach us how their people view the world.
The appearance of new ways of thinking and communicating, between 70,000 and 30,000 years ago, constitutes the Cognitive Revolution. What caused it? We’re not sure. The most commonly believed theory argues that accidental genetic mutations changed the inner wiring of the brains of Sapiens, enabling them to think in unprecedented ways and to communicate using an altogether new type of language. We might call it the Tree of Knowledge mutation. Why did it occur in Sapiens DNA rather than in that of Neanderthals? It was a matter of pure chance, as far as we can tell. But it’s more important to understand the consequences of the Tree of Knowledge mutation than its causes. What was so special about the new Sapiens language that it enabled us to conquer the world?*
It was not the first language. Every animal has some kind of language. Even insects, such as bees and ants, know how to communicate in sophisticated ways, informing one another of the whereabouts of food. Neither was it the first vocal language. Many animals, including all ape and monkey species, have vocal languages. For example, green monkeys use calls of various kinds to communicate. Zoologists have identified one call that means, ‘Careful! An eagle!’ A slightly different call warns, ‘Careful! A lion!’ When researchers played a recording of the first call to a group of monkeys, the monkeys stopped what they were doing and looked upwards in fear. When the same group heard a recording of the second call, the lion warning, they quickly scrambled up a tree. Sapiens can produce many more distinct sounds than green monkeys, but whales and elephants have equally impressive abilities. A parrot can say anything Albert Einstein could say, as well as mimicking the sounds of phones ringing, doors slamming and sirens wailing. Whatever advantage Einstein had over a parrot, it wasn’t vocal. What, then, is so special about our language?
The most common answer is that our language is amazingly supple. We can connect a limited number of sounds and signs to produce an infinite number of sentences, each with a distinct meaning. We can thereby ingest, store and communicate a prodigious amount of information about the surrounding world. A green monkey can yell to its comrades, ‘Careful! A lion!’ But a modern human can tell her friends that this morning, near the bend in the river, she saw a lion tracking a herd of bison. She can then describe the exact location, including the different paths leading to the area. With this information, the members of her band can put their heads together and discuss whether they ought to approach the river in order to chase away the lion and hunt the bison.
A second theory agrees that our unique language evolved as a means of sharing information about the world. But the most important information that needed to be conveyed was about humans, not about lions and bison. Our language evolved as a way of gossiping. According to this theory Homo sapiens is primarily a social animal. Social cooperation is our key for survival and reproduction. It is not enough for individual men and women to know the whereabouts of lions and bison. It’s much more important for them to know who in their band hates whom, who is sleeping with whom, who is honest, and who is a cheat.

4. An ivory figurine of a ‘lion-man’ (or ‘lioness-woman’) from the Stadel Cave in Germany (c.32,000 years ago). The body is human, but the head is leonine. This is one of the first indisputable examples of art, and probably of religion, and of the ability of the human mind to imagine things that do not really exist.

The amount of information that one must obtain and store in order to track the ever-changing relationships of a few dozen individuals is staggering. (In a band of fifty individuals, there are 1,225 one-on-one relationships, and countless more complex social combinations.) All apes show a keen interest in such social information, but they have trouble gossiping effectively. Neanderthals and archaic Homo sapiens probably also had a hard time talking behind each other’s backs – a much maligned ability which is in fact essential for cooperation in large numbers. The new linguistic skills that modern Sapiens acquired about seventy millennia ago enabled them to gossip for hours on end. Reliable information about who could be trusted meant that small bands could expand into larger bands, and Sapiens could develop tighter and more sophisticated types of cooperation.1
The gossip theory might sound like a joke, but numerous studies support it. Even today the vast majority of human communication – whether in the form of emails, phone calls or newspaper columns – is gossip. It comes so naturally to us that it seems as if our language evolved for this very purpose. Do you think that history professors chat about the reasons for World War One when they meet for lunch, or that nuclear physicists spend their coffee breaks at scientific conferences talking about quarks? Sometimes. But more often, they gossip about the professor who caught her husband cheating, or the quarrel between the head of the department and the dean, or the rumours that a colleague used his research funds to buy a Lexus. Gossip usually focuses on wrongdoings. Rumour-mongers are the original fourth estate, journalists who inform society about and thus protect it from cheats and freeloaders.
Most likely, both the gossip theory and the there-is-a-lion-near-the-river theory are valid. Yet the truly unique feature of our language is not its ability to transmit information about men and lions. Rather, it’s the ability to transmit information about things that do not exist at all. As far as we know, only Sapiens can talk about entire kinds of entities that they have never seen, touched or smelled.
Legends, myths, gods and religions appeared for the first time with the Cognitive Revolution. Many animals and human species could previously say, ‘Careful! A lion!’ Thanks to the Cognitive Revolution, Homo sapiens acquired the ability to say, ‘The lion is the guardian spirit of our tribe.’ This ability to speak about fictions is the most unique feature of Sapiens language.
It’s relatively easy to agree that only Homo sapiens can speak about things that don’t really exist, and believe six impossible things before breakfast. You could never convince a monkey to give you a banana by promising him limitless bananas after death in monkey heaven. But why is it important? After all, fiction can be dangerously misleading or distracting. People who go to the forest looking for fairies and unicorns would seem to have less chance of survival than people who go looking for mushrooms and deer. And if you spend hours praying to non-existing guardian spirits, aren’t you wasting precious time, time better spent foraging, fighting and fornicating?
But fiction has enabled us not merely to imagine things, but to do so collectively. We can weave common myths such as the biblical creation story, the Dreamtime myths of Aboriginal Australians, and the nationalist myths of modern states. Such myths give Sapiens the unprecedented ability to cooperate flexibly in large numbers. Ants and bees can also work together in huge numbers, but they do so in a very rigid manner and only with close relatives. Wolves and chimpanzees cooperate far more flexibly than ants, but they can do so only with small numbers of other individuals that they know intimately. Sapiens can cooperate in extremely flexible ways with countless numbers of strangers. That’s why Sapiens rule the world, whereas ants eat our leftovers and chimps are locked up in zoos and research laboratories.
The Legend of Peugeot

Our chimpanzee cousins usually live in small troops of several dozen individuals. They form close friendships, hunt together and fight shoulder to shoulder against baboons, cheetahs and enemy chimpanzees. Their social structure tends to be hierarchical. The dominant member, who is almost always a male, is termed the ‘alpha male’. Other males and females exhibit their submission to the alpha male by bowing before him while making grunting sounds, not unlike human subjects kowtowing before a king. The alpha male strives to maintain social harmony within his troop. When two individuals fight, he will intervene and stop the violence. Less benevolently, he might monopolise particularly coveted foods and prevent lower-ranking males from mating with the females.
When two males are contesting the alpha position, they usually do so by forming extensive coalitions of supporters, both male and female, from within the group. Ties between coalition members are based on intimate daily contact – hugging, touching, kissing, grooming and mutual favours. Just as human politicians on election campaigns go around shaking hands and kissing babies, so aspirants to the top position in a chimpanzee group spend much time hugging, back-slapping and kissing baby chimps. The alpha male usually wins his position not because he is physically stronger, but because he leads a large and stable coalition. These coalitions play a central part not only during overt struggles for the alpha position, but in almost all day-to-day activities. Members of a coalition spend more time together, share food, and help one another in times of trouble.
There are clear limits to the size of groups that can be formed and maintained in such a way. In order to function, all members of a group must know each other intimately. Two chimpanzees who have never met, never fought, and never engaged in mutual grooming will not know whether they can trust one another, whether it would be worthwhile to help one another, and which of them ranks higher. Under natural conditions, a typical chimpanzee troop consists of about twenty to fifty individuals. As the number of chimpanzees in a troop increases, the social order destabilises, eventually leading to a rupture and the formation of a new troop by some of the animals. Only in a handful of cases have zoologists observed groups larger than a hundred. Separate groups seldom cooperate, and tend to compete for territory and food. Researchers have documented prolonged warfare between groups, and even one case of ‘genocidal’ activity in which one troop systematically slaughtered most members of a neighbouring band.2
Similar patterns probably dominated the social lives of early humans, including archaic Homo sapiens. Humans, like chimps, have social instincts that enabled our ancestors to form friendships and hierarchies, and to hunt or fight together. However, like the social instincts of chimps, those of humans were adapted only for small intimate groups. When the group grew too large, its social order destabilised and the band split. Even if a particularly fertile valley could feed 500 archaic Sapiens, there was no way that so many strangers could live together. How could they agree who should be leader, who should hunt where, or who should mate with whom?
In the wake of the Cognitive Revolution, gossip helped Homo sapiens to form larger and more stable bands. But even gossip has its limits. Sociological research has shown that the maximum ‘natural’ size of a group bonded by gossip is about 150 individuals. Most people can neither intimately know, nor gossip effectively about, more than 150 human beings.
Even today, a critical threshold in human organisations falls somewhere around this magic number. Below this threshold, communities, businesses, social networks and military units can maintain themselves based mainly on intimate acquaintance and rumour-mongering. There is no need for formal ranks, titles and law books to keep order.3 A platoon of thirty soldiers or even a company of a hundred soldiers can function well on the basis of intimate relations, with a minimum of formal discipline. A well-respected sergeant can become ‘king of the company and exercise authority even over commissioned officers. A small family business can survive and flourish without a board of directors, a CEO or an accounting department.
But once the threshold of 150 individuals is crossed, things can no longer work that way. You cannot run a division with thousands of soldiers the same way you run a platoon. Successful family businesses usually face a crisis when they grow larger and hire more personnel. If they cannot reinvent themselves, they go bust.
How did Homo sapiens manage to cross this critical threshold, eventually founding cities comprising tens of thousands of inhabitants and empires ruling hundreds of millions? The secret was probably the appearance of fiction. Large numbers of strangers can cooperate successfully by believing in common myths.
Any large-scale human cooperation – whether a modern state, a medieval church, an ancient city or an archaic tribe – is rooted in common myths that exist only in peoples collective imagination. Churches are rooted in common religious myths. Two Catholics who have never met can nevertheless go together on crusade or pool funds to build a hospital because they both believe that God was incarnated in human flesh and allowed Himself to be crucified to redeem our sins. States are rooted in common national myths. Two Serbs who have never met might risk their lives to save one another because both believe in the existence of the Serbian nation, the Serbian homeland and the Serbian flag. Judicial systems are rooted in common legal myths. Two lawyers who have never met can nevertheless combine efforts to defend a complete stranger because they both believe in the existence of laws, justice, human rights – and the money paid out in fees.
Yet none of these things exists outside the stories that people invent and tell one another. There are no gods in the universe, no nations, no money, no human rights, no laws, and no justice outside the common imagination of human beings.
People easily understand that ‘primitives’ cement their social order by believing in ghosts and spirits, and gathering each full moon to dance together around the campfire. What we fail to appreciate is that our modern institutions function on exactly the same basis. Take for example the world of business corporations. Modern business-people and lawyers are, in fact, powerful sorcerers. The principal difference between them and tribal shamans is that modern lawyers tell far stranger tales. The legend of Peugeot affords us a good example.
An icon that somewhat resembles the Stadel lion-man appears today on cars, trucks and motorcycles from Paris to Sydney. It’s the hood ornament that adorns vehicles made by Peugeot, one of the oldest and largest of Europe’s carmakers. Peugeot began as a small family business in the village of Valentigney, just 300 kilometres from the Stadel Cave. Today the company employs about 200,000 people worldwide, most of whom are complete strangers to each other. These strangers cooperate so effectively that in 2008 Peugeot produced more than 1.5 million automobiles, earning revenues of about 55 billion euros.
In what sense can we say that Peugeot SA (the company’s official name) exists? There are many Peugeot vehicles, but these are obviously not the company. Even if every Peugeot in the world were simultaneously junked and sold for scrap metal, Peugeot SA would not disappear. It would continue to manufacture new cars and issue its annual report. The company owns factories, machinery and showrooms, and employs mechanics, accountants and secretaries, but all these together do not comprise Peugeot. A disaster might kill every single one of Peugeot’s employees, and go on to destroy all of its assembly lines and executive offices. Even then, the company could borrow money, hire new employees, build new factories and buy new machinery. Peugeot has managers and shareholders, but neither do they constitute the company. All the managers could be dismissed and all its shares sold, but the company itself would remain intact.

5. The Peugeot Lion

It doesn’t mean that Peugeot SA is invulnerable or immortal. If a judge were to mandate the dissolution of the company, its factories would remain standing and its workers, accountants, managers and shareholders would continue to live – but Peugeot SA would immediately vanish. In short, Peugeot SA seems to have no essential connection to the physical world. Does it really exist?
Peugeot is a figment of our collective imagination. Lawyers call this a ‘legal fiction’. It can’t be pointed at; it is not a physical object. But it exists as a legal entity. Just like you or me, it is bound by the laws of the countries in which it operates. It can open a bank account and own property. It pays taxes, and it can be sued and even prosecuted separately from any of the people who own or work for it.
Peugeot belongs to a particular genre of legal fictions called ‘limited liability companies’. The idea behind such companies is among humanity’s most ingenious inventions. Homo sapiens lived for untold millennia without them. During most of recorded history property could be owned only by flesh-and-blood humans, the kind that stood on two legs and had big brains. If in thirteenth-century France Jean set up a wagon-manufacturing workshop, he himself was the business. If a wagon he’d made broke down a week after purchase, the disgruntled buyer would have sued Jean personally. If Jean had borrowed 1,000 gold coins to set up his workshop and the business failed, he would have had to repay the loan by selling his private property – his house, his cow, his land. He might even have had to sell his children into servitude. If he couldn’t cover the debt, he could be thrown in prison by the state or enslaved by his creditors. He was fully liable, without limit, for all obligations incurred by his workshop.
If you had lived back then, you would probably have thought twice before you opened an enterprise of your own. And indeed this legal situation discouraged entrepreneurship. People were afraid to start new businesses and take economic risks. It hardly seemed worth taking the chance that their families could end up utterly destitute.
This is why people began collectively to imagine the existence of limited liability companies. Such companies were legally independent of the people who set them up, or invested money in them, or managed them. Over the last few centuries such companies have become the main players in the economic arena, and we have grown so used to them that we forget they exist only in our imagination. In the US, the technical term for a limited liability company is a ‘corporation’, which is ironic, because the term derives from ‘corpus’ (‘body’ in Latin) – the one thing these corporations lack. Despite their having no real bodies, the American legal system treats corporations as legal persons, as if they were flesh-and-blood human beings.
And so did the French legal system back in 1896, when Armand Peugeot, who had inherited from his parents a metalworking shop that produced springs, saws and bicycles, decided to go into the automobile business. To that end, he set up a limited liability company. He named the company after himself, but it was independent of him. If one of the cars broke down, the buyer could sue Peugeot, but not Armand Peugeot. If the company borrowed millions of francs and then went bust, Armand Peugeot did not owe its creditors a single franc. The loan, after all, had been given to Peugeot, the company, not to Armand Peugeot, the Homo sapiens. Armand Peugeot died in 1915. Peugeot, the company, is still alive and well.
How exactly did Armand Peugeot, the man, create Peugeot, the company? In much the same way that priests and sorcerers have created gods and demons throughout history, and in which thousands of French curés were still creating Christ’s body every Sunday in the parish churches. It all revolved around telling stories, and convincing people to believe them. In the case of the French curés, the crucial story was that of Christ’s life and death as told by the Catholic Church. According to this story, if a Catholic priest dressed in his sacred garments solemnly said the right words at the right moment, mundane bread and wine turned into God’s flesh and blood. The priest exclaimed ‘Hoc est corpus meum!’ (Latin for ‘This is my body!’) and hocus pocus – the bread turned into Christ’s flesh. Seeing that the priest had properly and assiduously observed all the procedures, millions of devout French Catholics behaved as if God really existed in the consecrated bread and wine.
In the case of Peugeot SA the crucial story was the French legal code, as written by the French parliament. According to the French legislators, if a certified lawyer followed all the proper liturgy and rituals, wrote all the required spells and oaths on a wonderfully decorated piece of paper, and affixed his ornate signature to the bottom of the document, then hocus pocus – a new company was incorporated. When in 1896 Armand Peugeot wanted to create his company, he paid a lawyer to go through all these sacred procedures. Once the lawyer had performed all the right rituals and pronounced all the necessary spells and oaths, millions of upright French citizens behaved as if the Peugeot company really existed.
Telling effective stories is not easy. The difficulty lies not in telling the story, but in convincing everyone else to believe it. Much of history revolves around this question: how does one convince millions of people to believe particular stories about gods, or nations, or limited liability companies? Yet when it succeeds, it gives Sapiens immense power, because it enables millions of strangers to cooperate and work towards common goals. Just try to imagine how difficult it would have been to create states, or churches, or legal systems if we could speak only about things that really exist, such as rivers, trees and lions.
Over the years, people have woven an incredibly complex network of stories. Within this network, fictions such as Peugeot not only exist, but also accumulate immense power. The kinds of things that people create through this network of stories are known in academic circles as ‘fictions’, ‘social constructs’, or ‘imagined realities’. An imagined reality is not a lie. I lie when I say that there is a lion near the river when I know perfectly well that there is no lion there. There is nothing special about lies. Green monkeys and chimpanzees can lie. A green monkey, for example, has been observed calling ‘Careful! A lion!’ when there was no lion around. This alarm conveniently frightened away a fellow monkey who had just found a banana, leaving the liar all alone to steal the prize for itself.
Unlike lying, an imagined reality is something that everyone believes in, and as long as this communal belief persists, the imagined reality exerts force in the world. The sculptor from the Stadel Cave may sincerely have believed in the existence of the lion-man guardian spirit. Some sorcerers are charlatans, but most sincerely believe in the existence of gods and demons. Most millionaires sincerely believe in the existence of money and limited liability companies. Most human-rights activists sincerely believe in the existence of human rights. No one was lying when, in 2011, the UN demanded that the Libyan government respect the human rights of its citizens, even though the UN, Libya and human rights are all figments of our fertile imaginations.
Ever since the Cognitive Revolution, Sapiens has thus been living in a dual reality. On the one hand, the objective reality of rivers, trees and lions; and on the other hand, the imagined reality of gods, nations and corporations. As time went by, the imagined reality became ever more powerful, so that today the very survival of rivers, trees and lions depends on the grace of imagined entities such as gods, nations and corporations.
Bypassing the Genome

The ability to create an imagined reality out of words enabled large numbers of strangers to cooperate effectively. But it also did something more. Since large-scale human cooperation is based on myths, the way people cooperate can be altered by changing the myths – by telling different stories. Under the right circumstances myths can change rapidly. In 1789 the French population switched almost overnight from believing in the myth of the divine right of kings to believing in the myth of the sovereignty of the people. Consequently,ever since the Cognitive Revolution Homo sapiens has been able to revise its behaviour rapidly in accordance with changing needs. This opened a fast lane of cultural evolution, bypassing the traffic jams of genetic evolution. Speeding down this fast lane, Homo sapiens soon far outstripped all other human and animal species in its ability to cooperate.
The behaviour of other social animals is determined to a large extent by their genes. DNA is not an autocrat. Animal behaviour is also influenced by environmental factors and individual quirks. Nevertheless, in a given environment, animals of the same species will tend to behave in a similar way. Significant changes in social behaviour cannot occur, in general, without genetic mutations. For example, common chimpanzees have a genetic tendency to live in hierarchical groups headed by an alpha male. Members of a closely related chimpanzee species, bonobos, usually live in more egalitarian groups dominated by female alliances. Female common chimpanzees cannot take lessons from their bonobo relatives and stage a feminist revolution. Male chimps cannot gather in a constitutional assembly to abolish the office of alpha male and declare that from here on out all chimps are to be treated as equals. Such dramatic changes in behaviour would occur only if something changed in the chimpanzees’ DNA.
For similar reasons, archaic humans did not initiate any revolutions. As far as we can tell, changes in social patterns, the invention of new technologies and the settlement of alien habitats resulted from genetic mutations and environmental pressures more than from cultural initiatives. This is why it took humans hundreds of thousands of years to make these steps. Two million years ago, genetic mutations resulted in the appearance of a new human species called Homo erectus. Its emergence was accompanied by the development of a new stone tool technology, now recognised as a defining feature of this species. As long as Homo erectus did not undergo further genetic alterations, its stone tools remained roughly the same – for close to 2 million years!
In contrast, ever since the Cognitive Revolution, Sapiens have been able to change their behaviour quickly, transmitting new behaviours to future generations without any need of genetic or environmental change. As a prime example, consider the repeated appearance of childless elites, such as the Catholic priesthood, Buddhist monastic orders and Chinese eunuch bureaucracies. The existence of such elites goes against the most fundamental principles of natural selection, since these dominant members of society willingly give up procreation. Whereas chimpanzee alpha males use their power to have sex with as many females as possible – and consequently sire a large proportion of their troop’s young – the Catholic alpha male abstains completely from sexual intercourse and childcare. This abstinence does not result from unique environmental conditions such as a severe lack of food or want of potential mates. Nor is it the result of some quirky genetic mutation. The Catholic Church has survived for centuries, not by passing on a ‘celibacy gene’ from one pope to the next, but by passing on the stories of the New Testament and of Catholic canon law.
In other words, while the behaviour patterns of archaic humans remained fixed for tens of thousands of years, Sapiens could transform their social structures, the nature of their interpersonal relations, their economic activities and a host of other behaviours within a decade or two. Consider a resident of Berlin, born in 1900 and living to the ripe age of one hundred. She spent her childhood in the Hohenzollern Empire of Wilhelm II; her adult years in the Weimar Republic, the Nazi Third Reich and Communist East Germany; and she died a citizen of a democratic and reunified Germany. She had managed to be a part of five very different sociopolitical systems, though her DNA remained exactly the same.
This was the key to Sapiens’ success. In a one-on-one brawl, a Neanderthal would probably have beaten a Sapiens. But in a conflict of hundreds, Neanderthals wouldn’t stand a chance. Neanderthals could share information about the whereabouts of lions, but they probably could not tell – and revise – stories about tribal spirits. Without an ability to compose fiction, Neanderthals were unable to cooperate effectively in large numbers, nor could they adapt their social behaviour to rapidly changing challenges.
While we can’t get inside a Neanderthal mind to understand how they thought, we have indirect evidence of the limits to their cognition compared with their Sapiens rivals. Archaeologists excavating 30,000-year-old Sapiens sites in the European heartland occasionally find there seashells from the Mediterranean and Atlantic coasts. In all likelihood, these shells got to the continental interior through long-distance trade between different Sapiens bands. Neanderthal sites lack any evidence of such trade. Each group manufactured its own tools from local materials.4

6. The Catholic alpha male abstains from sexual intercourse and childcare, even though there is no genetic or ecological reason for him to do so.

Another example comes from the South Pacific. Sapiens bands that lived on the island of New Ireland, north of New Guinea, used a volcanic glass called obsidian to manufacture particularly strong and sharp tools. New Ireland, however, has no natural deposits of obsidian. Laboratory tests revealed that the obsidian they used was brought from deposits on New Britain, an island 400 kilometres away. Some of the inhabitants of these islands must have been skilled navigators who traded from island to island over long distances.5
Trade may seem a very pragmatic activity, one that needs no fictive basis. Yet the fact is that no animal other than Sapiens engages in trade, and all the Sapiens trade neworks about which we have detailed evidence were based on fictions. Trade cannot exist without trust, and it is very difficult to trust strangers. The global trade network of today is based on our trust in such fictional entities as the dollar, the Federal Reserve Bank, and the totemic trademarks of corporations. When two strangers in a tribal society want to trade, they will often establish trust by appealing to a common god, mythical ancestor or totem animal.
If archaic Sapiens believing in such fictions traded shells and obsidian, it stands to reason that they could also have traded information, thus creating a much denser and wider knowledge network than the one that served Neanderthals and other archaic humans.
Hunting techniques provide another illustration of these differences. Neanderthals usually hunted alone or in small groups. Sapiens, on the other hand, developed techniques that relied on cooperation between many dozens of individuals, and perhaps even between different bands. One particularly effective method was to surround an entire herd of animals, such as wild horses, then chase them into a narrow gorge, where it was easy to slaughter them en masse. If all went according to plan, the bands could harvest tons of meat, fat and animal skins in a single afternoon of collective effort, and either consume these riches in a giant potlatch, or dry, smoke or (in Arctic areas) freeze them for later usage. Archaeologists have discovered sites where entire herds were butchered annually in such ways. There are even sites where fences and obstacles were erected in order to create artificial traps and slaughtering grounds.
We may presume that Neanderthals were not pleased to see their traditional hunting grounds turned into Sapiens-controlled slaughterhouses. However, if violence broke out between the two species, Neanderthals were not much better off than wild horses. Fifty Neanderthals cooperating in traditional and static patterns were no match for 500 versatile and innovative Sapiens. And even if the Sapiens lost the first round, they could quickly invent new stratagems that would enable them to win the next time.
What happened in the Cognitive Revolution?

New ability	Wider consequences
The ability to transmit larger quantities of information about the world surrounding Homo sapiens	Planning and carrying out complex actions, such as avoiding lions and hunting bison
The ability to transmit larger quantities of information about Sapiens social relationships	Larger and more cohesive groups, numbering up to 150 individuals
The ability to transmit information about things that do not really exist, such as tribal spirits, nations, limited liability companies, and human rights	a. Cooperation between very large numbers of strangers
b. Rapid innovation of social behaviour
History and Biology

The immense diversity of imagined realities that Sapiens invented, and the resulting diversity of behaviour patterns, are the main components of what we call ‘cultures’. Once cultures appeared, they never ceased to change and develop, and these unstoppable alterations are what we call ‘history’.
The Cognitive Revolution is accordingly the point when history declared its independence from biology. Until the Cognitive Revolution, the doings of all human species belonged to the realm of biology, or, if you so prefer, prehistory (I tend to avoid the term ‘prehistory’, because it wrongly implies that even before the Cognitive Revolution, humans were in a category of their own). From the Cognitive Revolution onwards, historical narratives replace biological theories as our primary means of explaining the development of Homo sapiens. To understand the rise of Christianity or the French Revolution, it is not enough to comprehend the interaction of genes, hormones and organisms. It is necessary to take into account the interaction of ideas, images and fantasies as well.
This does not mean that Homo sapiens and human culture became exempt from biological laws. We are still animals, and our physical, emotional and cognitive abilities are still shaped by our DNA. Our societies are built from the same building blocks as Neanderthal or chimpanzee societies, and the more we examine these building blocks – sensations, emotions, family ties – the less difference we find between us and other apes.
It is, however, a mistake to look for the differences at the level of the individual or the family. One on one, even ten on ten, we are embarrassingly similar to chimpanzees. Significant differences begin to appear only when we cross the threshold of 150 individuals, and when we reach 1,000–2,000 individuals, the differences are astounding. If you tried to bunch together thousands of chimpanzees into Tiananmen Square, Wall Street, the Vatican or the headquarters of the United Nations, the result would be pandemonium. By contrast, Sapiens regularly gather by the thousands in such places. Together, they create orderly patterns – such as trade networks, mass celebrations and political institutions – that they could never have created in isolation. The real difference between us and chimpanzees is the mythical glue that binds together large numbers of individuals, families and groups. This glue has made us the masters of creation.
Of course, we also needed other skills, such as the ability to make and use tools. Yet tool-making is of little consequence unless it is coupled with the ability to cooperate with many others. How is it that we now have intercontinental missiles with nuclear warheads, whereas 30,000 years ago we had only sticks with flint spearheads? Physiologically, there has been no significant improvement in our tool-making capacity over the last 30,000 years. Albert Einstein was far less dexterous with his hands than was an ancient hunter-gatherer. However, our capacity to cooperate with large numbers of strangers has improved dramatically. The ancient flint spearhead was manufactured in minutes by a single person, who relied on the advice and help of a few intimate friends. The production of a modern nuclear warhead requires the cooperation of millions of strangers all over the world – from the workers who mine the uranium ore in the depths of the earth to theoretical physicists who write long mathematical formulas to describe the interactions of subatomic particles.
To summarise the relationship between biology and history after the Cognitive Revolution:
a. Biology sets the basic parameters for the behaviour and capacities of Homo sapiens. The whole of history takes place within the bounds of this biological arena.
b. However, this arena is extraordinarily large, allowing Sapiens to play an astounding variety of games. Thanks to their ability to invent fiction, Sapiens create more and more complex games, which each generation develops and elaborates even further.
c. Consequently, in order to understand how Sapiens behave, we must describe the historical evolution of their actions. Referring only to our biological constraints would be like a radio sports-caster who, attending the World Cup football championships, offers his listeners a detailed description of the playing field rather than an account of what the players are doing.
What games did our Stone Age ancestors play in the arena of history? As far as we know, the people who carved the Stadel lion-man some 30,000 years ago had the same physical, emotional and intellectual abilities we have. What did they do when they woke up in the morning? What did they eat for breakfast – and lunch? What were their societies like? Did they have monogamous relationships and nuclear families? Did they have ceremonies, moral codes, sports contests and religious rituals? Did they fight wars? The next chapter takes a peek behind the curtain of the ages, examining what life was like in the millennia separating the Cognitive Revolution from the Agricultural Revolution.
* Here and in the following pages, when speaking about Sapiens language, I refer to the basic linguistic abilities of our species and not to a particular dialect. English, Hindi and Chinese are all variants of Sapiens language. Apparently, even at the time of the Cognitive Revolution, different Sapiens groups had different dialects.3

A Day in the Life of Adam and Eve

TO UNDERSTAND OUR NATURE, HISTORY and psychology, we must get inside the heads of our hunter-gatherer ancestors. For nearly the entire history of our species, Sapiens lived as foragers. The past 200 years, during which ever increasing numbers of Sapiens have obtained their daily bread as urban labourers and office workers, and the preceding 10,000 years, during which most Sapiens lived as farmers and herders, are the blink of an eye compared to the tens of thousands of years during which our ancestors hunted and gathered.
The flourishing field of evolutionary psychology argues that many of our present-day social and psychological characteristics were shaped during this long pre-agricultural era. Even today, scholars in this field claim, our brains and minds are adapted to a life of hunting and gathering. Our eating habits, our conflicts and our sexuality are all the result of the way our hunter-gatherer minds interact with our current post-industrial environment, with its mega-cities, aeroplanes, telephones and computers. This environment gives us more material resources and longer lives than those enjoyed by any previous generation, but it often makes us feel alienated, depressed and pressured. To understand why, evolutionary psychologists argue, we need to delve into the hunter-gatherer world that shaped us, the world that we subconsciously still inhabit.
Why, for example, do people gorge on high-calorie food that is doing little good to their bodies? Today’s affluent societies are in the throes of a plague of obesity, which is rapidly spreading to developing countries. It’s a puzzle why we binge on the sweetest and greasiest food we can find, until we consider the eating habits of our forager forebears. In the savannahs and forests they inhabited, high-calorie sweets were extremely rare and food in general was in short supply. A typical forager 30,000 years ago had access to only one type of sweet food – ripe fruit. If a Stone Age woman came across a tree groaning with figs, the most sensible thing to do was to eat as many of them as she could on the spot, before the local baboon band picked the tree bare. The instinct to gorge on high-calorie food was hard-wired into our genes. Today we may be living in high-rise apartments with over-stuffed refrigerators, but our DNA still thinks we are in the savannah. That’s what makes us spoon down an entire tub of Ben & Jerry’s when we find one in the freezer and wash it down with a jumbo Coke.
This ‘gorging gene’ theory is widely accepted. Other theories are far more contentious. For example, some evolutionary psychologists argue that ancient foraging bands were not composed of nuclear families centred on monogamous couples. Rather, foragers lived in communes devoid of private property, monogamous relationships and even fatherhood. In such a band, a woman could have sex and form intimate bonds with several men (and women) simultaneously, and all of the band’s adults cooperated in parenting its children. Since no man knew definitively which of the children were his, men showed equal concern for all youngsters.
Such a social structure is not an Aquarian utopia. It’s well documented among animals, notably our closest relatives, the chimpanzees and bonobos. There are even a number of present-day human cultures in which collective fatherhood is practised, as for example among the Barí Indians. According to the beliefs of such societies, a child is not born from the sperm of a single man, but from the accumulation of sperm in a woman’s womb. A good mother will make a point of having sex with several different men, especially when she is pregnant, so that her child will enjoy the qualities (and paternal care) not merely of the best hunter, but also of the best storyteller, the strongest warrior and the most considerate lover. If this sounds silly, bear in mind that before the development of modern embryological studies, people had no solid evidence that babies are always sired by a single father rather than by many.
The proponents of this ‘ancient commune’ theory argue that the frequent infidelities that characterise modern marriages, and the high rates of divorce, not to mention the cornucopia of psychological complexes from which both children and adults suffer, all result from forcing humans to live in nuclear families and monogamous relationships that are incompatible with our biological software.1
Many scholars vehemently reject this theory, insisting that both monogamy and the forming of nuclear families are core human behaviours. Though ancient hunter-gatherer societies tended to be more communal and egalitarian than modern societies, these researchers argue, they were nevertheless comprised of separate cells, each containing a jealous couple and the children they held in common. This is why today monogamous relationships and nuclear families are the norm in the vast majority of cultures, why men and women tend to be very possessive of their partners and children, and why even in modern states such as North Korea and Syria political authority passes from father to son.
In order to resolve this controversy and understand our sexuality, society and politics, we need to learn something about the living conditions of our ancestors, to examine how Sapiens lived between the Cognitive Revolution of 70,000 years ago, and the start of the Agricultural Revolution about 12,000 years ago.
Unfortunately, there are few certainties regarding the lives of our forager ancestors. The debate between the ‘ancient commune’ and ‘eternal monogamy schools is based on flimsy evidence. We obviously have no written records from the age of foragers, and the archaeological evidence consists mainly of fossilised bones and stone tools. Artefacts made of more perishable materials – such as wood, bamboo or leather – survive only under unique conditions. The common impression that pre-agricultural humans lived in an age of stone is a misconception based on this archaeological bias. The Stone Age should more accurately be called the Wood Age, because most of the tools used by ancient hunter-gatherers were made of wood.
Any reconstruction of the lives of ancient hunter-gatherers from the surviving artefacts is extremely problematic. One of the most glaring differences between the ancient foragers and their agricultural and industrial descendants is that foragers had very few artefacts to begin with, and these played a comparatively modest role in their lives. Over the course of his or her life, a typical member of a modern affluent society will own several million artefacts – from cars and houses to disposable nappies and milk cartons. There’s hardly an activity, a belief, or even an emotion that is not mediated by objects of our own devising. Our eating habits are mediated by a mind-boggling collection of such items, from spoons and glasses to genetic engineering labs and gigantic ocean-going ships. In play, we use a plethora of toys, from plastic cards to 100,000-seater stadiums. Our romantic and sexual relations are accoutred by rings, beds, nice clothes, sexy underwear, condoms, fashionable restaurants, cheap motels, airport lounges, wedding halls and catering companies. Religions bring the sacred into our lives with Gothic churches, Muslim mosques, Hindu ashrams, Torah scrolls, Tibetan prayer wheels, priestly cassocks, candles, incense, Christmas trees, matzah balls, tombstones and icons.
We hardly notice how ubiquitous our stuff is until we have to move it to a new house. Foragers moved house every month, every week, and sometimes even every day, toting whatever they had on their backs. There were no moving companies, wagons, or even pack animals to share the burden. They consequently had to make do with only the most essential possessions. It’s reasonable to presume, then, that the greater part of their mental, religious and emotional lives was conducted without the help of artefacts. An archaeologist working 100,000 years from now could piece together a reasonable picture of Muslim belief and practice from the myriad objects he unearthed in a ruined mosque. But we are largely at a loss in trying to comprehend the beliefs and rituals of ancient hunter-gatherers. It’s much the same dilemma that a future historian would face if he had to depict the social world of twenty-first-century teenagers solely on the basis of their surviving snail mail – since no records will remain of their phone conversations, emails, blogs and text messages.
A reliance on artefacts will thus bias an account of ancient hunter-gatherer life. One way to remedy this is to look at modern forager societies. These can be studied directly, by anthropological observation. But there are good reasons to be very careful in extrapolating from modern forager societies to ancient ones.
Firstly, all forager societies that have survived into the modern era have been influenced by neighbouring agricultural and industrial societies. Consequently, it’s risky to assume that what is true of them was also true tens of thousands of years ago.
Secondly, modern forager societies have survived mainly in areas with difficult climatic conditions and inhospitable terrain, ill-suited for agriculture. Societies that have adapted to the extreme conditions of places such as the Kalahari Desert in southern Africa may well provide a very misleading model for understanding ancient societies in fertile areas such as the Yangtze River Valley. In particular, population density in an area like the Kalahari Desert is far lower than it was around the ancient Yangtze, and this has far-reaching implications for key questions about the size and structure of human bands and the relations between them.
Thirdly, the most notable characteristic of hunter-gatherer societies is how different they are one from the other. They differ not only from one part of the world to another but even in the same region. One good example is the huge variety the first European settlers found among the Aborigine peoples of Australia. Just before the British conquest, between 300,000 and 700,000 hunter-gatherers lived on the continent in 200–600 tribes, each of which was further divided into several bands.2 Each tribe had its own language, religion, norms and customs. Living around what is now Adelaide in southern Australia were several patrilineal clans that reckoned descent from the father’s side. These clans bonded together into tribes on a strictly territorial basis. In contrast, some tribes in northern Australia gave more importance to a person’s maternal ancestry, and a person’s tribal identity depended on his or her totem rather than his territory.
It stands to reason that the ethnic and cultural variety among ancient hunter-gatherers was equally impressive, and that the 5 million to 8 million foragers who populated the world on the eve of the Agricultural Revolution were divided into thousands of separate tribes with thousands of different languages and cultures.3 This, after all, was one of the main legacies of the Cognitive Revolution. Thanks to the appearance of fiction, even people with the same genetic make-up who lived under similar ecological conditions were able to create very different imagined realities, which manifested themselves in different norms and values.
For example, there’s every reason to believe that a forager band that lived 30,000 years ago on the spot where Oxford University now stands would have spoken a different language from one living where Cambridge is now situated. One band might have been belligerent and the other peaceful. Perhaps the Cambridge band was communal while the one at Oxford was based on nuclear families. The Cantabrigians might have spent long hours carving wooden statues of their guardian spirits, whereas the Oxonians may have worshipped through dance. The former perhaps believed in reincarnation, while the latter thought this was nonsense. In one society, homosexual relationships might have been accepted, while in the other they were taboo.
In other words, while anthropological observations of modern foragers can help us understand some of the possibilities available to ancient foragers, the ancient horizon of possibilities was much broader, and most of it is hidden from our view.* The heated debates about Homo sapiens’ ‘natural way of life’ miss the main point. Ever since the Cognitive Revolution, there hasn’t been a single natural way of life for Sapiens. There are only cultural choices, from among a bewildering palette of possibilities.
The Original Affluent Society

What generalisations can we make about life in the pre-agricultural world nevertheless? It seems safe to say that the vast majority of people lived in small bands numbering several dozen or at most several hundred individuals, and that all these individuals were humans. It is important to note this last point, because it is far from obvious. Most members of agricultural and industrial societies are domesticated animals. They are not equal to their masters, of course, but they are members all the same. Today, the society called New Zealand is composed of 4.5 million Sapiens and 50 million sheep.
There was just one exception to this general rule: the dog. The dog was the first animal domesticated by Homo sapiens, and this occurred before the Agricultural Revolution. Experts disagree about the exact date, but we have incontrovertible evidence of domesticated dogs from about 15,000 years ago. They may have joined the human pack thousands of years earlier.
Dogs were used for hunting and fighting, and as an alarm system against wild beasts and human intruders. With the passing of generations, the two species co-evolved to communicate well with each other. Dogs that were most attentive to the needs and feelings of their human companions got extra care and food, and were more likely to survive. Simultaneously, dogs learned to manipulate people for their own needs. A 15,000-year bond has yielded a much deeper understanding and affection between humans and dogs than between humans and any other animal.4 In some cases dead dogs were even buried ceremoniously, much like humans.
Members of a band knew each other very intimately, and were surrounded throughout their lives by friends and relatives. Loneliness and privacy were rare. Neighbouring bands probably competed for resources and even fought one another, but they also had friendly contacts. They exchanged members, hunted together, traded rare luxuries, cemented political alliances and celebrated religious festivals. Such cooperation was one of the important trademarks of Homo sapiens, and gave it a crucial edge over other human species. Sometimes relations with neighbouring bands were tight enough that together they constituted a single tribe, sharing a common language, common myths, and common norms and values.
Yet we should not overestimate the importance of such external relations. Even if in times of crisis neighbouring bands drew closer together, and even if they occasionally gathered to hunt or feast together, they still spent the vast majority of their time in complete isolation and independence. Trade was mostly limited to prestige items such as shells, amber and pigments. There is no evidence that people traded staple goods like fruits and meat, or that the existence of one band depended on the importing of goods from another. Sociopolitical relations, too, tended to be sporadic. The tribe did not serve as a permanent political framework, and even if it had seasonal meeting places, there were no permanent towns or institutions. The average person lived many months without seeing or hearing a human from outside of her own band, and she encountered throughout her life no more than a few hundred humans. The Sapiens population was thinly spread over vast territories. Before the Agricultural Revolution, the human population of the entire planet was smaller than that of today’s Cairo.

7. First pet? A 12,000-year-old tomb found in northern Israel. It contains the skeleton of a fifty-year-old woman next to that of a puppy (bottom left corner). The puppy was buried close to the woman’s head. Her left hand is resting on the dog in a way that might indicate an emotional connection. There are, of course, other possible explanations. Perhaps, for example, the puppy was a gift to the gatekeeper of the next world.

Most Sapiens bands lived on the road, roaming from place to place in search of food. Their movements were influenced by the changing seasons, the annual migrations of animals and the growth cycles of plants. They usually travelled back and forth across the same home territory, an area of between several dozen and many hundreds of square kilometres.
Occasionally, bands wandered outside their turf and explored new lands, whether due to natural calamities, violent conflicts, demographic pressures or the initiative of a charismatic leader. These wanderings were the engine of human worldwide expansion. If a forager band split once every forty years and its splinter group migrated to a new territory a hundred kilometres to the east, the distance from East Africa to China would have been covered in about 10,000 years.
In some exceptional cases, when food sources were particularly rich, bands settled down in seasonal and even permanent camps. Techniques for drying, smoking and freezing food also made it possible to stay put for longer periods. Most importantly, alongside seas and rivers rich in seafood and waterfowl, humans set up permanent fishing villages – the first permanent settlements in history, long predating the Agricultural Revolution. Fishing villages might have appeared on the coasts of Indonesian islands as early as 45,000 years ago. These may have been the base from which Homo sapiens launched its first transoceanic enterprise: the invasion of Australia.
In most habitats, Sapiens bands fed themselves in an elastic and opportunistic fashion. They scrounged for termites, picked berries, dug for roots, stalked rabbits and hunted bison and mammoth. Notwithstanding the popular image of ‘man the hunter’, gathering was Sapiens’ main activity, and it provided most of their calories, as well as raw materials such as flint, wood and bamboo.
Sapiens did not forage only for food and materials. They foraged for knowledge as well. To survive, they needed a detailed mental map of their territory. To maximise the efficiency of their daily search for food, they required information about the growth patterns of each plant and the habits of each animal. They needed to know which foods were nourishing, which made you sick, and how to use others as cures. They needed to know the progress of the seasons and what warning signs preceded a thunderstorm or a dry spell. They studied every stream, every walnut tree, every bear cave, and every flint-stone deposit in their vicinity. Each individual had to understand how to make a stone knife, how to mend a torn cloak, how to lay a rabbit trap, and how to face avalanches, snakebites or hungry lions. Mastery of each of these many skills required years of apprenticeship and practice. The average ancient forager could turn a flint stone into a spear point within minutes. When we try to imitate this feat, we usually fail miserably. Most of us lack expert knowledge of the flaking properties of flint and basalt and the fine motor skills needed to work them precisely.
In other words, the average forager had wider, deeper and more varied knowledge of her immediate surroundings than most of her modern descendants. Today, most people in industrial societies don’t need to know much about the natural world in order to survive. What do you really need to know in order to get by as a computer engineer, an insurance agent, a history teacher or a factory worker? You need to know a lot about your own tiny field of expertise, but for the vast majority of life’s necessities you rely blindly on the help of other experts, whose own knowledge is also limited to a tiny field of expertise. The human collective knows far more today than did the ancient bands. But at the individual level, ancient foragers were the most knowledgeable and skilful people in history.
There is some evidence that the size of the average Sapiens brain has actually decreased since the age of foraging.5 Survival in that era required superb mental abilities from everyone. When agriculture and industry came along people could increasingly rely on the skills of others for survival, and new ‘niches for imbeciles’ were opened up. You could survive and pass your unremarkable genes to the next generation by working as a water carrier or an assembly-line worker.
Foragers mastered not only the surrounding world of animals, plants and objects, but also the internal world of their own bodies and senses. They listened to the slightest movement in the grass to learn whether a snake might be lurking there. They carefully observed the foliage of trees in order to discover fruits, beehives and bird nests. They moved with a minimum of effort and noise, and knew how to sit, walk and run in the most agile and efficient manner. Varied and constant use of their bodies made them as fit as marathon runners. They had physical dexterity that people today are unable to achieve even after years of practising yoga or t’ai chi.
The hunter-gatherer way of life differed significantly from region to region and from season to season, but on the whole foragers seem to have enjoyed a more comfortable and rewarding lifestyle than most of the peasants, shepherds, labourers and office clerks who followed in their footsteps.
While people in today’s affluent societies work an average of forty to forty-five hours a week, and people in the developing world work sixty and even eighty hours a week, hunter-gatherers living today in the most inhospitable of habitats – such as the Kalahari Desert work on average for just thirty-five to forty-five hours a week. They hunt only one day out of three, and gathering takes up just three to six hours daily. In normal times, this is enough to feed the band. It may well be that ancient hunter-gatherers living in zones more fertile than the Kalahari spent even less time obtaining food and raw materials. On top of that, foragers enjoyed a lighter load of household chores. They had no dishes to wash, no carpets to vacuum, no floors to polish, no nappies to change and no bills to pay.
The forager economy provided most people with more interesting lives than agriculture or industry do. Today, a Chinese factory hand leaves home around seven in the morning, makes her way through polluted streets to a sweatshop, and there operates the same machine, in the same way, day in, day out, for ten long and mind-numbing hours, returning home around seven in the evening in order to wash dishes and do the laundry. Thirty thousand years ago, a Chinese forager might leave camp with her companions at, say, eight in the morning. They’d roam the nearby forests and meadows, gathering mushrooms, digging up edible roots, catching frogs and occasionally running away from tigers. By early afternoon, they were back at the camp to make lunch. That left them plenty of time to gossip, tell stories, play with the children and just hang out. Of course the tigers sometimes caught them, or a snake bit them, but on the other hand they didn’t have to deal with automobile accidents and industrial pollution.
In most places and at most times, foraging provided ideal nutrition. That is hardly surprising – this had been the human diet for hundreds of thousands of years, and the human body was well adapted to it. Evidence from fossilised skeletons indicates that ancient foragers were less likely to suffer from starvation or malnutrition, and were generally taller and healthier than their peasant descendants. Average life expectancy was apparently just thirty to forty years, but this was due largely to the high incidence of child mortality. Children who made it through the perilous first years had a good chance of reaching the age of sixty, and some even made it to their eighties. Among modern foragers, forty-five-year-old women can expect to live another twenty years, and about 5–8 per cent of the population is over sixty.6
The foragers’ secret of success, which protected them from starvation and malnutrition, was their varied diet. Farmers tend to eat a very limited and unbalanced diet. Especially in premodern times, most of the calories feeding an agricultural population came from a single crop – such as wheat, potatoes or rice – that lacks some of the vitamins, minerals and other nutritional materials humans need. The typical peasant in traditional China ate rice for breakfast, rice for lunch, and rice for dinner. If she were lucky, she could expect to eat the same on the following day. By contrast, ancient foragers regularly ate dozens of different foodstuffs. The peasant’s ancient ancestor, the forager, may have eaten berries and mushrooms for breakfast; fruits, snails and turtle for lunch; and rabbit steak with wild onions for dinner. Tomorrows menu might have been completely different. This variety ensured that the ancient foragers received all the necessary nutrients.
Furthermore, by not being dependent on any single kind of food, they were less liable to suffer when one particular food source failed. Agricultural societies are ravaged by famine when drought, fire or earthquake devastates the annual rice or potato crop. Forager societies were hardly immune to natural disasters, and suffered from periods of want and hunger, but they were usually able to deal with such calamities more easily. If they lost some of their staple foodstuffs, they could gather or hunt other species, or move to a less affected area.
Ancient foragers also suffered less from infectious diseases. Most of the infectious diseases that have plagued agricultural and industrial societies (such as smallpox, measles and tuberculosis) originated in domesticated animals and were transferred to humans only after the Agricultural Revolution. Ancient foragers, who had domesticated only dogs, were free of these scourges. Moreover, most people in agricultural and industrial societies lived in dense, unhygienic permanent settlements – ideal hotbeds for disease. Foragers roamed the land in small bands that could not sustain epidemics.
The wholesome and varied diet, the relatively short working week, and the rarity of infectious diseases have led many experts to define pre-agricultural forager societies as ‘the original affluent societies’. It would be a mistake, however, to idealise the lives of these ancients. Though they lived better lives than most people in agricultural and industrial societies, their world could still be harsh and unforgiving. Periods of want and hardship were not uncommon, child mortality was high, and an accident which would be minor today could easily become a death sentence. Most people probably enjoyed the close intimacy of the roaming band, but those unfortunates who incurred the hostility or mockery of their fellow band members probably suffered terribly. Modern foragers occasionally abandon and even kill old or disabled people who cannot keep up with the band. Unwanted babies and children may be slain, and there are even cases of religiously inspired human sacrifice.
The Aché people, hunter-gatherers who lived in the jungles of Paraguay until the 1960s, offer a glimpse into the darker side of foraging. When a valued band member died, the Aché customarily killed a little girl and buried the two together. Anthropologists who interviewed the Aché recorded a case in which a band abandoned a middle-aged man who fell sick and was unable to keep up with the others. He was left under a tree. Vultures perched above him, expecting a hearty meal. But the man recuperated, and, walking briskly, he managed to rejoin the band. His body was covered with the birds’ faeces, so he was henceforth nicknamed ‘Vulture Droppings’.
When an old Aché woman became a burden to the rest of the band, one of the younger men would sneak behind her and kill her with an axe-blow to the head. An Aché man told the inquisitive anthropologists stories of his prime years in the jungle. ‘I customarily killed old women. I used to kill my aunts … The women were afraid of me … Now, here with the whites, I have become weak.’ Babies born without hair, who were considered underdeveloped, were killed immediately. One woman recalled that her first baby girl was killed because the men in the band did not want another girl. On another occasion a man killed a small boy because he was ‘in a bad mood and the child was crying’. Another child was buried alive because ‘it was funny-looking and the other children laughed at it’.7
We should be careful, though, not to judge the Aché too quickly. Anthropologists who lived with them for years report that violence between adults was very rare. Both women and men were free to change partners at will. They smiled and laughed constantly, had no leadership hierarchy, and generally shunned domineering people. They were extremely generous with their few possessions, and were not obsessed with success or wealth. The things they valued most in life were good social interactions and high-quality friendships.8 They viewed the killing of children, sick people and the elderly as many people today view abortion and euthanasia. It should also be noted that the Aché were hunted and killed without mercy by Paraguayan farmers. The need to evade their enemies probably caused the Aché to adopt an exceptionally harsh attitude towards anyone who might become a liability to the band.
The truth is that Aché society, like every human society, was very complex. We should beware of demonising or idealising it on the basis of a superficial acquaintance. The Aché were neither angels nor fiends – they were humans. So, too, were the ancient hunter-gatherers.
Talking Ghosts

What can we say about the spiritual and mental life of the ancient hunter-gatherers? The basics of the forager economy can be reconstructed with some confidence based on quantifiable and objective factors. For example, we can calculate how many calories per day a person needed in order to survive, how many calories were obtained from a kilogram of walnuts, and how many walnuts could be gathered from a square kilometre of forest. With this data, we can make an educated guess about the relative importance of walnuts in their diet.
But did they consider walnuts a delicacy or a humdrum staple? Did they believe that walnut trees were inhabited by spirits? Did they find walnut leaves pretty? If a forager boy wanted to take a forager girl to a romantic spot, did the shade of a walnut tree suffice? The world of thought, belief and feeling is by definition far more difficult to decipher.
Most scholars agree that animistic beliefs were common among ancient foragers. Animism (from ‘anima’, ‘soul’ or ‘spirit’ in Latin) is the belief that almost every place, every animal, every plant and every natural phenomenon has awareness and feelings, and can communicate directly with humans. Thus, animists may believe that the big rock at the top of the hill has desires and needs. The rock might be angry about something that people did and rejoice over some other action. The rock might admonish people or ask for favours. Humans, for their part, can address the rock, to mollify or threaten it. Not only the rock, but also the oak tree at the bottom of the hill is an animated being, and so is the stream flowing below the hill, the spring in the forest clearing, the bushes growing around it, the path to the clearing, and the field mice, wolves and crows that drink there. In the animist world, objects and living things are not the only animated beings. There are also immaterial entities – the spirits of the dead, and friendly and malevolent beings, the kind that we today call demons, fairies and angels.
Animists believe that there is no barrier between humans and other beings. They can all communicate directly through speech, song, dance and ceremony. A hunter may address a herd of deer and ask that one of them sacrifice itself. If the hunt succeeds, the hunter may ask the dead animal to forgive him. When someone falls sick, a shaman can contact the spirit that caused the sickness and try to pacify it or scare it away. If need be, the shaman may ask for help from other spirits. What characterises all these acts of communication is that the entities being addressed are local beings. They are not universal gods, but rather a particular deer, a particular tree, a particular stream, a particular ghost.
Just as there is no barrier between humans and other beings, neither is there a strict hierarchy. Non-human entities do not exist merely to provide for the needs of man. Nor are they all-powerful gods who run the world as they wish. The world does not revolve around humans or around any other particular group of beings.
Animism is not a specific religion. It is a generic name for thousands of very different religions, cults and beliefs. What makes all of them ‘animist’ is this common approach to the world and to man’s place in it. Saying that ancient foragers were probably animists is like saying that premodern agriculturists were mostly theists. Theism (from ‘theos’, ‘god’ in Greek) is the view that the universal order is based on a hierarchical relationship between humans and a small group of ethereal entities called gods. It is certainly true to say that premodern agriculturists tended to be theists, but it does not teach us much about the particulars. The generic rubric ‘theists’ covers Jewish rabbis from eighteenth-century Poland, witch-burning Puritans from seventeenth-century Massachusetts, Aztec priests from fifteenth-century Mexico, Sufi mystics from twelfth-century Iran, tenth-century Viking warriors, second-century Roman legionnaires, and first-century Chinese bureaucrats. Each of these viewed the others’ beliefs and practices as weird and heretical. The differences between the beliefs and practices of groups of ‘animistic’ foragers were probably just as big. Their religious experience may have been turbulent and filled with controversies, reforms and revolutions.
But these cautious generalisations are about as far as we can go. Any attempt to describe the specifics of archaic spirituality is highly speculative, as there is next to no evidence to go by and the little evidence we have – a handful of artefacts and cave paintings – can be interpreted in myriad ways. The theories of scholars who claim to know what the foragers felt shed much more light on the prejudices of their authors than on Stone Age religions.
Instead of erecting mountains of theory over a molehill of tomb relics, cave paintings and bone statuettes, it is better to be frank and admit that we have only the haziest notions about the religions of ancient foragers. We assume that they were animists, but that’s not very informative. We don’t know which spirits they prayed to, which festivals they celebrated, or which taboos they observed. Most importantly, we don’t know what stories they told. It’s one of the biggest holes in our understanding of human history.
The sociopolitical world of the foragers is another area about which we know next to nothing. As explained above, scholars cannot even agree on the basics, such as the existence of private property, nuclear families and monogamous relationships. It’s likely that different bands had different structures. Some may have been as hierarchical, tense and violent as the nastiest chimpanzee group, while others were as laid-back, peaceful and lascivious as a bunch of bonobos.

8. A painting from Lascaux Cave, c.15,000–20,000 years ago. What exactly do we see, and what is the painting’s meaning? Some argue that we see a man with the head of a bird and an erect penis, being killed by a bison. Beneath the man is another bird which might symbolise the soul, released from the body at the moment of death. If so, the picture depicts not a prosaic hunting accident, but rather the passage from this world to the next. But we have no way of knowing whether any of these speculations are true. It’s a Rorschach test that reveals much about the preconceptions of modern scholars, and little about the beliefs of ancient foragers.

In Sungir, Russia, archaeologists discovered in 1955 a 30,000-year-old burial site belonging to a mammoth-hunting culture. In one grave they found the skeleton of a fifty-year-old man, covered with strings of mammoth ivory beads, containing about 3,000 beads in total. On the dead man’s head was a hat decorated with fox teeth, and on his wrists twenty-five ivory bracelets. Other graves from the same site contained far fewer goods. Scholars deduced that the Sungir mammoth-hunters lived in a hierarchical society, and that the dead man was perhaps the leader of a band or of an entire tribe comprising several bands. It is unlikely that a few dozen members of a single band could have produced so many grave goods by themselves.

9. Hunter-gatherers made these handprints about 9,000 years ago in the ‘Hands Cave’, in Argentina. It looks as if these long-dead hands are reaching towards us from within the rock. This is one of the most moving relics of the ancient forager world – but nobody knows what it means.

Archaeologists then discovered an even more interesting tomb. It contained two skeletons, buried head to head. One belonged to a boy aged about twelve or thirteen, and the other to a girl of about nine or ten. The boy was covered with 5,000 ivory beads. He wore a fox-tooth hat and a belt with 250 fox teeth (at least sixty foxes had to have their teeth pulled to get that many). The girl was adorned with 5,250 ivory beads. Both children were surrounded by statuettes and various ivory objects. A skilled craftsman (or craftswoman) probably needed about forty-five minutes to prepare a single ivory bead. In other words, fashioning the 10,000 ivory beads that covered the two children, not to mention the other objects, required some 7,500 hours of delicate work, well over three years of labour by an experienced artisan!
It is highly unlikely that at such a young age the Sungir children had proved themselves as leaders or mammoth-hunters. Only cultural beliefs can explain why they received such an extravagant burial. One theory is that they owed their rank to their parents. Perhaps they were the children of the leader, in a culture that believed in either family charisma or strict rules of succession. According to a second theory, the children had been identified at birth as the incarnations of some long-dead spirits. A third theory argues that the children’s burial reflects the way they died rather than their status in life. They were ritually sacrificed – perhaps as part of the burial rites of the leader – and then entombed with pomp and circumstance.9
Whatever the correct answer, the Sungir children are among the best pieces of evidence that 30,000 years ago Sapiens could invent sociopolitical codes that went far beyond the dictates of our DNA and the behaviour patterns of other human and animal species.
Peace or War?

Finally, there’s the thorny question of the role of war in forager societies. Some scholars imagine ancient hunter-gatherer societies as peaceful paradises, and argue that war and violence began only with the Agricultural Revolution, when people started to accumulate private property. Other scholars maintain that the world of the ancient foragers was exceptionally cruel and violent. Both schools of thought are castles in the air, connected to the ground by the thin strings of meagre archaeological remains and anthropological observations of present-day foragers.
The anthropological evidence is intriguing but very problematic. Foragers today live mainly in isolated and inhospitable areas such as the Arctic or the Kalahari, where population density is very low and opportunities to fight other people are limited. Moreover, in recent generations, foragers have been increasingly subject to the authority of modern states, which prevent the eruption of large-scale conflicts. European scholars have had only two opportunities to observe large and relatively dense populations of independent foragers: in north-western North America in the nineteenth century, and in northern Australia during the nineteenth and early twentieth centuries. Both Amerindian and Aboriginal Australian cultures witnessed frequent armed conflicts. It is debatable, however, whether this represents a ‘timeless’ condition or the impact of European imperialism.
The archaeological findings are both scarce and opaque. What telltale clues might remain of any war that took place tens of thousands of years ago? There were no fortifications and walls back then, no artillery shells or even swords and shields. An ancient spear point might have been used in war, but it could have been used in a hunt as well. Fossilised human bones are no less hard to interpret. A fracture might indicate a war wound or an accident. Nor is the absence of fractures and cuts on an ancient skeleton conclusive proof that the person to whom the skeleton belonged did not die a violent death. Death can be caused by trauma to soft tissues that leaves no marks on bone. Even more importantly, during pre-industrial warfare more than 90 per cent of war dead were killed by starvation, cold and disease rather than by weapons. Imagine that 30,000 years ago one tribe defeated its neighbour and expelled it from coveted foraging grounds. In the decisive battle, ten members of the defeated tribe were killed. In the following year, another hundred members of the losing tribe died from starvation, cold and disease. Archaeologists who come across these no skeletons may too easily conclude that most fell victim to some natural disaster. How would we be able to tell that they were all victims of a merciless war?
Duly warned, we can now turn to the archaeological findings. In Portugal, a survey was made of 400 skeletons from the period immediately predating the Agricultural Revolution. Only two skeletons showed clear marks of violence. A similar survey of 400 skeletons from the same period in Israel discovered a single crack in a single skull that could be attributed to human violence. A third survey of 400 skeletons from various pre-agricultural sites in the Danube Valley found evidence of violence on eighteen skeletons. Eighteen out of 400 may not sound like a lot, but it’s actually a very high percentage. If all eighteen indeed died violently, it means that about 4.5 per cent of deaths in the ancient Danube Valley were caused by human violence. Today, the global average is only 1.5 per cent, taking war and crime together. During the twentieth century, only 5 per cent of human deaths resulted from human violence – and this in a century that saw the bloodiest wars and most massive genocides in history. If this revelation is typical, the ancient Danube Valley was as violent as the twentieth century.*
The depressing findings from the Danube Valley are supported by a string of equally depressing findings from other areas. At Jabl Sahaba in Sudan, a 12,000-year-old cemetery containing fifty-nine skeletons was discovered. Arrowheads and spear points were found embedded in or lying near the bones of twenty-four skeletons, 40 per cent of the find. The skeleton of one woman revealed twelve injuries. In Ofnet Cave in Bavaria, archaeologists discovered the remains of thirty-eight foragers, mainly women and children, who had been thrown into two burial pits. Half the skeletons, including those of children and babies, bore clear signs of damage by human weapons such as clubs and knives. The few skeletons belonging to mature males bore the worst marks of violence. In all probability, an entire forager band was massacred at Ofnet.
Which better represents the world of the ancient foragers: the peaceful skeletons from Israel and Portugal, or the abattoirs of Jabl Sahaba and Ofnet? The answer is neither. Just as foragers exhibited a wide array of religions and social structures, so, too, did they probably demonstrate a variety of violence rates. While some areas and some periods of time may have enjoyed peace and tranquillity, others were riven by ferocious conflicts.10
The Curtain of Silence

If the larger picture of ancient forager life is hard to reconstruct, particular events are largely irretrievable. When a Sapiens band first entered a valley inhabited by Neanderthals, the following years might have witnessed a breathtaking historical drama. Unfortunately, nothing would have survived from such an encounter except, at best, a few fossilised bones and a handful of stone tools that remain mute under the most intense scholarly inquisitions. We may extract from them information about human anatomy, human technology, human diet, and perhaps even human social structure. But they reveal nothing about the political alliance forged between neighbouring Sapiens bands, about the spirits of the dead that blessed this alliance, or about the ivory beads secretly given to the local witch doctor in order to secure the blessing of the spirits.
This curtain of silence shrouds tens of thousands of years of history. These long millennia may well have witnessed wars and revolutions, ecstatic religious movements, profound philosophical theories, incomparable artistic masterpieces. The foragers may have had their all-conquering Napoleons, who ruled empires half the size of Luxembourg; gifted Beethovens who lacked symphony orchestras but brought people to tears with the sound of their bamboo flutes; and charismatic prophets who revealed the words of a local oak tree rather than those of a universal creator god. But these are all mere guesses. The curtain of silence is so thick that we cannot even be sure such things occurred – let alone describe them in detail.
Scholars tend to ask only those questions that they can reasonably expect to answer. Without the discovery of as yet unavailable research tools, we will probably never know what the ancient foragers believed or what political dramas they experienced. Yet it is vital to ask questions for which no answers are available, otherwise we might be tempted to dismiss 60,000 of 70,000 years of human history with the excuse that ‘the people who lived back then did nothing of importance’.
The truth is that they did a lot of important things. In particular, they shaped the world around us to a much larger degree than most people realise. Trekkers visiting the Siberian tundra, the deserts of central Australia and the Amazonian rainforest believe that they have entered pristine landscapes, virtually untouched by human hands. But that’s an illusion. The foragers were there before us and they brought about dramatic changes even in the densest jungles and the most desolate wildernesses. The next chapter explains how the foragers completely reshaped the ecology of our planet long before the first agricultural village was built. The wandering bands of storytelling Sapiens were the most important and most destructive force the animal kingdom had ever produced.
* A ‘horizon of possibilities’ means the entire spectrum of beliefs, practices and experiences that are open before a particular society, given its ecological, technological and cultural limitations. Each society and each individual usually explore only a tiny fraction of their horizon of possibilities.
* It might be argued that not all eighteen ancient Danubians actually died from the violence whose marks can be seen on their remains. Some were only injured. However, this is probably counterbalanced by deaths from trauma to soft tissues and from the invisible deprivations that accompany war.4

The Flood

PRIOR TO THE COGNITIVE REVOLUTION, humans of all species lived exclusively on the Afro-Asian landmass. True, they had settled a few islands by swimming short stretches of water or crossing them on improvised rafts. Flores, for example, was colonised as far back as 850,000 years ago. Yet they were unable to venture into the open sea, and none reached America, Australia, or remote islands such as Madagascar, New Zealand and Hawaii.
The sea barrier prevented not just humans but also many other Afro-Asian animals and plants from reaching this ‘Outer World’. As a result, the organisms of distant lands like Australia and Madagascar evolved in isolation for millions upon millions of years, taking on shapes and natures very different from those of their distant Afro-Asian relatives. Planet Earth was separated into several distinct ecosystems, each made up of a unique assembly of animals and plants. Homo sapiens was about to put an end to this biological exuberance.
Following the Cognitive Revolution, Sapiens acquired the technology, the organisational skills, and perhaps even the vision necessary to break out of Afro-Asia and settle the Outer World. Their first achievement was the colonisation of Australia some 45,000 years ago. Experts are hard-pressed to explain this feat. In order to reach Australia, humans had to cross a number of sea channels, some more than a hundred kilometres wide, and upon arrival they had to adapt nearly overnight to a completely new ecosystem.
The most reasonable theory suggests that, about 45,000 years ago, the Sapiens living in the Indonesian archipelago (a group of islands separated from Asia and from each other by only narrow straits) developed the first seafaring societies. They learned how to build and manoeuvre ocean-going vessels and became long-distance fishermen, traders and explorers. This would have brought about an unprecedented transformation in human capabilities and lifestyles. Every other mammal that went to sea – seals, sea cows, dolphins – had to evolve for aeons to develop specialised organs and a hydrodynamic body. The Sapiens in Indonesia, descendants of apes who lived on the African savannah, became Pacific seafarers without growing flippers and without having to wait for their noses to migrate to the top of their heads as whales did. Instead, they built boats and learned how to steer them. And these skills enabled them to reach and settle Australia.
True, archaeologists have yet to unearth rafts, oars or fishing villages that date back as far as 45,000 years ago (they would be difficult to discover, because rising sea levels have buried the ancient Indonesian shoreline under a hundred metres of ocean). Nevertheless, there is strong circumstantial evidence to support this theory, especially the fact that in the thousands of years following the settlement of Australia, Sapiens colonised a large number of small and isolated islands to its north. Some, such as Buka and Manus, were separated from the closest land by 200 kilometres of open water. It’s hard to believe that anyone could have reached and colonised Manus without sophisticated vessels and sailing skills. As mentioned earlier, there is also firm evidence for regular sea trade between some of these islands, such as New Ireland and New Britain.1
The journey of the first humans to Australia is one of the most important events in history, at least as important as Columbus’ journey to America or the Apollo 11 expedition to the moon. It was the first time any human had managed to leave the Afro-Asian ecological system – indeed, the first time any large terrestrial mammal had managed to cross from Afro-Asia to Australia. Of even greater importance was what the human pioneers did in this new world. The moment the first hunter-gatherer set foot on an Australian beach was the moment that Homo sapiens climbed to the top rung in the food chain on a particular landmass and thereafter became the deadliest species in the annals of planet Earth.
Up until then humans had displayed some innovative adaptations and behaviours, but their effect on their environment had been negligible. They had demonstrated remarkable success in moving into and adjusting to various habitats, but they did so without drastically changing those habitats. The settlers of Australia, or more accurately, its conquerors, didn’t just adapt, they transformed the Australian ecosystem beyond recognition.
The first human footprint on a sandy Australian beach was immediately washed away by the waves. Yet when the invaders advanced inland, they left behind a different footprint, one that would never be expunged. As they pushed on, they encountered a strange universe of unknown creatures that included a 200-kilogram, two-metre kangaroo, and a marsupial lion, as massive as a modern tiger, that was the continent’s largest predator. Koalas far too big to be cuddly and cute rustled in the trees and flightless birds twice the size of ostriches sprinted on the plains. Dragon-like lizards and snakes five metres long slithered through the undergrowth. The giant diprotodon, a two-and-a-half-ton wombat, roamed the forests. Except for the birds and reptiles, all these animals were marsupials – like kangaroos, they gave birth to tiny, helpless, fetus-like young which they then nurtured with milk in abdominal pouches. Marsupial mammals were almost unknown in Africa and Asia, but in Australia they reigned supreme.
Within a few thousand years, virtually all of these giants vanished. Of the twenty-four Australian animal species weighing fifty kilograms or more, twenty-three became extinct.2 A large number of smaller species also disappeared. Food chains throughout the entire Australian ecosystem were broken and rearranged. It was the most important transformation of the Australian ecosystem for millions of years. Was it all the fault of Homo sapiens?
Guilty as Charged

Some scholars try to exonerate our species, placing the blame on the vagaries of the climate (the usual scapegoat in such cases). Yet it is hard to believe that Homo sapiens was completely innocent. There are three pieces of evidence that weaken the climate alibi, and implicate our ancestors in the extinction of the Australian megafauna.
Firstly, even though Australia’s climate changed some 45,000 years ago, it wasn’t a very remarkable upheaval. It’s hard to see how the new weather patterns alone could have caused such a massive extinction. It’s common today to explain anything and everything as the result of climate change, but the truth is that earth’s climate never rests. It is in constant flux. Every event in history occurred against the background of some climate change.
In particular, our planet has experienced numerous cycles of cooling and warming. During the last million years, there has been an ice age on average every 100,000 years. The last one ran from about 75,000 to 15,000 years ago. Not unusually severe for an ice age, it had twin peaks, the first about 70,000 years ago and the second at about 20,000 years ago. The giant diprotodon appeared in Australia more than 1.5 million years ago and successfully weathered at least ten previous ice ages. It also survived the first peak of the last ice age, around 70,000 years ago. Why, then, did it disappear 45,000 years ago? Of course, if diprotodons had been the only large animal to disappear at this time, it might have been just a fluke. But more than 90 per cent of Australia’s megafauna disappeared along with the diprotodon. The evidence is circumstantial, but it’s hard to imagine that Sapiens, just by coincidence, arrived in Australia at the precise point that all these animals were dropping dead of the chills.3
Secondly, when climate change causes mass extinctions, sea creatures are usually hit as hard as land dwellers. Yet there is no evidence of any significant disappearance of oceanic fauna 45,000 years ago. Human involvement can easily explain why the wave of extinction obliterated the terrestrial megafauna of Australia while sparing that of the nearby oceans. Despite its burgeoning navigational abilities, Homo sapiens was still overwhelmingly a terrestrial menace.
Thirdly, mass extinctions akin to the archetypal Australian decimation occurred again and again in the ensuing millennia – whenever people settled another part of the Outer World. In these cases Sapiens guilt is irrefutable. For example, the megafauna of New Zealand – which had weathered the alleged ‘climate change’ of c.45,000 years ago without a scratch – suffered devastating blows immediately after the first humans set foot on the islands. The Maoris, New Zealand’s first Sapiens colonisers, reached the islands about 800 years ago. Within a couple of centuries, the majority of the local megafauna was extinct, along with 60 per cent of all bird species.
A similar fate befell the mammoth population of Wrangel Island in the Arctic Ocean (200 kilometres north of the Siberian coast). Mammoths had flourished for millions of years over most of the northern hemisphere, but as Homo sapiens spread – first over Eurasia and then over North America – the mammoths retreated. By 10,000 years ago there was not a single mammoth to be found in the world, except on a few remote Arctic islands, most conspicuously Wrangel. The mammoths of Wrangel continued to prosper for a few more millennia, then suddenly disappeared about 4,000 years ago, just when the first humans reached the island.
Were the Australian extinction an isolated event, we could grant humans the benefit of the doubt. But the historical record makes Homo sapiens look like an ecological serial killer.
All the settlers of Australia had at their disposal was Stone Age technology. How could they cause an ecological disaster? There are three explanations that mesh quite nicely.
Large animals – the primary victims of the Australian extinction – breed slowly. Pregnancy is long, offspring per pregnancy are few, and there are long breaks between pregnancies. Consequently, if humans cut down even one diprotodon every few months, it would be enough to cause diprotodon deaths to outnumber births. Within a few thousand years the last, lonesome diprotodon would pass away, and with her the entire species.4
In fact, for all their size, diprotodons and Australia’s other giants probably wouldn’t have been that hard to hunt because they would have been taken totally by surprise by their two-legged assailants. Various human species had been prowling and evolving in Afro-Asia for 2 million years. They slowly honed their hunting skills, and began going after large animals around 400,000 years ago. The big beasts of Africa and Asia learned to avoid humans, so when the new mega-predator – Homo sapiens – appeared on the Afro-Asian scene, the large animals already knew to keep their distance from creatures that looked like it. In contrast, the Australian giants had no time to learn to run away. Humans don’t come across as particularly dangerous. They don’t have long, sharp teeth or muscular, lithe bodies. So when a diprotodon, the largest marsupial ever to walk the earth, set eyes for the first time on this frail-looking ape, he gave it one glance and then went back to chewing leaves. These animals had to evolve a fear of humankind, but before they could do so they were gone.
The second explanation is that by the time Sapiens reached Australia, they had already mastered fire agriculture. Faced with an alien and threatening environment, they deliberately burned vast areas of impassable thickets and dense forests to create open grasslands, which attracted more easily hunted game, and were better suited to their needs. They thereby completely changed the ecology of large parts of Australia within a few short millennia.
One body of evidence supporting this view is the fossil plant record. Eucalyptus trees were rare in Australia 45,000 years ago. But the arrival of Homo sapiens inaugurated a golden age for the species. Since eucalyptuses are particularly resistant to fire, they spread far and wide while other trees and shrubs disappeared.
These changes in vegetation influenced the animals that ate the plants and the carnivores that ate the vegetarians. Koalas, which subsist exclusively on eucalyptus leaves, happily munched their way into new territories. Most other animals suffered greatly. Many Australian food chains collapsed, driving the weakest links into extinction.5
A third explanation agrees that hunting and fire agriculture played a significant role in the extinction, but emphasises that we can’t completely ignore the role of climate. The climate changes that beset Australia about 45,000 years ago destabilised the ecosystem and made it particularly vulnerable. Under normal circumstances the system would probably have recuperated, as had happened many times previously. However, humans appeared on the stage at just this critical juncture and pushed the brittle ecosystem into the abyss. The combination of climate change and human hunting is particularly devastating for large animals, since it attacks them from different angles. It is hard to find a good survival strategy that will work simultaneously against multiple threats.
Without further evidence, there’s no way of deciding between the three scenarios. But there are certainly good reasons to believe that if Homo sapiens had never gone Down Under, it would still be home to marsupial lions, diprotodons and giant kangaroos.
The End of Sloth

The extinction of the Australian megafauna was probably the first significant mark Homo sapiens left on our planet. It was followed by an even larger ecological disaster, this time in America. Homo sapiens was the first and only human species to reach the western hemisphere landmass, arriving about 16,000 years ago, that is in or around 14,000 BC. The first Americans arrived on foot, which they could do because, at the time, sea levels were low enough that a land bridge connected north-eastern Siberia with north-western Alaska. Not that it was easy – the journey was an arduous one, perhaps harder than the sea passage to Australia. To make the crossing, Sapiens first had to learn how to withstand the extreme Arctic conditions of northern Siberia, an area on which the sun never shines in winter, and where temperatures can drop to minus fifty degrees Celsius.
No previous human species had managed to penetrate places like northern Siberia. Even the cold-adapted Neanderthals restricted themselves to relatively warmer regions further south. But Homo sapiens, whose body was adapted to living in the African savannah rather than in the lands of snow and ice, devised ingenious solutions. When roaming bands of Sapiens foragers migrated into colder climates, they learned to make snowshoes and effective thermal clothing composed of layers of furs and skins, sewn together tightly with the help of needles. They developed new weapons and sophisticated hunting techniques that enabled them to track and kill mammoths and the other big game of the far north. As their thermal clothing and hunting techniques improved, Sapiens dared to venture deeper and deeper into the frozen regions. And as they moved north, their clothes, hunting strategies and other survival skills continued to improve.
But why did they bother? Why banish oneself to Siberia by choice? Perhaps some bands were driven north by wars, demographic pressures or natural disasters. Others might have been lured northwards by more positive reasons, such as animal protein. The Arctic lands were full of large, juicy animals such as reindeer and mammoths. Every mammoth was a source of a vast quantity of meat (which, given the frosty temperatures, could even be frozen for later use), tasty fat, warm fur and valuable ivory. As the findings from Sungir testify, mammoth-hunters did not just survive in the frozen north – they thrived. As time passed, the bands spread far and wide, pursuing mammoths, mastodons, rhinoceroses and reindeer. Around 14,000 BC, the chase took some of them from north-eastern Siberia to Alaska. Of course, they didn’t know they were discovering a new world. For mammoth and man alike, Alaska was a mere extension of Siberia.
At first, glaciers blocked the way from Alaska to the rest of America, allowing no more than perhaps a few isolated pioneers to investigate the lands further south. However, around 12,000 BC global warming melted the ice and opened an easier passage. Making use of the new corridor, people moved south en masse, spreading over the entire continent. Though originally adapted to hunting large game in the Arctic, they soon adjusted to an amazing variety of climates and ecosystems. Descendants of the Siberians settled the thick forests of the eastern United States, the swamps of the Mississippi Delta, the deserts of Mexico and steaming jungles of Central America. Some made their homes in the river world of the Amazon basin, others struck roots in Andean mountain valleys or the open pampas of Argentina. And all this happened in a mere millennium or two! By 10,000 BC, humans already inhabited the most southern point in America, the island of Tierra del Fuego at the continent’s southern tip. The human blitzkrieg across America testifies to the incomparable ingenuity and the unsurpassed adaptability of Homo sapiens. No other animal had ever moved into such a huge variety of radically different habitats so quickly, everywhere using virtually the same genes.6
The settling of America was hardly bloodless. It left behind a long trail of victims. American fauna 14,000 years ago was far richer than it is today. When the first Americans marched south from Alaska into the plains of Canada and the western United States, they encountered mammoths and mastodons, rodents the size of bears, herds of horses and camels, oversized lions and dozens of large species the likes of which are completely unknown today, among them fearsome sabre-tooth cats and giant ground sloths that weighed up to eight tons and reached a height of six metres. South America hosted an even more exotic menagerie of large mammals, reptiles and birds. The Americas were a great laboratory of evolutionary experimentation, a place where animals and plants unknown in Africa and Asia had evolved and thrived.
But no longer. Within 2,000 years of the Sapiens arrival, most of these unique species were gone. According to current estimates, within that short interval, North America lost thirty-four out of its forty-seven genera of large mammals. South America lost fifty out of sixty. The sabre-tooth cats, after flourishing for more than 30 million years, disappeared, and so did the giant ground sloths, the oversized lions, native American horses, native American camels, the giant rodents and the mammoths. Thousands of species of smaller mammals, reptiles, birds, and even insects and parasites also became extinct (when the mammoths died out, all species of mammoth ticks followed them to oblivion).
For decades, palaeontologists and zooarchaeologists – people who search for and study animal remains – have been combing the plains and mountains of the Americas in search of the fossilised bones of ancient camels and the petrified faeces of giant ground sloths. When they find what they seek, the treasures are carefully packed up and sent to laboratories, where every bone and every coprolite (the technical name for fossilised turds) is meticulously studied and dated. Time and again, these analyses yield the same results: the freshest dung balls and the most recent camel bones date to the period when humans flooded America, that is, between approximately 12,000 and 9000 BC. Only in one area have scientists discovered younger dung balls: on several Caribbean islands, in particular Cuba and Hispaniola, they found petrified ground-sloth scat dating to about 5000 BC. This is exactly the time when the first humans managed to cross the Caribbean Sea and settle these two large islands.
Again, some scholars try to exonerate Homo sapiens and blame climate change (which requires them to posit that, for some mysterious reason, the climate in the Caribbean islands remained static for 7,000 years while the rest of the western hemisphere warmed). But in America, the dung ball cannot be dodged. We are the culprits. There is no way around that truth. Even if climate change abetted us, the human contribution was decisive.7
Noah’s Ark

If we combine the mass extinctions in Australia and America, and add the smaller-scale extinctions that took place as Homo sapiens spread over Afro-Asia – such as the extinction of all other human species – and the extinctions that occurred when ancient foragers settled remote islands such as Cuba, the inevitable conclusion is that the first wave of Sapiens colonisation was one of the biggest and swiftest ecological disasters to befall the animal kingdom. Hardest hit were the large furry creatures. At the time of the Cognitive Revolution, the planet was home to about 200 genera of large terrestrial mammals weighing over fifty kilograms. At the time of the Agricultural Revolution, only about a hundred remained. Homo sapiens drove to extinction about half of the planet’s big beasts long before humans invented the wheel, writing, or iron tools.
This ecological tragedy was restaged in miniature countless times after the Agricultural Revolution. The archaeological record of island after island tells the same sad story. The tragedy opens with a scene showing a rich and varied population of large animals, without any trace of humans. In scene two, Sapiens appear, evidenced by a human bone, a spear point, or perhaps a potsherd. Scene three quickly follows, in which men and women occupy centre stage and most large animals, along with many smaller ones, are gone.
The large island of Madagascar, about 400 kilometres east of the African mainland, offers a famous example. Through millions of years of isolation, a unique collection of animals evolved there. These included the elephant bird, a flightless creature three metres tall and weighing almost half a ton – the largest bird in the world – and the giant lemurs, the globe’s largest primates. The elephant birds and the giant lemurs, along with most of the other large animals of Madagascar, suddenly vanished about 1,500 years ago – precisely when the first humans set foot on the island.

10. Reconstructions of two giant ground sloths (Megatherium) and behind them two giant armadillos (Glyptodon). Now extinct, giant armadillos measured over three metres in length and weighed up to two tons, whereas giant ground sloths reached heights of up to six metres, and weighed up to eight tons.

In the Pacific Ocean, the main wave of extinction began in about 1500 BC, when Polynesian farmers settled the Solomon Islands, Fiji and New Caledonia. They killed off, directly or indirectly, hundreds of species of birds, insects, snails and other local inhabitants. From there, the wave of extinction moved gradually to the east, the south and the north, into the heart of the Pacific Ocean, obliterating on its way the unique fauna of Samoa and Tonga (1200 BC); the Marquis Islands (AD 1); Easter Island, the Cook Islands and Hawaii (AD 500); and finally New Zealand (AD 1200).
Similar ecological disasters occurred on almost every one of the thousands of islands that pepper the Atlantic Ocean, Indian Ocean, Arctic Ocean and Mediterranean Sea. Archaeologists have discovered on even the tiniest islands evidence of the existence of birds, insects and snails that lived there for countless generations, only to vanish when the first human farmers arrived. None but a few extremely remote islands escaped man’s notice until the modern age, and these islands kept their fauna intact. The Galapagos Islands, to give one famous example, remained uninhabited by humans until the nineteenth century, thus preserving their unique menagerie, including their giant tortoises, which, like the ancient diprotodons, show no fear of humans.
The First Wave Extinction, which accompanied the spread of the foragers, was followed by the Second Wave Extinction, which accompanied the spread of the farmers, and gives us an important perspective on the Third Wave Extinction, which industrial activity is causing today. Don’t believe tree-huggers who claim that our ancestors lived in harmony with nature. Long before the Industrial Revolution, Homo sapiens held the record among all organisms for driving the most plant and animal species to their extinctions. We have the dubious distinction of being the deadliest species in the annals of biology.
Perhaps if more people were aware of the First Wave and Second Wave extinctions, they’d be less nonchalant about the Third Wave they are part of. If we knew how many species we’ve already eradicated, we might be more motivated to protect those that still survive. This is especially relevant to the large animals of the oceans. Unlike their terrestrial counterparts, the large sea animals suffered relatively little from the Cognitive and Agricultural Revolutions. But many of them are on the brink of extinction now as a result of industrial pollution and human overuse of oceanic resources. If things continue at the present pace, it is likely that whales, sharks, tuna and dolphins will follow the diprotodons, ground sloths and mammoths to oblivion. Among all the world’s large creatures, the only survivors of the human flood will be humans themselves, and the farmyard animals that serve as galley slaves in Noah’s Ark.5

History’s Biggest Fraud

FOR 2.5 MILLION YEARS HUMANS FED themselves by gathering plants and hunting animals that lived and bred without their intervention. Homo erectus, Homo ergaster and the Neanderthals plucked wild figs and hunted wild sheep without deciding where fig trees would take root, in which meadow a herd of sheep should graze, or which billy goat would inseminate which nanny goat. Homo sapiens spread from East Africa to the Middle East, to Europe and Asia, and finally to Australia and America – but everywhere they went, Sapiens too continued to live by gathering wild plants and hunting wild animals. Why do anything else when your lifestyle feeds you amply and supports a rich world of social structures, religious beliefs and political dynamics?
All this changed about 10,000 years ago, when Sapiens began to devote almost all their time and effort to manipulating the lives of a few animal and plant species. From sunrise to sunset humans sowed seeds, watered plants, plucked weeds from the ground and led sheep to prime pastures. This work, they thought, would provide them with more fruit, grain and meat. It was a revolution in the way humans lived – the Agricultural Revolution.
The transition to agriculture began around 9500–8500 BC in the hill country of south-eastern Turkey, western Iran, and the Levant. It began slowly and in a restricted geographical area. Wheat and goats were domesticated by approximately 9000 BC; peas and lentils around 8000 BC; olive trees by 5000 BC; horses by 4000 BC; and grapevines in 3500 BC. Some animals and plants, such as camels and cashew nuts, were domesticated even later, but by 3500 BC the main wave of domestication was over. Even today, with all our advanced technologies, more than 90 per cent of the calories that feed humanity come from the handful of plants that our ancestors domesticated between 9500 and 3500 BC – wheat, rice, maize (called ‘corn’ in the US), potatoes, millet and barley. No noteworthy plant or animal has been domesticated in the last 2,000 years. If our minds are those of hunter-gatherers, our cuisine is that of ancient farmers.
Scholars once believed that agriculture spread from a single Middle Eastern point of origin to the four corners of the world. Today, scholars agree that agriculture sprang up in other parts of the world not by the action of Middle Eastern farmers exporting their revolution but entirely independently. People in Central America domesticated maize and beans without knowing anything about wheat and pea cultivation in the Middle East. South Americans learned how to raise potatoes and llamas, unaware of what was going on in either Mexico or the Levant. Chinas first revolutionaries domesticated rice, millet and pigs. North America’s first gardeners were those who got tired of combing the undergrowth for edible gourds and decided to cultivate pumpkins. New Guineans tamed sugar cane and bananas, while the first West African farmers made African millet, African rice, sorghum and wheat conform to their needs. From these initial focal points, agriculture spread far and wide. By the first century AD the vast majority of people throughout most of the world were agriculturists.
Why did agricultural revolutions erupt in the Middle East, China and Central America but not in Australia, Alaska or South Africa? The reason is simple: most species of plants and animals can’t be domesticated. Sapiens could dig up delicious truffles and hunt down woolly mammoths, but domesticating either species was out of the question. The fungi were far too elusive, the giant beasts too ferocious. Of the thousands of species that our ancestors hunted and gathered, only a few were suitable candidates for farming and herding. Those few species lived in particular places, and those are the places where agricultural revolutions occurred.
Scholars once proclaimed that the agricultural revolution was a great leap forward for humanity. They told a tale of progress fuelled by human brain power. Evolution gradually produced ever more intelligent people. Eventually, people were so smart that they were able to decipher nature’s secrets, enabling them to tame sheep and cultivate wheat. As soon as this happened, they cheerfully abandoned the gruelling, dangerous, and often spartan life of hunter-gatherers, settling down to enjoy the pleasant, satiated life of farmers.

Map 2. Locations and dates of agricultural revolutions. The data is contentious, and the map is constantly being redrawn to incorporate the latest archaeological discoveries.1

That tale is a fantasy. There is no evidence that people became more intelligent with time. Foragers knew the secrets of nature long before the Agricultural Revolution, since their survival depended on an intimate knowledge of the animals they hunted and the plants they gathered. Rather than heralding a new era of easy living, the Agricultural Revolution left farmers with lives generally more difficult and less satisfying than those of foragers. Hunter-gatherers spent their time in more stimulating and varied ways, and were less in danger of starvation and disease. The Agricultural Revolution certainly enlarged the sum total of food at the disposal of humankind, but the extra food did not translate into a better diet or more leisure. Rather, it translated into population explosions and pampered elites. The average farmer worked harder than the average forager, and got a worse diet in return. The Agricultural Revolution was history’s biggest fraud.2
Who was responsible? Neither kings, nor priests, nor merchants. The culprits were a handful of plant species, including wheat, rice and potatoes. These plants domesticated Homo sapiens, rather than vice versa.
Think for a moment about the Agricultural Revolution from the viewpoint of wheat. Ten thousand years ago wheat was just a wild grass, one of many, confined to a small range in the Middle East. Suddenly, within just a few short millennia, it was growing all over the world. According to the basic evolutionary criteria of survival and reproduction, wheat has become one of the most successful plants in the history of the earth. In areas such as the Great Plains of North America, where not a single wheat stalk grew 10,000 years ago, you can today walk for hundreds upon hundreds of kilometres without encountering any other plant. Worldwide, wheat covers about 2.25 million square kilometres of the globes surface, almost ten times the size of Britain. How did this grass turn from insignificant to ubiquitous?
Wheat did it by manipulating Homo sapiens to its advantage. This ape had been living a fairly comfortable life hunting and gathering until about 10,000 years ago, but then began to invest more and more effort in cultivating wheat. Within a couple of millennia, humans in many parts of the world were doing little from dawn to dusk other than taking care of wheat plants. It wasn’t easy. Wheat demanded a lot of them. Wheat didn’t like rocks and pebbles, so Sapiens broke their backs clearing fields. Wheat didn’t like sharing its space, water and nutrients with other plants, so men and women laboured long days weeding under the scorching sun. Wheat got sick, so Sapiens had to keep a watch out for worms and blight. Wheat was defenceless against other organisms that liked to eat it, from rabbits to locust swarms, so the farmers had to guard and protect it. Wheat was thirsty, so humans lugged water from springs and streams to water it. Its hunger even impelled Sapiens to collect animal faeces to nourish the ground in which wheat grew.
The body of Homo sapiens had not evolved for such tasks. It was adapted to climbing apple trees and running after gazelles, not to clearing rocks and carrying water buckets. Human spines, knees, necks and arches paid the price. Studies of ancient skeletons indicate that the transition to agriculture brought about a plethora of ailments, such as slipped discs, arthritis and hernias. Moreover, the new agricultural tasks demanded so much time that people were forced to settle permanently next to their wheat fields. This completely changed their way of life. We did not domesticate wheat. It domesticated us. The word ‘domesticate’ comes from the Latin domus, which means ‘house’. Who’s the one living in a house? Not the wheat. It’s the Sapiens.
How did wheat convince Homo sapiens to exchange a rather good life for a more miserable existence? What did it offer in return? It did not offer a better diet. Remember, humans are omnivorous apes who thrive on a wide variety of foods. Grains made up only a small fraction of the human diet before the Agricultural Revolution. A diet based on cereals is poor in minerals and vitamins, hard to digest, and really bad for your teeth and gums.
Wheat did not give people economic security. The life of a peasant is less secure than that of a hunter-gatherer. Foragers relied on dozens of species to survive, and could therefore weather difficult years even without stocks of preserved food. If the availability of one species was reduced, they could gather and hunt more of other species. Farming societies have, until very recently, relied for the great bulk of their calorie intake on a small variety of domesticated plants. In many areas, they relied on just a single staple, such as wheat, potatoes or rice. If the rains failed or clouds of locusts arrived or if a fungus learned how to infect that staple species, peasants died by the thousands and millions.
Nor could wheat offer security against human violence. The early farmers were at least as violent as their forager ancestors, if not more so. Farmers had more possessions and needed land for planting. The loss of pasture land to raiding neighbours could mean the difference between subsistence and starvation, so there was much less room for compromise. When a foraging band was hard-pressed by a stronger rival, it could usually move on. It was difficult and dangerous, but it was feasible. When a strong enemy threatened an agricultural village, retreat meant giving up fields, houses and granaries. In many cases, this doomed the refugees to starvation. Farmers, therefore, tended to stay put and fight to the bitter end.

12. Tribal warfare in New Guinea between two farming communities (1960). Such scenes were probably widespread in the thousands of years following the Agricultural Revolution.

Many anthropological and archaeological studies indicate that in simple agricultural societies with no political frameworks beyond village and tribe, human violence was responsible for about 15 per cent of deaths, including 25 per cent of male deaths. In contemporary New Guinea, violence accounts for 30 per cent of male deaths in one agricultural tribal society, the Dani, and 35 per cent in another, the Enga. In Ecuador, perhaps 50 per cent of adult Waoranis meet a violent death at the hands of another human!3 In time, human violence was brought under control through the development of larger social frameworks – cities, kingdoms and states. But it took thousands of years to build such huge and effective political structures.
Village life certainly brought the first farmers some immediate benefits, such as better protection against wild animals, rain and cold. Yet for the average person, the disadvantages probably outweighed the advantages. This is hard for people in today’s prosperous societies to appreciate. Since we enjoy affluence and security, and since our affluence and security are built on foundations laid by the Agricultural Revolution, we assume that the Agricultural Revolution was a wonderful improvement. Yet it is wrong to judge thousands of years of history from the perspective of today. A much more representative viewpoint is that of a three-year-old girl dying from malnutrition in first-century China because her father’s crops have failed. Would she say ‘I am dying from malnutrition, but in 2,000 years, people will have plenty to eat and live in big air-conditioned houses, so my suffering is a worthwhile sacrifice’?
What then did wheat offer agriculturists, including that malnourished Chinese girl? It offered nothing for people as individuals. Yet it did bestow something on Homo sapiens as a species. Cultivating wheat provided much more food per unit of territory, and thereby enabled Homo sapiens to multiply exponentially. Around 13,000 BC, when people fed themselves by gathering wild plants and hunting wild animals, the area around the oasis of Jericho, in Palestine, could support at most one roaming band of about a hundred relatively healthy and well-nourished people. Around 8500 BC, when wild plants gave way to wheat fields, the oasis supported a large but cramped village of 1,000 people, who suffered far more from disease and malnourishment.
The currency of evolution is neither hunger nor pain, but rather copies of DNA helixes. Just as the economic success of a company is measured only by the number of dollars in its bank account, not by the happiness of its employees, so the evolutionary success of a species is measured by the number of copies of its DNA. If no more DNA copies remain, the species is extinct, just as a company without money is bankrupt. If a species boasts many DNA copies, it is a success, and the species flourishes. From such a perspective, 1,000 copies are always better than a hundred copies. This is the essence of the Agricultural Revolution: the ability to keep more people alive under worse conditions.
Yet why should individuals care about this evolutionary calculus? Why would any sane person lower his or her standard of living just to multiply the number of copies of the Homo sapiens genome? Nobody agreed to this deal: the Agricultural Revolution was a trap.
The Luxury Trap

The rise of farming was a very gradual affair spread over centuries and millennia. A band of Homo sapiens gathering mushrooms and nuts and hunting deer and rabbit did not all of a sudden settle in a permanent village, ploughing fields, sowing wheat and carrying water from the river. The change proceeded by stages, each of which involved just a small alteration in daily life.
Homo sapiens reached the Middle East around 70,000 years ago. For the next 50,000 years our ancestors flourished there without agriculture. The natural resources of the area were enough to support its human population. In times of plenty people had a few more children, and in times of need a few less. Humans, like many mammals, have hormonal and genetic mechanisms that help control procreation. In good times females reach puberty earlier, and their chances of getting pregnant are a bit higher. In bad times puberty is late and fertility decreases.
To these natural population controls were added cultural mechanisms. Babies and small children, who move slowly and demand much attention, were a burden on nomadic foragers. People tried to space their children three to four years apart. Women did so by nursing their children around the clock and until a late age (around-the-clock suckling significantly decreases the chances of getting pregnant). Other methods included full or partial sexual abstinence (backed perhaps by cultural taboos), abortions and occasionally infanticide.4
During these long millennia people occasionally ate wheat grain, but this was a marginal part of their diet. About 18,000 years ago, the last ice age gave way to a period of global warming. As temperatures rose, so did rainfall. The new climate was ideal for Middle Eastern wheat and other cereals, which multiplied and spread. People began eating more wheat, and in exchange they inadvertently spread its growth. Since it was impossible to eat wild grains without first winnowing, grinding and cooking them, people who gathered these grains carried them back to their temporary campsites for processing. Wheat grains are small and numerous, so some of them inevitably fell on the way to the campsite and were lost. Over time, more and more wheat grew along favourite human trails and near campsites.
When humans burned down forests and thickets, this also helped wheat. Fire cleared away trees and shrubs, allowing wheat and other grasses to monopolise the sunlight, water and nutrients. Where wheat became particularly abundant, and game and other food sources were also plentiful, human bands could gradually give up their nomadic lifestyle and settle down in seasonal and even permanent camps.
At first they might have camped for four weeks during the harvest. A generation later, as wheat plants multiplied and spread, the harvest camp might have lasted for five weeks, then six, and finally it became a permanent village. Evidence of such settlements has been discovered throughout the Middle East, particularly in the Levant, where the Natufian culture flourished from 12,500 BC to 9500 BC. The Natufians were hunter-gatherers who subsisted on dozens of wild species, but they lived in permanent villages and devoted much of their time to the intensive gathering and processing of wild cereals. They built stone houses and granaries. They stored grain for times of need. They invented new tools such as stone scythes for harvesting wild wheat, and stone pestles and mortars to grind it.
In the years following 9500 BC, the descendants of the Natufians continued to gather and process cereals, but they also began to cultivate them in more and more elaborate ways. When gathering wild grains, they took care to lay aside part of the harvest to sow the fields next season. They discovered that they could achieve much better results by sowing the grains deep in the ground rather than haphazardly scattering them on the surface. So they began to hoe and plough. Gradually they also started to weed the fields, to guard them against parasites, and to water and fertilise them. As more effort was directed towards cereal cultivation, there was less time to gather and hunt wild species. The foragers became farmers.
No single step separated the woman gathering wild wheat from the woman farming domesticated wheat, so it’s hard to say exactly when the decisive transition to agriculture took place. But, by 8500 BC, the Middle East was peppered with permanent villages such as Jericho, whose inhabitants spent most of their time cultivating a few domesticated species.
With the move to permanent villages and the increase in food supply, the population began to grow. Giving up the nomadic lifestyle enabled women to have a child every year. Babies were weaned at an earlier age – they could be fed on porridge and gruel. The extra hands were sorely needed in the fields. But the extra mouths quickly wiped out the food surpluses, so even more fields had to be planted. As people began living in disease-ridden settlements, as children fed more on cereals and less on mother’s milk, and as each child competed for his or her porridge with more and more siblings, child mortality soared. In most agricultural societies at least one out of every three children died before reaching twenty.5 Yet the increase in births still outpaced the increase in deaths; humans kept having larger numbers of children.
With time, the ‘wheat bargain’ became more and more burdensome. Children died in droves, and adults ate bread by the sweat of their brows. The average person in Jericho of 8500 BC lived a harder life than the average person in Jericho of 9500 BC or 13,000 BC. But nobody realised what was happening. Every generation continued to live like the previous generation, making only small improvements here and there in the way things were done. Paradoxically, a series of ‘improvements’, each of which was meant to make life easier, added up to a millstone around the necks of these farmers.
Why did people make such a fateful miscalculation? For the same reason that people throughout history have miscalculated. People were unable to fathom the full consequences of their decisions. Whenever they decided to do a bit of extra work – say, to hoe the fields instead of scattering seeds on the surface – people thought, ‘Yes, we will have to work harder. But the harvest will be so bountiful! We won’t have to worry any more about lean years. Our children will never go to sleep hungry.’ It made sense. If you worked harder, you would have a better life. That was the plan.
The first part of the plan went smoothly. People indeed worked harder. But people did not foresee that the number of children would increase, meaning that the extra wheat would have to be shared between more children. Neither did the early farmers understand that feeding children with more porridge and less breast milk would weaken their immune system, and that permanent settlements would be hotbeds for infectious diseases. They did not foresee that by increasing their dependence on a single source of food, they were actually exposing themselves even more to the depredations of drought. Nor did the farmers foresee that in good years their bulging granaries would tempt thieves and enemies, compelling them to start building walls and doing guard duty.
Then why didn’t humans abandon farming when the plan backfired? Partly because it took generations for the small changes to accumulate and transform society and, by then, nobody remembered that they had ever lived differently. And partly because population growth burned humanity’s boats. If the adoption of ploughing increased a village’s population from a hundred to no, which ten people would have volunteered to starve so that the others could go back to the good old times? There was no going back. The trap snapped shut.
The pursuit of an easier life resulted in much hardship, and not for the last time. It happens to us today. How many young college graduates have taken demanding jobs in high-powered firms, vowing that they will work hard to earn money that will enable them to retire and pursue their real interests when they are thirty-five? But by the time they reach that age, they have large mortgages, children to school, houses in the suburbs that necessitate at least two cars per family, and a sense that life is not worth living without really good wine and expensive holidays abroad. What are they supposed to do, go back to digging up roots? No, they double their efforts and keep slaving away.
One of history’s few iron laws is that luxuries tend to become necessities and to spawn new obligations. Once people get used to a certain luxury, they take it for granted. Then they begin to count on it. Finally they reach a point where they can’t live without it. Let’s take another familiar example from our own time. Over the last few decades, we have invented countless time-saving devices that are supposed to make life more relaxed – washing machines, vacuum cleaners, dishwashers, telephones, mobile phones, computers, email. Previously it took a lot of work to write a letter, address and stamp an envelope, and take it to the mailbox. It took days or weeks, maybe even months, to get a reply. Nowadays I can dash off an email, send it halfway around the globe, and (if my addressee is online) receive a reply a minute later. I’ve saved all that trouble and time, but do I live a more relaxed life?
Sadly not. Back in the snail-mail era, people usually only wrote letters when they had something important to relate. Rather than writing the first thing that came into their heads, they considered carefully what they wanted to say and how to phrase it. They expected to receive a similarly considered answer. Most people wrote and received no more than a handful of letters a month and seldom felt compelled to reply immediately. Today I receive dozens of emails each day, all from people who expect a prompt reply. We thought we were saving time; instead we revved up the treadmill of life to ten times its former speed and made our days more anxious and agitated.
Here and there a Luddite holdout refuses to open an email account, just as thousands of years ago some human bands refused to take up farming and so escaped the luxury trap. But the Agricultural Revolution didn’t need every band in a given region to join up. It only took one. Once one band settled down and started tilling, whether in the Middle East or Central America, agriculture was irresistible. Since farming created the conditions for swift demographic growth, farmers could usually overcome foragers by sheer weight of numbers. The foragers could either run away, abandoning their hunting grounds to field and pasture, or take up the ploughshare themselves. Either way, the old life was doomed.
The story of the luxury trap carries with it an important lesson. Humanity’s search for an easier life released immense forces of change that transformed the world in ways nobody envisioned or wanted. Nobody plotted the Agricultural Revolution or sought human dependence on cereal cultivation. A series of trivial decisions aimed mostly at filling a few stomachs and gaining a little security had the cumulative effect of forcing ancient foragers to spend their days carrying water buckets under a scorching sun.
Divine Intervention

The above scenario explains the Agricultural Revolution as a miscalculation. It’s very plausible. History is full of far more idiotic miscalculations. But there’s another possibility. Maybe it wasn’t the search for an easier life that brought about the transformation. Maybe Sapiens had other aspirations, and were consciously willing to make their lives harder in order to achieve them.
Scientists usually seek to attribute historical developments to cold economic and demographic factors. It sits better with their rational and mathematical methods. In the case of modern history, scholars cannot avoid taking into account non-material factors such as ideology and culture. The written evidence forces their hand. We have enough documents, letters and memoirs to prove that World War Two was not caused by food shortages or demographic pressures. But we have no documents from the Natufian culture, so when dealing with ancient periods the materialist school reigns supreme. It is difficult to prove that preliterate people were motivated by faith rather than economic necessity.
Yet, in some rare cases, we are lucky enough to find telltale clues. In 1995 archaeologists began to excavate a site in south-east Turkey called Göbekli Tepe. In the oldest stratum they discovered no signs of a settlement, houses or daily activities. They did, however, find monumental pillared structures decorated with spectacular engravings. Each stone pillar weighed up to seven tons and reached a height of five metres. In a nearby quarry they found a half-chiselled pillar weighing fifty tons. Altogether, they uncovered more than ten monumental structures, the largest of them nearly thirty metres across.
Archaeologists are familiar with such monumental structures from sites around the world – the best-known example is Stonehenge in Britain. Yet as they studied Göbekli Tepe, they discovered an amazing fact. Stonehenge dates to 2500 BC, and was built by a developed agricultural society. The structures at Göbekli Tepe are dated to about 9500 BC, and all available evidence indicates that they were built by hunter-gatherers. The archaeological community initially found it difficult to credit these findings, but one test after another confirmed both the early date of the structures and the pre-agricultural society of their builders. The capabilities of ancient foragers, and the complexity of their cultures, seem to be far more impressive than was previously suspected.

13. Opposite: The remains of a monumental structure from Göbekli Tepe. Right: One of the decorated stone pillars (about five metres high).

Why would a foraging society build such structures? They had no obvious utilitarian purpose. They were neither mammoth slaughterhouses nor places to shelter from rain or hide from lions. That leaves us with the theory that they were built for some mysterious cultural purpose that archaeologists have a hard time deciphering. Whatever it was, the foragers thought it worth a huge amount of effort and time. The only way to build Göbekli Tepe was for thousands of foragers belonging to different bands and tribes to cooperate over an extended period of time. Only a sophisticated religious or ideological system could sustain such efforts.
Göbekli Tepe held another sensational secret. For many years, geneticists have been tracing the origins of domesticated wheat. Recent discoveries indicate that at least one domesticated variant, einkorn wheat, originated in the Karaçadag Hills – about thirty kilometres from Göbekli Tepe.6

This can hardly be a coincidence. It’s likely that the cultural centre of Göbekli Tepe was somehow connected to the initial domestication of wheat by humankind and of humankind by wheat. In order to feed the people who built and used the monumental structures, particularly large quantities of food were required. It may well be that foragers switched from gathering wild wheat to intense wheat cultivation, not to increase their normal food supply, but rather to support the building and running of a temple. In the conventional picture, pioneers first built a village, and when it prospered, they set up a temple in the middle. But Göbekli Tepe suggests that the temple may have been built first, and that a village later grew up around it.
Victims of the Revolution

The Faustian bargain between humans and grains was not the only deal our species made. Another deal was struck concerning the fate of animals such as sheep, goats, pigs and chickens. Nomadic bands that stalked wild sheep gradually altered the constitutions of the herds on which they preyed. This process probably began with selective hunting. Humans learned that it was to their advantage to hunt only adult rams and old or sick sheep. They spared fertile females and young lambs in order to safeguard the long-term vitality of the local herd. The second step might have been to actively defend the herd against predators, driving away lions, wolves and rival human bands. The band might next have corralled the herd into a narrow gorge in order to better control and defend it. Finally, people began to make a more careful selection among the sheep in order to tailor them to human needs. The most aggressive rams, those that showed the greatest resistance to human control, were slaughtered first. So were the skinniest and most inquisitive females. (Shepherds are not fond of sheep whose curiosity takes them far from the herd.) With each passing generation, the sheep became fatter, more submissive and less curious. Voilà! Mary had a little lamb and everywhere that Mary went the lamb was sure to go.
Alternatively, hunters may have caught and adopted’ a lamb, fattening it during the months of plenty and slaughtering it in the leaner season. At some stage they began keeping a greater number of such lambs. Some of these reached puberty and began to procreate. The most aggressive and unruly lambs were first to the slaughter. The most submissive, most appealing lambs were allowed to live longer and procreate. The result was a herd of domesticated and submissive sheep.
Such domesticated animals – sheep, chickens, donkeys and others – supplied food (meat, milk, eggs), raw materials (skins, wool), and muscle power. Transportation, ploughing, grinding and other tasks, hitherto performed by human sinew, were increasingly carried out by animals. In most farming societies people focused on plant cultivation; raising animals was a secondary activity. But a new kind of society also appeared in some places, based primarily on the exploitation of animals: tribes of pastoralist herders.
As humans spread around the world, so did their domesticated animals. Ten thousand years ago, not more than a few million sheep, cattle, goats, boars and chickens lived in restricted Afro-Asian niches. Today the world contains about a billion sheep, a billion pigs, more than a billion cattle, and more than 25 billion chickens. And they are all over the globe. The domesticated chicken is the most widespread fowl ever. Following Homo sapiens, domesticated cattle, pigs and sheep are the second, third and fourth most widespread large mammals in the world. From a narrow evolutionary perspective, which measures success by the number of DNA copies, the Agricultural Revolution was a wonderful boon for chickens, cattle, pigs and sheep.
Unfortunately, the evolutionary perspective is an incomplete measure of success. It judges everything by the criteria of survival and reproduction, with no regard for individual suffering and happiness. Domesticated chickens and cattle may well be an evolutionary success story, but they are also among the most miserable creatures that ever lived. The domestication of animals was founded on a series of brutal practices that only became crueller with the passing of the centuries.
The natural lifespan of wild chickens is about seven to twelve years, and of cattle about twenty to twenty-five years. In the wild, most chickens and cattle died long before that, but they still had a fair chance of living for a respectable number of years. In contrast, the vast majority of domesticated chickens and cattle are slaughtered at the age of between a few weeks and a few months, because this has always been the optimal slaughtering age from an economic perspective. (Why keep feeding a cock for three years if it has already reached its maximum weight after three months?)
Egg-laying hens, dairy cows and draught animals are sometimes allowed to live for many years. But the price is subjugation to a way of life completely alien to their urges and desires. It’s reasonable to assume, for example, that bulls prefer to spend their days wandering over open prairies in the company of other bulls and cows rather than pulling carts and ploughshares under the yoke of a whip-wielding ape.
In order to turn bulls, horses, donkeys and camels into obedient draught animals, their natural instincts and social ties had to be broken, their aggression and sexuality contained, and their freedom of movement curtailed. Farmers developed techniques such as locking animals inside pens and cages, bridling them in harnesses and leashes, training them with whips and cattle prods, and mutilating them. The process of taming almost always involves the castration of males. This restrains male aggression and enables humans selectively to control the herd’s procreation.

14. A painting from an Egyptian grave, c.1200 BC: A pair of oxen ploughing a field. In the wild, cattle roamed as they pleased in herds with a complex social structure. The castrated and domesticated ox wasted away his life under the lash and in a narrow pen, labouring alone or in pairs in a way that suited neither its body nor its social and emotional needs. When an ox could no longer pull the plough, it was slaughtered. (Note the hunched position of the Egyptian farmer who, much like the ox, spent his life in hard labour oppressive to his body, his mind and his social relationships.)

In many New Guinean societies, the wealth of a person has traditionally been determined by the number of pigs he or she owns. To ensure that the pigs can’t run away, farmers in northern New Guinea slice off a chunk of each pig’s nose. This causes severe pain whenever the pig tries to sniff. Since the pigs cannot find food or even find their way around without sniffing, this mutilation makes them completely dependent on their human owners. In another area of New Guinea, it has been customary to gouge out pigs’ eyes, so that they cannot even see where they’re going.7
The dairy industry has its own ways of forcing animals to do its will. Cows, goats and sheep produce milk only after giving birth to calves, kids and lambs, and only as long as the youngsters are suckling. To continue a supply of animal milk, a farmer needs to have calves, kids or lambs for suckling, but must prevent them from monopolising the milk. One common method throughout history was to simply slaughter the calves and kids shortly after birth, milk the mother for all she was worth, and then get her pregnant again. This is still a very widespread technique. In many modern dairy farms a milk cow usually lives for about five years before being slaughtered. During these five years she is almost constantly pregnant, and is fertilised within 60 to 120 days after giving birth in order to preserve maximum milk production. Her calves are separated from her shortly after birth. The females are reared to become the next generation of dairy cows, whereas the males are handed over to the care of the meat industry.8
Another method is to keep the calves and kids near their mothers, but prevent them by various stratagems from suckling too much milk. The simplest way to do that is to allow the kid or calf to start suckling, but drive it away once the milk starts flowing. This method usually encounters resistance from both kid and mother. Some shepherd tribes used to kill the offspring, eat its flesh, and then stuff the skin. The stuffed offspring was then presented to the mother so that its presence would encourage her milk production. The Nuer tribe in the Sudan went so far as to smear stuffed animals with their mother’s urine, to give the counterfeit calves a familiar, live scent. Another Nuer technique was to tie a ring of thorns around a calf’s mouth, so that it pricks the mother and causes her to resist suckling.9 Tuareg camel breeders in the Sahara used to puncture or cut off parts of the nose and upper lip of young camels in order to make suckling painful, thereby discouraging them from consuming too much milk.10
Not all agricultural societies were this cruel to their farm animals. The lives of some domesticated animals could be quite good. Sheep raised for wool, pet dogs and cats, war horses and race horses often enjoyed comfortable conditions. The Roman emperor Caligula allegedly planned to appoint his favourite horse, Incitatus, to the consulship. Shepherds and farmers throughout history showed affection for their animals and have taken great care of them, just as many slaveholders felt affection and concern for their slaves. It was no accident that kings and prophets styled themselves as shepherds and likened the way they and the gods cared for their people to a shepherd’s care for his flock.

15. A modern calf in an industrial meat farm. Immediately after birth the calf is separated from its mother and locked inside a tiny cage not much bigger than the calf’s own body. There the calf spends its entire life – about four months on average. It never leaves its cage, nor is it allowed to play with other calves or even walk – all so that its muscles will not grow strong. Soft muscles mean a soft and juicy steak. The first time the calf has a chance to walk, stretch its muscles and touch other calves is on its way to the slaughterhouse. In evolutionary terms, cattle represent one of the most successful animal species ever to exist. At the same time, they are some of the most miserable animals on the planet.

Yet from the viewpoint of the herd, rather than that of the shepherd, it’s hard to avoid the impression that for the vast majority of domesticated animals, the Agricultural Revolution was a terrible catastrophe. Their evolutionary ‘success’ is meaningless. A rare wild rhinoceros on the brink of extinction is probably more satisfied than a calf who spends its short life inside a tiny box, fattened to produce juicy steaks. The contented rhinoceros is no less content for being among the last of its kind. The numerical success of the calf’s species is little consolation for the suffering the individual endures.
This discrepancy between evolutionary success and individual suffering is perhaps the most important lesson we can draw from the Agricultural Revolution. When we study the narrative of plants such as wheat and maize, maybe the purely evolutionary perspective makes sense. Yet in the case of animals such as cattle, sheep and Sapiens, each with a complex world of sensations and emotions, we have to consider how evolutionary success translates into individual experience. In the following chapters we will see time and again how a dramatic increase in the collective power and ostensible success of our species went hand in hand with much individual suffering.6

Building Pyramids

THE AGRICULTURAL REVOLUTION IS ONE of the most controversial events in history. Some partisans proclaim that it set humankind on the road to prosperity and progress. Others insist that it led to perdition. This was the turning point, they say, where Sapiens cast off its intimate symbiosis with nature and sprinted towards greed and alienation. Whichever direction the road led, there was no going back. Farming enabled populations to increase so radically and rapidly that no complex agricultural society could ever again sustain itself if it returned to hunting and gathering. Around 10,000 BC, before the transition to agriculture, earth was home to about 5–8 million nomadic foragers. By the first century AD, only 1–2 million foragers remained (mainly in Australia, America and Africa), but their numbers were dwarfed by the world’s 250 million farmers.1
The vast majority of farmers lived in permanent settlements; only a few were nomadic shepherds. Settling down caused most peoples turf to shrink dramatically. Ancient hunter-gatherers usually lived in territories covering many dozens and even hundreds of square kilometres. ‘Home’ was the entire territory, with its hills, streams, woods and open sky. Peasants, on the other hand, spent most of their days working a small field or orchard, and their domestic lives centred on a cramped structure of wood, stone or mud, measuring no more than a few dozen metres – the house. The typical peasant developed a very strong attachment to this structure. This was a far-reaching revolution, whose impact was psychological as much as architectural. Henceforth, attachment to ‘my house’ and separation from the neighbours became the psychological hallmark of a much more self-centred creature.
The new agricultural territories were not only far smaller than those of ancient foragers, but also far more artificial. Aside from the use of fire, hunter-gatherers made few deliberate changes to the lands in which they roamed. Farmers, on the other hand, lived in artificial human islands that they laboriously carved out of the surrounding wilds. They cut down forests, dug canals, cleared fields, built houses, ploughed furrows, and planted fruit trees in tidy rows. The resulting artificial habitat was meant only for humans and ‘their’ plants and animals, and was often fenced off by walls and hedges. Farmer families did all they could to keep out wayward weeds and wild animals. If such interlopers made their way in, they were driven out. If they persisted, their human antagonists sought ways to exterminate them. Particularly strong defences were erected around the home. From the dawn of agriculture until this very day, billions of humans armed with branches, swatters, shoes and poison sprays have waged relentless war against the diligent ants, furtive roaches, adventurous spiders and misguided beetles that constantly infiltrate the human domicile.
For most of history these man-made enclaves remained very small, surrounded by expanses of untamed nature. The earth’s surface measures about 510 million square kilometres, of which 155 million is land. As late as AD 1400, the vast majority of farmers, along with their plants and animals, clustered together in an area of just 11 million square kilometres – 2 per cent of the planet’s surface.2 Everywhere else was too cold, too hot, too dry, too wet, or otherwise unsuited for cultivation. This minuscule 2 per cent of the earth’s surface constituted the stage on which history unfolded.
People found it difficult to leave their artificial islands. They could not abandon their houses, fields and granaries without grave risk of loss. Furthermore, as time went on they accumulated more and more things – objects, not easily transportable, that tied them down. Ancient farmers might seem to us dirt poor, but a typical family possessed more artefacts than an entire forager tribe.
The Coming of the Future

While agricultural space shrank, agricultural time expanded. Foragers usually didn’t waste much time thinking about next week or next month. Farmers sailed in their imagination years and decades into the future.
Foragers discounted the future because they lived from hand to mouth and could only preserve food or accumulate possessions with difficulty. Of course, they clearly engaged in some advanced planning. The creators of the cave paintings of Chauvet, Lascaux and Altamira almost certainly intended them to last for generations. Social alliances and political rivalries were long-term affairs. It often took years to repay a favour or to avenge a wrong. Nevertheless, in the subsistence economy of hunting and gathering, there was an obvious limit to such long-term planning. Paradoxically, it saved foragers a lot of anxieties. There was no sense in worrying about things that they could not influence.
The Agricultural Revolution made the future far more important than it had ever been before. Farmers must always keep the future in mind and must work in its service. The agricultural economy was based on a seasonal cycle of production, comprising long months of cultivation followed by short peak periods of harvest. On the night following the end of a plentiful harvest the peasants might celebrate for all they were worth, but within a week or so they were again up at dawn for a long day in the field. Although there was enough food for today, next week, and even next month, they had to worry about next year and the year after that.
Concern about the future was rooted not only in seasonal cycles of production, but also in the fundamental uncertainty of agriculture. Since most villages lived by cultivating a very limited variety of domesticated plants and animals, they were at the mercy of droughts, floods and pestilence. Peasants were obliged to produce more than they consumed so that they could build up reserves. Without grain in the silo, jars of olive oil in the cellar, cheese in the pantry and sausages hanging from the rafters, they would starve in bad years. And bad years were bound to come, sooner or later. A peasant living on the assumption that bad years would not come didn’t live long.
Consequently, from the very advent of agriculture, worries about the future became major players in the theatre of the human mind. Where farmers depended on rains to water their fields, the onset of the rainy season meant that each morning the farmers gazed towards the horizon, sniffing the wind and straining their eyes. Is that a cloud? Would the rains come on time? Would there be enough? Would violent storms wash the seeds from the fields and batter down seedlings? Meanwhile, in the valleys of the Euphrates, Indus and Yellow rivers, other peasants monitored, with no less trepidation, the height of the water. They needed the rivers to rise in order to spread the fertile topsoil washed down from the highlands, and to enable their vast irrigation systems to fill with water. But floods that surged too high or came at the wrong time could destroy their fields as much as a drought.
Peasants were worried about the future not just because they had more cause for worry, but also because they could do something about it. They could clear another field, dig another irrigation canal, sow more crops. The anxious peasant was as frenetic and hardworking as a harvester ant in the summer, sweating to plant olive trees whose oil would be pressed by his children and grandchildren, putting off until the winter or the following year the eating of the food he craved today.
The stress of farming had far-reaching consequences. It was the foundation of large-scale political and social systems. Sadly, the diligent peasants almost never achieved the future economic security they so craved through their hard work in the present. Everywhere, rulers and elites sprang up, living off the peasants’ surplus food and leaving them with only a bare subsistence.
These forfeited food surpluses fuelled politics, wars, art and philosophy. They built palaces, forts, monuments and temples. Until the late modern era, more than 90 per cent of humans were peasants who rose each morning to till the land by the sweat of their brows. The extra they produced fed the tiny minority of elites – kings, government officials, soldiers, priests, artists and thinkers – who fill the history books. History is something that very few people have been doing while everyone else was ploughing fields and carrying water buckets.
An Imagined Order

The food surpluses produced by peasants, coupled with new transportation technology, eventually enabled more and more people to cram together first into large villages, then into towns, and finally into cities, all of them joined together by new kingdoms and commercial networks.
Yet in order to take advantage of these new opportunities, food surpluses and improved transportation were not enough. The mere fact that one can feed a thousand people in the same town or a million people in the same kingdom does not guarantee that they can agree how to divide the land and water, how to settle disputes and conflicts, and how to act in times of drought or war. And if no agreement can be reached, strife spreads, even if the storehouses are bulging. It was not food shortages that caused most of history’s wars and revolutions. The French Revolution was spearheaded by affluent lawyers, not by famished peasants. The Roman Republic reached the height of its power in the first century BC, when treasure fleets from throughout the Mediterranean enriched the Romans beyond their ancestors’ wildest dreams. Yet it was at that moment of maximum affluence that the Roman political order collapsed into a series of deadly civil wars. Yugoslavia in 1991 had more than enough resources to feed all its inhabitants, and still disintegrated into a terrible bloodbath.
The problem at the root of such calamities is that humans evolved for millions of years in small bands of a few dozen individuals. The handful of millennia separating the Agricultural Revolution from the appearance of cities, kingdoms and empires was not enough time to allow an instinct for mass cooperation to evolve.
Despite the lack of such biological instincts, during the foraging era, hundreds of strangers were able to cooperate thanks to their shared myths. However, this cooperation was loose and limited. Every Sapiens band continued to run its life independently and to provide for most of its own needs. An archaic sociologist living 20,000 years ago, who had no knowledge of events following the Agricultural Revolution, might well have concluded that mythology had a fairly limited scope. Stories about ancestral spirits and tribal totems were strong enough to enable 500 people to trade seashells, celebrate the odd festival, and join forces to wipe out a Neanderthal band, but no more than that. Mythology, the ancient sociologist would have thought, could not possibly enable millions of strangers to cooperate on a daily basis.
But that turned out to be wrong. Myths, it transpired, are stronger than anyone could have imagined. When the Agricultural Revolution opened opportunities for the creation of crowded cities and mighty empires, people invented stories about great gods, motherlands and joint stock companies to provide the needed social links. While human evolution was crawling at its usual snail’s pace, the human imagination was building astounding networks of mass cooperation, unlike any other ever seen on earth.
Around 8500 BC the largest settlements in the world were villages such as Jericho, which contained a few hundred individuals. By 7000 BC the town of Çatalhöyük in Anatolia numbered between 5,000 and 10,000 individuals. It may well have been the world’s biggest settlement at the time. During the fifth and fourth millennia BC, cities with tens of thousands of inhabitants sprouted in the Fertile Crescent, and each of these held sway over many nearby villages. In 3100 BC the entire lower Nile Valley was united into the first Egyptian kingdom. Its pharaohs ruled thousands of square kilometres and hundreds of thousands of people. Around 2250 BC Sargon the Great forged the first empire, the Akkadian. It boasted over a million subjects and a standing army of 5,400 soldiers. Between 1000 BC and 500 BC, the first mega-empires appeared in the Middle East: the Late Assyrian Empire, the Babylonian Empire, and the Persian Empire. They ruled over many millions of subjects and commanded tens of thousands of soldiers.
In 221 BC the Qin dynasty united China, and shortly afterwards Rome united the Mediterranean basin. Taxes levied on 40 million Qin subjects paid for a standing army of hundreds of thousands of soldiers and a complex bureaucracy that employed more than 100,000 officials. The Roman Empire at its zenith collected taxes from up to 100 million subjects. This revenue financed a standing army of 250,000–500,000 soldiers, a road network still in use 1,500 years later, and theatres and amphitheatres that host spectacles to this day.

16. A stone stela inscribed with the Code of Hammurabi, c.1776 BC.

Impressive, no doubt, but we mustn’t harbour rosy illusions about ‘mass cooperation networks’ operating in pharaonic Egypt or the Roman Empire. ‘Cooperation’ sounds very altruistic, but is not always voluntary and seldom egalitarian. Most human cooperation networks have been geared towards oppression and exploitation. The peasants paid for the burgeoning cooperation networks with their precious food surpluses, despairing when the tax collector wiped out an entire year of hard labour with a single stroke of his imperial pen. The famed Roman amphitheatres were often built by slaves so that wealthy and idle Romans could watch other slaves engage in vicious gladiatorial combat. Even prisons and concentration camps are cooperation networks, and can function only because thousands of strangers somehow manage to coordinate their actions.

17. The Declaration of Independence of the United States, signed 4 July 1776.

All these cooperation networks – from the cities of ancient Mesopotamia to the Qin and Roman empires – were ‘imagined orders’. The social norms that sustained them were based neither on ingrained instincts nor on personal acquaintances, but rather on belief in shared myths.
How can myths sustain entire empires? We have already discussed one such example: Peugeot. Now let’s examine two of the best-known myths of history: the Code of Hammurabi of c.1776 BC, which served as a cooperation manual for hundreds of thousands of ancient Babylonians; and the American Declaration of Independence of 1776 AD, which today still serves as a cooperation manual for hundreds of millions of modern Americans.
In 1776 BC Babylon was the world’s biggest city. The Babylonian Empire was probably the world’s largest, with more than a million subjects. It ruled most of Mesopotamia, including the bulk of modern Iraq and parts of present-day Syria and Iran. The Babylonian king most famous today was Hammurabi. His fame is due primarily to the text that bears his name, the Code of Hammurabi. This was a collection of laws and judicial decisions whose aim was to present Hammurabi as a role model of a just king, serve as a basis for a more uniform legal system across the Babylonian Empire, and teach future generations what justice is and how a just king acts.
Future generations took notice. The intellectual and bureaucratic elite of ancient Mesopotamia canonised the text, and apprentice scribes continued to copy it long after Hammurabi died and his empire lay in ruins. Hammurabi’s Code is therefore a good source for understanding the ancient Mesopotamians’ ideal of social order.3
The text begins by saying that the gods Anu, Enlil and Marduk – the leading deities of the Mesopotamian pantheon – appointed Hammurabi ‘to make justice prevail in the land, to abolish the wicked and the evil, to prevent the strong from oppressing the weak’.4 It then lists about 300 judgements, given in the set formula ‘If such and such a thing happens, such is the judgment.’ For example, judgements 196–9 and 209–14 read:

196.   	If a superior man should blind the eye of another superior man, they shall blind his eye.
197.	If he should break the bone of another superior man, they shall break his bone.
198.	If he should blind the eye of a commoner or break the bone of a commoner, he shall weigh and deliver 60 shekels of silver.
199.	If he should blind the eye of a slave of a superior man or break the bone of a slave of a superior man, he shall weigh and deliver one-half of the slave’s value (in silver).5
209.	If a superior man strikes a woman of superior class and thereby causes her to miscarry her fetus, he shall weigh and deliver ten shekels of silver for her fetus.
210.	If that woman should die, they shall kill his daughter.
211.	If he should cause a woman of commoner class to miscarry her fetus by the beating, he shall weigh and deliver five shekels of silver.
212.	If that woman should die, he shall weigh and deliver thirty shekels of silver.
213.	If he strikes a slave-woman of a superior man and thereby causes her to miscarry her fetus, he shall weigh and deliver two shekels of silver.
214.	If that slave-woman should die, he shall weigh and deliver twenty shekels of silver.6

After listing his judgements, Hammurabi again declares that
These are the just decisions which Hammurabi, the able king, has established and thereby has directed the land along the course of truth and the correct way of life … I am Hammurabi, noble king. I have not been careless or negligent toward humankind, granted to my care by the god Enlil, and with whose shepherding the god Marduk charged me.7
Hammurabi’s Code asserts that Babylonian social order is rooted in universal and eternal principles of justice, dictated by the gods. The principle of hierarchy is of paramount importance. According to the code, people are divided into two genders and three classes: superior people, commoners and slaves. Members of each gender and class have different values. The life of a female commoner is worth thirty silver shekels and that of a slave-woman twenty silver shekels, whereas the eye of a male commoner is worth sixty silver shekels.
The code also establishes a strict hierarchy within families, according to which children are not independent persons, but rather the property of their parents. Hence, if one superior man kills the daughter of another superior man, the killer’s daughter is executed in punishment. To us it may seem strange that the killer remains unharmed whereas his innocent daughter is killed, but to Hammurabi and the Babylonians this seemed perfectly just. Hammurabi’s Code was based on the premise that if the king’s subjects all accepted their positions in the hierarchy and acted accordingly, the empire’s million inhabitants would be able to cooperate effectively. Their society could then produce enough food for its members, distribute it efficiently, protect itself against its enemies, and expand its territory so as to acquire more wealth and better security.
About 3,500 years after Hammurabi’s death, the inhabitants of thirteen British colonies in North America felt that the king of England was treating them unjustly. Their representatives gathered in the city of Philadelphia, and on 4 July 1776 the colonies declared that their inhabitants were no longer subjects of the British Crown. Their Declaration of Independence proclaimed universal and eternal principles of justice, which, like those of Hammurabi, were inspired by a divine power. However, the most important principle dictated by the American god was somewhat different from the principle dictated by the gods of Babylon. The American Declaration of Independence asserts that:
We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable rights, that among these are life, liberty, and the pursuit of happiness.
Like Hammurabi’s Code, the American founding document promises that if humans act according to its sacred principles, millions of them would be able to cooperate effectively, living safely and peacefully in a just and prosperous society. Like the Code of Hammurabi, the American Declaration of Independence was not just a document of its time and place – it was accepted by future generations as well. For more than 200 years, American schoolchildren have been copying and learning it by heart.
The two texts present us with an obvious dilemma. Both the Code of Hammurabi and the American Declaration of Independence claim to outline universal and eternal principles of justice, but according to the Americans all people are equal, whereas according to the Babylonians people are decidedly unequal. The Americans would, of course, say that they are right, and that Hammurabi is wrong. Hammurabi, naturally, would retort that he is right, and that the Americans are wrong. In fact, they are both wrong. Hammurabi and the American Founding Fathers alike imagined a reality governed by universal and immutable principles of justice, such as equality or hierarchy. Yet the only place where such universal principles exist is in the fertile imagination of Sapiens, and in the myths they invent and tell one another. These principles have no objective validity.
It is easy for us to accept that the division of people into ‘superiors’ and commoners’ is a figment of the imagination. Yet the idea that all humans are equal is also a myth. In what sense do all humans equal one another? Is there any objective reality, outside the human imagination, in which we are truly equal? Are all humans equal to one another biologically? Let us try to translate the most famous line of the American Declaration of Independence into biological terms:
We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable rights, that among these are life, liberty, and the pursuit of happiness.
According to the science of biology, people were not created’. They have evolved. And they certainly did not evolve to be ‘equal’. The idea of equality is inextricably intertwined with the idea of creation. The Americans got the idea of equality from Christianity, which argues that every person has a divinely created soul, and that all souls are equal before God. However, if we do not believe in the Christian myths about God, creation and souls, what does it mean that all people are ‘equal’? Evolution is based on difference, not on equality. Every person carries a somewhat different genetic code, and is exposed from birth to different environmental influences. This leads to the development of different qualities that carry with them different chances of survival. ‘Created equal’ should therefore be translated into ‘evolved differently’.
Just as people were never created, neither, according to the science of biology, is there a ‘Creator’ who ‘endows’ them with anything. There is only a blind evolutionary process, devoid of any purpose, leading to the birth of individuals. ‘Endowed by their creator’ should be translated simply into ‘born.
Equally, there are no such things as rights in biology. There are only organs, abilities and characteristics. Birds fly not because they have a right to fly, but because they have wings. And it’s not true that these organs, abilities and characteristics are ‘unalienable’. Many of them undergo constant mutations, and may well be completely lost over time. The ostrich is a bird that lost its ability to fly. So ‘unalienable rights’ should be translated into ‘mutable characteristics’.
And what are the characteristics that evolved in humans? ‘Life’, certainly. But ‘liberty’? There is no such thing in biology. Just like equality, rights and limited liability companies, liberty is something that people invented and that exists only in their imagination. From a biological viewpoint, it is meaningless to say that humans in democratic societies are free, whereas humans in dictatorships are unfree. And what about ‘happiness’? So far biological research has failed to come up with a clear definition of happiness or a way to measure it objectively. Most biological studies acknowledge only the existence of pleasure, which is more easily defined and measured. So ‘life, liberty, and the pursuit of happiness’ should be translated into ‘life and the pursuit of pleasure’.
So here is that line from the American Declaration of Independence translated into biological terms:
We hold these truths to be self-evident, that all men evolved differently, that they are born with certain mutable characteristics, and that among these are life and the pursuit of pleasure.
Advocates of equality and human rights may be outraged by this line of reasoning. Their response is likely to be, ‘We know that people are not equal biologically! But if we believe that we are all equal in essence, it will enable us to create a stable and prosperous society.’ I have no argument with that. This is exactly what I mean by ‘imagined order’. We believe in a particular order not because it is objectively true, but because believing in it enables us to cooperate effectively and forge a better society. Imagined orders are not evil conspiracies or useless mirages. Rather, they are the only way large numbers of humans can cooperate effectively. Bear in mind, though, that Hammurabi might have defended his principle of hierarchy using the same logic: ‘I know that superiors, commoners and slaves are not inherently different kinds of people. But if we believe that they are, it will enable us to create a stable and prosperous society.’
True Believers

It’s likely that more than a few readers squirmed in their chairs while reading the preceding paragraphs. Most of us today are educated to react in such a way. It is easy to accept that Hammurabi’s Code was a myth, but we do not want to hear that human rights are also a myth. If people realise that human rights exist only in the imagination, isn’t there a danger that our society will collapse? Voltaire said about God that ‘there is no God, but don’t tell that to my servant, lest he murder me at night’. Hammurabi would have said the same about his principle of hierarchy, and Thomas Jefferson about human rights. Homo sapiens has no natural rights, just as spiders, hyenas and chimpanzees have no natural rights. But don’t tell that to our servants, lest they murder us at night.
Such fears are well justified. A natural order is a stable order. There is no chance that gravity will cease to function tomorrow, even if people stop believing in it. In contrast, an imagined order is always in danger of collapse, because it depends upon myths, and myths vanish once people stop believing in them. In order to safeguard an imagined order, continuous and strenuous efforts are imperative. Some of these efforts take the shape of violence and coercion. Armies, police forces, courts and prisons are ceaselessly at work forcing people to act in accordance with the imagined order. If an ancient Babylonian blinded his neighbour, some violence was usually necessary in order to enforce the law of ‘an eye for an eye’. When, in 1860, a majority of American citizens concluded that African slaves are human beings and must therefore enjoy the right of liberty, it took a bloody civil war to make the southern states acquiesce.
However, an imagined order cannot be sustained by violence alone. It requires some true believers as well. Prince Talleyrand, who began his chameleon-like career under Louis XVI, later served the revolutionary and Napoleonic regimes, and switched loyalties in time to end his days working for the restored monarchy, summed up decades of governmental experience by saying that ‘You can do many things with bayonets, but it is rather uncomfortable to sit on them.’ A single priest often does the work of a hundred soldiers far more cheaply and effectively. Moreover, no matter how efficient bayonets are, somebody must wield them. Why should the soldiers, jailors, judges and police maintain an imagined order in which they do not believe? Of all human collective activities, the one most difficult to organise is violence. To say that a social order is maintained by military force immediately raises the question: what maintains the military order? It is impossible to organise an army solely by coercion. At least some of the commanders and soldiers must truly believe in something, be it God, honour, motherland, manhood or money.
An even more interesting question concerns those standing at the top of the social pyramid. Why should they wish to enforce an imagined order if they themselves don’t believe in it? It is quite common to argue that the elite may do so out of cynical greed. Yet a cynic who believes in nothing is unlikely to be greedy. It does not take much to provide the objective biological needs of Homo sapiens. After those needs are met, more money can be spent on building pyramids, taking holidays around the world, financing election campaigns, funding your favourite terrorist organisation, or investing in the stock market and making yet more money – all of which are activities that a true cynic would find utterly meaningless. Diogenes, the Greek philosopher who founded the Cynical school, lived in a barrel. When Alexander the Great once visited Diogenes as he was relaxing in the sun, and asked if there were anything he might do for him, the Cynic answered the all-powerful conqueror, ‘Yes, there is something you can do for me. Please move a little to the side. You are blocking the sunlight.’
This is why cynics don’t build empires and why an imagined order can be maintained only if large segments of the population – and in particular large segments of the elite and the security forces – truly believe in it. Christianity would not have lasted 2,000 years if the majority of bishops and priests failed to believe in Christ. American democracy would not have lasted 250 years if the majority of presidents and congressmen failed to believe in human rights. The modern economic system would not have lasted a single day if the majority of investors and bankers failed to believe in capitalism.
The Prison Walls

How do you cause people to believe in an imagined order such as Christianity, democracy or capitalism? First, you never admit that the order is imagined. You always insist that the order sustaining society is an objective reality created by the great gods or by the laws of nature. People are unequal, not because Hammurabi said so, but because Enlil and Marduk decreed it. People are equal, not because Thomas Jefferson said so, but because God created them that way. Free markets are the best economic system, not because Adam Smith said so, but because these are the immutable laws of nature.
You also educate people thoroughly. From the moment they are born, you constantly remind them of the principles of the imagined order, which are incorporated into anything and everything. They are incorporated into fairy tales, dramas, paintings, songs, etiquette, political propaganda, architecture, recipes and fashions. For example, today people believe in equality, so it’s fashionable for rich kids to wear jeans, which were originally working-class attire. In the Middle Ages people believed in class divisions, so no young nobleman would have worn a peasant’s smock. Back then, to be addressed as ‘Sir’ or ‘Madam’ was a rare privilege reserved for the nobility, and often purchased with blood. Today all polite correspondence, regardless of the recipient, begins with ‘Dear Sir or Madam’.
The humanities and social sciences devote most of their energies to explaining exactly how the imagined order is woven into the tapestry of life. In the limited space at our disposal we can only scratch the surface. Three main factors prevent people from realising that the order organising their lives exists only in their imagination:
a. The imagined order is embedded in the material world. Though the imagined order exists only in our minds, it can be woven into the material reality around us, and even set in stone. Most Westerners today believe in individualism. They believe that every human is an individual, whose worth does not depend on what other people think of him or her. Each of us has within ourselves a brilliant ray of light that gives value and meaning to our lives. In modern Western schools teachers and parents tell children that if their classmates make fun of them, they should ignore it. Only they themselves, not others, know their true worth.
In modern architecture, this myth leaps out of the imagination to take shape in stone and mortar. The ideal modern house is divided into many small rooms so that each child can have a private space, hidden from view, providing for maximum autonomy. This private room almost invariably has a door, and in many households it is accepted practice for the child to close, and perhaps lock, the door. Even parents are forbidden to enter without knocking and asking permission. The room is decorated as the child sees fit, with rock-star posters on the wall and dirty socks on the floor. Somebody growing up in such a space cannot help but imagine himself ‘an individual’, his true worth emanating from within rather than from without.
Medieval noblemen did not believe in individualism. Someone’s worth was determined by their place in the social hierarchy, and by what other people said about them. Being laughed at was a horrible indignity. Noblemen taught their children to protect their good name whatever the cost. Like modern individualism, the medieval value system left the imagination and was manifested in the stone of medieval castles. The castle rarely contained private rooms for children (or anyone else, for that matter). The teenage son of a medieval baron did not have a private room on the castle’s second floor, with posters of Richard the Lionheart and King Arthur on the walls and a locked door that his parents were not allowed to open. He slept alongside many other youths in a large hall. He was always on display and always had to take into account what others saw and said. Someone growing up in such conditions naturally concluded that a man’s true worth was determined by his place in the social hierarchy and by what other people said of him.8
b. The imagined order shapes our desires. Most people do not wish to accept that the order governing their lives is imaginary, but in fact every person is born into a pre-existing imagined order, and his or her desires are shaped from birth by its dominant myths. Our personal desires thereby become the imagined order’s most important defences.
For instance, the most cherished desires of present-day Westerners are shaped by romantic, nationalist, capitalist and humanist myths that have been around for centuries. Friends giving advice often tell each other, ‘Follow your heart.’ But the heart is a double agent that usually takes its instructions from the dominant myths of the day, and the very recommendation to ‘Follow your heart’ was implanted in our minds by a combination of nineteenth-century Romantic myths and twentieth-century consumerist myths. The Coca-Cola Company, for example, has marketed Diet Coke around the world under the slogan, ‘Diet Coke. Do what feels good.’
Even what people take to be their most personal desires are usually programmed by the imagined order. Let’s consider, for example, the popular desire to take a holiday abroad. There is nothing natural or obvious about this. A chimpanzee alpha male would never think of using his power in order to go on holiday into the territory of a neighbouring chimpanzee band. The elite of ancient Egypt spent their fortunes building pyramids and having their corpses mummified, but none of them thought of going shopping in Babylon or taking a skiing holiday in Phoenicia. People today spend a great deal of money on holidays abroad because they are true believers in the myths of romantic consumerism.
Romanticism tells us that in order to make the most of our human potential we must have as many different experiences as we can. We must open ourselves to a wide spectrum of emotions; we must sample various kinds of relationships; we must try different cuisines; we must learn to appreciate different styles of music. One of the best ways to do all that is to break free from our daily routine, leave behind our familiar setting, and go travelling in distant lands, where we can ‘experience’ the culture, the smells, the tastes and the norms of other people. We hear again and again the romantic myths about ‘how a new experience opened my eyes and changed my life’.
Consumerism tells us that in order to be happy we must consume as many products and services as possible. If we feel that something is missing or not quite right, then we probably need to buy a product (a car, new clothes, organic food) or a service (housekeeping, relationship therapy, yoga classes). Every television commercial is another little legend about how consuming some product or service will make life better.
Romanticism, which encourages variety, meshes perfectly with consumerism. Their marriage has given birth to the infinite ‘market of experiences’, on which the modern tourism industry is founded. The tourism industry does not sell flight tickets and hotel bedrooms. It sells experiences. Paris is not a city, nor India a country – they are both experiences, the consumption of which is supposed to widen our horizons, fulfil our human potential, and make us happier. Consequently, when the relationship between a millionaire and his wife is going through a rocky patch, he takes her on an expensive trip to Paris. The trip is not a reflection of some independent desire, but rather of an ardent belief in the myths of romantic consumerism. A wealthy man in ancient Egypt would never have dreamed of solving a relationship crisis by taking his wife on holiday to Babylon. Instead, he might have built for her the sumptuous tomb she had always wanted.

18. The Great Pyramid of Giza. The kind of thing rich people in ancient Egypt did with their money.

Like the elite of ancient Egypt, most people in most cultures dedicate their lives to building pyramids. Only the names, shapes and sizes of these pyramids change from one culture to the other. They may take the form, for example, of a suburban cottage with a swimming pool and an evergreen lawn, or a gleaming penthouse with an enviable view. Few question the myths that cause us to desire the pyramid in the first place.
c. The imagined order is inter-subjective. Even if by some superhuman effort I succeed in freeing my personal desires from the grip of the imagined order, I am just one person. In order to change the imagined order I must convince millions of strangers to cooperate with me. For the imagined order is not a subjective order existing in my own imagination – it is rather an inter-subjective order, existing in the shared imagination of thousands and millions of people.
In order to understand this, we need to understand the difference between ‘objective’, ‘subjective’, and ‘inter-subjective’.
An objective phenomenon exists independently of human consciousness and human beliefs. Radioactivity, for example, is not a myth. Radioactive emissions occurred long before people discovered them, and they are dangerous even when people do not believe in them. Marie Curie, one of the discoverers of radioactivity, did not know, during her long years of studying radioactive materials, that they could harm her body. While she did not believe that radioactivity could kill her, she nevertheless died of aplastic anaemia, a disease caused by overexposure to radioactive materials.
The subjective is something that exists depending on the consciousness and beliefs of a single individual. It disappears or changes if that particular individual changes his or her beliefs. Many a child believes in the existence of an imaginary friend who is invisible and inaudible to the rest of the world. The imaginary friend exists solely in the child’s subjective consciousness, and when the child grows up and ceases to believe in it, the imaginary friend fades away.
The inter-subjective is something that exists within the communication network linking the subjective consciousness of many individuals. If a single individual changes his or her beliefs, or even dies, it is of little importance. However, if most individuals in the network die or change their beliefs, the inter-subjective phenomenon will mutate or disappear. Inter-subjective phenomena are neither malevolent frauds nor insignificant charades. They exist in a different way from physical phenomena such as radioactivity, but their impact on the world may still be enormous. Many of history’s most important drivers are inter-subjective: law, money, gods, nations.
Peugeot, for example, is not the imaginary friend of Peugeot’s CEO. The company exists in the shared imagination of millions of people. The CEO believes in the company’s existence because the board of directors also believes in it, as do the company’s lawyers, the secretaries in the nearby office, the tellers in the bank, the brokers on the stock exchange, and car dealers from France to Australia. If the CEO alone were suddenly to stop believing in Peugeot’s existence, he’d quickly land in the nearest mental hospital and someone else would occupy his office.
Similarly, the dollar, human rights and the United States of America exist in the shared imagination of billions, and no single individual can threaten their existence. If I alone were to stop believing in the dollar, in human rights, or in the United States, it wouldn’t much matter. These imagined orders are inter-subjective, so in order to change them we must simultaneously change the consciousness of billions of people, which is not easy. A change of such magnitude can be accomplished only with the help of a complex organisation, such as a political party, an ideological movement, or a religious cult. However, in order to establish such complex organisations, it’s necessary to convince many strangers to cooperate with one another. And this will happen only if these strangers believe in some shared myths. It follows that in order to change an existing imagined order, we must first believe in an alternative imagined order.
In order to dismantle Peugeot, for example, we need to imagine something more powerful, such as the French legal system. In order to dismantle the French legal system we need to imagine something even more powerful, such as the French state. And if we would like to dismantle that too, we will have to imagine something yet more powerful.
There is no way out of the imagined order. When we break down our prison walls and run towards freedom, we are in fact running into the more spacious exercise yard of a bigger prison.7

Memory Overload

EVOLUTION DID NOT ENDOW HUMANS with the ability to play football. True, it produced legs for kicking, elbows for fouling and mouths for cursing, but all that this enables us to do is perhaps practise penalty kicks by ourselves. To get into a game with the strangers we find in the schoolyard on any given afternoon, we not only have to work in concert with ten teammates we may never have met before, we also need to know that the eleven players on the opposing team are playing by the same rules. Other animals that engage strangers in ritualised aggression do so largely by instinct – puppies throughout the world have the rules for rough-and-tumble play hard-wired into their genes. But human teenagers have no genes for football. They can nevertheless play the game with complete strangers because they have all learned an identical set of ideas about football. These ideas are entirely imaginary, but if everyone shares them, we can all play the game.
The same applies, on a larger scale, to kingdoms, churches and trade networks, with one important difference. The rules of football are relatively simple and concise, much like those necessary for cooperation in a forager band or small village. Each player can easily store them in his brain and still have room for songs, images and shopping lists. But large systems of cooperation that involve not twenty-two but thousands or even millions of humans require the handling and storage of huge amounts of information, much more than any single human brain can contain and process.
The large societies found in some other species, such as ants and bees, are stable and resilient because most of the information needed to sustain them is encoded in the genome. A female honeybee larva can, for example, grow up to be either a queen or a worker, depending on what food it is fed. Its DNA programmes the necessary behaviours for whatever role it will fulfil in life. Hives can be very complex social structures, containing many different kinds of workers, such as harvesters, nurses and cleaners. But so far researchers have failed to locate lawyer bees. Bees don’t need lawyers, because there is no danger that they might forget or violate the hive constitution. The queen does not cheat the cleaner bees of their food, and they never go on strike demanding higher wages.
But humans do such things all the time. Because the Sapiens social order is imagined, humans cannot preserve the critical information for running it simply by making copies of their DNA and passing these on to their progeny. A conscious effort has to be made to sustain laws, customs, procedures and manners, otherwise the social order would quickly collapse. For example, King Hammurabi decreed that people are divided into superiors, commoners and slaves. Unlike the beehive class system, this is not a natural division – there is no trace of it in the human genome. If the Babylonians could not keep this ‘truth’ in mind, their society would have ceased to function. Similarly, when Hammurabi passed his DNA to his offspring, it did not encode his ruling that a superior man who killed a commoner woman must pay thirty silver shekels. Hammurabi deliberately had to instruct his sons in the laws of his empire, and his sons and grandsons had to do the same.
Empires generate huge amounts of information. Beyond laws, empires have to keep accounts of transactions and taxes, inventories of military supplies and merchant vessels, and calendars of festivals and victories. For millions of years people stored information in a single place – their brains. Unfortunately, the human brain is not a good storage device for empire-sized databases, for three main reasons.
First, its capacity is limited. True, some people have astonishing memories, and in ancient times there were memory professionals who could store in their heads the topographies of whole provinces and the law codes of entire states. Nevertheless, there is a limit that even master mnemonists cannot transcend. A lawyer might know by heart the entire law code of the Commonwealth of Massachusetts, but not the details of every legal proceeding that took place in Massachusetts from the Salem witch trials onward.
Secondly, humans die, and their brains die with them. Any information stored in a brain will be erased in less than a century. It is, of course, possible to pass memories from one brain to another, but after a few transmissions, the information tends to get garbled or lost.
Thirdly and most importantly, the human brain has been adapted to store and process only particular types of information. In order to survive, ancient hunter-gatherers had to remember the shapes, qualities and behaviour patterns of thousands of plant and animal species. They had to remember that a wrinkled yellow mushroom growing in autumn under an elm tree is most probably poisonous, whereas a similar-looking mushroom growing in winter under an oak tree is a good stomach-ache remedy. Hunter-gatherers also had to bear in mind the opinions and relations of several dozen band members. If Lucy needed a band member’s help to get John to stop harassing her, it was important for her to remember that John had fallen out last week with Mary, who would thus be a likely and enthusiastic ally. Consequently, evolutionary pressures have adapted the human brain to store immense quantities of botanical, zoological, topographical and social information.
But when particularly complex societies began to appear in the wake of the Agricultural Revolution, a completely new type of information became vital – numbers. Foragers were never obliged to handle large amounts of mathematical data. No forager needed to remember, say, the number of fruit on each tree in the forest. So human brains did not adapt to storing and processing numbers. Yet in order to maintain a large kingdom, mathematical data was vital. It was never enough to legislate laws and tell stories about guardian gods. One also had to collect taxes. In order to tax hundreds of thousands of people, it was imperative to collect data about peoples incomes and possessions; data about payments made; data about arrears, debts and fines; data about discounts and exemptions. This added up to millions of data bits, which had to be stored and processed. Without this capacity, the state would never know what resources it had and what further resources it could tap. When confronted with the need to memorise, recall and handle all these numbers, most human brains overdosed or fell asleep.
This mental limitation severely constrained the size and complexity of human collectives. When the amount of people and property in a particular society crossed a critical threshold, it became necessary to store and process large amounts of mathematical data. Since the human brain could not do it, the system collapsed. For thousands of years after the Agricultural Revolution, human social networks remained relatively small and simple.
The first to overcome the problem were the ancient Sumerians, who lived in southern Mesopotamia. There, a scorching sun beating upon rich muddy plains produced plentiful harvests and prosperous towns. As the number of inhabitants grew, so did the amount of information required to coordinate their affairs. Between the years 3500 BC and 3000 BC, some unknown Sumerian geniuses invented a system for storing and processing information outside their brains, one that was custom-built to handle large amounts of mathematical data. The Sumerians thereby released their social order from the limitations of the human brain, opening the way for the appearance of cities, kingdoms and empires. The data-processing system invented by the Sumerians is called ‘writing’.
Signed, Kushim

Writing is a method for storing information through material signs. The Sumerian writing system did so by combining two types of signs, which were pressed in clay tablets. One type of signs represented numbers. There were signs for 1, 10, 60, 600, 3,600 and 36,000. (The Sumerians used a combination of base-6 and base-10 numeral systems. Their base-6 system bestowed on us several important legacies, such as the division of the day into twenty-four hours and of the circle into 360 degrees.) The other type of signs represented people, animals, merchandise, territories, dates and so forth. By combining both types of signs the Sumerians were able to preserve far more data than any human brain could remember or any DNA chain could encode.

19. A clay tablet with an administrative text from the city of Uruk, c.3400–3000 BC. ‘Kushim’ may be the generic title of an officeholder, or the name of a particular individual. If Kushim was indeed a person, he may be the first individual in history whose name is known to us! All the names applied earlier in human history – the Neanderthals, the Natufians, Chauvet Cave, Göbekli Tepe – are modern inventions. We have no idea what the builders of Göbekli Tepe actually called the place. With the appearance of writing, we are beginning to hear history through the ears of its protagonists. When Kushim’s neighbours called out to him, they might really have shouted ‘Kushim!’ It is telling that the first recorded name in history belongs to an accountant, rather than a prophet, a poet or a great conqueror.1

At this early stage, writing was limited to facts and figures. The great Sumerian novel, if there ever was one, was never committed to clay tablets. Writing was time-consuming and the reading public tiny, so no one saw any reason to use it for anything other than essential record-keeping. If we look for the first words of wisdom reaching us from our ancestors, 5,000 years ago, we’re in for a big disappointment. The earliest messages our ancestors have left us read, for example, ‘29,086 measures barley 37 months Kushim.’ The most probable reading of this sentence is: ‘A total of 29,086 measures of barley were received over the course of 37 months. Signed, Kushim.’ Alas, the first texts of history contain no philosophical insights, no poetry, legends, laws, or even royal triumphs. They are humdrum economic documents, recording the payment of taxes, the accumulation of debts and the ownership of property.

Partial script cannot express the entire spectrum of a spoken language, but it can express things that fall outside the scope of spoken language. Partial scripts such as the Sumerian and mathematical scripts cannot be used to write poetry, but they can keep tax accounts very effectively.

Only one other type of text survived from these ancient days, and it is even less exciting: lists of words, copied over and over again by apprentice scribes as training exercises. Even had a bored student wanted to write out some of his poems instead of copy a bill of sale, he could not have done so. The earliest Sumerian writing was a partial rather than a full script. Full script is a system of material signs that can represent spoken language more or less completely. It can therefore express everything people can say, including poetry. Partial script, on the other hand, is a system of material signs that can represent only particular types of information, belonging to a limited field of activity. Latin script, ancient Egyptian hieroglyphics and Braille are full scripts. You can use them to write tax registers, love poems, history books, food recipes and business law. In contrast, the earliest Sumerian script, like modern mathematical symbols and musical notation, are partial scripts. You can use mathematical script to make calculations, but you cannot use it to write love poems.

20. A man holding a quipu, as depicted in a Spanish manuscript following the fall of the Inca Empire.

It didn’t disturb the Sumerians that their script was ill-suited for writing poetry. They didn’t invent it in order to copy spoken language, but rather to do things that spoken language failed at. There were some cultures, such as those of the pre-Columbian Andes, which used only partial scripts throughout their entire histories, unfazed by their scripts’ limitations and feeling no need for a full version. Andean script was very different from its Sumerian counterpart. In fact, it was so different that many people would argue it wasn’t a script at all. It was not written on clay tablets or pieces of paper. Rather, it was written by tying knots on colourful cords called quipus. Each quipu consisted of many cords of different colours, made of wool or cotton. On each cord, several knots were tied in different places. A single quipu could contain hundreds of cords and thousands of knots. By combining different knots on different cords with different colours, it was possible to record large amounts of mathematical data relating to, for example, tax collection and property ownership.2
For hundreds, perhaps thousands of years, quipus were essential to the business of cities, kingdoms and empires.3 They reached their full potential under the Inca Empire, which ruled 10–12 million people and covered today’s Peru, Ecuador and Bolivia, as well as chunks of Chile, Argentina and Colombia. Thanks to quipus, the Incas could save and process large amounts of data, without which they would not have been able to maintain the complex administrative machinery that an empire of that size requires.
In fact, quipus were so effective and accurate that in the early years following the Spanish conquest of South America, the Spaniards themselves employed quipus in the work of administering their new empire. The problem was that the Spaniards did not themselves know how to record and read quipus, making them dependent on local professionals. The continent’s new rulers realised that this placed them in a tenuous position – the native quipu experts could easily mislead and cheat their overlords. So once Spain’s dominion was more firmly established, quipus were phased out and the new empire’s records were kept entirely in Latin script and numerals. Very few quipus survived the Spanish occupation, and most of those remaining are undecipherable, since, unfortunately, the art of reading quipus has been lost.
The Wonders of Bureaucracy

The Mesopotamians eventually started to want to write down things other than monotonous mathematical data. Between 3000 BC and 2500 BC more and more signs were added to the Sumerian system, gradually transforming it into a full script that we today call cuneiform. By 2500 BC, kings were using cuneiform to issue decrees, priests were using it to record oracles, and less exalted citizens were using it to write personal letters. At roughly the same time, Egyptians developed another full script known as hieroglyphics. Other full scripts were developed in China around 1200 BC and in Central America around 1000–500 BC.
From these initial centres, full scripts spread far and wide, taking on various new forms and novel tasks. People began to write poetry, history books, romances, dramas, prophecies and cookbooks. Yet writing’s most important task continued to be the storage of reams of mathematical data, and that task remained the prerogative of partial script. The Hebrew Bible, the Greek Iliad, the Hindu Mahabharata and the Buddhist Tipitika all began as oral works. For many generations they were transmitted orally and would have lived on even had writing never been invented. But tax registries and complex bureaucracies were born together with partial script, and the two remain inexorably linked to this day like Siamese twins – think of the cryptic entries in computerised data bases and spreadsheets.
As more and more things were written, and particularly as administrative archives grew to huge proportions, new problems appeared. Information stored in a persons brain is easy to retrieve. My brain stores billions of bits of data, yet I can quickly, almost instantaneously, recall the name of Italy’s capital, immediately afterwards recollect what I did on 11 September 2001, and then reconstruct the route leading from my house to the Hebrew University in Jerusalem. Exactly how the brain does it remains a mystery, but we all know that the brain’s retrieval system is amazingly efficient, except when you are trying to remember where you put your car keys.
How, though, do you find and retrieve information stored on quipu cords or clay tablets? If you have just ten tablets or a hundred tablets, it’s not a problem. But what if you have accumulated thousands of them, as did one of Hammurabi’s contemporaries, King Zimrilim of Mari?
Imagine for a moment that it’s 1776 BC. Two Marians are quarrelling over possession of a wheat field. Jacob insists that he bought the field from Esau thirty years ago. Esau retorts that he in fact rented the field to Jacob for a term of thirty years, and that now, the term being up, he intends to reclaim it. They shout and wrangle and start pushing one another before they realise that they can resolve their dispute by going to the royal archive, where are housed the deeds and bills of sale that apply to all the kingdom’s real estate. Upon arriving at the archive they are shuttled from one official to the other. They wait through several herbal tea breaks, are told to come back tomorrow, and eventually are taken by a grumbling clerk to look for the relevant clay tablet. The clerk opens a door and leads them into a huge room lined, floor to ceiling, with thousands of clay tablets. No wonder the clerk is sour-faced. How is he supposed to locate the deed to the disputed wheat field written thirty years ago? Even if he finds it, how will he be able to cross-check to ensure that the one from thirty years ago is the latest document relating to the field in question? If he can’t find it, does that prove that Esau never sold or rented out the field? Or just that the document got lost, or turned to mush when some rain leaked into the archive?
Clearly, just imprinting a document in clay is not enough to guarantee efficient, accurate and convenient data processing. That requires methods of organisation like catalogues, methods of reproduction like photocopy machines, methods of rapid and accurate retrieval like computer algorithms, and pedantic (but hopefully cheerful) librarians who know how to use these tools.
Inventing such methods proved to be far more difficult than inventing writing. Many writing systems developed independently in cultures distant in time and place from each other. Every decade archaeologists discover another few forgotten scripts. Some of them might prove to be even older than the Sumerian scratches in clay. But most of them remain curiosities because those who invented them failed to invent efficient ways of cataloguing and retrieving data. What set apart Sumer, as well as pharaonic Egypt, ancient China and the Inca Empire, is that these cultures developed good techniques of archiving, cataloguing and retrieving written records. They also invested in schools for scribes, clerks, librarians and accountants.
A writing exercise from a school in ancient Mesopotamia discovered by modern archaeologists gives us a glimpse into the lives of these students, some 4,000 years ago:
I went in and sat down, and my teacher read my tablet. He said, ‘There’s something missing!’
And he caned me.
One of the people in charge said, ‘Why did you open your mouth without my permission?’
And he caned me.
The one in charge of rules said, ‘Why did you get up without my permission?’
And he caned me.
The gatekeeper said, ‘Why are you going out without my permission?’ And he caned me.
The keeper of the beer jug said, ‘Why did you get some without my permission?’
And he caned me.
The Sumerian teacher said, ‘Why did you speak Akkadian?’*
And he caned me.
My teacher said, ‘Your handwriting is no good!’
And he caned me.4
Ancient scribes learned not merely to read and write, but also to use catalogues, dictionaries, calendars, forms and tables. They studied and internalised techniques of cataloguing, retrieving and processing information very different from those used by the brain. In the brain, all data is freely associated. When I go with my spouse to sign on a mortgage for our new home, I am reminded of the first place we lived together, which reminds me of our honeymoon in New Orleans, which reminds me of alligators, which remind me of dragons, which remind me of The Ring of the Nibelungen, and suddenly, before I know it, there I am humming the Siegfried leitmotif to a puzzled bank clerk. In bureaucracy, things must be kept apart. There is one drawer for home mortgages, another for marriage certificates, a third for tax registers, and a fourth for lawsuits. Otherwise, how can you find anything? Things that belong in more than one drawer, like Wagnerian music dramas (do I file them under ‘music’, ‘theatre’, or perhaps invent a new category altogether?), are a terrible headache. So one is forever adding, deleting and rearranging drawers.
In order to function, the people who operate such a system of drawers must be reprogrammed to stop thinking as humans and to start thinking as clerks and accountants. As everyone from ancient times till today knows, clerks and accountants think in a non-human fashion. They think like filing cabinets. This is not their fault. If they don’t think that way their drawers will all get mixed up and they won’t be able to provide the services their government, company or organisation requires. The most important impact of script on human history is precisely this: it has gradually changed the way humans think and view the world. Free association and holistic thought have given way to compartmentalisation and bureaucracy.
The Language of Numbers

As the centuries passed, bureaucratic methods of data processing grew ever more different from the way humans naturally think – and ever more important. A critical step was made sometime before the ninth century AD, when a new partial script was invented, one that could store and process mathematical data with unprecedented efficiency. This partial script was composed of ten signs, representing the numbers from 0 to 9. Confusingly, these signs are known as Arabic numerals even though they were first invented by the Hindus (even more confusingly, modern Arabs use a set of digits that look quite different from Western ones). But the Arabs get the credit because when they invaded India they encountered the system, understood its usefulness, refined it, and spread it through the Middle East and then to Europe. When several other signs were later added to the Arab numerals (such as the signs for addition, subtraction and multiplication), the basis of modern mathematical notation came into being.
Although this system of writing remains a partial script, it has become the world’s dominant language. Almost all states, companies, organisations and institutions – whether they speak Arabic, Hindi, English or Norwegian – use mathematical script to record and process data. Every piece of information that can be translated into mathematical script is stored, spread and processed with mind-boggling speed and efficiency.
A person who wishes to influence the decisions of governments, organisations and companies must therefore learn to speak in numbers. Experts do their best to translate even ideas such as ‘poverty’, ‘happiness’ and ‘honesty’ into numbers (‘the poverty line’, ‘subjective well-being levels’, ‘credit rating’). Entire fields of knowledge, such as physics and engineering, have already lost almost all touch with the spoken human language, and are maintained solely by mathematical script.

An equation for calculating the acceleration of mass i under the influence of gravity, according to the Theory of Relativity. When most laypeople see such an equation, they usually panic and freeze, like a deer caught in the headlights of a speeding vehicle. The reaction is quite natural, and does not betray a lack of intelligence or curiosity. With rare exceptions, human brains are simply incapable of thinking through concepts like relativity and quantum mechanics. Physicists nevertheless manage to do so, because they set aside the traditional human way of thinking, and learn to think anew with the help of external data-processing systems. Crucial parts of their thought process take place not in the head, but inside computers or on classroom blackboards.

More recently, mathematical script has given rise to an even more revolutionary writing system, a computerised binary script consisting of only two signs: 0 and 1. The words I am now typing on my keyboard are written within my computer by different combinations of 0 and 1.
Writing was born as the maidservant of human consciousness, but is increasingly becoming its master. Our computers have trouble understanding how Homo sapiens talks, feels and dreams. So we are teaching Homo sapiens to talk, feel and dream in the language of numbers, which can be understood by computers.
And this is not the end of the story. The field of artificial intelligence is seeking to create a new kind of intelligence based solely on the binary script of computers. Science-fiction movies such as The Matrix and The Terminator tell of a day when the binary script throws off the yoke of humanity. When humans try to regain control of the rebellious script, it responds by attempting to wipe out the human race.
* Even after Akkadian became the spoken language, Sumerian remained the language of administration and thus the language recorded with writing. Aspiring scribes thus had to speak Sumerian.8

There is No Justice in History

UNDERSTANDING HUMAN HISTORY IN THE millennia following the Agricultural Revolution boils down to a single question: how did humans organise themselves in mass-cooperation networks, when they lacked the biological instincts necessary to sustain such networks? The short answer is that humans created imagined orders and devised scripts. These two inventions filled the gaps left by our biological inheritance.
However, the appearance of these networks was, for many, a dubious blessing. The imagined orders sustaining these networks were neither neutral nor fair. They divided people into make-believe groups, arranged in a hierarchy. The upper levels enjoyed privileges and power, while the lower ones suffered from discrimination and oppression. Hammurabi’s Code, for example, established a pecking order of superiors, commoners and slaves. Superiors got all the good things in life. Commoners got what was left. Slaves got a beating if they complained.
Despite its proclamation of the equality of all men, the imagined order established by the Americans in 1776 also established a hierarchy. It created a hierarchy between men, who benefited from it, and women, whom it left disempowered. It created a hierarchy between whites, who enjoyed liberty, and blacks and American Indians, who were considered humans of a lesser type and therefore did not share in the equal rights of men. Many of those who signed the Declaration of Independence were slaveholders. They did not release their slaves upon signing the Declaration, nor did they consider themselves hypocrites. In their view, the rights of men had little to do with Negroes.
The American order also consecrated the hierarchy between rich and poor. Most Americans at that time had little problem with the inequality caused by wealthy parents passing their money and businesses on to their children. In their view, equality meant simply that the same laws applied to rich and poor. It had nothing to do with unemployment benefits, integrated education or health insurance. Liberty, too, carried very different connotations than it does today. In 1776, it did not mean that the disempowered (certainly not blacks or Indians or, God forbid, women) could gain and exercise power. It meant simply that the state could not, except in unusual circumstances, confiscate a citizen’s private property or tell him what to do with it. The American order thereby upheld the hierarchy of wealth, which some thought was mandated by God and others viewed as representing the immutable laws of nature. Nature, it was claimed, rewarded merit with wealth while penalising indolence.
All the above-mentioned distinctions – between free persons and slaves, between whites and blacks, between rich and poor – are rooted in fictions. (The hierarchy of men and women will be discussed later.) Yet it is an iron rule of history that every imagined hierarchy disavows its fictional origins and claims to be natural and inevitable. For instance, many people who have viewed the hierarchy of free persons and slaves as natural and correct have argued that slavery is not a human invention. Hammurabi saw it as ordained by the gods. Aristotle argued that slaves have a ‘slavish nature’ whereas free people have a ‘free nature’. Their status in society is merely a reflection of their innate nature.
Ask white supremacists about the racial hierarchy, and you are in for a pseudoscientific lecture concerning the biological differences between the races. You are likely to be told that there is something in Caucasian blood or genes that makes whites naturally more intelligent, moral and hardworking. Ask a diehard capitalist about the hierarchy of wealth, and you are likely to hear that it is the inevitable outcome of objective differences in abilities. The rich have more money, in this view, because they are more capable and diligent. No one should be bothered, then, if the wealthy get better health care, better education and better nutrition. The rich richly deserve every perk they enjoy.

21. A sign on a South African beach from the period of apartheid, restricting its usage to whites’ only. People with lighter skin colour are typically more in danger of sunburn than people with darker skin. Yet there was no biological logic behind the division of South African beaches. Beaches reserved for people with lighter skin were not characterised by lower levels of ultraviolet radiation.

Hindus who adhere to the caste system believe that cosmic forces have made one caste superior to another. According to a famous Hindu creation myth, the gods fashioned the world out of the body of a primeval being, the Purusa. The sun was created from the Purusa’s eye, the moon from the Purusa’s brain, the Brahmins (priests) from its mouth, the Kshatriyas (warriors) from its arms, the Vaishyas (peasants and merchants) from its thighs, and the Shudras (servants) from its legs. Accept this explanation and the sociopolitical differences between Brahmins and Shudras are as natural and eternal as the differences between the sun and the moon.1 The ancient Chinese believed that when the goddess Nü Wa created humans from earth, she kneaded aristocrats from fine yellow soil, whereas commoners were formed from brown mud.2
Yet, to the best of our understanding, these hierarchies are all the product of human imagination. Brahmins and Shudras were not really created by the gods from different body parts of a primeval being. Instead, the distinction between the two castes was created by laws and norms invented by humans in northern India about 3,000 years ago. Contrary to Aristotle, there is no known biological difference between slaves and free people. Human laws and norms have turned some people into slaves and others into masters. Between blacks and whites there are some objective biological differences, such as skin colour and hair type, but there is no evidence that the differences extend to intelligence or morality.
Most people claim that their social hierarchy is natural and just, while those of other societies are based on false and ridiculous criteria. Modern Westerners are taught to scoff at the idea of racial hierarchy. They are shocked by laws prohibiting blacks to live in white neighbourhoods, or to study in white schools, or to be treated in white hospitals. But the hierarchy of rich and poor – which mandates that rich people live in separate and more luxurious neighbourhoods, study in separate and more prestigious schools, and receive medical treatment in separate and better-equipped facilities – seems perfectly sensible to many Americans and Europeans. Yet it’s a proven fact that most rich people are rich for the simple reason that they were born into a rich family, while most poor people will remain poor throughout their lives simply because they were born into a poor family.
Unfortunately, complex human societies seem to require imagined hierarchies and unjust discrimination. Of course not all hierarchies are morally identical, and some societies suffered from more extreme types of discrimination than others, yet scholars know of no large society that has been able to dispense with discrimination altogether. Time and again people have created order in their societies by classifying the population into imagined categories, such as superiors, commoners and slaves; whites and blacks; patricians and plebeians; Brahmins and Shudras; or rich and poor. These categories have regulated relations between millions of humans by making some people legally, politically or socially superior to others.
Hierarchies serve an important function. They enable complete strangers to know how to treat one another without wasting the time and energy needed to become personally acquainted. In George Bernard Shaw’s Pygmalion, Henry Higgins doesn’t need to establish an intimate acquaintance with Eliza Doolittle in order to understand how he should relate to her. Just hearing her talk tells him that she is a member of the underclass with whom he can do as he wishes – for example, using her as a pawn in his bet to pass off a flower girl as a duchess. A modern Eliza working at a florist’s needs to know how much effort to put into selling roses and gladioli to the dozens of people who enter the shop each day. She can’t make a detailed enquiry into the tastes and wallets of each individual. Instead, she uses social cues – the way the person is dressed, his or her age, and if she’s not politically correct his skin colour. That is how she immediately distinguishes between the accounting-firm partner who’s likely to place a large order for expensive roses, and a messenger boy who can only afford a bunch of daisies.
Of course, differences in natural abilities also play a role in the formation of social distinctions. But such diversities of aptitudes and character are usually mediated through imagined hierarchies. This happens in two important ways. First and foremost, most abilities have to be nurtured and developed. Even if somebody is born with a particular talent, that talent will usually remain latent if it is not fostered, honed and exercised. Not all people get the same chance to cultivate and refine their abilities. Whether or not they have such an opportunity will usually depend on their place within their society’s imagined hierarchy. Harry Potter is a good example. Removed from his distinguished wizard family and brought up by ignorant muggles, he arrives at Hogwarts without any experience in magic. It takes him seven books to gain a firm command of his powers and knowledge of his unique abilities.
Second, even if people belonging to different classes develop exactly the same abilities, they are unlikely to enjoy equal success because they will have to play the game by different rules. If, in British-ruled India, an Untouchable, a Brahmin, a Catholic Irishman and a Protestant Englishman had somehow developed exactly the same business acumen, they still would not have had the same chance of becoming rich. The economic game was rigged by legal restrictions and unofficial glass ceilings.
The Vicious Circle

All societies are based on imagined hierarchies, but not necessarily on the same hierarchies. What accounts for the differences? Why did traditional Indian society classify people according to caste, Ottoman society according to religion, and American society according to race? In most cases the hierarchy originated as the result of a set of accidental historical circumstances and was then perpetuated and refined over many generations as different groups developed vested interests in it.
For instance, many scholars surmise that the Hindu caste system took shape when Indo-Aryan people invaded the Indian subcontinent about 3,000 years ago, subjugating the local population. The invaders established a stratified society, in which they – of course – occupied the leading positions (priests and warriors), leaving the natives to live as servants and slaves. The invaders, who were few in number, feared losing their privileged status and unique identity. To forestall this danger, they divided the population into castes, each of which was required to pursue a specific occupation or perform a specific role in society. Each had different legal status, privileges and duties. Mixing of castes – social interaction, marriage, even the sharing of meals – was prohibited. And the distinctions were not just legal – they became an inherent part of religious mythology and practice.
The rulers argued that the caste system reflected an eternal cosmic reality rather than a chance historical development. Concepts of purity and impurity were essential elements in Hindu religion, and they were harnessed to buttress the social pyramid. Pious Hindus were taught that contact with members of a different caste could pollute not only them personally, but society as a whole, and should therefore be abhorred. Such ideas are hardly unique to Hindus. Throughout history, and in almost all societies, concepts of pollution and purity have played a leading role in enforcing social and political divisions and have been exploited by numerous ruling classes to maintain their privileges. The fear of pollution is not a complete fabrication of priests and princes, however. It probably has its roots in biological survival mechanisms that make humans feel an instinctive revulsion towards potential disease carriers, such as sick persons and dead bodies. If you want to keep any human group isolated – women, Jews, Roma, gays, blacks – the best way to do it is convince everyone that these people are a source of pollution.
The Hindu caste system and its attendant laws of purity became deeply embedded in Indian culture. Long after the Indo-Aryan invasion was forgotten, Indians continued to believe in the caste system and to abhor the pollution caused by caste mixing. Castes were not immune to change. In fact, as time went by, large castes were divided into sub-castes. Eventually the original four castes turned into 3,000 different groupings called jati (literally ‘birth’). But this proliferation of castes did not change the basic principle of the system, according to which every person is born into a particular rank, and any infringement of its rules pollutes the person and society as a whole. A persons jati determines her profession, the food she can eat, her place of residence and her eligible marriage partners. Usually a person can marry only within his or her caste, and the resulting children inherit that status.
Whenever a new profession developed or a new group of people appeared on the scene, they had to be recognised as a caste in order to receive a legitimate place within Hindu society. Groups that failed to win recognition as a caste were, literally, outcasts – in this stratified society, they did not even occupy the lowest rung. They became known as Untouchables. They had to live apart from all other people and scrape together a living in humiliating and disgusting ways, such as sifting through garbage dumps for scrap material. Even members of the lowest caste avoided mingling with them, eating with them, touching them and certainly marrying them. In modern India, matters of marriage and work are still heavily influenced by the caste system, despite all attempts by the democratic government of India to break down such distinctions and convince Hindus that there is nothing polluting in caste mixing.3
Purity in America

A similar vicious circle perpetuated the racial hierarchy in modern America. From the sixteenth to the eighteenth century, the European conquerors imported millions of African slaves to work the mines and plantations of America. They chose to import slaves from Africa rather than from Europe or East Asia due to three circumstantial factors. Firstly, Africa was closer, so it was cheaper to import slaves from Senegal than from Vietnam.
Secondly, in Africa there already existed a well-developed slave trade (exporting slaves mainly to the Middle East), whereas in Europe slavery was very rare. It was obviously far easier to buy slaves in an existing market than to create a new one from scratch.
Thirdly, and most importantly, American plantations in places such as Virginia, Haiti and Brazil were plagued by malaria and yellow fever, which had originated in Africa. Africans had acquired over the generations a partial genetic immunity to these diseases, whereas Europeans were totally defenceless and died in droves. It was consequently wiser for a plantation owner to invest his money in an African slave than in a European slave or indentured labourer. Paradoxically, genetic superiority (in terms of immunity) translated into social inferiority: precisely because Africans were fitter in tropical climates than Europeans, they ended up as the slaves of European masters! Due to these circumstantial factors, the burgeoning new societies of America were to be divided into a ruling caste of white Europeans and a subjugated caste of black Africans.
But people don’t like to say that they keep slaves of a certain race or origin simply because it’s economically expedient. Like the Aryan conquerors of India, white Europeans in the Americas wanted to be seen not only as economically successful but also as pious, just and objective. Religious and scientific myths were pressed into service to justify this division. Theologians argued that Africans descend from Ham, son of Noah, saddled by his father with a curse that his offspring would be slaves. Biologists argued that blacks are less intelligent than whites and their moral sense less developed. Doctors alleged that blacks live in filth and spread diseases – in other words, they are a source of pollution.
These myths struck a chord in American culture, and in Western culture generally. They continued to exert their influence long after the conditions that created slavery had disappeared. In the early nineteenth century imperial Britain outlawed slavery and stopped the Atlantic slave trade, and in the decades that followed slavery was gradually outlawed throughout the American continent. Notably, this was the first and only time in history that slaveholding societies voluntarily abolished slavery. But, even though the slaves were freed, the racist myths that justified slavery persisted. Separation of the races was maintained by racist legislation and social custom.
The result was a self-reinforcing cycle of cause and effect, a vicious circle. Consider, for example, the southern United States immediately after the Civil War. In 1865 the Thirteenth Amendment to the US Constitution outlawed slavery and the Fourteenth Amendment mandated that citizenship and the equal protection of the law could not be denied on the basis of race. However, two centuries of slavery meant that most black families were far poorer and far less educated than most white families. A black person born in Alabama in 1865 thus had much less chance of getting a good education and a well-paid job than did his white neighbours. His children, born in the 1880S and 1890s, started life with the same disadvantage – they, too, were born to an uneducated, poor family.
But economic disadvantage was not the whole story. Alabama was also home to many poor whites who lacked the opportunities available to their better-off racial brothers and sisters. In addition, the Industrial Revolution and the waves of immigration made the United States an extremely fluid society, where rags could quickly turn into riches. If money was all that mattered, the sharp divide between the races should soon have blurred, not least through intermarriage.
But that did not happen. By 1865 whites, as well as many blacks, took it to be a simple matter of fact that blacks were less intelligent, more violent and sexually dissolute, lazier and less concerned about personal cleanliness than whites. They were thus the agents of violence, theft, rape and disease – in other words, pollution. If a black Alabaman in 1895 miraculously managed to get a good education and then applied for a respectable job such as a bank teller, his odds of being accepted were far worse than those of an equally qualified white candidate. The stigma that labelled blacks as, by nature, unreliable, lazy and less intelligent conspired against him.
You might think that people would gradually understand that these stigmas were myth rather than fact and that blacks would be able, over time, to prove themselves just as competent, law-abiding and clean as whites. In fact, the opposite happened – these prejudices became more and more entrenched as time went by. Since all the best jobs were held by whites, it became easier to believe that blacks really are inferior. ‘Look,’ said the average white citizen, ‘blacks have been free for generations, yet there are almost no black professors, lawyers, doctors or even bank tellers. Isn’t that proof that blacks are simply less intelligent and hard-working?’ Trapped in this vicious circle, blacks were not hired for white-collar jobs because they were deemed unintelligent, and the proof of their inferiority was the paucity of blacks in white-collar jobs.
The vicious circle did not stop there. As anti-black stigmas grew stronger, they were translated into a system of ‘Jim Crow’ laws and norms that were meant to safeguard the racial order. Blacks were forbidden to vote in elections, to study in white schools, to buy in white stores, to eat in white restaurants, to sleep in white hotels. The justification for all of this was that blacks were foul, slothful and vicious, so whites had to be protected from them. Whites did not want to sleep in the same hotel as blacks or to eat in the same restaurant, for fear of diseases. They did not want their children learning in the same school as black children, for fear of brutality and bad influences. They did not want blacks voting in elections, since blacks were ignorant and immoral. These fears were substantiated by scientific studies that ‘proved’ that blacks were indeed less educated, that various diseases were more common among them, and that their crime rate was far higher (the studies ignored the fact that these ‘facts’ resulted from discrimination against blacks).
By the mid-twentieth century, segregation in the former Confederate states was probably worse than in the late nineteenth century. Clennon King, a black student who applied to the University of Mississippi in 1958, was forcefully committed to a mental asylum. The presiding judge ruled that a black person must surely be insane to think that he could be admitted to the University of Mississippi.
 
The vicious circle: a chance histotical situation is translated into a rigid social system.

Nothing was as revolting to American southerners (and many northerners) as sexual relations and marriage between black men and white women. Sex between the races became the greatest taboo and any violation, or suspected violation, was viewed as deserving immediate and summary punishment in the form of lynching. The Ku Klux Klan, a white supremacist secret society, perpetrated many such killings. They could have taught the Hindu Brahmins a thing or two about purity laws.
With time, the racism spread to more and more cultural arenas. American aesthetic culture was built around white standards of beauty. The physical attributes of the white race – for example light skin, fair and straight hair, a small upturned nose – came to be identified as beautiful. Typical black features – dark skin, dark and bushy hair, a flattened nose – were deemed ugly. These preconceptions ingrained the imagined hierarchy at an even deeper level of human consciousness.
Such vicious circles can go on for centuries and even millennia, perpetuating an imagined hierarchy that sprang from a chance historical occurrence. Unjust discrimination often gets worse, not better, with time. Money comes to money, and poverty to poverty. Education comes to education, and ignorance to ignorance. Those once victimised by history are likely to be victimised yet again. And those whom history has privileged are more likely to be privileged again.
Most sociopolitical hierarchies lack a logical or biological basis – they are nothing but the perpetuation of chance events supported by myths. That is one good reason to study history. If the division into blacks and whites or Brahmins and Shudras was grounded in biological realities – that is, if Brahmins really had better brains than Shudras – biology would be sufficient for understanding human society. Since the biological distinctions between different groups of Homo sapiens are, in fact, negligible, biology can’t explain the intricacies of Indian society or American racial dynamics. We can only understand those phenomena by studying the events, circumstances, and power relations that transformed figments of imagination into cruel – and very real – social structures.
He and She

Different societies adopt different kinds of imagined hierarchies. Race is very important to modern Americans but was relatively insignificant to medieval Muslims. Caste was a matter of life and death in medieval India, whereas in modern Europe it is practically non-existent. One hierarchy, however, has been of supreme importance in all known human societies: the hierarchy of gender. People everywhere have divided themselves into men and women. And almost everywhere men have got the better deal, at least since the Agricultural Revolution.
Some of the earliest Chinese texts are oracle bones, dating to 1200 BC, used to divine the future. On one was engraved the question: ‘Will Lady Hao’s childbearing be lucky?’ To which was written the reply: ‘If the child is born on a ding day, lucky; if on a geng day, vastly auspicious.’ However, Lady Hao was to give birth on a jiayin day. The text ends with the morose observation: ‘Three weeks and one day later, on jiayin day, the child was born. Not lucky. It was a girl.’4 More than 3,000 years later, when Communist China enacted the ‘one child’ policy, many Chinese families continued to regard the birth of a girl as a misfortune. Parents would occasionally abandon or murder newborn baby girls in order to have another shot at getting a boy.
In many societies women were simply the property of men, most often their fathers, husbands or brothers. Rape, in many legal systems, falls under property violation – in other words, the victim is not the woman who was raped but the male who owns her. This being the case, the legal remedy was the transfer of ownership – the rapist was required to pay a bride price to the woman’s father or brother, upon which she became the rapist’s property. The Bible decrees that ‘If a man meets a virgin who is not betrothed, and seizes her and lies with her, and they are found, then the man who lay with her shall give to the father of the young woman fifty shekels of silver, and she shall be his wife’ (Deuteronomy 22:28–9). The ancient Hebrews considered this a reasonable arrangement.
Raping a woman who did not belong to any man was not considered a crime at all, just as picking up a lost coin on a busy street is not considered theft. And if a husband raped his own wife, he had committed no crime. In fact, the idea that a husband could rape his wife was an oxymoron. To be a husband was to have full control of your wife’s sexuality. To say that a husband ‘raped’ his wife was as illogical as saying that a man stole his own wallet. Such thinking was not confined to the ancient Middle East. As of 2006, there were still fifty-three countries where a husband could not be prosecuted for the rape of his wife. Even in Germany, rape laws were amended only in 1997 to create a legal category of marital rape.5
Is the division into men and women a product of the imagination, like the caste system in India and the racial system in America, or is it a natural division with deep biological roots? And if it is indeed a natural division, are there also biological explanations for the preference given to men over women?
Some of the cultural, legal and political disparities between men and women reflect the obvious biological differences between the sexes. Childbearing has always been women’s job, because men don’t have wombs. Yet around this hard universal kernel, every society accumulated layer upon layer of cultural ideas and norms that have little to do with biology. Societies associate a host of attributes with masculinity and femininity that, for the most part, lack a firm biological basis.
For instance, in democratic Athens of the fifth century BC, an individual possessing a womb had no independent legal status and was forbidden to participate in popular assemblies or to be a judge. With few exceptions, such an individual could not benefit from a good education, nor engage in business or in philosophical discourse. None of Athens’ political leaders, none of its great philosophers, orators, artists or merchants had a womb. Does having a womb make a person unfit, biologically, for these professions? The ancient Athenians thought so. Modern Athenians disagree. In present-day Athens, women vote, are elected to public office, make speeches, design everything from jewellery to buildings to software, and go to university. Their wombs do not keep them from doing any of these things as successfully as men do. True, they are still under-represented in politics and business – only about 12 per cent of the members of Greece’s parliament are women. But there is no legal barrier to their participation in politics, and most modern Greeks think it is quite normal for a woman to serve in public office.
Many modern Greeks also think that an integral part of being a man is being sexually attracted to women only, and having sexual relations exclusively with the opposite sex. They don’t see this as a cultural bias, but rather as a biological reality – relations between two people of the opposite sex are natural, and between two people of the same sex unnatural. In fact, though, Mother Nature does not mind if men are sexually attracted to one another. It’s only human mothers steeped in particular cultures who make a scene if their son has a fling with the boy next door. The mother’s tantrums are not a biological imperative. A significant number of human cultures have viewed homosexual relations as not only legitimate but even socially constructive, ancient Greece being the most notable example. The Iliad does not mention that Thetis had any objection to her son Achilles’ relations with Patroclus. Queen Olympias of Macedon was one of the most temperamental and forceful women of the ancient world, and even had her own husband, King Philip, assassinated. Yet she didn’t have a fit when her son, Alexander the Great, brought his lover Hephaestion home for dinner.
How can we distinguish what is biologically determined from what people merely try to justify through biological myths? A good rule of thumb is ‘Biology enables, Culture forbids.’ Biology is willing to tolerate a very wide spectrum of possibilities. It’s culture that obliges people to realise some possibilities while forbidding others. Biology enables women to have children – some cultures oblige women to realise this possibility. Biology enables men to enjoy sex with one another – some cultures forbid them to realise this possibility.
Culture tends to argue that it forbids only that which is unnatural. But from a biological perspective, nothing is unnatural. Whatever is possible is by definition also natural. A truly unnatural behaviour, one that goes against the laws of nature, simply cannot exist, so it would need no prohibition. No culture has ever bothered to forbid men to photosynthesise, women to run faster than the speed of light, or negatively charged electrons to be attracted to each other.
In truth, our concepts ‘natural’ and unnatural’ are taken not from biology, but from Christian theology. The theological meaning of ‘natural’ is ‘in accordance with the intentions of the God who created nature’. Christian theologians argued that God created the human body, intending each limb and organ to serve a particular purpose. If we use our limbs and organs for the purpose envisioned by God, then it is a natural activity. To use them differently than God intends is unnatural. But evolution has no purpose. Organs have not evolved with a purpose, and the way they are used is in constant flux. There is not a single organ in the human body that only does the job its prototype did when it first appeared hundreds of millions of years ago. Organs evolve to perform a particular function, but once they exist, they can be adapted for other usages as well. Mouths, for example, appeared because the earliest multicellular organisms needed a way to take nutrients into their bodies. We still use our mouths for that purpose, but we also use them to kiss, speak and, if we are Rambo, to pull the pins out of hand grenades. Are any of these uses unnatural simply because our worm-like ancestors 600 million years ago didn’t do those things with their mouths?
Similarly, wings didn’t suddenly appear in all their aerodynamic glory. They developed from organs that served another purpose. According to one theory, insect wings evolved millions of years ago from body protrusions on flightless bugs. Bugs with bumps had a larger surface area than those without bumps, and this enabled them to absorb more sunlight and thus stay warmer. In a slow evolutionary process, these solar heaters grew larger. The same structure that was good for maximum sunlight absorption – lots of surface area, little weight – also, by coincidence, gave the insects a bit of a lift when they skipped and jumped. Those with bigger protrusions could skip and jump farther. Some insects started using the things to glide, and from there it was a small step to wings that could actually propel the bug through the air. Next time a mosquito buzzes in your ear, accuse her of unnatural behaviour. If she were well behaved and content with what God gave her, she’d use her wings only as solar panels.
The same sort of multitasking applies to our sexual organs and behaviour. Sex first evolved for procreation and courtship rituals as a way of sizing up the fitness of a potential mate. But many animals now put both to use for a multitude of social purposes that have little to do with creating little copies of themselves. Chimpanzees, for example, use sex to cement political alliances, establish intimacy and defuse tensions. Is that unnatural?
Sex and Gender

There is little sense, then, in arguing that the natural function of women is to give birth, or that homosexuality is unnatural. Most of the laws, norms, rights and obligations that define manhood and womanhood reflect human imagination more than biological reality.
Biologically, humans are divided into males and females. A male Homo sapiens is one who has one X chromosome and one Y chromosome; a female is one with two Xs. But ‘man’ and woman’ name social, not biological, categories. While in the great majority of cases in most human societies men are males and women are females, the social terms carry a lot of baggage that has only a tenuous, if any, relationship to the biological terms. A man is not a Sapiens with particular biological qualities such as XY chromosomes, testicles and lots of testosterone. Rather, he fits into a particular slot in his society’s imagined human order. His culture’s myths assign him particular masculine roles (like engaging in politics), rights (like voting) and duties (like military service). Likewise, a woman is not a Sapiens with two X chromosomes, a womb and plenty of oestrogen. Rather, she is a female member of an imagined human order. The myths of her society assign her unique feminine roles (raising children), rights (protection against violence) and duties (obedience to her husband). Since myths, rather than biology, define the roles, rights and duties of men and women, the meaning of ‘manhood’ and ‘womanhood’ have varied immensely from one society to another.
 

22. Eighteenth-century masculinity: an official portrait of King Louis XIV of France. Note the long wig, stockings, high-heeled shoes, dancers posture – and huge sword. In contemporary Europe, all these (except for the sword) would be considered marks of effeminacy. But in his time Louis was a European paragon of manhood and virility.


23. Twenty-first-century masculinity: an official portrait of Barack Obama. What happened to the wig, stockings, high heels – and sword? Dominant men have never looked so dull and dreary as they do today. During most of history, dominant men have been colourful and flamboyant, such as American Indian chiefs with their feathered headdresses and Hindu maharajas decked out in silks and diamonds. Throughout the animal kingdom males tend to be more colourful and accessorised than females – think of peacocks’ tails and lions’ manes.

To make things less confusing, scholars usually distinguish between ‘sex’, which is a biological category, and ‘gender’, a cultural category. Sex is divided between males and females, and the qualities of this division are objective and have remained constant throughout history. Gender is divided between men and women (and some cultures recognise other categories). So-called ‘masculine’ and ‘feminine’ qualities are inter-subjective and undergo constant changes. For example, there are far-reaching differences in the behaviour, desires, dress and even body posture expected from women in classical Athens and women in modern Athens.6
Sex is child’s play; but gender is serious business. To get to be a member of the male sex is the simplest thing in the world. You just need to be born with an X and a Y chromosome. To get to be a female is equally simple. A pair of X chromosomes will do it. In contrast, becoming a man or a woman is a very complicated and demanding undertaking. Since most masculine and feminine qualities are cultural rather than biological, no society automatically crowns each male a man, or every female a woman. Nor are these titles laurels that can be rested on once they are acquired. Males must prove their masculinity constantly, throughout their lives, from cradle to grave, in an endless series of rites and performances. And a woman’s work is never done – she must continually convince herself and others that she is feminine enough.
Success is not guaranteed. Males in particular live in constant dread of losing their claim to manhood. Throughout history, males have been willing to risk and even sacrifice their lives, just so that people will say ‘He’s a real man!’
What’s So Good About Men?

At least since the Agricultural Revolution, most human societies have been patriarchal societies that valued men more highly than women. No matter how a society defined ‘man’ and ‘woman’, to be a man was always better. Patriarchal societies educate men to think and act in a masculine way and women to think and act in a feminine way, punishing anyone who dares cross those boundaries. Yet they do not equally reward those who conform. Qualities considered masculine are more valued than those considered feminine, and members of a society who personify the feminine ideal get less than those who exemplify the masculine ideal. Fewer resources are invested in the health and education of women; they have fewer economic opportunities, less political power, and less freedom of movement. Gender is a race in which some of the runners compete only for the bronze medal.
True, a handful of women have made it to the alpha position, such as Cleopatra of Egypt, Empress Wu Zetian of China (c. AD 700) and Elizabeth I of England. Yet they are the exceptions that prove the rule. Throughout Elizabeth’s forty-five-year reign, all Members of Parliament were men, all officers in the Royal Navy and army were men, all judges and lawyers were men, all bishops and archbishops were men, all theologians and priests were men, all doctors and surgeons were men, all students and professors in all universities and colleges were men, all mayors and sheriffs were men, and almost all the writers, architects, poets, philosophers, painters, musicians and scientists were men.
Patriarchy has been the norm in almost all agricultural and industrial societies. It has tenaciously weathered political upheavals, social revolutions and economic transformations. Egypt, for example, was conquered numerous times over the centuries. Assyrians, Persians, Macedonians, Romans, Arabs, Mameluks, Turks and British occupied it – and its society always remained patriarchal. Egypt was governed by pharaonic law, Greek law, Roman law, Muslim law, Ottoman law and British law – and they all discriminated against people who were not ‘real men’.
Since patriarchy is so universal, it cannot be the product of some vicious circle that was kick-started by a chance occurrence. It is particularly noteworthy that even before 1492, most societies in both America and Afro-Asia were patriarchal, even though they had been out of contact for thousands of years. If patriarchy in Afro-Asia resulted from some chance occurrence, why were the Aztecs and Incas patriarchal? It is far more likely that even though the precise definition of ‘man’ and ‘woman’ varies between cultures, there is some universal biological reason why almost all cultures valued manhood over womanhood. We do not know what this reason is. There are plenty of theories, none of them convincing.
Muscle Power

The most common theory points to the fact that men are stronger than women, and that they have used their greater physical power to force women into submission. A more subtle version of this claim argues that their strength allows men to monopolise tasks that demand hard manual labour, such as ploughing and harvesting. This gives them control of food production, which in turn translates into political clout.
There are two problems with this emphasis on muscle power. First, the statement that men are stronger than women’ is true only on average, and only with regard to certain types of strength. Women are generally more resistant to hunger, disease and fatigue than men. There are also many women who can run faster and lift heavier weights than many men. Furthermore, and most problematically for this theory, women have, throughout history, been excluded mainly from jobs that require little physical effort (such as the priesthood, law and politics), while engaging in hard manual labour in the fields, in crafts and in the household. If social power were divided in direct relation to physical strength or stamina, women should have got far more of it.
Even more importantly, there simply is no direct relation between physical strength and social power among humans. People in their sixties usually exercise power over people in their twenties, even though twentysomethings are much stronger than their elders. The typical plantation owner in Alabama in the mid-nineteenth century could have been wrestled to the ground in seconds by any of the slaves cultivating his cotton fields. Boxing matches were not used to select Egyptian pharaohs or Catholic popes. In forager societies, political dominance generally resides with the person possessing the best social skills rather than the most developed musculature. In organised crime, the big boss is not necessarily the strongest man. He is often an older man who very rarely uses his own fists; he gets younger and fitter men to do the dirty jobs for him. A guy who thinks that the way to take over the syndicate is to beat up the don is unlikely to live long enough to learn from his mistake. Even among chimpanzees, the alpha male wins his position by building a stable coalition with other males and females, not through mindless violence.
In fact, human history shows that there is often an inverse relation between physical prowess and social power. In most societies, it’s the lower classes who do the manual labour. This may reflect Homo sapiens position in the food chain. If all that counted were raw physical abilities, Sapiens would have found themselves on a middle rung of the ladder. But their mental and social skills placed them at the top. It is therefore only natural that the chain of power within the species will also be determined by mental and social abilities more than by brute force. It is therefore hard to believe that the most influential and most stable social hierarchy in history is founded on men’s ability physically to coerce women.
The Scum of Society

Another theory explains that masculine dominance results not from strength but from aggression. Millions of years of evolution have made men far more violent than women. Women can match men as far as hatred, greed and abuse are concerned, but when push comes to shove, the theory goes, men are more willing to engage in raw physical violence. This is why throughout history warfare has been a masculine prerogative.
In times of war, men’s control of the armed forces has made them the masters of civilian society, too. They then used their control of civilian society to fight more and more wars, and the greater the number of wars, the greater men’s control of society. This feedback loop explains both the ubiquity of war and the ubiquity of patriarchy.
Recent studies of the hormonal and cognitive systems of men and women strengthen the assumption that men indeed have more aggressive and violent tendencies, and are therefore, on average, better suited to serve as common soldiers. Yet granted that the common soldiers are all men, does it follow that the ones managing the war and enjoying its fruits must also be men? That makes no sense. It’s like assuming that because all the slaves cultivating cotton fields are black, plantation owners will be black as well. Just as an all-black workforce might be controlled by an all-white management, why couldn’t an all-male soldiery be controlled by an all-female or at least partly female government? In fact, in numerous societies throughout history, the top officers did not work their way up from the rank of private. Aristocrats, the wealthy and the educated were automatically assigned officer rank and never served a day in the ranks.
When the Duke of Wellington, Napoleon’s nemesis, enlisted in the British army at the age of eighteen, he was immediately commissioned as an officer. He didn’t think much of the plebeians under his command. ‘We have in the service the scum of the earth as common soldiers,’ he wrote to a fellow aristocrat during the wars against France. These common soldiers were usually recruited from among the very poorest, or from ethnic minorities (such as the Irish Catholics). Their chances of ascending the military ranks were negligible. The senior ranks were reserved for dukes, princes and kings. But why only for dukes, and not for duchesses?
The French Empire in Africa was established and defended by the sweat and blood of Senegalese, Algerians and working-class Frenchmen. The percentage of well-born Frenchmen within the ranks was negligible. Yet the percentage of well-born Frenchmen within the small elite that led the French army, ruled the empire and enjoyed its fruits was very high. Why just Frenchmen, and not French women?
In China there was a long tradition of subjugating the army to the civilian bureaucracy, so mandarins who had never held a sword often ran the wars. ‘You do not waste good iron to make nails,’ went a common Chinese saying, meaning that really talented people join the civil bureaucracy, not the army. Why, then, were all of these mandarins men?
One can’t reasonably argue that their physical weakness or low testosterone levels prevented women from being successful mandarins, generals and politicians. In order to manage a war, you surely need stamina, but not much physical strength or aggressiveness. Wars are not a pub brawl. They are very complex projects that require an extraordinary degree of organisation, cooperation and appeasement. The ability to maintain peace at home, acquire allies abroad, and understand what goes through the minds of other people (particularly your enemies) is usually the key to victory. Hence an aggressive brute is often the worst choice to run a war. Much better is a cooperative person who knows how to appease, how to manipulate and how to see things from different perspectives. This is the stuff empire-builders are made of. The militarily incompetent Augustus succeeded in establishing a stable imperial regime, achieving something that eluded both Julius Caesar and Alexander the Great, who were much better generals. Both his admiring contemporaries and modern historians often attribute this feat to his virtue of clementia – mildness and clemency.
Women are often stereotyped as better manipulators and appeasers than men, and are famed for their superior ability to see things from the perspective of others. If there’s any truth in these stereotypes, then women should have made excellent politicians and empire-builders, leaving the dirty work on the battlefields to testosterone-charged but simple-minded machos. Popular myths notwithstanding, this rarely happened in the real world. It is not at all clear why not.
Patriarchal Genes

A third type of biological explanation gives less importance to brute force and violence, and suggests that through millions of years of evolution, men and women evolved different survival and reproduction strategies. As men competed against each other for the opportunity to impregnate fertile women, an individual’s chances of reproduction depended above all on his ability to outperform and defeat other men. As time went by, the masculine genes that made it to the next generation were those belonging to the most ambitious, aggressive and competitive men.
A woman, on the other hand, had no problem finding a man willing to impregnate her. However, if she wanted her children to provide her with grandchildren, she needed to carry them in her womb for nine arduous months, and then nurture them for years. During that time she had fewer opportunities to obtain food, and required a lot of help. She needed a man. In order to ensure her own survival and the survival of her children, the woman had little choice but to agree to whatever conditions the man stipulated so that he would stick around and share some of the burden. As time went by, the feminine genes that made it to the next generation belonged to women who were submissive caretakers. Women who spent too much time fighting for power did not leave any of those powerful genes for future generations.
The result of these different survival strategies – so the theory goes – is that men have been programmed to be ambitious and competitive, and to excel in politics and business, whereas women have tended to move out of the way and dedicate their lives to raising children.
But this approach also seems to be belied by the empirical evidence. Particularly problematic is the assumption that women’s dependence on external help made them dependent on men, rather than on other women, and that male competitiveness made men socially dominant. There are many species of animals, such as elephants and bonobo chimpanzees, in which the dynamics between dependent females and competitive males results in a matriarchal society. Since females need external help, they are obliged to develop their social skills and learn how to cooperate and appease. They construct all-female social networks that help each member raise her children. Males, meanwhile, spend their time fighting and competing. Their social skills and social bonds remain underdeveloped. Bonobo and elephant societies are controlled by strong networks of cooperative females, while the self-centred and uncooperative males are pushed to the sidelines. Though bonobo females are weaker on average than the males, the females often gang up to beat males who overstep their limits.
If this is possible among bonobos and elephants, why not among Homo sapiens? Sapiens are relatively weak animals, whose advantage rests in their ability to cooperate in large numbers. If so, we should expect that dependent women, even if they are dependent on men, would use their superior social skills to cooperate to outmanoeuvre and manipulate aggressive, autonomous and self-centred men.
How did it happen that in the one species whose success depends above all on cooperation, individuals who are supposedly less cooperative (men) control individuals who are supposedly more cooperative (women)? At present, we have no good answer. Maybe the common assumptions are just wrong. Maybe males of the species Homo sapiens are characterised not by physical strength, aggressiveness and competitiveness, but rather by superior social skills and a greater tendency to cooperate. We just don’t know.
What we do know, however, is that during the last century gender roles have undergone a tremendous revolution. More and more societies today not only give men and women equal legal status, political rights and economic opportunities, but also completely rethink their most basic conceptions of gender and sexuality. Though the gender gap is still significant, events have been moving at a breathtaking speed. At the beginning of the twentieth century the idea of giving voting rights to women was generally seen in the USA as outrageous; the prospect of a female cabinet secretary or Supreme Court justice was simply ridiculous; whereas homosexuality was such a taboo subject that it could not even be openly discussed. At the beginning of the twenty-first century women’s voting rights are taken for granted; female cabinet secretaries are hardly a cause for comment; and in 2013 five US Supreme Court justices, three of them women, decided in favour of legalising same-sex marriages (overruling the objections of four male justices).
These dramatic changes are precisely what makes the history of gender so bewildering. If, as is being demonstrated today so clearly, the patriarchal system has been based on unfounded myths rather than on biological facts, what accounts for the universality and stability of this system?9

The Arrow of History

AFTER THE AGRICULTURAL REVOLUTION, human societies grew ever larger and more complex, while the imagined constructs sustaining the social order also became more elaborate. Myths and fictions accustomed people, nearly from the moment of birth, to think in certain ways, to behave in accordance with certain standards, to want certain things, and to observe certain rules. They thereby created artificial instincts that enabled millions of strangers to cooperate effectively. This network of artificial instincts is called culture’.
During the first half of the twentieth century, scholars taught that every culture was complete and harmonious, possessing an unchanging essence that defined it for all time. Each human group had its own world view and system of social, legal and political arrangements that ran as smoothly as the planets going around the sun. In this view, cultures left to their own devices did not change. They just kept going at the same pace and in the same direction. Only a force applied from outside could change them. Anthropologists, historians and politicians thus referred to ‘Samoan Culture’ or ‘Tasmanian Culture’ as if the same beliefs, norms and values had characterised Samoans and Tasmanians from time immemorial.
Today, most scholars of culture have concluded that the opposite is true. Every culture has its typical beliefs, norms and values, but these are in constant flux. The culture may transform itself in response to changes in its environment or through interaction with neighbouring cultures. But cultures also undergo transitions due to their own internal dynamics. Even a completely isolated culture existing in an ecologically stable environment cannot avoid change. Unlike the laws of physics, which are free of inconsistencies, every man-made order is packed with internal contradictions. Cultures are constantly trying to reconcile these contradictions, and this process fuels change.
For instance, in medieval Europe the nobility believed in both Christianity and chivalry. A typical nobleman went to church in the morning, and listened as the priest held forth on the lives of the saints. ‘Vanity of vanities,’ said the priest, ‘all is vanity. Riches, lust and honour are dangerous temptations. You must rise above them, and follow in Christ’s footsteps. Be meek like Him, avoid violence and extravagance, and if attacked – just turn the other cheek.’ Returning home in a meek and pensive mood, the nobleman would change into his best silks and go to a banquet in his lord’s castle. There the wine flowed like water, the minstrel sang of Lancelot and Guinevere, and the guests exchanged dirty jokes and bloody war tales. ‘It is better to die,’ declared the barons, ‘than to live with shame. If someone questions your honour, only blood can wipe out the insult. And what is better in life than to see your enemies flee before you, and their pretty daughters tremble at your feet?’
The contradiction was never fully resolved. But as the European nobility, clergy and commoners grappled with it, their culture changed. One attempt to figure it out produced the Crusades. On crusade, knights could demonstrate their military prowess and their religious devotion at one stroke. The same contradiction produced military orders such as the Templars and Hospitallers, who tried to mesh Christian and chivalric ideals even more tightly. It was also responsible for a large part of medieval art and literature, such as the tales of King Arthur and the Holy Grail. What was Camelot but an attempt to prove that a good knight can and should be a good Christian, and that good Christians make the best knights?
Another example is the modern political order. Ever since the French Revolution, people throughout the world have gradually come to see both equality and individual freedom as fundamental values. Yet the two values contradict each other. Equality can be ensured only by curtailing the freedoms of those who are better off. Guaranteeing that every individual will be free to do as he wishes inevitably short-changes equality. The entire political history of the world since 1789 can be seen as a series of attempts to reconcile this contradiction.
Anyone who has read a novel by Charles Dickens knows that the liberal regimes of nineteenth-century Europe gave priority to individual freedom even if it meant throwing insolvent poor families in prison and giving orphans little choice but to join schools for pickpockets. Anyone who has read a novel by Alexander Solzhenitsyn knows how Communisms egalitarian ideal produced brutal tyrannies that tried to control every aspect of daily life.
Contemporary American politics also revolve around this contradiction. Democrats want a more equitable society, even if it means raising taxes to fund programmes to help the poor, elderly and infirm. But that infringes on the freedom of individuals to spend their money as they wish. Why should the government force me to buy health insurance if I prefer using the money to put my kids through college? Republicans, on the other hand, want to maximise individual freedom, even if it means that the income gap between rich and poor will grow wider and that many Americans will not be able to afford health care.
Just as medieval culture did not manage to square chivalry with Christianity, so the modern world fails to square liberty with equality. But this is no defect. Such contradictions are an inseparable part of every human culture. In fact, they are culture’s engines, responsible for the creativity and dynamism of our species. Just as when two clashing musical notes played together force a piece of music forward, so discord in our thoughts, ideas and values compel us to think, reevaluate and criticise. Consistency is the playground of dull minds.
If tensions, conflicts and irresolvable dilemmas are the spice of every culture, a human being who belongs to any particular culture must hold contradictory beliefs and be riven by incompatible values. It’s such an essential feature of any culture that it even has a name: cognitive dissonance. Cognitive dissonance is often considered a failure of the human psyche. In fact, it is a vital asset. Had people been unable to hold contradictory beliefs and values, it would probably have been impossible to establish and maintain any human culture.
If, say, a Christian really wants to understand the Muslims who attend that mosque down the street, he shouldn’t look for a pristine set of values that every Muslim holds dear. Rather, he should enquire into the catch-22s of Muslim culture, those places where rules are at war and standards scuffle. It’s at the very spot where the Muslims teeter between two imperatives that you’ll understand them best.
The Spy Satellite

Human cultures are in constant flux. Is this flux completely random, or does it have some overall pattern? In other words, does history have a direction?
The answer is yes. Over the millennia, small, simple cultures gradually coalesce into bigger and more complex civilisations, so that the world contains fewer and fewer mega-cultures, each of which is bigger and more complex. This is of course a very crude generalisation, true only at the macro level. At the micro level, it seems that for every group of cultures that coalesces into a mega-culture, there’s a mega-culture that breaks up into pieces. The Mongol Empire expanded to dominate a huge swathe of Asia and even parts of Europe, only to shatter into fragments. Christianity converted hundreds of millions of people at the same time that it splintered into innumerable sects. The Latin language spread through western and central Europe, then split into local dialects that themselves eventually became national languages. But these break-ups are temporary reversals in an inexorable trend towards unity.
Perceiving the direction of history is really a question of vantage point. When we adopt the proverbial bird’s-eye view of history, which examines developments in terms of decades or centuries, it’s hard to say whether history moves in the direction of unity or of diversity. However, to understand long-term processes the bird’s-eye view is too myopic. We would do better to adopt instead the viewpoint of a cosmic spy satellite, which scans millennia rather than centuries. From such a vantage point it becomes crystal clear that history is moving relentlessly towards unity. The sectioning of Christianity and the collapse of the Mongol Empire are just speed bumps on history’s highway.
*

The best way to appreciate the general direction of history is to count the number of separate human worlds that coexisted at any given moment on planet Earth. Today, we are used to thinking about the whole planet as a single unit, but for most of history, earth was in fact an entire galaxy of isolated human worlds.
Consider Tasmania, a medium-sized island south of Australia. It was cut off from the Australian mainland in about 10,000 BC as the end of the Ice Age caused the sea level to rise. A few thousand hunter-gatherers were left on the island, and had no contact with any other humans until the arrival of the Europeans in the nineteenth century. For 12,000 years, nobody else knew the Tasmanians were there, and they didn’t know that there was anyone else in the world. They had their wars, political struggles, social oscillations and cultural developments. Yet as far as the emperors of China or the rulers of Mesopotamia were concerned, Tasmania could just as well have been located on one of Jupiter’s moons. The Tasmanians lived in a world of their own.
America and Europe, too, were separate worlds for most of their histories. In AD 378, the Roman emperor Valence was defeated and killed by the Goths at the battle of Adrianople. In the same year, King Chak Tok Ich’aak of Tikal was defeated and killed by the army of Teotihuacan. (Tikal was an important Mayan city state, while Teotihuacan was then the largest city in America, with almost 250,000 inhabitants – of the same order of magnitude as its contemporary, Rome.) There was absolutely no connection between the defeat of Rome and the rise of Teotihuacan. Rome might just as well have been located on Mars, and Teotihuacan on Venus.
How many different human worlds coexisted on earth? Around 10.000 BC our planet contained many thousands of them. By 2000 BC, their numbers had dwindled to the hundreds, or at most a few thousand. By AD 1450, their numbers had declined even more drastically. At that time, just prior to the age of European exploration, earth still contained a significant number of dwarf worlds such as Tasmania. But close to 90 per cent of humans lived in a single mega-world: the world of Afro-Asia. Most of Asia, most of Europe, and most of Africa (including substantial chunks of sub-Saharan Africa) were already connected by significant cultural, political and economic ties.
Most of the remaining tenth of the world’s human population was divided between four worlds of considerable size and complexity:
1. The Mesoamerican World, which encompassed most of Central America and parts of North America.
2. The Andean World, which encompassed most of western South America.
3. The Australian World, which encompassed the continent of Australia.
4. The Oceanic World, which encompassed most of the islands of the south-western Pacific Ocean, from Hawaii to New Zealand.
Over the next 300 years, the Afro-Asian giant swallowed up all the other worlds. It consumed the Mesoamerican World in 1521, when the Spanish conquered the Aztec Empire. It took its first bite out of the Oceanic World at the same time, during Ferdinand Magellan’s circumnavigation of the globe, and soon after that completed its conquest. The Andean World collapsed in 1532, when Spanish conquistadors crushed the Inca Empire. The first European landed on the Australian continent in 1606, and that pristine world came to an end when British colonisation began in earnest in 1788. Fifteen years later the Britons established their first settlement in Tasmania, thus bringing the last autonomous human world into the Afro-Asian sphere of influence.
It took the Afro-Asian giant several centuries to digest all that it had swallowed, but the process was irreversible. Today almost all humans share the same geopolitical system (the entire planet is divided into internationally recognised states); the same economic system (capitalist market forces shape even the remotest corners of the globe); the same legal system (human rights and international law are valid everywhere, at least theoretically); and the same scientific system (experts in Iran, Israel, Australia and Argentina have exactly the same views about the structure of atoms or the treatment of tuberculosis).
The single global culture is not homogeneous. Just as a single organic body contains many different kinds of organs and cells, so our single global culture contains many different types of lifestyles and people, from New York stockbrokers to Afghan shepherds. Yet they are all closely connected and they influence one another in myriad ways. They still argue and fight, but they argue using the same concepts and fight using the same weapons. A real ‘clash of civilisations’ is like the proverbial dialogue of the deaf. Nobody can grasp what the other is saying. Today when Iran and the United States rattle swords at one another, they both speak the language of nation states, capitalist economies, international rights and nuclear physics.

Map 3. Earth in AD 1450. The named locations within the Afro-Asian World were places visited by the fourteenth-century Muslim traveller Ibn Battuta. A native of Tangier, in Morocco, Ibn Battuta visited Timbuktu, Zanzibar, southern Russia, Central Asia, India, China and Indonesia. His travels illustrate the unity of Afro-Asia on the eve of the modern era.

We still talk a lot about ‘authentic’ cultures, but if by authentic’ we mean something that developed independently, and that consists of ancient local traditions free of external influences, then there are no authentic cultures left on earth. Over the last few centuries, all cultures were changed almost beyond recognition by a flood of global influences.
One of the most interesting examples of this globalisation is ‘ethnic’ cuisine. In an Italian restaurant we expect to find spaghetti in tomato sauce; in Polish and Irish restaurants lots of potatoes; in an Argentinian restaurant we can choose between dozens of kinds of beefsteaks; in an Indian restaurant hot chillies are incorporated into just about everything; and the highlight at any Swiss café is thick hot chocolate under an alp of whipped cream. But none of these foods is native to those nations. Tomatoes, chilli peppers and cocoa are all Mexican in origin; they reached Europe and Asia only after the Spaniards conquered Mexico. Julius Caesar and Dante Alighieri never twirled tomato-drenched spaghetti on their forks (even forks hadn’t been invented yet), William Tell never tasted chocolate, and Buddha never spiced up his food with chilli. Potatoes reached Poland and Ireland no more than 400 years ago. The only steak you could obtain in Argentina in 1492 was from a llama.
Hollywood films have perpetuated an image of the Plains Indians as brave horsemen, courageously charging the wagons of European pioneers to protect the customs of their ancestors. However, these Native American horsemen were not the defenders of some ancient, authentic culture. Instead, they were the product of a major military and political revolution that swept the plains of western North America in the seventeenth and eighteenth centuries, a consequence of the arrival of European horses. In 1492 there were no horses in America. The culture of the nineteenth-century Sioux and Apache has many appealing features, but it was a modern culture – a result of global forces – much more than authentic’.
The Global Vision

From a practical perspective, the most important stage in the process of global unification occurred in the last few centuries, when empires grew and trade intensified. Ever-tightening links were formed between the people of Afro-Asia, America, Australia and Oceania. Thus Mexican chilli peppers made it into Indian food and Spanish cattle began grazing in Argentina. Yet from an ideological perspective, an even more important development occurred during the first millennium BC, when the idea of a universal order took root. For thousands of years previously, history was already moving slowly in the direction of global unity, but the idea of a universal order governing the entire world was still alien to most people.

25. Sioux chiefs (1905). Neither the Sioux nor any other Great Plains tribe had horses prior to 1492.

Homo sapiens evolved to think of people as divided into us and them. ‘Us’ was the group immediately around you, whoever you were, and ‘them’ was everyone else. In fact, no social animal is ever guided by the interests of the entire species to which it belongs. No chimpanzee cares about the interests of the chimpanzee species, no snail will lift a tentacle for the global snail community, no lion alpha male makes a bid for becoming the king of all lions, and at the entrance of no beehive can one find the slogan: ‘Worker bees of the world – unite!’
But beginning with the Cognitive Revolution, Homo sapiens became more and more exceptional in this respect. People began to cooperate on a regular basis with complete strangers, whom they imagined as ‘brothers’ or ‘friends’. Yet this brotherhood was not universal. Somewhere in the next valley, or beyond the mountain range, one could still sense ‘them’. When the first pharaoh, Menes, united Egypt around 3000 BC, it was clear to the Egyptians that Egypt had a border, and beyond the border lurked ‘barbarians’. The barbarians were alien, threatening, and interesting only to the extent that they had land or natural resources that the Egyptians wanted. All the imagined orders people created tended to ignore a substantial part of humankind.
The first millennium BC witnessed the appearance of three potentially universal orders, whose devotees could for the first time imagine the entire world and the entire human race as a single unit governed by a single set of laws. Everyone was ‘us’, at least potentially. There was no longer ‘them’. The first universal order to appear was economic: the monetary order. The second universal order was political: the imperial order. The third universal order was religious: the order of universal religions such as Buddhism, Christianity and Islam.
Merchants, conquerors and prophets were the first people who managed to transcend the binary evolutionary division, ‘us vs them’, and to foresee the potential unity of humankind. For the merchants, the entire world was a single market and all humans were potential customers. They tried to establish an economic order that would apply to all, everywhere. For the conquerors, the entire world was a single empire and all humans were potential subjects, and for the prophets, the entire world held a single truth and all humans were potential believers. They too tried to establish an order that would be applicable for everyone everywhere.
During the last three millennia, people made more and more ambitious attempts to realise that global vision. The next three chapters discuss how money, empires and universal religions spread, and how they laid the foundation of the united world of today. We begin with the story of the greatest conqueror in history, a conqueror possessed of extreme tolerance and adaptability, thereby turning people into ardent disciples. This conqueror is money. People who do not believe in the same god or obey the same king are more than willing to use the same money. Osama Bin Laden, for all his hatred of American culture, American religion and American politics, was very fond of American dollars. How did money succeed where gods and kings failed?10

The Scent of Money

IN 1519 HERNÁN CORTÉS AND HIS CONQUISTADORS invaded Mexico, hitherto an isolated human world. The Aztecs, as the people who lived there called themselves, quickly noticed that the aliens showed an extraordinary interest in a certain yellow metal. In fact, they never seemed to stop talking about it. The natives were not unfamiliar with gold – it was pretty and easy to work, so they used it to make jewellery and statues, and they occasionally used gold dust as a medium of exchange. But when an Aztec wanted to buy something, he generally paid in cocoa beans or bolts of cloth. The Spanish obsession with gold thus seemed inexplicable. What was so important about a metal that could not be eaten, drunk or woven, and was too soft to use for tools or weapons? When the natives questioned Cortés as to why the Spaniards had such a passion for gold, the conquistador answered, ‘Because I and my companions suffer from a disease of the heart which can be cured only with gold.’1
In the Afro-Asian world from which the Spaniards came, the obsession for gold was indeed an epidemic. Even the bitterest of enemies lusted after the same useless yellow metal. Three centuries before the conquest of Mexico, the ancestors of Cortés and his army waged a bloody war of religion against the Muslim kingdoms in Iberia and North Africa. The followers of Christ and the followers of Allah killed each other by the thousands, devastated fields and orchards, and turned prosperous cities into smouldering ruins – all for the greater glory of Christ or Allah.
As the Christians gradually gained the upper hand, they marked their victories not only by destroying mosques and building churches,but also by issuing new gold and silver coins bearing the sign of the cross and thanking God for His help in combating the infidels. Yet alongside the new currency, the victors minted another type of coin, called the millares, which carried a somewhat different message. These square coins made by the Christian conquerors were emblazoned with flowing Arabic script that declared: ‘There is no god except Allah, and Muhammad is Allah’s messenger.’ Even the Catholic bishops of Melgueil and Agde issued these faithful copies of popular Muslim coins, and God-fearing Christians happily used them.2
Tolerance flourished on the other side of the hill too. Muslim merchants in North Africa conducted business using Christian coins such as the Florentine florin, the Venetian ducat and the Neapolitan gigliato. Even Muslim rulers who called for jihad against the infidel Christians were glad to receive taxes in coins that invoked Christ and His Virgin Mother.3
How Much is It?

Hunter-gatherers had no money. Each band hunted, gathered and manufactured almost everything it required, from meat to medicine, from sandals to sorcery. Different band members may have specialised in different tasks, but they shared their goods and services through an economy of favours and obligations. A piece of meat given for free would carry with it the assumption of reciprocity – say, free medical assistance. The band was economically independent; only a few rare items that could not be found locally – seashells, pigments, obsidian and the like – had to be obtained from strangers. This could usually be done by simple barter: ‘We’ll give you pretty seashells, and you’ll give us high-quality flint.’
Little of this changed with the onset of the Agricultural Revolution. Most people continued to live in small, intimate communities. Much like a hunter-gatherer band, each village was a self-sufficient economic unit, maintained by mutual favours and obligations plus a little barter with outsiders. One villager may have been particularly adept at making shoes, another at dispensing medical care, so villagers knew where to turn when barefoot or sick. But villages were small and their economies limited, so there could be no full-time shoemakers and doctors.
The rise of cities and kingdoms and the improvement in transport infrastructure brought about new opportunities for specialisation. Densely populated cities provided full-time employment not just for professional shoemakers and doctors, but also for carpenters, priests, soldiers and lawyers. Villages that gained a reputation for producing really good wine, olive oil or ceramics discovered that it was worth their while to specialise nearly exclusively in that product and trade it with other settlements for all the other goods they needed. This made a lot of sense. Climates and soils differ, so why drink mediocre wine from your backyard if you can buy a smoother variety from a place whose soil and climate is much better suited to grape vines? If the clay in your backyard makes stronger and prettier pots, then you can make an exchange. Furthermore, full-time specialist vintners and potters, not to mention doctors and lawyers, can hone their expertise to the benefit of all. But specialisation created a problem – how do you manage the exchange of goods between the specialists?
An economy of favours and obligations doesn’t work when large numbers of strangers try to cooperate. It’s one thing to provide free assistance to a sister or a neighbour, a very different thing to take care of foreigners who might never reciprocate the favour. One can fall back on barter. But barter is effective only when exchanging a limited range of products. It cannot form the basis for a complex economy.4
In order to understand the limitations of barter, imagine that you own an apple orchard in the hill country that produces the crispest, sweetest apples in the entire province. You work so hard in your orchard that your shoes wear out. So you harness up your donkey cart and head to the market town down by the river. Your neighbour told you that a shoemaker on the south end of the marketplace made him a really sturdy pair of boots that’s lasted him through five seasons. You find the shoemaker’s shop and offer to barter some of your apples in exchange for the shoes you need.
The shoemaker hesitates. How many apples should he ask for in payment? Every day he encounters dozens of customers, a few of whom bring along sacks of apples, while others carry wheat, goats or cloth – all of varying quality. Still others offer their expertise in petitioning the king or curing backaches. The last time the shoemaker exchanged shoes for apples was three months ago, and back then he asked for three sacks of apples. Or was it four? But come to think of it, those apples were sour valley apples, rather than prime hill apples. On the other hand, on that previous occasion, the apples were given in exchange for small women’s shoes. This fellow is asking for man-size boots. Besides, in recent weeks a disease has decimated the flocks around town, and skins are becoming scarce. The tanners are starting to demand twice as many finished shoes in exchange for the same quantity of leather. Shouldn’t that be taken into consideration?
In a barter economy, every day the shoemaker and the apple grower will have to learn anew the relative prices of dozens of commodities. If one hundred different commodities are traded in the market, then buyers and sellers will have to know 4,950 different exchange rates. And if 1,000 different commodities are traded, buyers and sellers must juggle 499,500 different exchange rates!5 How do you figure it out?
It gets worse. Even if you manage to calculate how many apples equal one pair of shoes, barter is not always possible. After all, a trade requires that each side want what the other has to offer. What happens if the shoemaker doesn’t like apples and, if at the moment in question, what he really wants is a divorce? True, the farmer could look for a lawyer who likes apples and set up a three-way deal. But what if the lawyer is full up on apples but really needs a haircut?
Some societies tried to solve the problem by establishing a central barter system that collected products from specialist growers and manufacturers and distributed them to those who needed them. The largest and most famous such experiment was conducted in the Soviet Union, and it failed miserably. ‘Everyone would work according to their abilities, and receive according to their needs’ turned out in practice into ‘everyone would work as little as they can get away with, and receive as much as they could grab’. More moderate and more successful experiments were made on other occasions, for example in the Inca Empire. Yet most societies found a more easy way to connect large numbers of experts – they developed money.
Shells and Cigarettes

Money was created many times in many places. Its development required no technological breakthroughs – it was a purely mental revolution. It involved the creation of a new inter-subjective reality that exists solely in people’s shared imagination.
Money is not coins and banknotes. Money is anything that people are willing to use in order to represent systematically the value of other things for the purpose of exchanging goods and services. Money enables people to compare quickly and easily the value of different commodities (such as apples, shoes and divorces), to easily exchange one thing for another, and to store wealth conveniently. There have been many types of money. The most familiar is the coin, which is a standardised piece of imprinted metal. Yet money existed long before the invention of coinage, and cultures have prospered using other things as currency, such as shells, cattle, skins, salt, grain, beads, cloth and promissory notes. Cowry shells were used as money for about 4,000 years all over Africa, South Asia, East Asia and Oceania. Taxes could still be paid in cowry shells in British Uganda in the early twentieth century.
 
26. In ancient Chinese script the cowry-shell sign represented money, in words such as ‘to sell’ or ‘reward’.

In modern prisons and POW camps, cigarettes have often served as money. Even non-smoking prisoners have been willing to accept cigarettes in payment, and to calculate the value of all other goods and services in cigarettes. One Auschwitz survivor described the cigarette currency used in the camp: ‘We had our own currency, whose value no one questioned: the cigarette. The price of every article was stated in cigarettes … In “normal” times, that is, when the candidates to the gas chambers were coming in at a regular pace, a loaf of bread cost twelve cigarettes; a 300-gram package of margarine, thirty; a watch, eighty to 200; a litre of alcohol, 400 cigarettes!’6
In fact, even today coins and banknotes are a rare form of money. In 2006, the sum total of money in the world is about $60 trillion, yet the sum total of coins and banknotes was less than $6 trillion.7 More than 90 per cent of all money – more than $50 trillion appearing in our accounts – exists only on computer servers. Accordingly, most business transactions are executed by moving electronic data from one computer file to another, without any exchange of physical cash. Only a criminal buys a house, for example, by handing over a suitcase full of banknotes. As long as people are willing to trade goods and services in exchange for electronic data, it’s even better than shiny coins and crisp banknotes – lighter, less bulky, and easier to keep track of.
For complex commercial systems to function, some kind of money is indispensable. A shoemaker in a money economy needs to know only the prices charged for various kinds of shoes – there is no need to memorise the exchange rates between shoes and apples or goats. Money also frees apple experts from the need to search out apple-craving shoemakers, because everyone always wants money. This is perhaps its most basic quality. Everyone always wants money because everyone else also always wants money, which means you can exchange money for whatever you want or need. The shoemaker will always be happy to take your money, because no matter what he really wants – apples, goats or a divorce – he can get it in exchange for money.
Money is thus a universal medium of exchange that enables people to convert almost everything into almost anything else. Brawn gets converted to brain when a discharged soldier finances his college tuition with his military benefits. Land gets converted into loyalty when a baron sells property to support his retainers. Health is converted to justice when a physician uses her fees to hire a lawyer – or bribe a judge. It is even possible to convert sex into salvation, as fifteenth-century prostitutes did when they slept with men for money, which they in turn used to buy indulgences from the Catholic Church.
Ideal types of money enable people not merely to turn one thing into another, but to store wealth as well. Many valuables cannot be stored – such as time or beauty. Some things can be stored only for a short time, such as strawberries. Other things are more durable, but take up a lot of space and require expensive facilities and care. Grain, for example, can be stored for years, but to do so you need to build huge storehouses and guard against rats, mould, water, fire and thieves. Money, whether paper, computer bits or cowry shells, solves these problems. Cowry shells don’t rot, are unpalatable to rats, can survive fires and are compact enough to be locked up in a safe.
In order to use wealth it is not enough just to store it. It often needs to be transported from place to place. Some forms of wealth, such as real estate, cannot be transported at all. Commodities such as wheat and rice can be transported only with difficulty. Imagine a wealthy farmer living in a moneyless land who emigrates to a distant province. His wealth consists mainly of his house and rice paddies. The farmer cannot take with him the house or the paddies. He might exchange them for tons of rice, but it would be very burdensome and expensive to transport all that rice. Money solves these problems. The farmer can sell his property in exchange for a sack of cowry shells, which he can easily carry wherever he goes.
Because money can convert, store and transport wealth easily and cheaply, it made a vital contribution to the appearance of complex commercial networks and dynamic markets. Without money, commercial networks and markets would have been doomed to remain very limited in their size, complexity and dynamism.
How Does Money Work?

Cowry shells and dollars have value only in our common imagination. Their worth is not inherent in the chemical structure of the shells and paper, or their colour, or their shape. In other words, money isn’t a material reality – it is a psychological construct. It works by converting matter into mind. But why does it succeed? Why should anyone be willing to exchange a fertile rice paddy for a handful of useless cowry shells? Why are you willing to flip hamburgers, sell health insurance or babysit three obnoxious brats when all you get for your exertions is a few pieces of coloured paper?
People are willing to do such things when they trust the figments of their collective imagination. Trust is the raw material from which all types of money are minted. When a wealthy farmer sold his possessions for a sack of cowry shells and travelled with them to another province, he trusted that upon reaching his destination other people would be willing to sell him rice, houses and fields in exchange for the shells. Money is accordingly a system of mutual trust, and not just any system of mutual trust: money is the most universal and most efficient system of mutual trust ever devised.
What created this trust was a very complex and long-term network of political, social and economic relations. Why do I believe in the cowry shell or gold coin or dollar bill? Because my neighbours believe in them. And my neighbours believe in them because I believe in them. And we all believe in them because our king believes in them and demands them in taxes, and because our priest believes in them and demands them in tithes. Take a dollar bill and look at it carefully. You will see that it is simply a colourful piece of paper with the signature of the US secretary of the treasury on one side, and the slogan ‘In God We Trust’ on the other. We accept the dollar in payment, because we trust in God and the US secretary of the treasury. The crucial role of trust explains why our financial systems are so tightly bound up with our political, social and ideological systems, why financial crises are often triggered by political developments, and why the stock market can rise or fall depending on the way traders feel on a particular morning.
Initially, when the first versions of money were created, people didn’t have this sort of trust, so it was necessary to define as ‘money’ things that had real intrinsic value. History’s first known money Sumerian barley money – is a good example. It appeared in Sumer around 3000 BC, at the same time and place, and under the same circumstances, in which writing appeared. Just as writing developed to answer the needs of intensifying administrative activities, so barley money developed to answer the needs of intensifying economic activities.
Barley money was simply barley – fixed amounts of barley grains used as a universal measure for evaluating and exchanging all other goods and services. The most common measurement was the sila, equivalent to roughly one litre. Standardised bowls, each capable of containing one sila, were mass-produced so that whenever people needed to buy or sell anything, it was easy to measure the necessary amounts of barley. Salaries, too, were set and paid in silas of barley. A male labourer earned sixty silas a month, a female labourer thirty silas. A foreman could earn between 1,200 and 5,000 silas. Not even the most ravenous foreman could eat 5,000 litres of barley a month, but he could use the silas he didn’t eat to buy all sorts of other commodities – oil, goats, slaves, and something else to eat besides barley.8
Even though barley has intrinsic value, it was not easy to convince people to use it as money rather than as just another commodity. In order to understand why, just think what would happen if you took a sack full of barley to your local shopping centre, and tried to buy a shirt or a pizza. The vendors would probably call security. Still, it was somewhat easier to build trust in barley as the first type of money, because barley has an inherent biological value. Humans can eat it. On the other hand, it was difficult to store and transport barley. The real breakthrough in monetary history occurred when people gained trust in money that lacked inherent value, but was easier to store and transport. Such money appeared in ancient Mesopotamia in the middle of the third millennium BC. This was the silver shekel.
The silver shekel was not a coin, but rather 8.33 grams of silver. When Hammurabi’s Code declared that a superior man who killed a slave woman must pay her owner twenty silver shekels, it meant that he had to pay 166 grams of silver, not twenty coins. Most monetary terms in the Old Testament are given in terms of silver rather than coins. Josephs brothers sold him to the Ishmaelites for twenty silver shekels, or rather 166 grams of silver (the same price as a slave woman – he was a youth, after all).
Unlike the barley sila, the silver shekel had no inherent value. You cannot eat, drink or clothe yourself in silver, and it’s too soft for making useful tools – ploughshares or swords of silver would crumple almost as fast as ones made out of aluminium foil. When they are used for anything, silver and gold are made into jewellery, crowns and other status symbols – luxury goods that members of a particular culture identify with high social status. Their value is purely cultural.
Set weights of precious metals eventually gave birth to coins. The first coins in history were struck around 640 BC by King Alyattes of Lydia, in western Anatolia. These coins had a standardised weight of gold or silver, and were imprinted with an identification mark. The mark testified to two things. First, it indicated how much precious metal the coin contained. Second, it identified the authority that issued the coin and that guaranteed its contents. Almost all coins in use today are descendants of the Lydian coins.
Coins had two important advantages over unmarked metal ingots. First, the latter had to be weighed for every transaction. Second, weighing the ingot is not enough. How does the shoemaker know that the silver ingot I put down for my boots is really made of pure silver, and not of lead covered on the outside by a thin silver coating? Coins help solve these problems. The mark imprinted on them testifies to their exact value, so the shoemaker doesn’t have to keep a scale on his cash register. More importantly, the mark on the coin is the signature of some political authority that guarantees the coin’s value.
The shape and size of the mark varied tremendously throughout history, but the message was always the same: ‘I, the Great King So-And-So, give you my personal word that this metal disc contains exactly five grams of gold. If anyone dares counterfeit this coin, it means he is fabricating my own signature, which would be a blot on my reputation. I will punish such a crime with the utmost severity.’ That’s why counterfeiting money has always been considered a much more serious crime than other acts of deception. Counterfeiting is not just cheating – it’s a breach of sovereignty, an act of subversion against the power, privileges and person of the king. The legal term is lese-majesty (violating majesty), and was typically punished by torture and death. As long as people trusted the power and integrity of the king, they trusted his coins. Total strangers could easily agree on the worth of a Roman denarius coin, because they trusted the power and integrity of the Roman emperor, whose name and picture adorned it.

27. One of the earliest coins in history, from Lydia of the seventh century BC.

In turn, the power of the emperor rested on the denarius. Just think how difficult it would have been to maintain the Roman Empire without coins – if the emperor had to raise taxes and pay salaries in barley and wheat. It would have been impossible to collect barley taxes in Syria, transport the funds to the central treasury in Rome, and transport them again to Britain in order to pay the legions there. It would have been equally difficult to maintain the empire if the inhabitants of the city of Rome believed in gold coins, but the subject populations rejected this belief, putting their trust instead in cowry shells, ivory beads or rolls of cloth.
The Gospel of Gold

The trust in Rome’s coins was so strong that even outside the empire’s borders, people were happy to receive payment in denarii. In the first century AD, Roman coins were an accepted medium of exchange in the markets of India, even though the closest Roman legion was thousands of kilometres away. The Indians had such a strong confidence in the denarius and the image of the emperor that when local rulers struck coins of their own they closely imitated the denarius, down to the portrait of the Roman emperor! The name ‘denarius’ became a generic name for coins. Muslim caliphs Arabicised this name and issued ‘dinars’. The dinar is still the official name of the currency in Jordan, Iraq, Serbia, Macedonia, Tunisia and several other countries.
As Lydian-style coinage was spreading from the Mediterranean to the Indian Ocean, China developed a slightly different monetary system, based on bronze coins and unmarked silver and gold ingots. Yet the two monetary systems had enough in common (especially the reliance on gold and silver) that close monetary and commercial relations were established between the Chinese zone and the Lydian zone. Muslim and European merchants and conquerors gradually spread the Lydian system and the gospel of gold to the far corners of the earth. By the late modern era the entire world was a single monetary zone, relying first on gold and silver, and later on a few trusted currencies such as the British pound and the American dollar.
The appearance of a single transnational and transcultural monetary zone laid the foundation for the unification of Afro-Asia, and eventually of the entire globe, into a single economic and political sphere. People continued to speak mutually incomprehensible languages, obey different rulers and worship distinct gods, but all believed in gold and silver and in gold and silver coins. Without this shared belief, global trading networks would have been virtually impossible. The gold and silver that sixteenth-century conquistadors found in America enabled European merchants to buy silk, porcelain and spices in East Asia, thereby moving the wheels of economic growth in both Europe and East Asia. Most of the gold and silver mined in Mexico and the Andes slipped through European fingers to find a welcome home in the purses of Chinese silk and porcelain manufacturers. What would have happened to the global economy if the Chinese hadn’t suffered from the same ‘disease of the heart’ that afflicted Cortés and his companions – and had refused to accept payment in gold and silver?
Yet why should Chinese, Indians, Muslims and Spaniards – who belonged to very different cultures that failed to agree about much of anything – nevertheless share the belief in gold? Why didn’t it happen that Spaniards believed in gold, while Muslims believed in barley, Indians in cowry shells, and Chinese in rolls of silk? Economists have a ready answer. Once trade connects two areas, the forces of supply and demand tend to equalise the prices of transportable goods. In order to understand why, consider a hypothetical case. Assume that when regular trade opened between India and the Mediterranean, Indians were uninterested in gold, so it was almost worthless. But in the Mediterranean, gold was a coveted status symbol, hence its value was high. What would happen next?
Merchants travelling between India and the Mediterranean would notice the difference in the value of gold. In order to make a profit, they would buy gold cheaply in India and sell it dearly in the Mediterranean. Consequently, the demand for gold in India would skyrocket, as would its value. At the same time the Mediterranean would experience an influx of gold, whose value would consequently drop. Within a short time the value of gold in India and the Mediterranean would be quite similar. The mere fact that Mediterranean people believed in gold would cause Indians to start believing in it as well. Even if Indians still had no real use for gold, the fact that Mediterranean people wanted it would be enough to make the Indians value it.
Similarly, the fact that another person believes in cowry shells, or dollars, or electronic data, is enough to strengthen our own belief in them, even if that person is otherwise hated, despised or ridiculed by us. Christians and Muslims who could not agree on religious beliefs could nevertheless agree on a monetary belief, because whereas religion asks us to believe in something, money asks us to believe that other people believe in something.
For thousands of years, philosophers, thinkers and prophets have besmirched money and called it the root of all evil. Be that as it may, money is also the apogee of human tolerance. Money is more open-minded than language, state laws, cultural codes, religious beliefs and social habits. Money is the only trust system created by humans that can bridge almost any cultural gap, and that does not discriminate on the basis of religion, gender, race, age or sexual orientation. Thanks to money, even people who don’t know each other and don’t trust each other can nevertheless cooperate effectively.
The Price of Money

Money is based on two universal principles:
a. Universal convertibility: with money as an alchemist, you can turn land into loyalty, justice into health, and violence into knowledge.
b. Universal trust: with money as a go-between, any two people can cooperate on any project.
These principles have enabled millions of strangers to cooperate effectively in trade and industry. But these seemingly benign principles have a dark side. When everything is convertible, and when trust depends on anonymous coins and cowry shells, it corrodes local traditions, intimate relations and human values, replacing them with the cold laws of supply and demand.
Human communities and families have always been based on belief in ‘priceless’ things, such as honour, loyalty, morality and love. These things lie outside the domain of the market, and they shouldn’t be bought or sold for money. Even if the market offers a good price, certain things just aren’t done. Parents mustn’t sell their children into slavery; a devout Christian must not commit a mortal sin; a loyal knight must never betray his lord; and ancestral tribal lands shall never be sold to foreigners.
Money has always tried to break through these barriers, like water seeping through cracks in a dam. Parents have been reduced to selling some of their children into slavery in order to buy food for the others. Devout Christians have murdered, stolen and cheated – and later used their spoils to buy forgiveness from the church. Ambitious knights auctioned their allegiance to the highest bidder, while securing the loyalty of their own followers by cash payments. Tribal lands were sold to foreigners from the other side of the world in order to purchase an entry ticket into the global economy.
Money has an even darker side. For although money builds universal trust between strangers, this trust is invested not in humans, communities or sacred values, but in money itself and in the impersonal systems that back it. We do not trust the stranger, or the next-door neighbour – we trust the coin they hold. If they run out of coins, we run out of trust. As money brings down the dams of community, religion and state, the world is in danger of becoming one big and rather heartless marketplace.
Hence the economic history of humankind is a delicate dance. People rely on money to facilitate cooperation with strangers, but they’re afraid it will corrupt human values and intimate relations. With one hand people willingly destroy the communal dams that held at bay the movement of money and commerce for so long. Yet with the other hand they build new dams to protect society, religion and the environment from enslavement to market forces.
It is common nowadays to believe that the market always prevails, and that the dams erected by kings, priests and communities cannot long hold back the tides of money. This is naïve. Brutal warriors, religious fanatics and concerned citizens have repeatedly managed to trounce calculating merchants, and even to reshape the economy. It is therefore impossible to understand the unification of humankind as a purely economic process. In order to understand how thousands of isolated cultures coalesced over time to form the global village of today, we must take into account the role of gold and silver, but we cannot disregard the equally crucial role of steel.II

Imperial Visions

THE ANCIENT ROMANS WERE USED TO being defeated. Like the rulers of most of history’s great empires, they could lose battle after battle but still win the war. An empire that cannot sustain a blow and remain standing is not really an empire. Yet even the Romans found it hard to stomach the news arriving from northern Iberia in the middle of the second century BC. A small, insignificant mountain town called Numantia, inhabited by the peninsula’s native Celts, had dared to throw off the Roman yoke. Rome at the time was the unquestioned master of the entire Mediterranean basin, having vanquished the Macedonian and Seleucid empires, subjugated the proud city states of Greece, and turned Carthage into a smouldering ruin. The Numantians had nothing on their side but their fierce love of freedom and their inhospitable terrain. Yet they forced legion after legion to surrender or retreat in shame.
Eventually, in 134 BC, Roman patience snapped. The Senate decided to send Scipio Aemilianus, Rome’s foremost general and the man who had levelled Carthage, to take care of the Numantians. He was given a massive army of more than 30,000 soldiers. Scipio, who respected the fighting spirit and martial skill of the Numantians, preferred not to waste his soldiers in unnecessary combat. Instead, he encircled Numantia with a line of fortifications, blocking the town’s contact with the outside world. Hunger did his work for him. After more than a year, the food supply ran out. When the Numantians realised that all hope was lost, they burned down their town; according to Roman accounts, most of them killed themselves so as not to become Roman slaves.
Numantia later became a symbol of Spanish independence and courage. Miguel de Cervantes, the author of Don Quixote, wrote a tragedy called The Siege of Numantia which ends with the town’s destruction, but also with a vision of Spain’s future greatness. Poets composed paeans to its fierce defenders and painters committed majestic depictions of the siege to canvas. In 1882, its ruins were declared a national monument’ and became a pilgrimage site for Spanish patriots. In the 1950s and 1960s, the most popular comic books in Spain weren’t about Superman and Spiderman – they told of the adventures of El Jabato, an imaginary ancient Iberian hero who fought against the Roman oppressors. The ancient Numantians are to this day Spain’s paragons of heroism and patriotism, cast as role models for the country’s young people.
Yet Spanish patriots extol the Numantians in Spanish – a romance language that is a progeny of Scipio’s Latin. The Numantians spoke a now dead and lost Celtic language. Cervantes wrote The Siege of Numantia in Latin script, and the play follows Graeco-Roman artistic models. Numantia had no theatres. Spanish patriots who admire Numantian heroism tend also to be loyal followers of the Roman Catholic Church – don’t miss that first word – a church whose leader still sits in Rome and whose God prefers to be addressed in Latin. Similarly, modern Spanish law derives from Roman law; Spanish politics is built on Roman foundations; and Spanish cuisine and architecture owe a far greater debt to Roman legacies than to those of the Celts of Iberia. Nothing is really left of Numantia save ruins. Even its story has reached us thanks only to the writings of Roman historians. It was tailored to the tastes of Roman audiences which relished tales of freedom-loving barbarians. The victory of Rome over Numantia was so complete that the victors co-opted the very memory of the vanquished.
It’s not our kind of story. We like to see underdogs win. But there is no justice in history. Most past cultures have sooner or later fallen prey to the armies of some ruthless empire, which have consigned them to oblivion. Empires, too, ultimately fall, but they tend to leave behind rich and enduring legacies. Almost all people in the twenty-first century are the offspring of one empire or another.
What is an Empire?

An empire is a political order with two important characteristics. First, to qualify for that designation you have to rule over a significant number of distinct peoples, each possessing a different cultural identity and a separate territory. How many peoples exactly? Two or three is not sufficient. Twenty or thirty is plenty. The imperial threshold passes somewhere in between.
Second, empires are characterised by flexible borders and a potentially unlimited appetite. They can swallow and digest more and more nations and territories without altering their basic structure or identity. The British state of today has fairly clear borders that cannot be exceeded without altering the fundamental structure and identity of the state. A century ago almost any place on earth could have become part of the British Empire.
Cultural diversity and territorial flexibility give empires not only their unique character, but also their central role in history. It’s thanks to these two characteristics that empires have managed to unite diverse ethnic groups and ecological zones under a single political umbrella, thereby fusing together larger and larger segments of the human species and of planet Earth.
It should be stressed that an empire is defined solely by its cultural diversity and flexible borders, rather than by its origins, its form of government, its territorial extent, or the size of its population. An empire need not emerge from military conquest. The Athenian Empire began its life as a voluntary league, and the Habsburg Empire was born in wedlock, cobbled together by a string of shrewd marriage alliances. Nor must an empire be ruled by an autocratic emperor. The British Empire, the largest empire in history, was ruled by a democracy. Other democratic (or at least republican) empires have included the modern Dutch, French, Belgian and American empires, as well as the premodern empires of Novgorod, Rome, Carthage and Athens.
Size, too, does not really matter. Empires can be puny. The Athenian Empire at its zenith was much smaller in size and population than today’s Greece. The Aztec Empire was smaller than today’s Mexico. Both were nevertheless empires, whereas modern Greece and modern Mexico are not, because the former gradually subdued dozens and even hundreds of different polities while the latter have not. Athens lorded it over more than a hundred formerly independent city states, whereas the Aztec Empire, if we can trust its taxation records, ruled 371 different tribes and peoples.1
How was it possible to squeeze such a human potpourri into the territory of a modest modern state? It was possible because in the past there were many more distinct peoples in the world, each of which had a smaller population and occupied less territory than today’s typical people. The land between the Mediterranean and the Jordan River, which today struggles to satisfy the ambitions of just two peoples, easily accommodated in biblical times dozens of nations, tribes, petty kingdoms and city states.
Empires were one of the main reasons for the drastic reduction in human diversity. The imperial steamroller gradually obliterated the unique characteristics of numerous peoples (such as the Numantians), forging out of them new and much larger groups.
Evil Empires?

In our time, ‘imperialist’ ranks second only to ‘fascist’ in the lexicon of political swear words. The contemporary critique of empires commonly takes two forms:
1. Empires do not work. In the long run, it is not possible to rule effectively over a large number of conquered peoples.
2. Even if it can be done, it should not be done, because empires are evil engines of destruction and exploitation. Every people has a right to self-determination, and should never be subject to the rule of another.
From a historical perspective, the first statement is plain nonsense, and the second is deeply problematic.
The truth is that empire has been the world’s most common form of political organisation for the last 2,500 years. Most humans during these two and a half millennia have lived in empires. Empire is also a very stable form of government. Most empires have found it alarmingly easy to put down rebellions. In general, they have been toppled only by external invasion or by a split within the ruling elite. Conversely, conquered peoples don’t have a very good record of freeing themselves from their imperial overlords. Most have remained subjugated for hundreds of years. Typically, they have been slowly digested by the conquering empire, until their distinct cultures fizzled out.
For example, when the Western Roman Empire finally fell to invading Germanic tribes in 476 AD, the Numantians, Arverni, Helvetians, Samnites, Lusitanians, Umbrians, Etruscans and hundreds of other forgotten peoples whom the Romans conquered centuries earlier did not emerge from the empires eviscerated carcass like Jonah from the belly of the great fish. None of them were left. The biological descendants of the people who had identified themselves as members of those nations, who had spoken their languages, worshipped their gods and told their myths and legends, now thought, spoke and worshipped as Romans.
In many cases, the destruction of one empire hardly meant independence for subject peoples. Instead, a new empire stepped into the vacuum created when the old one collapsed or retreated. Nowhere has this been more obvious than in the Middle East. The current political constellation in that region – a balance of power between many independent political entities with more or less stable borders – is almost without parallel any time in the last several millennia. The last time the Middle East experienced such a situation was in the eighth century BC – almost 3,000 years ago! From the rise of the Neo-Assyrian Empire in the eighth century BC until the collapse of the British and French empires in the mid-twentieth century AD, the Middle East passed from the hands of one empire into the hands of another, like a baton in a relay race. And by the time the British and French finally dropped the baton, the Aramaeans, the Ammonites, the Phoenicians, the Philistines, the Moabites, the Edomites and the other peoples conquered by the Assyrians had long disappeared.
True, today’s Jews, Armenians and Georgians claim with some measure of justice that they are the offspring of ancient Middle Eastern peoples. Yet these are only exceptions that prove the rule, and even these claims are somewhat exaggerated. It goes without saying that the political, economic and social practices of modern Jews, for example, owe far more to the empires under which they lived during the past two millennia than to the traditions of the ancient kingdom of Judaea. If King David were to show up in an ultra-Orthodox synagogue in present-day Jerusalem, he would be utterly bewildered to find people dressed in East European clothes, speaking in a German dialect (Yiddish) and having endless arguments about the meaning of a Babylonian text (the Talmud). There were neither synagogues, volumes of Talmud, nor even Torah scrolls in ancient Judaea.
Building and maintaining an empire usually required the vicious slaughter of large populations and the brutal oppression of everyone who was left. The standard imperial toolkit included wars, enslavement, deportation and genocide. When the Romans invaded Scotland in AD 83, they were met by fierce resistance from local Caledonian tribes, and reacted by laying waste to the country. In reply to Roman peace offers, the chieftain Calgacus called the Romans ‘the ruffians of the world’, and said that ‘to plunder, slaughter and robbery they give the lying name of empire; they make a desert and call it peace’.2
This does not mean, however, that empires leave nothing of value in their wake. To colour all empires black and to disavow all imperial legacies is to reject most of human culture. Imperial elites used the profits of conquest to finance not only armies and forts but also philosophy, art, justice and charity. A significant proportion of humanity’s cultural achievements owe their existence to the exploitation of conquered populations. The profits and prosperity brought by Roman imperialism provided Cicero, Seneca and St Augustine with the leisure and wherewithal to think and write; the Taj Mahal could not have been built without the wealth accumulated by Mughal exploitation of their Indian subjects; and the Habsburg Empire’s profits from its rule over its Slavic, Hungarian and Romanian-speaking provinces paid Haydn’s salaries and Mozart’s commissions. No Caledonian writer preserved Calgacus’ speech for posterity. We know of it thanks to the Roman historian Tacitus. In fact, Tacitus probably made it up. Most scholars today agree that Tacitus not only fabricated the speech but invented the character of Calgacus, the Caledonian chieftain, to serve as a mouthpiece for what he and other upper-class Romans thought about their own country.
Even if we look beyond elite culture and high art, and focus instead on the world of common people, we find imperial legacies in the majority of modern cultures. Today most of us speak, think and dream in imperial languages that were forced upon our ancestors by the sword. Most East Asians speak and dream in the language of the Han Empire. No matter what their origins, nearly all the inhabitants of the two American continents, from Alaska’s Barrow Peninsula to the Straits of Magellan, communicate in one of four imperial languages: Spanish, Portuguese, French or English. Present-day Egyptians speak Arabic, think of themselves as Arabs, and identify wholeheartedly with the Arab Empire that conquered Egypt in the seventh century and crushed with an iron fist the repeated revolts that broke out against its rule. About 10 million Zulus in South Africa hark back to the Zulu age of glory in the nineteenth century, even though most of them descend from tribes who fought against the Zulu Empire, and were incorporated into it only through bloody military campaigns.
It’s for Your Own Good

The first empire about which we have definitive information was the Akkadian Empire of Sargon the Great (c.2250 BC). Sargon began his career as the king of Kish, a small city state in Mesopotamia. Within a few decades he managed to conquer not only all other Mesopotamian city states, but also large territories outside the Mesopotamian heartland. Sargon boasted that he had conquered the entire world. In reality, his dominion stretched from the Persian Gulf to the Mediterranean, and included most of today’s Iraq and Syria, along with a few slices of modern Iran and Turkey.
The Akkadian Empire did not last long after its founder’s death, but Sargon left behind an imperial mantle that seldom remained unclaimed. For the next 1,700 years, Assyrian, Babylonian and Hittite kings adopted Sargon as a role model, boasting that they, too, had conquered the entire world. Then, around 550 BC, Cyrus the Great of Persia came along with an even more impressive boast.

Map 4. The Akkadian Empire and the Persian Empire.

The kings of Assyria always remained the kings of Assyria. Even when they claimed to rule the entire world, it was obvious that they were doing it for the greater glory of Assyria, and they were not apologetic about it. Cyrus, on the other hand, claimed not merely to rule the whole world, but to do so for the sake of all people. ‘We are conquering you for your own benefit,’ said the Persians. Cyrus wanted the peoples he subjected to love him and to count themselves lucky to be Persian vassals. The most famous example of Cyrus’ innovative efforts to gain the approbation of a nation living under the thumb of his empire was his command that the Jewish exiles in Babylonia be allowed to return to their Judaean homeland and rebuild their temple. He even offered them financial assistance. Cyrus did not see himself as a Persian king ruling over Jews – he was also the king of the Jews, and thus responsible for their welfare.
The presumption to rule the entire world for the benefit of all its inhabitants was startling. Evolution has made Homo sapiens, like other social mammals, a xenophobic creature. Sapiens instinctively divide humanity into two parts, ‘we’ and ‘they’. We are people like you and me, who share our language, religion and customs. We are all responsible for each other, but not responsible for them. We were always distinct from them, and owe them nothing. We don’t want to see any of them in our territory, and we don’t care an iota what happens in their territory. They are barely even human. In the language of the Dinka people of the Sudan, ‘Dinka’ simply means ‘people’. People who are not Dinka are not people. The Dinka’s bitter enemies are the Nuer. What does the word Nuer mean in Nuer language? It means ‘original people’. Thousands of kilometres from the Sudan deserts, in the frozen ice-lands of Alaska and north-eastern Siberia, live the Yupiks. What does Yupik mean in Yupik language? It means ‘real people’.3
In contrast with this ethnic exclusiveness, imperial ideology from Cyrus onward has tended to be inclusive and all-encompassing. Even though it has often emphasised racial and cultural differences between rulers and ruled, it has still recognised the basic unity of the entire world, the existence of a single set of principles governing all places and times, and the mutual responsibilities of all human beings. Humankind is seen as a large family: the privileges of the parents go hand in hand with responsibility for the welfare of the children.
This new imperial vision passed from Cyrus and the Persians to Alexander the Great, and from him to Hellenistic kings, Roman emperors, Muslim caliphs, Indian dynasts, and eventually even to Soviet premiers and American presidents. This benevolent imperial vision has justified the existence of empires, and negated not only attempts by subject peoples to rebel, but also attempts by independent peoples to resist imperial expansion.
Similar imperial visions were developed independently of the Persian model in other parts of the world, most notably in Central America, in the Andean region, and in China. According to traditional Chinese political theory, Heaven (Tian) is the source of all legitimate authority on earth. Heaven chooses the most worthy person or family and gives them the Mandate of Heaven. This person or family then rules over All Under Heaven (Tianxia) for the benefit of all its inhabitants. Thus, a legitimate authority is – by definition – universal. If a ruler lacks the Mandate of Heaven, then he lacks legitimacy to rule even a single city. If a ruler enjoys the mandate, he is obliged to spread justice and harmony to the entire world. The Mandate of Heaven could not be given to several candidates simultaneously, and consequently one could not legitimise the existence of more than one independent state.
The first emperor of the united Chinese empire, Qín Shǐ Huángdì, boasted that ‘throughout the six directions [of the universe] everything belongs to the emperor … wherever there is a human footprint, there is not one who did not become a subject [of the emperor] … his kindness reaches even oxen and horses. There is not one who did not benefit. Every man is safe under his own roof.’4 In Chinese political thinking as well as Chinese historical memory, imperial periods were henceforth seen as golden ages of order and justice. In contradiction to the modern Western view that a just world is composed of separate nation states, in China periods of political fragmentation were seen as dark ages of chaos and injustice. This perception has had far-reaching implications for Chinese history. Every time an empire collapsed, the dominant political theory goaded the powers that be not to settle for paltry independent principalities, but to attempt reunification. Sooner or later these attempts always succeeded.
When They Become Us

Empires have played a decisive part in amalgamating many small cultures into fewer big cultures. Ideas, people, goods and technology spread more easily within the borders of an empire than in a politically fragmented region. Often enough, it was the empires themselves which deliberately spread ideas, institutions, customs and norms. One reason was to make life easier for themselves. It is difficult to rule an empire in which every little district has its own set of laws, its own form of writing, its own language and its own money. Standardisation was a boon to emperors.
A second and equally important reason why empires actively spread a common culture was to gain legitimacy. At least since the days of Cyrus and Qín Shǐ Huángdì, empires have justified their actions – whether road-building or bloodshed – as necessary to spread a superior culture from which the conquered benefit even more than the conquerors.
The benefits were sometimes salient – law enforcement, urban planning, standardisation of weights and measures – and sometimes questionable – taxes, conscription, emperor worship. But most imperial elites earnestly believed that they were working for the general welfare of all the empires inhabitants. China’s ruling class treated their country’s neighbours and its foreign subjects as miserable barbarians to whom the empire must bring the benefits of culture. The Mandate of Heaven was bestowed upon the emperor not in order to exploit the world, but in order to educate humanity. The Romans, too, justified their dominion by arguing that they were endowing the barbarians with peace, justice and refinement. The wild Germans and painted Gauls had lived in squalor and ignorance until the Romans tamed them with law, cleaned them up in public bathhouses, and improved them with philosophy. The Mauryan Empire in the third century BC took as its mission the dissemination of Buddha’s teachings to an ignorant world. The Muslim caliphs received a divine mandate to spread the Prophet’s revelation, peacefully if possible but by the sword if necessary. The Spanish and Portuguese empires proclaimed that it was not riches they sought in the Indies and America, but converts to the true faith. The sun never set on the British mission to spread the twin gospels of liberalism and free trade. The Soviets felt duty-bound to facilitate the inexorable historical march from capitalism towards the utopian dictatorship of the proletariat. Many Americans nowadays maintain that their government has a moral imperative to bring Third World countries the benefits of democracy and human rights, even if these goods are delivered by cruise missiles and F-16s.
The cultural ideas spread by empire were seldom the exclusive creation of the ruling elite. Since the imperial vision tends to be universal and inclusive, it was relatively easy for imperial elites to adopt ideas, norms and traditions from wherever they found them, rather than to stick fanatically to a single hidebound tradition. While some emperors sought to purify their cultures and return to what they viewed as their roots, for the most part empires have begot hybrid civilisations that absorbed much from their subject peoples. The imperial culture of Rome was Greek almost as much as Roman. The imperial Abbasid culture was part Persian, part Greek, part Arab. Imperial Mongol culture was a Chinese copycat. In the imperial United States, an American president of Kenyan blood can munch on Italian pizza while watching his favourite film, Lawrence of Arabia, a British epic about the Arab rebellion against the Turks.
Not that this cultural melting pot made the process of cultural assimilation any easier for the vanquished. The imperial civilisation may well have absorbed numerous contributions from various conquered peoples, but the hybrid result was still alien to the vast majority. The process of assimilation was often painful and traumatic. It is not easy to give up a familiar and loved local tradition, just as it is difficult and stressful to understand and adopt a new culture. Worse still, even when subject peoples were successful in adopting the imperial culture, it could take decades, if not centuries, until the imperial elite accepted them as part of ‘us’. The generations between conquest and acceptance were left out in the cold. They had already lost their beloved local culture, but they were not allowed to take an equal part in the imperial world. On the contrary, their adopted culture continued to view them as barbarians.
Imagine an Iberian of good stock living a century after the fall of Numantia. He speaks his native Celtic dialect with his parents, but has acquired impeccable Latin, with only a slight accent, because he needs it to conduct his business and deal with the authorities. He indulges his wife’s penchant for elaborately ornate baubles, but is a bit embarrassed that she, like other local women, retains this relic of Celtic taste – he’d rather have her adopt the clean simplicity of the jewellery worn by the Roman governor’s wife. He himself wears Roman tunics and, thanks to his success as a cattle merchant, due in no small part to his expertise in the intricacies of Roman commercial law, he has been able to build a Roman-style villa. Yet, even though he can recite Book III of Virgil’s Georgics by heart, the Romans still treat him as though he’s semi-barbarian. He realises with frustration that he’ll never get a government appointment, or one of the really good seats in the amphitheatre.
In the late nineteenth century, many educated Indians were taught the same lesson by their British masters. One famous anecdote tells of an ambitious Indian who mastered the intricacies of the English language, took lessons in Western-style dance, and even became accustomed to eating with a knife and fork. Equipped with his new manners, he travelled to England, studied law at University College London, and became a qualified barrister. Yet this young man of law, bedecked in suit and tie, was thrown off a train in the British colony of South Africa for insisting on travelling first class instead of settling for third class, where ‘coloured’ men like him were supposed to ride. His name was Mohandas Karamchand Gandhi.
In some cases the processes of acculturation and assimilation eventually broke down the barriers between the newcomers and the old elite. The conquered no longer saw the empire as an alien system of occupation, and the conquerors came to view their subjects as equal to themselves. Rulers and ruled alike came to see ‘them’ as ‘us’. All the subjects of Rome eventually, after centuries of imperial rule, were granted Roman citizenship. Non-Romans rose to occupy the top ranks in the officer corps of the Roman legions and were appointed to the Senate. In AD 48 the emperor Claudius admitted to the Senate several Gallic notables, who, he noted in a speech, through ‘customs, culture, and the ties of marriage have blended with ourselves’. Snobbish senators protested introducing these former enemies into the heart of the Roman political system. Claudius reminded them of an inconvenient truth. Most of their own senatorial families descended from Italian tribes who once fought against Rome, and were later granted Roman citizenship. Indeed, the emperor reminded them, his own family was of Sabine ancestry.5
During the second century AD, Rome was ruled by a line of emperors born in Iberia, in whose veins probably flowed at least a few drops of local Iberian blood. The reigns of Trajan, Hadrian, Antoninius Pius and Marcus Aurelius are generally thought to constitute the empire’s golden age. After that, all the ethnic dams were let down. Emperor Septimius Severus (193–211) was the scion of a Punic family from Libya. Elagabalus (218–22) was a Syrian. Emperor Philip (244–9) was known colloquially as ‘Philip the Arab’. The empire’s new citizens adopted Roman imperial culture with such zest that, for centuries and even millennia after the empire itself collapsed, they continued to speak the empire’s language, to believe in the Christian God that the empire had adopted from one of its Levantine provinces, and to live by the empire’s laws.
A similar process occurred in the Arab Empire. When it was established in the mid-seventh century AD, it was based on a sharp division between the ruling Arab–Muslim elite and the subjugated Egyptians, Syrians, Iranians and Berbers, who were neither Arabs nor Muslim. Many of the empire’s subjects gradually adopted the Muslim faith, the Arabic language and a hybrid imperial culture. The old Arab elite looked upon these parvenus with deep hostility, fearing to lose its unique status and identity. The frustrated converts clamoured for an equal share within the empire and in the world of Islam. Eventually they got their way. Egyptians, Syrians and Mesopotamians were increasingly seen as ‘Arabs’. Arabs, in their turn – whether authentic’ Arabs from Arabia or newly minted Arabs from Egypt and Syria – came to be increasingly dominated by non-Arab Muslims, in particular by Iranians, Turks and Berbers. The great success of the Arab imperial project was that the imperial culture it created was wholeheartedly adopted by numerous non-Arab people, who continued to uphold it, develop it and spread it – even after the original empire collapsed and the Arabs as an ethnic group lost their dominion.
In China the success of the imperial project was even more thorough. For more than 2,000 years, a welter of ethnic and cultural groups first termed barbarians were successfully integrated into imperial Chinese culture and became Han Chinese (so named after the Han Empire that ruled China from 206 BC to AD 220). The ultimate achievement of the Chinese Empire is that it is still alive and kicking, yet it is hard to see it as an empire except in outlying areas such as Tibet and Xinjiang. More than 90 per cent of the population of China are seen by themselves and by others as Han.
We can understand the decolonisation process of the last few decades in a similar way. During the modern era Europeans conquered much of the globe under the guise of spreading a superior Western culture. They were so successful that billions of people gradually adopted significant parts of that culture. Indians, Africans, Arabs, Chinese and Maoris learned French, English and Spanish. They began to believe in human rights and the principle of self-determination, and they adopted Western ideologies such as liberalism, capitalism, Communism, feminism and nationalism.
The Imperial Cycle

 

During the twentieth century, local groups that had adopted Western values claimed equality with their European conquerors in the name of these very values. Many anti-colonial struggles were waged under the banners of self-determination, socialism and human rights, all of which are Western legacies. Just as Egyptians, Iranians and Turks adopted and adapted the imperial culture that they inherited from the original Arab conquerors, so today’s Indians, Africans and Chinese have accepted much of the imperial culture of their former Western overlords, while seeking to mould it in accordance with their needs and traditions.
Good Guys and Bad Guys in History

It is tempting to divide history neatly into good guys and bad guys, with all empires among the bad guys. For the vast majority of empires were founded on blood, and maintained their power through oppression and war. Yet most of today’s cultures are based on imperial legacies. If empires are by definition bad, what does that say about us?
There are schools of thought and political movements that seek to purge human culture of imperialism, leaving behind what they claim is a pure, authentic civilisation, untainted by sin. These ideologies are at best naïve; at worst they serve as disingenuous window-dressing for crude nationalism and bigotry. Perhaps you could make a case that some of the myriad cultures that emerged at the dawn of recorded history were pure, untouched by sin and unadulterated by other societies. But no culture since that dawn can reasonably make that claim, certainly no culture that exists now on earth. All human cultures are at least in part the legacy of empires and imperial civilisations, and no academic or political surgery can cut out the imperial legacies without killing the patient.
Think, for example, about the love-hate relationship between the independent Indian republic of today and the British Raj. The British conquest and occupation of India cost the lives of millions of Indians, and was responsible for the continuous humiliation and exploitation of hundreds of millions more. Yet many Indians adopted, with the zest of converts, Western ideas such as self-determination and human rights, and were dismayed when the British refused to live up to their own declared values by granting native Indians either equal rights as British subjects or independence.
Nevertheless, the modern Indian state is a child of the British Empire. The British killed, injured and persecuted the inhabitants of the subcontinent, but they also united a bewildering mosaic of warring kingdoms, principalities and tribes, creating a shared national consciousness and a country that functioned more or less as a single political unit. They laid the foundations of the Indian judicial system, created its administrative structure, and built the railroad network that was critical for economic integration. Independent India adopted Western democracy, in its British incarnation, as its form of government. English is still the subcontinent’s lingua franca, a neutral tongue that native speakers of Hindi, Tamil and Malayalam can use to communicate. Indians are passionate cricket players and chai (tea) drinkers, and both game and beverage are British legacies. Commercial tea farming did not exist in India until the mid-nineteenth century, when it was introduced by the British East India Company. It was the snobbish British sahibs who spread the custom of tea drinking throughout the subcontinent.

28. The Chhatrapati Shivaji train station in Mumbai. It began its life as Victoria Station, Bombay. The British built it in the Neo-Gothic style that was popular in late nineteenth-century Britain. A Hindu nationalist government changed the names of both city and station, but showed no appetite for razing such a magnificent building, even if it was built by foreign oppressors.

How many Indians today would want to call a vote to divest themselves of democracy, English, the railway network, the legal system, cricket and tea on the grounds that they are imperial legacies? And if they did, wouldn’t the very act of calling a vote to decide the issue demonstrate their debt to their former overlords?

29. The Taj Mahal. An example of ‘authentic’ Indian culture, or the alien creation of Muslim imperialism?

Even if we were to completely disavow the legacy of a brutal empire in the hope of reconstructing and safeguarding the ‘authentic’ cultures that preceded it, in all probability what we will be defending is nothing but the legacy of an older and no less brutal empire. Those who resent the mutilation of Indian culture by the British Raj inadvertently sanctify the legacies of the Mughal Empire and the conquering sultanate of Delhi. And whoever attempts to rescue ‘authentic Indian culture’ from the alien influences of these Muslim empires sanctifies the legacies of the Gupta Empire, the Kushan Empire and the Maurya Empire. If an extreme Hindu nationalist were to destroy all the buildings left by the British conquerors, such as Mumbai’s main train station, what about the structures left by India’s Muslim conquerors, such as the Taj Mahal?
Nobody really knows how to solve this thorny question of cultural inheritance. Whatever path we take, the first step is to acknowledge the complexity of the dilemma and to accept that simplistically dividing the past into good guys and bad guys leads nowhere. Unless, of course, we are willing to admit that we usually follow the lead of the bad guys.
The New Global Empire

Since around 200 BC, most humans have lived in empires. It seems likely that in the future, too, most humans will live in one. But this time the empire will be truly global. The imperial vision of dominion over the entire world could be imminent.
As the twenty-first century unfolds, nationalism is fast losing ground. More and more people believe that all of humankind is the legitimate source of political authority, rather than the members of a particular nationality, and that safeguarding human rights and protecting the interests of the entire human species should be the guiding light of politics. If so, having close to 200 independent states is a hindrance rather than a help. Since Swedes, Indonesians and Nigerians deserve the same human rights, wouldn’t it be simpler for a single global government to safeguard them?
The appearance of essentially global problems, such as melting ice caps, nibbles away at whatever legitimacy remains to the independent nation states. No sovereign state will be able to overcome global warming on its own. The Chinese Mandate of Heaven was given by Heaven to solve the problems of humankind. The modern Mandate of Heaven will be given by humankind to solve the problems of heaven, such as the hole in the ozone layer and the accumulation of greenhouse gases. The colour of the global empire may well be green.
As of 2014, the world is still politically fragmented, but states are fast losing their independence. Not one of them is really able to execute independent economic policies, to declare and wage wars as it pleases, or even to run its own internal affairs as it sees fit. States are increasingly open to the machinations of global markets, to the interference of global companies and NGOs, and to the supervision of global public opinion and the international judicial system. States are obliged to conform to global standards of financial behaviour, environmental policy and justice. Immensely powerful currents of capital, labour and information turn and shape the world, with a growing disregard for the borders and opinions of states.
The global empire being forged before our eyes is not governed by any particular state or ethnic group. Much like the Late Roman Empire, it is ruled by a multi-ethnic elite, and is held together by a common culture and common interests. Throughout the world, more and more entrepreneurs, engineers, experts, scholars, lawyers and managers are called to join the empire. They must ponder whether to answer the imperial call or to remain loyal to their state and their people. More and more choose the empire.12

The Law of Religion

IN THE MEDIEVAL MARKET IN SAMARKAND, a city built on a Central Asian oasis, Syrian merchants ran their hands over fine Chinese silks, fierce tribesmen from the steppes displayed the latest batch of straw-haired slaves from the far west, and shopkeepers pocketed shiny gold coins imprinted with exotic scripts and the profiles of unfamiliar kings. Here, at one of that era’s major crossroads between east and west, north and south, the unification of humankind was an everyday fact. The same process could be observed at work when Kublai Khan’s army mustered to invade Japan in 1281. Mongol cavalrymen in skins and furs rubbed shoulders with Chinese foot soldiers in bamboo hats, drunken Korean auxiliaries picked fights with tattooed sailors from the South China Sea, engineers from Central Asia listened with dropping jaws to the tall tales of European adventurers, and all obeyed the command of a single emperor.
Meanwhile, around the holy Ka’aba in Mecca, human unification was proceeding by other means. Had you been a pilgrim to Mecca, circling Islam’s holiest shrine in the year 1300, you might have found yourself in the company of a party from Mesopotamia, their robes floating in the wind, their eyes blazing with ecstasy, and their mouths repeating one after the other the ninety-nine names of God. Just ahead you might have seen a weather-beaten Turkish patriarch from the Asian steppes, hobbling on a stick and stroking his beard thoughtfully. To one side, gold jewellery shining against jet-black skin, might have been a group of Muslims from the African kingdom of Mali. The aroma of clove, turmeric, cardamom and sea salt would have signalled the presence of brothers from India, or perhaps from the mysterious spice islands further east.
Today religion is often considered a source of discrimination, disagreement and disunion. Yet, in fact, religion has been the third great unifier of humankind, alongside money and empires. Since all social orders and hierarchies are imagined, they are all fragile, and the larger the society, the more fragile it is. The crucial historical role of religion has been to give superhuman legitimacy to these fragile structures. Religions assert that our laws are not the result of human caprice, but are ordained by an absolute and supreme authority. This helps place at least some fundamental laws beyond challenge, thereby ensuring social stability.
Religion can thus be defined as a system of human norms and values that is founded on a belief in a superhuman order. This involves two distinct criteria:
1. Religions hold that there is a superhuman order, which is not the product of human whims or agreements. Professional football is not a religion, because despite its many laws, rites and often bizarre rituals, everyone knows that human beings invented football themselves, and FIFA may at any moment enlarge the size of the goal or cancel the offside rule.
2. Based on this superhuman order, religion establishes norms and values that it considers binding. Many Westerners today believe in ghosts, fairies and reincarnation, but these beliefs are not a source of moral and behavioural standards. As such, they do not constitute a religion.
Despite their ability to legitimise widespread social and political orders, not all religions have actuated this potential. In order to unite under its aegis a large expanse of territory inhabited by disparate groups of human beings, a religion must possess two further qualities. First, it must espouse a universal superhuman order that is true always and everywhere. Second, it must insist on spreading this belief to everyone. In other words, it must be universal and missionary.
The best-known religions of history, such as Islam and Buddhism, are universal and missionary. Consequently people tend to believe that all religions are like them. In fact, the majority of ancient religions were local and exclusive. Their followers believed in local deities and spirits, and had no interest in converting the entire human race. As far as we know, universal and missionary religions began to appear only in the first millennium BC. Their emergence was one of the most important revolutions in history, and made a vital contribution to the unification of humankind, much like the emergence of universal empires and universal money.
Silencing the Lambs

When animism was the dominant belief system, human norms and values had to take into consideration the outlook and interests of a multitude of other beings, such as animals, plants, fairies and ghosts. For example, a forager band in the Ganges Valley may have established a rule forbidding people to cut down a particularly large fig tree, lest the fig-tree spirit become angry and take revenge. Another forager band living in the Indus Valley may have forbidden people from hunting white-tailed foxes, because a white-tailed fox once revealed to a wise old woman where the band might find precious obsidian.
Such religions tended to be very local in outlook, and to emphasise the unique features of specific locations, climates and phenomena. Most foragers spent their entire lives within an area of no more than a thousand square kilometres. In order to survive, the inhabitants of a particular valley needed to understand the super-human order that regulated their valley, and to adjust their behaviour accordingly. It was pointless to try to convince the inhabitants of some distant valley to follow the same rules. The people of the Indus did not bother to send missionaries to the Ganges to convince locals not to hunt white-tailed foxes.
The Agricultural Revolution seems to have been accompanied by a religious revolution. Hunter-gatherers picked and pursued wild plants and animals, which could be seen as equal in status to Homo sapiens. The fact that man hunted sheep did not make sheep inferior to man, just as the fact that tigers hunted man did not make man inferior to tigers. Beings communicated with one another directly and negotiated the rules governing their shared habitat. In contrast, farmers owned and manipulated plants and animals, and could hardly degrade themselves by negotiating with their possessions. Hence the first religious effect of the Agricultural Revolution was to turn plants and animals from equal members of a spiritual round table into property.
This, however, created a big problem. Farmers may have desired absolute control of their sheep, but they knew perfectly well that their control was limited. They could lock the sheep in pens, castrate rams and selectively breed ewes, yet they could not ensure that the ewes conceived and gave birth to healthy lambs, nor could they prevent the eruption of deadly epidemics. How then to safeguard the fecundity of the flocks?
A leading theory about the origin of the gods argues that gods gained importance because they offered a solution to this problem. Gods such as the fertility goddess, the sky god and the god of medicine took centre stage when plants and animals lost their ability to speak, and the gods’ main role was to mediate between humans and the mute plants and animals. Much of ancient mythology is in fact a legal contract in which humans promise everlasting devotion to the gods in exchange for mastery over plants and animals – the first chapters of the book of Genesis are a prime example. For thousands of years after the Agricultural Revolution, religious liturgy consisted mainly of humans sacrificing lambs, wine and cakes to divine powers, who in exchange promised abundant harvests and fecund flocks.
The Agricultural Revolution initially had a far smaller impact on the status of other members of the animist system, such as rocks, springs, ghosts and demons. However, these too gradually lost status in favour of the new gods. As long as people lived their entire lives within limited territories of a few hundred square kilometres, most of their needs could be met by local spirits. But once kingdoms and trade networks expanded, people needed to contact entities whose power and authority encompassed a whole kingdom or an entire trade basin.
The attempt to answer these needs led to the appearance of polytheistic religions (from the Greek: poly = many, theos = god). These religions understood the world to be controlled by a group of powerful gods, such as the fertility goddess, the rain god and the war god. Humans could appeal to these gods and the gods might, if they received devotions and sacrifices, deign to bring rain, victory and health.
Animism did not entirely disappear at the advent of polytheism. Demons, fairies, ghosts, holy rocks, holy springs and holy trees remained an integral part of almost all polytheist religions. These spirits were far less important than the great gods, but for the mundane needs of many ordinary people, they were good enough. While the king in his capital city sacrificed dozens of fat rams to the great war god, praying for victory over the barbarians, the peasant in his hut lit a candle to the fig-tree fairy, praying that she help cure his sick son.
Yet the greatest impact of the rise of great gods was not on sheep or demons, but upon the status of Homo sapiens. Animists thought that humans were just one of many creatures inhabiting the world. Polytheists, on the other hand, increasingly saw the world as a reflection of the relationship between gods and humans. Our prayers, our sacrifices, our sins and our good deeds determined the fate of the entire ecosystem. A terrible flood might wipe out billions of ants, grasshoppers, turtles, antelopes, giraffes and elephants, just because a few stupid Sapiens made the gods angry. Polytheism thereby exalted not only the status of the gods, but also that of humankind. Less fortunate members of the old animist system lost their stature and became either extras or silent decor in the great drama of man’s relationship with the gods.
The Benefits of Idolatry

Two thousand years of monotheistic brainwashing have caused most Westerners to see polytheism as ignorant and childish idolatry. This is an unjust stereotype. In order to understand the inner logic of polytheism, it is necessary to grasp the central idea buttressing the belief in many gods.
Polytheism does not necessarily dispute the existence of a single power or law governing the entire universe. In fact, most polytheist and even animist religions recognised such a supreme power that stands behind all the different gods, demons and holy rocks. In classical Greek polytheism, Zeus, Hera, Apollo and their colleagues were subject to an omnipotent and all-encompassing power – Fate (Moira, Ananke). Nordic gods, too, were in thrall to fate, which doomed them to perish in the cataclysm of Ragnarök (the Twilight of the Gods). In the polytheistic religion of the Yoruba of West Africa, all gods were born of the supreme god Olodumare, and remained subject to him. In Hindu polytheism, a single principle, Atman, controls the myriad gods and spirits, humankind, and the biological and physical world. Atman is the eternal essence or soul of the entire universe, as well as of every individual and every phenomenon.
The fundamental insight of polytheism, which distinguishes it from monotheism, is that the supreme power governing the world is devoid of interests and biases, and therefore it is unconcerned with the mundane desires, cares and worries of humans. It’s pointless to ask this power for victory in war, for health or for rain, because from its all-encompassing vantage point, it makes no difference whether a particular kingdom wins or loses, whether a particular city prospers or withers, whether a particular person recuperates or dies. The Greeks did not waste any sacrifices on Fate, and Hindus built no temples to Atman.
The only reason to approach the supreme power of the universe would be to renounce all desires and embrace the bad along with the good – to embrace even defeat, poverty, sickness and death. Thus some Hindus, known as Sadhus or Sannyasis, devote their lives to uniting with Atman, thereby achieving enlightenment. They strive to see the world from the viewpoint of this fundamental principle, to realise that from its eternal perspective all mundane desires and fears are meaningless and ephemeral phenomena.
Most Hindus, however, are not Sadhus. They are sunk deep in the morass of mundane concerns, where Atman is not much help. For assistance in such matters, Hindus approach the gods with their partial powers. Precisely because their powers are partial rather than all-encompassing, gods such as Ganesha, Lakshmi and Saraswati have interests and biases. Humans can therefore make deals with these partial powers and rely on their help in order to win wars and recuperate from illness. There are necessarily many of these smaller powers, since once you start dividing up the all-encompassing power of a supreme principle, you’ll inevitably end up with more than one deity. Hence the plurality of gods.
The insight of polytheism is conducive to far-reaching religious tolerance. Since polytheists believe, on the one hand, in one supreme and completely disinterested power, and on the other hand in many partial and biased powers, there is no difficulty for the devotees of one god to accept the existence and efficacy of other gods. Polytheism is inherently open-minded, and rarely persecutes ‘heretics’ and ‘infidels’.
Even when polytheists conquered huge empires, they did not try to convert their subjects. The Egyptians, the Romans and the Aztecs did not send missionaries to foreign lands to spread the worship of Osiris, Jupiter or Huitzilopochtli (the chief Aztec god), and they certainly didn’t dispatch armies for that purpose. Subject peoples throughout the empire were expected to respect the empire’s gods and rituals, since these gods and rituals protected and legitimised the empire. Yet they were not required to give up their local gods and rituals. In the Aztec Empire, subject peoples were obliged to build temples for Huitzilopochtli, but these temples were built alongside those of local gods, rather than in their stead. In many cases the imperial elite itself adopted the gods and rituals of subject people. The Romans happily added the Asian goddess Cybele and the Egyptian goddess Isis to their pantheon.
The only god that the Romans long refused to tolerate was the monotheistic and evangelising god of the Christians. The Roman Empire did not require the Christians to give up their beliefs and rituals, but it did expect them to pay respect to the empire’s protector gods and to the divinity of the emperor. This was seen as a declaration of political loyalty. When the Christians vehemently refused to do so, and went on to reject all attempts at compromise, the Romans reacted by persecuting what they understood to be a politically subversive faction. And even this was done half-heartedly. In the 300 years from the crucifixion of Christ to the conversion of Emperor Constantine, polytheistic Roman emperors initiated no more than four general persecutions of Christians. Local administrators and governors incited some anti-Christian violence of their own. Still, if we combine all the victims of all these persecutions, it turns out that in these three centuries, the polytheistic Romans killed no more than a few thousand Christians.1 In contrast, over the course of the next 1,500 years, Christians slaughtered Christians by the millions to defend slightly different interpretations of the religion of love and compassion.
The religious wars between Catholics and Protestants that swept Europe in the sixteenth and seventeenth centuries are particularly notorious. All those involved accepted Christ’s divinity and His gospel of compassion and love. However, they disagreed about the nature of this love. Protestants believed that the divine love is so great that God was incarnated in flesh and allowed Himself to be tortured and crucified, thereby redeeming the original sin and opening the gates of heaven to all those who professed faith in Him. Catholics maintained that faith, while essential, was not enough. To enter heaven, believers had to participate in church rituals and do good deeds. Protestants refused to accept this, arguing that this quid pro quo belittles God’s greatness and love. Whoever thinks that entry to heaven depends upon his or her own good deeds magnifies his own importance, and implies that Christ’s suffering on the cross and God’s love for humankind are not enough.
These theological disputes turned so violent that during the sixteenth and seventeenth centuries, Catholics and Protestants killed each other by the hundreds of thousands. On 23 August 1572, French Catholics who stressed the importance of good deeds attacked communities of French Protestants who highlighted God’s love for humankind. In this attack, the St Bartholomew’s Day Massacre, between 5,000 and 10,000 Protestants were slaughtered in less than twenty-four hours. When the pope in Rome heard the news from France, he was so overcome by joy that he organised festive prayers to celebrate the occasion and commissioned Giorgio Vasari to decorate one of the Vatican’s rooms with a fresco of the massacre (the room is currently off-limits to visitors).2 More Christians were killed by fellow Christians in those twenty-four hours than by the polytheistic Roman Empire throughout its entire existence.
God is One

With time some followers of polytheist gods became so fond of their particular patron that they drifted away from the basic polytheist insight. They began to believe that their god was the only god, and that He was in fact the supreme power of the universe. Yet at the same time they continued to view Him as possessing interests and biases, and believed that they could strike deals with Him. Thus were born monotheist religions, whose followers beseech the supreme power of the universe to help them recover from illness, win the lottery and gain victory in war.
The first monotheist religion known to us appeared in Egypt, c.350 BC, when Pharaoh Akhenaten declared that one of the minor deities of the Egyptian pantheon, the god Aten, was, in fact, the supreme power ruling the universe. Akhenaten institutionalised the worship of Aten as the state religion and tried to check the worship of all other gods. His religious revolution, however, was unsuccessful. After his death, the worship of Aten was abandoned in favour of the old pantheon.
Polytheism continued to give birth here and there to other monotheist religions, but they remained marginal, not least because they failed to digest their own universal message. Judaism, for example, argued that the supreme power of the universe has interests and biases, yet His chief interest is in the tiny Jewish nation and in the obscure land of Israel. Judaism had little to offer other nations, and throughout most of its existence it has not been a missionary religion. This stage can be called the stage of ‘local monotheism’.
The big breakthrough came with Christianity. This faith began as an esoteric Jewish sect that sought to convince Jews that Jesus of Nazareth was their long-awaited messiah. However, one of the sect’s first leaders, Paul of Tarsus, reasoned that if the supreme power of the universe has interests and biases, and if He had bothered to incarnate Himself in the flesh and to die on the cross for the salvation of humankind, then this is something everyone should hear about, not just Jews. It was thus necessary to spread the good word – the gospel – about Jesus throughout the world.
Paul’s arguments fell on fertile ground. Christians began organising widespread missionary activities aimed at all humans. In one of history’s strangest twists, this esoteric Jewish sect took over the mighty Roman Empire.
Christian success served as a model for another monotheist religion that appeared in the Arabian peninsula in the seventh century – Islam. Like Christianity, Islam, too, began as a small sect in a remote corner of the world, but in an even stranger and swifter historical surprise it managed to break out of the deserts of Arabia and conquer an immense empire stretching from the Atlantic Ocean to India. Henceforth, the monotheist idea played a central role in world history.
Monotheists have tended to be far more fanatical and missionary than polytheists. A religion that recognises the legitimacy of other faiths implies either that its god is not the supreme power of the universe, or that it received from God just part of the universal truth. Since monotheists have usually believed that they are in possession of the entire message of the one and only God, they have been compelled to discredit all other religions. Over the last two millennia, monotheists repeatedly tried to strengthen their hand by violently exterminating all competition.
It worked. At the beginning of the first century AD, there were hardly any monotheists in the world. Around AD 500, one of the world’s largest empires – the Roman Empire – was a Christian polity, and missionaries were busy spreading Christianity to other parts of Europe, Asia and Africa. By the end of the first millennium AD, most people in Europe, West Asia and North Africa were monotheists, and empires from the Atlantic Ocean to the Himalayas claimed to be ordained by the single great God. By the early sixteenth century, monotheism dominated most of Afro-Asia, with the exception of East Asia and the southern parts of Africa, and it began extending long tentacles towards South Africa, America and Oceania. Today most people outside East Asia adhere to one monotheist religion or another, and the global political order is built on monotheistic foundations.
Yet just as animism continued to survive within polytheism, so polytheism continued to survive within monotheism. In theory, once a person believes that the supreme power of the universe has interests and biases, what’s the point in worshipping partial powers? Who would want to approach a lowly bureaucrat when the president’s office is open to you? Indeed, monotheist theology tends to deny the existence of all gods except the supreme God, and to pour hellfire and brimstone over anyone who dares worship them.

Map 5. The Spread of Christianity and Islam.

Yet there has always been a chasm between theological theories and historical realities. Most people have found it difficult to digest the monotheist idea fully. They have continued to divide the world into ‘we’ and ‘they’, and to see the supreme power of the universe as too distant and alien for their mundane needs. The monotheist religions expelled the gods through the front door with a lot of fanfare, only to take them back in through the side window. Christianity, for example, developed its own pantheon of saints, whose cults differed little from those of the polytheistic gods.
Just as the god Jupiter defended Rome and Huitzilopochtli protected the Aztec Empire, so every Christian kingdom had its own patron saint who helped it overcome difficulties and win wars. England was protected by St George, Scotland by St Andrew, Hungary by St Stephen, and France had St Martin. Cities and towns, professions, and even diseases – each had their own saint. The city of Milan had St Ambrose, while St Mark watched over Venice. St Florian protected chimney cleaners, whereas St Mathew lent a hand to tax collectors in distress. If you suffered from headaches you had to pray to St Agathius, but if from toothaches, then St Apollonia was a much better audience.
The Christian saints did not merely resemble the old polytheistic gods. Often they were these very same gods in disguise. For example, the chief goddess of Celtic Ireland prior to the coming of Christianity was Brigid. When Ireland was Christianised, Brigid too was baptised. She became St Brigit, who to this day is the most revered saint in Catholic Ireland.
The Battle of Good and Evil

Polytheism gave birth not merely to monotheist religions, but also to dualistic ones. Dualistic religions espouse the existence of two opposing powers: good and evil. Unlike monotheism, dualism believes that evil is an independent power, neither created by the good God, nor subordinate to it. Dualism explains that the entire universe is a battleground between these two forces, and that everything that happens in the world is part of the struggle.
Dualism is a very attractive world view because it has a short and simple answer to the famous Problem of Evil, one of the fundamental concerns of human thought. ‘Why is there evil in the world? Why is there suffering? Why do bad things happen to good people?’ Monotheists have to practise intellectual gymnastics to explain how an all-knowing, all-powerful and perfectly good God allows so much suffering in the world. One well-known explanation is that this is God’s way of allowing for human free will. Were there no evil, humans could not choose between good and evil, and hence there would be no free will. This, however, is a non-intuitive answer that immediately raises a host of new questions. Freedom of will allows humans to choose evil. Many indeed choose evil and, according to the standard monotheist account, this choice must bring divine punishment in its wake. If God knew in advance that a particular person would use her free will to choose evil, and that as a result she would be punished for this by eternal tortures in hell, why did God create her? Theologians have written countless books to answer such questions. Some find the answers convincing. Some don’t. What’s undeniable is that monotheists have a hard time dealing with the Problem of Evil.
For dualists, it’s easy to explain evil. Bad things happen even to good people because the world is not governed single-handedly by a good God. There is an independent evil power loose in the world. The evil power does bad things.
Dualism has its own drawbacks. While solving the Problem of Evil, it is unnerved by the Problem of Order. If the world was created by a single God, it’s clear why it is such an orderly place, where everything obeys the same laws. But if Good and Evil battle for control of the world, who enforces the laws governing this cosmic war? Two rival states can fight one another because both obey the same laws of physics. A missile launched from Pakistan can hit targets in India because gravity works the same way in both countries. When Good and Evil fight, what common laws do they obey, and who decreed these laws?
So, monotheism explains order, but is mystified by evil. Dualism explains evil, but is puzzled by order. There is one logical way of solving the riddle: to argue that there is a single omnipotent God who created the entire universe – and He’s evil. But nobody in history has had the stomach for such a belief.
Dualistic religions flourished for more than a thousand years. Sometime between 1500 BC and 1000 BC a prophet named Zoroaster (Zarathustra) was active somewhere in Central Asia. His creed passed from generation to generation until it became the most important of dualistic religions – Zoroastrianism. Zoroastrians saw the world as a cosmic battle between the good god Ahura Mazda and the evil god Angra Mainyu. Humans had to help the good god in this battle. Zoroastrianism was an important religion during the Achaemenid Persian Empire (550–330 BC) and later became the official religion of the Sassanid Persian Empire (AD 224–651). It exerted a major influence on almost all subsequent Middle Eastern and Central Asian religions, and it inspired a number of other dualist religions, such as Gnosticism and Manichaeanism.
During the third and fourth centuries AD, the Manichaean creed spread from China to North Africa, and for a moment it appeared that it would beat Christianity to achieve dominance in the Roman Empire. Yet the Manichaeans lost the soul of Rome to the Christians, the Zoroastrian Sassanid Empire was overrun by the monotheistic Muslims, and the dualist wave subsided. Today only a handful of dualist communities survive in India and the Middle East.
Nevertheless, the rising tide of monotheism did not really wipe out dualism. Jewish, Christian and Muslim monotheism absorbed numerous dualist beliefs and practices, and some of the most basic ideas of what we call ‘monotheism’ are, in fact, dualist in origin and spirit. Countless Christians, Muslims and Jews believe in a powerful evil force – like the one Christians call the Devil or Satan – who can act independently, fight against the good God, and wreak havoc without God’s permission.
How can a monotheist adhere to such a dualistic belief (which, by the way, is nowhere to be found in the Old Testament)? Logically, it is impossible. Either you believe in a single omnipotent God or you believe in two opposing powers, neither of which is omnipotent. Still, humans have a wonderful capacity to believe in contradictions. So it should not come as a surprise that millions of pious Christians, Muslims and Jews manage to believe at one and the same time in an omnipotent God and an independent Devil. Countless Christians, Muslims and Jews have gone so far as to imagine that the good God even needs our help in its struggle against the Devil, which inspired among other things the call for jihads and crusades.
Another key dualistic concept, particularly in Gnosticism and Manichaeanism, was the sharp distinction between body and soul, between matter and spirit. Gnostics and Manichaeans argued that the good god created the spirit and the soul, whereas matter and bodies are the creation of the evil god. Man, according to this view, serves as a battleground between the good soul and the evil body. From a monotheistic perspective, this is nonsense – why distinguish so sharply between body and soul, or matter and spirit? And why argue that body and matter are evil? After all, everything was created by the same good God. But monotheists could not help but be captivated by dualist dichotomies, precisely because they helped them address the problem of evil. So such oppositions eventually became cornerstones of Christian and Muslim thought. Belief in heaven (the realm of the good god) and hell (the realm of the evil god) was also dualist in origin. There is no trace of this belief in the Old Testament, which also never claims that the souls of people continue to live after the death of the body.
In fact, monotheism, as it has played out in history, is a kaleidoscope of monotheist, dualist, polytheist and animist legacies, jumbling together under a single divine umbrella. The average Christian believes in the monotheist God, but also in the dualist Devil, in polytheist saints, and in animist ghosts. Scholars of religion have a name for this simultaneous avowal of different and even contradictory ideas and the combination of rituals and practices taken from different sources. It’s called syncretism. Syncretism might, in fact, be the single great world religion.
The Law of Nature

All the religions we have discussed so far share one important characteristic: they all focus on a belief in gods and other supernatural entities. This seems obvious to Westerners, who are familiar mainly with monotheistic and polytheist creeds. In fact, however, the religious history of the world does not boil down to the history of gods. During the first millennium BC, religions of an altogether new kind began to spread through Afro-Asia. The newcomers, such as Jainism and Buddhism in India, Daoism and Confucianism in China, and Stoicism, Cynicism and Epicureanism in the Mediterranean basin, were characterised by their disregard of gods.
These creeds maintained that the superhuman order governing the world is the product of natural laws rather than of divine wills and whims. Some of these natural-law religions continued to espouse the existence of gods, but their gods were subject to the laws of nature no less than humans, animals and plants were. Gods had their niche in the ecosystem, just as elephants and porcupines had theirs, but could no more change the laws of nature than elephants can. A prime example is Buddhism, the most important of the ancient natural law religions, which remains one of the major faiths.
The central figure of Buddhism is not a god but a human being, Siddhartha Gautama. According to Buddhist tradition, Gautama was heir to a small Himalayan kingdom, sometime around 500 BC. The young prince was deeply affected by the suffering evident all around him. He saw that men and women, children and old people, all suffer not just from occasional calamities such as war and plague, but also from anxiety, frustration and discontent, all of which seem to be an inseparable part of the human condition. People pursue wealth and power, acquire knowledge and possessions, beget sons and daughters, and build houses and palaces. Yet no matter what they achieve, they are never content. Those who live in poverty dream of riches. Those who have a million want two million. Those who have two million want 10 million. Even the rich and famous are rarely satisfied. They too are haunted by ceaseless cares and worries, until sickness, old age and death put a bitter end to them. Everything that one has accumulated vanishes like smoke. Life is a pointless rat race. But how to escape it?
At the age of twenty-nine Gautama slipped away from his palace in the middle of the night, leaving behind his family and possessions. He travelled as a homeless vagabond throughout northern India, searching for a way out of suffering. He visited ashrams and sat at the feet of gurus but nothing liberated him entirely – some dissatisfaction always remained. He did not despair. He resolved to investigate suffering on his own until he found a method for complete liberation. He spent six years meditating on the essence, causes and cures for human anguish. In the end he came to the realisation that suffering is not caused by ill fortune, by social injustice, or by divine whims. Rather, suffering is caused by the behaviour patterns of one’s own mind.
Gautama’s insight was that no matter what the mind experiences, it usually reacts with craving, and craving always involves dissatisfaction. When the mind experiences something distasteful it craves to be rid of the irritation. When the mind experiences something pleasant, it craves that the pleasure will remain and will intensify. Therefore, the mind is always dissatisfied and restless. This is very clear when we experience unpleasant things, such as pain. As long as the pain continues, we are dissatisfied and do all we can to avoid it. Yet even when we experience pleasant things we are never content. We either fear that the pleasure might disappear, or we hope that it will intensify. People dream for years about finding love but are rarely satisfied when they find it. Some become anxious that their partner will leave; others feel that they have settled cheaply, and could have found someone better. And we all know people who manage to do both.

Map 6. The Spread of Buddhism.

Great gods can send us rain, social institutions can provide justice and good health care, and lucky coincidences can turn us into millionaires, but none of them can change our basic mental patterns. Hence even the greatest kings are doomed to live in angst, constantly fleeing grief and anguish, forever chasing after greater pleasures.
Gautama found that there was a way to exit this vicious circle. If, when the mind experiences something pleasant or unpleasant, it simply understands things as they are, then there is no suffering. If you experience sadness without craving that the sadness go away, you continue to feel sadness but you do not suffer from it. There can actually be richness in the sadness. If you experience joy without craving that the joy linger and intensify, you continue to feel joy without losing your peace of mind.
But how do you get the mind to accept things as they are, without craving? To accept sadness as sadness, joy as joy, pain as pain? Gautama developed a set of meditation techniques that train the mind to experience reality as it is, without craving. These practices train the mind to focus all its attention on the question, ‘What am I experiencing now?’ rather than on ‘What would I rather be experiencing?’ It is difficult to achieve this state of mind, but not impossible.
Gautama grounded these meditation techniques in a set of ethical rules meant to make it easier for people to focus on actual experience and to avoid falling into cravings and fantasies. He instructed his followers to avoid killing, promiscuous sex and theft, since such acts necessarily stoke the fire of craving (for power, for sensual pleasure, or for wealth). When the flames are completely extinguished, craving is replaced by a state of perfect contentment and serenity, known as nirvana (the literal meaning of which is ‘extinguishing the fire’). Those who have attained nirvana are fully liberated from all suffering. They experience reality with the utmost clarity, free of fantasies and delusions. While they will most likely still encounter unpleasantness and pain, such experiences cause them no misery. A person who does not crave cannot suffer.
According to Buddhist tradition, Gautama himself attained nirvana and was fully liberated from suffering. Henceforth he was known as ‘Buddha’, which means ‘The Enlightened One’. Buddha spent the rest of his life explaining his discoveries to others so that everyone could be freed from suffering. He encapsulated his teachings in a single law: suffering arises from craving; the only way to be fully liberated from suffering is to be fully liberated from craving; and the only way to be liberated from craving is to train the mind to experience reality as it is.
This law, known as dharma or dhamma, is seen by Buddhists as a universal law of nature. That ‘suffering arises from craving’ is always and everywhere true, just as in modern physics E always equals mc2. Buddhists are people who believe in this law and make it the fulcrum of all their activities. Belief in gods, on the other hand, is of minor importance to them. The first principle of monotheist religions is ‘God exists. What does He want from me?’ The first principle of Buddhism is ‘Suffering exists. How do I escape it?’
Buddhism does not deny the existence of gods – they are described as powerful beings who can bring rains and victories – but they have no influence on the law that suffering arises from craving. If the mind of a person is free of all craving, no god can make him miserable. Conversely, once craving arises in a person’s mind, all the gods in the universe cannot save him from suffering.
Yet much like the monotheist religions, premodern natural-law religions such as Buddhism never really rid themselves of the worship of gods. Buddhism told people that they should aim for the ultimate goal of complete liberation from suffering, rather than for stops along the way such as economic prosperity and political power. However, 99 per cent of Buddhists did not attain nirvana, and even if they hoped to do so in some future lifetime, they devoted most of their present lives to the pursuit of mundane achievements. So they continued to worship various gods, such as the Hindu gods in India, the Bon gods in Tibet, and the Shinto gods in Japan.
Moreover, as time went by several Buddhist sects developed pantheons of Buddhas and bodhisattvas. These are human and non-human beings with the capacity to achieve full liberation from suffering but who forego this liberation out of compassion, in order to help the countless beings still trapped in the cycle of misery. Instead of worshipping gods, many Buddhists began worshipping these enlightened beings, asking them for help not only in attaining nirvana, but also in dealing with mundane problems. Thus we find many Buddhas and bodhisattvas throughout East Asia who spend their time bringing rain, stopping plagues, and even winning bloody wars – in exchange for prayers, colourful flowers, fragrant incense and gifts of rice and candy.
The Worship of Man

The last 300 years are often depicted as an age of growing secularism, in which religions have increasingly lost their importance. If we are talking about theist religions, this is largely correct. But if we take into consideration natural-law religions, then modernity turns out to be an age of intense religious fervour, unparalleled missionary efforts, and the bloodiest wars of religion in history. The modern age has witnessed the rise of a number of new natural-law religions, such as liberalism, Communism, capitalism, nationalism and Nazism. These creeds do not like to be called religions, and refer to themselves as ideologies. But this is just a semantic exercise. If a religion is a system of human norms and values that is founded on belief in a superhuman order, then Soviet Communism was no less a religion than Islam.
Islam is of course different from Communism, because Islam sees the superhuman order governing the world as the edict of an omnipotent creator god, whereas Soviet Communism did not believe in gods. But Buddhism too gives short shrift to gods, and yet we commonly classify it as a religion. Like Buddhists, Communists believed in a superhuman order of natural and immutable laws that should guide human actions. Whereas Buddhists believe that the law of nature was discovered by Siddhartha Gautama, Communists believed that the law of nature was discovered by Karl Marx, Friedrich Engels and Vladimir Ilyich Lenin. The similarity does not end there. Like other religions, Communism too has its holy scripts and prophetic books, such as Marx’s Das Kapital, which foretold that history would soon end with the inevitable victory of the proletariat. Communism had its holidays and festivals, such as the First of May and the anniversary of the October Revolution. It had theologians adept at Marxist dialectics, and every unit in the Soviet army had a chaplain, called a commissar, who monitored the piety of soldiers and officers. Communism had martyrs, holy wars and heresies, such as Trotskyism. Soviet Communism was a fanatical and missionary religion. A devout Communist could not be a Christian or a Buddhist, and was expected to spread the gospel of Marx and Lenin even at the price of his or her life.

Religion is a system of human norms and values that is founded on belief in a superhuman order. The theory of relativity is not a religion, because (at least so far) there are no human norms and values that are founded on it. Football is not a religion because nobody argues that its rules reflect superhuman edicts. Islam, Buddhism and Communism are all religions, because all are systems of human norms and values that are founded on belief in a superhuman order. (Note the difference between ‘superhuman’ and ‘supernatural’. The Buddhist law of nature and the Marxist laws of history are superhuman, since they were not legislated by humans. Yet they are not supernatural.)

Some readers may feel very uncomfortable with this line of reasoning. If it makes you feel better, you are free to go on calling Communism an ideology rather than a religion. It makes no difference. We can divide creeds into god-centred religions and godless ideologies that claim to be based on natural laws. But then, to be consistent, we would need to catalogue at least some Buddhist, Daoist and Stoic sects as ideologies rather than religions. Conversely, we should note that belief in gods persists within many modern ideologies, and that some of them, most notably liberalism, make little sense without this belief.
*

It would be impossible to survey here the history of all the new modern creeds, especially because there are no clear boundaries between them. They are no less syncretic than monotheism and popular Buddhism. Just as a Buddhist could worship Hindu deities, and just as a monotheist could believe in the existence of Satan, so the typical American nowadays is simultaneously a nationalist (she believes in the existence of an American nation with a special role to play in history), a free-market capitalist (she believes that open competition and the pursuit of self-interest are the best ways to create a prosperous society), and a liberal humanist (she believes that humans have been endowed by their creator with certain inalienable rights). Nationalism will be discussed in Chapter 18. Capitalism – the most successful of the modern religions – gets a whole chapter, Chapter 16, which expounds its principal beliefs and rituals. In the remaining pages of this chapter I will address the humanist religions.
Theist religions focus on the worship of gods. Humanist religions worship humanity, or more correctly, Homo sapiens. Humanism is a belief that Homo sapiens has a unique and sacred nature, which is fundamentally different from the nature of all other animals and of all other phenomena. Humanists believe that the unique nature of Homo sapiens is the most important thing in the world, and it determines the meaning of everything that happens in the universe. The supreme good is the good of Homo sapiens. The rest of the world and all other beings exist solely for the benefit of this species.
All humanists worship humanity, but they do not agree on its definition. Humanism has split into three rival sects that fight over the exact definition of ‘humanity’, just as rival Christian sects fought over the exact definition of God. Today, the most important humanist sect is liberal humanism, which believes that ‘humanity’ is a quality of individual humans, and that the liberty of individuals is therefore sacrosanct. According to liberals, the sacred nature of humanity resides within each and every individual Homo sapiens. The inner core of individual humans gives meaning to the world, and is the source for all ethical and political authority. If we encounter an ethical or political dilemma, we should look inside and listen to our inner voice – the voice of humanity. The chief commandments of liberal humanism are meant to protect the liberty of this inner voice against intrusion or harm. These commandments are collectively known as ‘human rights’.
This, for example, is why liberals object to torture and the death penalty. In early modern Europe, murderers were thought to violate and destabilise the cosmic order. To bring the cosmos back to balance, it was necessary to torture and publicly execute the criminal, so that everyone could see the order re-established. Attending gruesome executions was a favourite pastime for Londoners and Parisians in the era of Shakespeare and Molière. In today’s Europe, murder is seen as a violation of the sacred nature of humanity. In order to restore order, present-day Europeans do not torture and execute criminals. Instead, they punish a murderer in what they see as the most ‘humane’ way possible, thus safeguarding and even rebuilding his human sanctity. By honouring the human nature of the murderer, everyone is reminded of the sanctity of humanity, and order is restored. By defending the murderer, we right what the murderer has wronged.
Even though liberal humanism sanctifies humans, it does not deny the existence of God, and is, in fact, founded on monotheist beliefs. The liberal belief in the free and sacred nature of each individual is a direct legacy of the traditional Christian belief in free and eternal individual souls. Without recourse to eternal souls and a Creator God, it becomes embarrassingly difficult for liberals to explain what is so special about individual Sapiens.
Another important sect is socialist humanism. Socialists believe that ‘humanity’ is collective rather than individualistic. They hold as sacred not the inner voice of each individual, but the species Homo sapiens as a whole. Whereas liberal humanism seeks as much freedom as possible for individual humans, socialist humanism seeks equality between all humans. According to socialists, inequality is the worst blasphemy against the sanctity of humanity, because it privileges peripheral qualities of humans over their universal essence. For example, when the rich are privileged over the poor, it means that we value money more than the universal essence of all humans, which is the same for rich and poor alike.
Like liberal humanism, socialist humanism is built on monotheist foundations. The idea that all humans are equal is a revamped version of the monotheist conviction that all souls are equal before God. The only humanist sect that has actually broken loose from traditional monotheism is evolutionary humanism, whose most famous representatives are the Nazis. What distinguished the Nazis from other humanist sects was a different definition of ‘humanity’, one deeply influenced by the theory of evolution. In contrast to other humanists, the Nazis believed that humankind is not something universal and eternal, but rather a mutable species that can evolve or degenerate. Man can evolve into superman, or degenerate into a subhuman.
The main ambition of the Nazis was to protect humankind from degeneration and encourage its progressive evolution. This is why the Nazis said that the Aryan race, the most advanced form of humanity, had to be protected and fostered, while degenerate kinds of Homo sapiens like Jews, Roma, homosexuals and the mentally ill had to be quarantined and even exterminated. The Nazis explained that Homo sapiens itself appeared when one ‘superior’ population of ancient humans evolved, whereas ‘inferior’ populations such as the Neanderthals became extinct. These different populations were at first no more than different races, but developed independently along their own evolutionary paths. This might well happen again. According to the Nazis, Homo sapiens had already divided into several distinct races, each with its own unique qualities. One of these races, the Aryan race, had the finest qualities – rationalism, beauty, integrity, diligence. The Aryan race therefore had the potential to turn man into superman. Other races, such as Jews and blacks, were today’s Neanderthals, possessing inferior qualities. If allowed to breed, and in particular to intermarry with Aryans, they would adulterate all human populations and doom Homo sapiens to extinction.
Biologists have since debunked Nazi racial theory. In particular, genetic research conducted after 1945 has demonstrated that the differences between the various human lineages are far smaller than the Nazis postulated. But these conclusions are relatively new. Given the state of scientific knowledge in 1933, Nazi beliefs were hardly outside the pale. The existence of different human races, the superiority of the white race, and the need to protect and cultivate this superior race were widely held beliefs among most Western elites. Scholars in the most prestigious Western universities, using the orthodox scientific methods of the day, published studies that allegedly proved that members of the white race were more intelligent, more ethical and more skilled than Africans or Indians. Politicians in Washington, London and Canberra took it for granted that it was their job to prevent the adulteration and degeneration of the white race, by, for example, restricting immigration from China or even Italy to ‘Aryan’ countries such as the USA and Australia.
Humanist Religions – Religions that Worship Humanity

Liberal humanism	Socialist humanism	Evolutionary humanism
Homo sapiens has a unique and sacred nature that is fundamentally different from the nature of all other beings and phenomena. The supreme good is the good of humanity.
‘Humanity’ is individualistic and resides within each individual Homo sapiens.	‘Humanity’ is collective and resides within the species Homo sapiens as a whole.	‘Humanity’ is a mutable species. Humans might degenerate into subhumans or evolve into superhumans.
The supreme commandment is to protect the inner core and freedom of each individual Homo sapiens.	The supreme commandment is to protect equality within the species Homo sapiens.	The supreme commandment is to protect humankind from degenerating into subhumans, and to encourage its evolution into superhumans.
These positions did not change simply because new scientific research was published. Sociological and political developments were far more powerful engines of change. In this sense, Hitler dug not just his own grave but that of racism in general. When he launched World War Two, he compelled his enemies to make clear distinctions between ‘us’ and ‘them’. Afterwards, precisely because Nazi ideology was so racist, racism became discredited in the West. But the change took time. White supremacy remained a mainstream ideology in American politics at least until the 1960s. The White Australia policy which restricted immigration of non-white people to Australia remained in force until 1973. Aboriginal Australians did not receive equal political rights until the 1960s, and most were prevented from voting in elections because they were deemed unfit to function as citizens.

30. A Nazi propaganda poster showing on the right a ‘racially pure Aryan’ and on the left a ‘cross-breed’. Nazi admiration for the human body is evident, as is their fear that the lower races might pollute humanity and cause its degeneration.

The Nazis did not loathe humanity. They fought liberal humanism, human rights and Communism precisely because they admired humanity and believed in the great potential of the human species. But following the logic of Darwinian evolution, they argued that natural selection must be allowed to weed out unfit individuals and leave only the fittest to survive and reproduce. By succouring the weak, liberalism and Communism not only allowed unfit individuals to survive, they actually gave them the opportunity to reproduce, thereby undermining natural selection. In such a world, the fittest humans would inevitably drown in a sea of unfit degenerates. Humankind would become less and less fit with each passing generation – which could lead to its extinction.

31. A Nazi cartoon of 1933. Hitler is presented as a sculptor who creates the superman. A bespectacled liberal intellectual is appalled by the violence needed to create the superman. (Note also the erotic glorification of the human body.)

A 1942 German biology textbook explains in the chapter ‘The Laws of Nature and Mankind’ that the supreme law of nature is that all beings are locked in a remorseless struggle for survival. After describing how plants struggle for territory, how beetles struggle to find mates and so forth, the textbook concludes that:
The battle for existence is hard and unforgiving, but is the only way to maintain life. This struggle eliminates everything that is unfit for life, and selects everything that is able to survive … These natural laws are incontrovertible; living creatures demonstrate them by their very survival. They are unforgiving. Those who resist them will be wiped out. Biology not only tells us about animals and plants, but also shows us the laws we must follow in our lives, and steels our wills to live and fight according to these laws. The meaning of life is struggle. Woe to him who sins against these laws.
Then follows a quotation from Mein Kampf: ‘The person who attempts to fight the iron logic of nature thereby fights the principles he must thank for his life as a human being. To fight against nature is to bring about one’s own destruction.’3
At the dawn of the third millennium, the future of evolutionary humanism is unclear. For sixty years after the end of the war against Hitler it was taboo to link humanism with evolution and to advocate using biological methods to upgrade’ Homo sapiens. But today such projects are back in vogue. No one speaks about exterminating lower races or inferior people, but many contemplate using our increasing knowledge of human biology to create superhumans.
At the same time, a huge gulf is opening between the tenets of liberal humanism and the latest findings of the life sciences, a gulf we cannot ignore much longer. Our liberal political and judicial systems are founded on the belief that every individual has a sacred inner nature, indivisible and immutable, which gives meaning to the world, and which is the source of all ethical and political authority. This is a reincarnation of the traditional Christian belief in a free and eternal soul that resides within each individual. Yet over the last 200 years, the life sciences have thoroughly undermined this belief. Scientists studying the inner workings of the human organism have found no soul there. They increasingly argue that human behaviour is determined by hormones, genes and synapses, rather than by free will – the same forces that determine the behaviour of chimpanzees, wolves, and ants. Our judicial and political systems largely try to sweep such inconvenient discoveries under the carpet. But in all frankness, how long can we maintain the wall separating the department of biology from the departments of law and political science?13

The Secret of Success

COMMERCE, EMPIRES AND UNIVERSAL religions eventually brought virtually every Sapiens on every continent into the global world we live in today. Not that this process of expansion and unification was linear or without interruptions. Looking at the bigger picture, though, the transition from many small cultures to a few large cultures and finally to a single global society was probably an inevitable result of the dynamics of human history.
But saying that a global society is inevitable is not the same as saying that the end result had to be the particular kind of global society we now have. We can certainly imagine other outcomes. Why is English so widespread today, and not Danish? Why are there about 2 billion Christians and 1.25 billion Muslims, but only 150,000 Zoroastrians and no Manichaeans? If we could go back in time to 10,000 years ago and set the process going again, time after time, would we always see the rise of monotheism and the decline of dualism?
We can’t do such an experiment, so we don’t really know. But an examination of two crucial characteristics of history can provide us with some clues.
1. The Hindsight Fallacy

Every point in history is a crossroads. A single travelled road leads from the past to the present, but myriad paths fork off into the future. Some of those paths are wider, smoother and better marked, and are thus more likely to be taken, but sometimes history – or the people who make history – takes unexpected turns.
At the beginning of the fourth century AD, the Roman Empire faced a wide horizon of religious possibilities. It could have stuck to its traditional and variegated polytheism. But its emperor, Constantine, looking back on a fractious century of civil war, seems to have thought that a single religion with a clear doctrine could help unify his ethnically diverse realm. He could have chosen any of a number of contemporary cults to be his national faith – Manichaeism, Mithraism, the cults of Isis or Cybele, Zoroastrianism, Judaism and even Buddhism were all available options. Why did he opt for Jesus? Was there something in Christian theology that attracted him personally, or perhaps an aspect of the faith that made him think it would be easier to use for his purposes? Did he have a religious experience, or did some of his advisers suggest that the Christians were quickly gaining adherents and that it would be best to jump on that wagon? Historians can speculate, but not provide any definitive answer. They can describe how Christianity took over the Roman Empire, but they cannot explain why this particular possibility was realised.
What is the difference between describing ‘how’ and explaining ‘why’? To describe ‘how’ means to reconstruct the series of specific events that led from one point to another. To explain ‘why means to find causal connections that account for the occurrence of this particular series of events to the exclusion of all others.
Some scholars do indeed provide deterministic explanations of events such as the rise of Christianity. They attempt to reduce human history to the workings of biological, ecological or economic forces. They argue that there was something about the geography, genetics or economy of the Roman Mediterranean that made the rise of a monotheist religion inevitable. Yet most historians tend to be sceptical of such deterministic theories. This is one of the distinguishing marks of history as an academic discipline – the better you know a particular historical period, the harder it becomes to explain why things happened one way and not another. Those who have only a superficial knowledge of a certain period tend to focus only on the possibility that was eventually realised. They offer a just-so story to explain with hindsight why that outcome was inevitable. Those more deeply informed about the period are much more cognisant of the roads not taken.
In fact, the people who knew the period best – those alive at the time – were the most clueless of all. For the average Roman in Constantine’s time, the future was a fog. It is an iron rule of history that what looks inevitable in hindsight was far from obvious at the time. Today is no different. Are we out of the global economic crisis, or is the worst still to come? Will China continue growing until it becomes the leading superpower? Will the United States lose its hegemony? Is the upsurge of monotheistic fundamentalism the wave of the future or a local whirlpool of little long-term significance? Are we heading towards ecological disaster or technological paradise? There are good arguments to be made for all of these outcomes, but no way of knowing for sure. In a few decades, people will look back and think that the answers to all of these questions were obvious.
It is particularly important to stress that possibilities which seem very unlikely to contemporaries often get realised. When Constantine assumed the throne in 306, Christianity was little more than an esoteric Eastern sect. If you were to suggest then that it was about to become the Roman state religion, you’d have been laughed out of the room just as you would be today if you were to suggest that by the year 2050 Hare Krishna would be the state religion of the USA. In October 1913, the Bolsheviks were a small radical Russian faction. No reasonable person would have predicted that within a mere four years they would take over the country. In AD 600, the notion that a band of desert-dwelling Arabs would soon conquer an expanse stretching from the Atlantic Ocean to India was even more preposterous. Indeed, had the Byzantine army been able to repel the initial onslaught, Islam would probably have remained an obscure cult of which only a handful of cognoscenti were aware. Scholars would then have a very easy job explaining why a faith based on a revelation to a middle-aged Meccan merchant could never have caught on.
Not that everything is possible. Geographical, biological and economic forces create constraints. Yet these constraints leave ample room for surprising developments, which do not seem bound by any deterministic laws.
This conclusion disappoints many people, who prefer history to be deterministic. Determinism is appealing because it implies that our world and our beliefs are a natural and inevitable product of history. It is natural and inevitable that we live in nation states, organise our economy along capitalist principles, and fervently believe in human rights. To acknowledge that history is not deterministic is to acknowledge that it is just a coincidence that most people today believe in nationalism, capitalism and human rights.
History cannot be explained deterministically and it cannot be predicted because it is chaotic. So many forces are at work and their interactions are so complex that extremely small variations in the strength of the forces and the way they interact produce huge differences in outcomes. Not only that, but history is what is called a ‘level two’ chaotic system. Chaotic systems come in two shapes. Level one chaos is chaos that does not react to predictions about it. The weather, for example, is a level one chaotic system. Though it is influenced by myriad factors, we can build computer models that take more and more of them into consideration, and produce better and better weather forecasts.
Level two chaos is chaos that reacts to predictions about it, and therefore can never be predicted accurately. Markets, for example, are a level two chaotic system. What will happen if we develop a computer program that forecasts with 100 per cent accuracy the price of oil tomorrow? The price of oil will immediately react to the forecast, which would consequently fail to materialise. If the current price of oil is $90 a barrel, and the infallible computer program predicts that tomorrow it will be $100, traders will rush to buy oil so that they can profit from the predicted price rise. As a result, the price will shoot up to $100 a barrel today rather than tomorrow. Then what will happen tomorrow? Nobody knows.
Politics, too, is a second-order chaotic system. Many people criticise Sovietologists for failing to predict the 1989 revolutions and castigate Middle East experts for not anticipating the Arab Spring revolutions of 2011. This is unfair. Revolutions are, by definition, unpredictable. A predictable revolution never erupts.
Why not? Imagine that it’s 2010 and some genius political scientists in cahoots with a computer wizard have developed an infallible algorithm that, incorporated into an attractive interface, can be marketed as a revolution predictor. They offer their services to President Hosni Mubarak of Egypt and, in return for a generous down payment, tell Mubarak that according to their forecasts a revolution would certainly break out in Egypt during the course of the following year. How would Mubarak react? Most likely, he would immediately lower taxes, distribute billions of dollars in handouts to the citizenry – and also beef up his secret police force, just in case. The pre-emptive measures work. The year comes and goes and, surprise, there is no revolution. Mubarak demands his money back. ‘Your algorithm is worthless!’ he shouts at the scientists. ‘In the end I could have built another palace instead of giving all that money away!’ ‘But the reason the revolution didn’t happen is because we predicted it,’ the scientists say in their defence. ‘Prophets who predict things that don’t happen?’ Mubarak remarks as he motions his guards to grab them. ‘I could have picked up a dozen of those for next to nothing in the Cairo marketplace.’
So why study history? Unlike physics or economics, history is not a means for making accurate predictions. We study history not to know the future but to widen our horizons, to understand that our present situation is neither natural nor inevitable, and that we consequently have many more possibilities before us than we imagine. For example, studying how Europeans came to dominate Africans enables us to realise that there is nothing natural or inevitable about the racial hierarchy, and that the world might well be arranged differently.
2. Blind Clio

We cannot explain the choices that history makes, but we can say something very important about them: history’s choices are not made for the benefit of humans. There is absolutely no proof that human well-being inevitably improves as history rolls along. There is no proof that cultures that are beneficial to humans must inexorably succeed and spread, while less beneficial cultures disappear. There is no proof that Christianity was a better choice than Manichaeism, or that the Arab Empire was more beneficial than that of the Sassanid Persians.
There is no proof that history is working for the benefit of humans because we lack an objective scale on which to measure such benefit. Different cultures define the good differently, and we have no objective yardstick by which to judge between them. The victors, of course, always believe that their definition is correct. But why should we believe the victors? Christians believe that the victory of Christianity over Manichaeism was beneficial to humankind, but if we do not accept the Christian world view then there is no reason to agree with them. Muslims believe that the fall of the Sassanid Empire into Muslim hands was beneficial to humankind. But these benefits are evident only if we accept the Muslim world view. It may well be that we’d all be better off if Christianity and Islam had been forgotten or defeated.
Ever more scholars see cultures as a kind of mental infection or parasite, with humans as its unwitting host. Organic parasites, such as viruses, live inside the body of their hosts. They multiply and spread from one host to the other, feeding off their hosts, weakening them, and sometimes even killing them. As long as the hosts live long enough to pass along the parasite, it cares little about the condition of its host. In just this fashion, cultural ideas live inside the minds of humans. They multiply and spread from one host to another, occasionally weakening the hosts and sometimes even killing them. A cultural idea – such as belief in Christian heaven above the clouds or Communist paradise here on earth – can compel a human to dedicate his or her life to spreading that idea, even at the price of death. The human dies, but the idea spreads. According to this approach, cultures are not conspiracies concocted by some people in order to take advantage of others (as Marxists tend to think). Rather, cultures are mental parasites that emerge accidentally, and thereafter take advantage of all people infected by them.
This approach is sometimes called memetics. It assumes that, just as organic evolution is based on the replication of organic information units called ‘genes’, so cultural evolution is based on the replication of cultural information units called ‘memes’.1 Successful cultures are those that excel in reproducing their memes, irrespective of the costs and benefits to their human hosts.
Most scholars in the humanities disdain memetics, seeing it as an amateurish attempt to explain cultural processes with crude biological analogies. But many of these same scholars adhere to memetics’ twin sister – postmodernism. Postmodernist thinkers speak about discourses rather than memes as the building blocks of culture. Yet they too see cultures as propagating themselves with little regard for the benefit of humankind. For example, postmodernist thinkers describe nationalism as a deadly plague that spread throughout the world in the nineteenth and twentieth centuries, causing wars, oppression, hate and genocide. The moment people in one country were infected with it, those in neighbouring countries were also likely to catch the virus. The nationalist virus presented itself as being beneficial for humans, yet it has been beneficial mainly to itself.
Similar arguments are common in the social sciences, under the aegis of game theory. Game theory explains how in multi-player systems, views and behaviour patterns that harm all players nevertheless manage to take root and spread. Arms races are a famous example. Many arms races bankrupt all those who take part in them, without really changing the military balance of power. When Pakistan buys advanced aeroplanes, India responds in kind. When India develops nuclear bombs, Pakistan follows suit. When Pakistan enlarges its navy, India counters. At the end of the process, the balance of power may remain much as it was, but meanwhile billions of dollars that could have been invested in education or health are spent on weapons. Yet the arms race dynamic is hard to resist. ‘Arms racing’ is a pattern of behaviour that spreads itself like a virus from one country to another, harming everyone, but benefiting itself, under the evolutionary criteria of survival and reproduction. (Keep in mind that an arms race, like a gene, has no awareness – it does not consciously seek to survive and reproduce. Its spread is the unintended result of a powerful dynamic.)
No matter what you call it – game theory, postmodernism or memetics – the dynamics of history are not directed towards enhancing human well-being. There is no basis for thinking that the most successful cultures in history are necessarily the best ones for Homo sapiens. Like evolution, history disregards the happiness of individual organisms. And individual humans, for their part, are usually far too ignorant and weak to influence the course of history to their own advantage.
History proceeds from one junction to the next, choosing for some mysterious reason to follow first this path, then another. Around AD 1500, history made its most momentous choice, changing not only the fate of humankind, but arguably the fate of all life on earth. We call it the Scientific Revolution. It began in western Europe, a large peninsula on the western tip of Afro-Asia, which up till then played no important role in history. Why did the Scientific Revolution begin there of all places, and not in China or India? Why did it begin at the midpoint of the second millennium AD rather than two centuries before or three centuries later? We don’t know. Scholars have proposed dozens of theories, but none of them is particularly convincing.
History has a very wide horizon of possibilities, and many possibilities are never realised. It is conceivable to imagine history going on for generations upon generations while bypassing the Scientific Revolution, just as it is conceivable to imagine history without Christianity, without a Roman Empire, and without gold coins.14

The Discovery of Ignorance

WERE, SAY, A SPANISH PEASANT TO HAVE fallen asleep in AD 1000 and woken up 500 years later, to the din of Columbus’ sailors boarding the Niña, Pinta and Santa Maria, the world would have seemed to him quite familiar. Despite many changes in technology, manners and political boundaries, this medieval Rip Van Winkle would have felt at home. But had one of Columbus’ sailors fallen into a similar slumber and woken up to the ringtone of a twenty-first-century iPhone, he would have found himself in a world strange beyond comprehension. ‘Is this heaven?’ he might well have asked himself. ‘Or perhaps – hell?’
The last 500 years have witnessed a phenomenal and unprecedented growth in human power. In the year 1500, there were about 500 million Homo sapiens in the entire world. Today, there are 7 billion.1 The total value of goods and services produced by humankind in the year 1500 is estimated at $250 billion, in today’s dollars.2 Nowadays the value of a year of human production is close to $60 trillion.3 In 1500, humanity consumed about 13 trillion calories of energy per day. Today, we consume 1,500 trillion calories a day.4 (Take a second look at those figures – human population has increased fourteen-fold, production 240-fold, and energy consumption 115-fold.)
Suppose a single modern battleship got transported back to Columbus’ time. In a matter of seconds it could make driftwood out of the Niña, Pinta and Santa Maria and then sink the navies of every great world power of the time without sustaining a scratch. Five modern freighters could have taken onboard all the cargo borne by the whole world’s merchant fleets.5 A modern computer could easily store every word and number in all the codex books and scrolls in every single medieval library with room to spare. Any large bank today holds more money than all the world’s premodern kingdoms put together.6
In 1500, few cities had more than 100,000 inhabitants. Most buildings were constructed of mud, wood and straw; a three-storey building was a skyscraper. The streets were rutted dirt tracks, dusty in summer and muddy in winter, plied by pedestrians, horses, goats, chickens and a few carts. The most common urban noises were human and animal voices, along with the occasional hammer and saw. At sunset, the cityscape went black, with only an occasional candle or torch flickering in the gloom. If an inhabitant of such a city could see modern Tokyo, New York or Mumbai, what would she think?
Prior to the sixteenth century, no human had circumnavigated the earth. This changed in 1522, when Magellan’s expedition returned to Spain after a journey of 72,000 kilometres. It took three years and cost the lives of almost all the crew members, Magellan included. In 1873, Jules Verne could imagine that Phileas Fogg, a wealthy British adventurer, might just be able to make it around the world in eighty days. Today anyone with a middle-class income can safely and easily circumnavigate the globe in just forty-eight hours.
In 1500, humans were confined to the earth’s surface. They could build towers and climb mountains, but the sky was reserved for birds, angels and deities. On 20 July 1969 humans landed on the moon. This was not merely a historical achievement, but an evolutionary and even cosmic feat. During the previous 4 billion years of evolution, no organism managed even to leave the earth’s atmosphere, and certainly none left a foot or tentacle print on the moon.
For most of history, humans knew nothing about 99.99 per cent of the organisms on the planet – namely, the microorganisms. This was not because they were of no concern to us. Each of us bears billions of one-celled creatures within us, and not just as free-riders. They are our best friends, and deadliest enemies. Some of them digest our food and clean our guts, while others cause illnesses and epidemics. Yet it was only in 1674 that a human eye first saw a microorganism, when Anton van Leeuwenhoek took a peek through his home-made microscope and was startled to see an entire world of tiny creatures milling about in a drop of water. During the subsequent 300 years, humans have made the acquaintance of a huge number of microscopic species. We’ve managed to defeat most of the deadliest contagious diseases they cause, and have harnessed microorganisms in the service of medicine and industry. Today we engineer bacteria to produce medications, manufacture biofuel and kill parasites.
But the single most remarkable and defining moment of the past 500 years came at 05:29:45 on 16 July 1945. At that precise second, American scientists detonated the first atomic bomb at Alamogordo, New Mexico. From that point onward, humankind had the capability not only to change the course of history, but to end it.
The historical process that led to Alamogordo and to the moon is known as the Scientific Revolution. During this revolution humankind has obtained enormous new powers by investing resources in scientific research. It is a revolution because, until about AD 1500, humans the world over doubted their ability to obtain new medical, military and economic powers. While government and wealthy patrons allocated funds to education and scholarship, the aim was, in general, to preserve existing capabilities rather than acquire new ones. The typical premodern ruler gave money to priests, philosophers and poets in the hope that they would legitimise his rule and maintain the social order. He did not expect them to discover new medications, invent new weapons or stimulate economic growth.
During the last five centuries, humans increasingly came to believe that they could increase their capabilities by investing in scientific research. This wasn’t just blind faith – it was repeatedly proven empirically. The more proofs there were, the more resources wealthy people and governments were willing to put into science. We would never have been able to walk on the moon, engineer microorganisms and split the atom without such investments. The US government, for example, has in recent decades allocated billions of dollars to the study of nuclear physics. The knowledge produced by this research has made possible the construction of nuclear power stations, which provide cheap electricity for American industries, which pay taxes to the US government, which uses some of these taxes to finance further research in nuclear physics.

The Scientific Revolution’s feedback loop. Science needs more than just research to make progress. It depends on the mutual reinforcement of science, politics and economics. Political and economic institutions provide the resources without which scientific research is almost impossible. In return, scientific research provides new powers that are used, among other things, to obtain new resources, some of which are reinvested in research.

Why did modern humans develop a growing belief in their ability to obtain new powers through research? What forged the bond between science, politics and economics? This chapter looks at the unique nature of modern science in order to provide part of the answer. The next two chapters examine the formation of the alliance between science, the European empires and the economics of capitalism.
Ignoramus

Humans have sought to understand the universe at least since the Cognitive Revolution. Our ancestors put a great deal of time and effort into trying to discover the rules that govern the natural world. But modern science differs from all previous traditions of knowledge in three critical ways:
a. The willingness to admit ignorance. Modern science is based on the Latin injunction ignoramus – ‘we do not know’. It assumes that we don’t know everything. Even more critically, it accepts that the things that we think we know could be proven wrong as we gain more knowledge. No concept, idea or theory is sacred and beyond challenge.
b. The centrality of observation and mathematics. Having admitted ignorance, modern science aims to obtain new knowledge. It does so by gathering observations and then using mathematical tools to connect these observations into comprehensive theories.
c. The acquisition of new powers. Modern science is not content with creating theories. It uses these theories in order to acquire new powers, and in particular to develop new technologies.
The Scientific Revolution has not been a revolution of knowledge. It has been above all a revolution of ignorance. The great discovery that launched the Scientific Revolution was the discovery that humans do not know the answers to their most important questions.
Premodern traditions of knowledge such as Islam, Christianity, Buddhism and Confucianism asserted that everything that is important to know about the world was already known. The great gods, or the one almighty God, or the wise people of the past possessed all-encompassing wisdom, which they revealed to us in scriptures and oral traditions. Ordinary mortals gained knowledge by delving into these ancient texts and traditions and understanding them properly. It was inconceivable that the Bible, the Qur’an or the Vedas were missing out on a crucial secret of the universe – a secret that might yet be discovered by flesh-and-blood creatures.
Ancient traditions of knowledge admitted only two kinds of ignorance. First, an individual might be ignorant of something important. To obtain the necessary knowledge, all he needed to do was ask somebody wiser. There was no need to discover something that nobody yet knew. For example, if a peasant in some thirteenth-century Yorkshire village wanted to know how the human race originated, he assumed that Christian tradition held the definitive answer. All he had to do was ask the local priest.
Second, an entire tradition might be ignorant of unimportant things. By definition, whatever the great gods or the wise people of the past did not bother to tell us was unimportant. For example, if our Yorkshire peasant wanted to know how spiders weave their webs, it was pointless to ask the priest, because there was no answer to this question in any of the Christian Scriptures. That did not mean, however, that Christianity was deficient. Rather, it meant that understanding how spiders weave their webs was unimportant. After all, God knew perfectly well how spiders do it. If this were a vital piece of information, necessary for human prosperity and salvation, God would have included a comprehensive explanation in the Bible.
Christianity did not forbid people to study spiders. But spider scholars – if there were any in medieval Europe – had to accept their peripheral role in society and the irrelevance of their findings to the eternal truths of Christianity. No matter what a scholar might discover about spiders or butterflies or Galapagos finches, that knowledge was little more than trivia, with no bearing on the fundamental truths of society, politics and economics.
In fact, things were never quite that simple. In every age, even the most pious and conservative, there were people who argued that there were important things of which their entire tradition was ignorant. Yet such people were usually marginalised or persecuted – or else they founded a new tradition and began arguing that they knew everything there is to know. For example, the prophet Muhammad began his religious career by condemning his fellow Arabs for living in ignorance of the divine truth. Yet Muhammad himself very quickly began to argue that he knew the full truth, and his followers began calling him ‘The Seal of the Prophets’. Henceforth, there was no need of revelations beyond those given to Muhammad.
Modern-day science is a unique tradition of knowledge, inasmuch as it openly admits collective ignorance regarding the most important questions. Darwin never argued that he was ‘The Seal of the Biologists’, and that he had solved the riddle of life once and for all. After centuries of extensive scientific research, biologists admit that they still don’t have any good explanation for how brains produce consciousness. Physicists admit that they don’t know what caused the Big Bang, or how to reconcile quantum mechanics with the theory of general relativity.
In other cases, competing scientific theories are vociferously debated on the basis of constantly emerging new evidence. A prime example is the debates about how best to run the economy. Though individual economists may claim that their method is the best, orthodoxy changes with every financial crisis and stock-exchange bubble, and it is generally accepted that the final word on economics is yet to be said.
In still other cases, particular theories are supported so consistently by the available evidence, that all alternatives have long since fallen by the wayside. Such theories are accepted as true – yet everyone agrees that were new evidence to emerge that contradicts the theory, it would have to be revised or discarded. Good examples of these are the plate tectonics theory and the theory of evolution.
The willingness to admit ignorance has made modern science more dynamic, supple and inquisitive than any previous tradition of knowledge. This has hugely expanded our capacity to understand how the world works and our ability to invent new technologies. But it presents us with a serious problem that most of our ancestors did not have to cope with. Our current assumption that we do not know everything, and that even the knowledge we possess is tentative, extends to the shared myths that enable millions of strangers to cooperate effectively. If the evidence shows that many of those myths are doubtful, how can we hold society together? How can our communities, countries and international system function?
All modern attempts to stabilise the sociopolitical order have had no choice but to rely on either of two unscientific methods:
a. Take a scientific theory, and in opposition to common scientific practices, declare that it is a final and absolute truth. This was the method used by Nazis (who claimed that their racial policies were the corollaries of biological facts) and Communists (who claimed that Marx and Lenin had divined absolute economic truths that could never be refuted).
b. Leave science out of it and live in accordance with a non-scientific absolute truth. This has been the strategy of liberal humanism, which is built on a dogmatic belief in the unique worth and rights of human beings – a doctrine which has embarrassingly little in common with the scientific study of Homo sapiens.
But that shouldn’t surprise us. Even science itself has to rely on religious and ideological beliefs to justify and finance its research.
Modern culture has nevertheless been willing to embrace ignorance to a much greater degree than has any previous culture. One of the things that has made it possible for modern social orders to hold together is the spread of an almost religious belief in technology and in the methods of scientific research, which have replaced to some extent the belief in absolute truths.
The Scientific Dogma

Modern science has no dogma. Yet it has a common core of research methods, which are all based on collecting empirical observations – those we can observe with at least one of our senses – and putting them together with the help of mathematical tools.
People throughout history collected empirical observations, but the importance of these observations was usually limited. Why waste precious resources obtaining new observations when we already have all the answers we need? But as modern people came to admit that they did not know the answers to some very important questions, they found it necessary to look for completely new knowledge. Consequently, the dominant modern research method takes for granted the insufficiency of old knowledge. Instead of studying old traditions, emphasis is now placed on new observations and experiments. When present observation collides with past tradition, we give precedence to the observation. Of course, physicists analysing the spectra of distant galaxies, archaeologists analysing the finds from a Bronze Age city, and political scientists studying the emergence of capitalism do not disregard tradition. They start by studying what the wise people of the past have said and written. But from their first year in college, aspiring physicists, archaeologists and political scientists are taught that it is their mission to go beyond what Einstein, Heinrich Schliemann and Max Weber ever knew.
Mere observations, however, are not knowledge. In order to understand the universe, we need to connect observations into comprehensive theories. Earlier traditions usually formulated their theories in terms of stories. Modern science uses mathematics.
There are very few equations, graphs and calculations in the Bible, the Qur’an, the Vedas or the Confucian classics. When traditional mythologies and scriptures laid down general laws, these were presented in narrative rather than mathematical form. Thus a fundamental principle of Manichaean religion asserted that the world is a battleground between good and evil. An evil force created matter, while a good force created spirit. Humans are caught between these two forces, and should choose good over evil. Yet the prophet Mani made no attempt to offer a mathematical formula that could be used to predict human choices by quantifying the respective strength of these two forces. He never calculated that ‘the force acting on a man is equal to the acceleration of his spirit divided by the mass of his body’.
This is exactly what scientists seek to accomplish. In 1687, Isaac Newton published The Mathematical Principles of Natural Philosophy, arguably the most important book in modern history. Newton presented a general theory of movement and change. The greatness of Newton’s theory was its ability to explain and predict the movements of all bodies in the universe, from falling apples to shooting stars, using three very simple mathematical laws:

Henceforth, anyone who wished to understand and predict the movement of a cannonball or a planet simply had to make measurements of the object’s mass, direction and acceleration, and the forces acting on it. By inserting these numbers into Newton’s equations, the future position of the object could be predicted. It worked like magic. Only around the end of the nineteenth century did scientists come across a few observations that did not fit well with Newton’s laws, and these led to the next revolutions in physics – the theory of relativity and quantum mechanics.
Newton showed that the book of nature is written in the language of mathematics. Some chapters (for example) boil down to a clear-cut equation; but scholars who attempted to reduce biology, economics and psychology to neat Newtonian equations have discovered that these fields have a level of complexity that makes such an aspiration futile. This did not mean, however, that they gave up on mathematics. A new branch of mathematics was developed over the last 200 years to deal with the more complex aspects of reality: statistics.
In 1744, two Presbyterian clergymen in Scotland, Alexander Webster and Robert Wallace, decided to set up a life-insurance fund that would provide pensions for the widows and orphans of dead clergymen. They proposed that each of their church’s ministers would pay a small portion of his income into the fund, which would invest the money. If a minister died, his widow would receive dividends on the fund’s profits. This would allow her to live comfortably for the rest of her life. But to determine how much the ministers had to pay in so that the fund would have enough money to live up to its obligations, Webster and Wallace had to be able to predict how many ministers would die each year, how many widows and orphans they would leave behind, and by how many years the widows would outlive their husbands.
Take note of what the two churchmen did not do. They did not pray to God to reveal the answer. Nor did they search for an answer in the Holy Scriptures or among the works of ancient theologians. Nor did they enter into an abstract philosophical disputation. Being Scots, they were practical types. So they contacted a professor of mathematics from the University of Edinburgh, Colin Maclaurin. The three of them collected data on the ages at which people died and used these to calculate how many ministers were likely to pass away in any given year.
Their work was founded on several recent breakthroughs in the fields of statistics and probability. One of these was Jacob Bernoulli’s Law of Large Numbers. Bernoulli had codified the principle that while it might be difficult to predict with certainty a single event, such as the death of a particular person, it was possible to predict with great accuracy the average outcome of many similar events. That is, while Maclaurin could not use maths to predict whether Webster and Wallace would die next year, he could, given enough data, tell Webster and Wallace how many Presbyterian ministers in Scotland would almost certainly die next year. Fortunately, they had ready-made data that they could use. Actuary tables published fifty years previously by Edmond Halley proved particularly useful. Halley had analysed records of 1,238 births and 1,174 deaths that he obtained from the city of Breslau, Germany. Halley’s tables made it possible to see that, for example, a twenty-year-old person has a 1:100 chance of dying in a given year, but a fifty-year-old person has a 1:39 chance.
Processing these numbers, Webster and Wallace concluded that, on average, there would be 930 living Scottish Presbyterian ministers at any given moment, and an average of twenty-seven ministers would die each year, eighteen of whom would be survived by widows. Five of those who did not leave widows would leave orphaned children, and two of those survived by widows would also be outlived by children from previous marriages who had not yet reached the age of sixteen. They further computed how much time was likely to go by before the widows’ death or remarriage (in both these eventualities, payment of the pension would cease). These figures enabled Webster and Wallace to determine how much money the ministers who joined their fund had to pay in order to provide for their loved ones. By contributing £2 12s. 2d. a year, a minister could guarantee that his widowed wife would receive at least £10 a year – a hefty sum in those days. If he thought that was not enough he could choose to pay in more, up to a level of £6 11s. 3d. a year – which would guarantee his widow the even more handsome sum of £25 a year.
According to their calculations, by the year 1765 the Fund for a Provision for the Widows and Children of the Ministers of the Church of Scotland would have capital totalling £58,348. Their calculations proved amazingly accurate. When that year arrived, the fund’s capital stood at £58,347 – just £1 less than the prediction! This was even better than the prophecies of Habakkuk, Jeremiah or St John. Today, Webster and Wallace’s fund, known simply as Scottish Widows, is one of the largest pension and insurance companies in the world. With assets worth £100 billion, it insures not only Scottish widows, but anyone willing to buy its policies.7
Probability calculations such as those used by the two Scottish ministers became the foundation not merely of actuarial science, which is central to the pension and insurance business, but also of the science of demography (founded by another clergyman, the Anglican Robert Malthus). Demography in its turn was the cornerstone on which Charles Darwin (who almost became an Anglican pastor) built his theory of evolution. While there are no equations that predict what kind of organism will evolve under a specific set of conditions, geneticists use probability calculations to compute the likelihood that a particular mutation will spread in a given population. Similar probabilistic models have become central to economics, sociology, psychology, political science and the other social and natural sciences. Even physics eventually supplemented Newton’s classical equations with the probability clouds of quantum mechanics.
We need merely look at the history of education to realise how far this process has taken us. Throughout most of history, mathematics was an esoteric field that even educated people rarely studied seriously. In medieval Europe, logic, grammar and rhetoric formed the educational core, while the teaching of mathematics seldom went beyond simple arithmetic and geometry. Nobody studied statistics. The undisputed monarch of all sciences was theology.
Today few students study rhetoric; logic is restricted to philosophy departments, and theology to seminaries. But more and more students are motivated – or forced – to study mathematics. There is an irresistible drift towards the exact sciences – defined as ‘exact’ by their use of mathematical tools. Even fields of study that were traditionally part of the humanities, such as the study of human language (linguistics) and the human psyche (psychology), rely increasingly on mathematics and seek to present themselves as exact sciences. Statistics courses are now part of the basic requirements not just in physics and biology, but also in psychology, sociology, economics and political science.
In the course catalogue of the psychology department at my own university, the first required course in the curriculum is ‘Introduction to Statistics and Methodology in Psychological Research’. Second-year psychology students must take ‘Statistical Methods in Psychological Research’. Confucius, Buddha, Jesus and Muhammad would have been bewildered if you told them that in order to understand the human mind and cure its illnesses you must first study statistics.
Knowledge is Power

Most people have a hard time digesting modern science because its mathematical language is difficult for our minds to grasp, and its findings often contradict common sense. Out of the 7 billion people in the world, how many really understand quantum mechanics, cell biology or macroeconomics? Science nevertheless enjoys immense prestige because of the new powers it gives us. Presidents and generals may not understand nuclear physics, but they have a good grasp of what nuclear bombs can do.
In 1620 Francis Bacon published a scientific manifesto tided The New Instrument. In it he argued that ‘knowledge is power’. The real test of ‘knowledge’ is not whether it is true, but whether it empowers us. Scientists usually assume that no theory is 100 per cent correct. Consequently, truth is a poor test for knowledge. The real test is utility. A theory that enables us to do new things constitutes knowledge.
Over the centuries, science has offered us many new tools. Some are mental tools, such as those used to predict death rates and economic growth. Even more important are technological tools. The connection forged between science and technology is so strong that today people tend to confuse the two. We often think that it is impossible to develop new technologies without scientific research, and that there is little point in research if it does not result in new technologies.
In fact, the relationship between science and technology is a very recent phenomenon. Prior to 1500, science and technology were totally separate fields. When Bacon connected the two in the early seventeenth century, it was a revolutionary idea. During the seventeenth and eighteenth centuries this relationship tightened, but the knot was tied only in the nineteenth century. Even in 1800, most rulers who wanted a strong army, and most business magnates who wanted a successful business, did not bother to finance research in physics, biology or economics.
I don’t mean to claim that there is no exception to this rule. A good historian can find precedent for everything. But an even better historian knows when these precedents are but curiosities that cloud the big picture. Generally speaking, most premodern rulers and business people did not finance research about the nature of the universe in order to develop new technologies, and most thinkers did not try to translate their findings into technological gadgets. Rulers financed educational institutions whose mandate was to spread traditional knowledge for the purpose of buttressing the existing order.
Here and there people did develop new technologies, but these were usually created by uneducated craftsmen using trial and error, not by scholars pursuing systematic scientific research. Cart manufacturers built the same carts from the same materials year in year out. They did not set aside a percentage of their annual profits in order to research and develop new cart models. Cart design occasionally improved, but it was usually thanks to the ingenuity of some local carpenter who never set foot in a university and did not even know how to read.
This was true of the public as well as the private sector. Whereas modern states call in their scientists to provide solutions in almost every area of national policy, from energy to health to waste disposal, ancient kingdoms seldom did so. The contrast between then and now is most pronounced in weaponry. When outgoing President Dwight Eisenhower warned in 1961 of the growing power of the military-industrial complex, he left out a part of the equation. He should have alerted his country to the military-industrial-scientific complex, because today’s wars are scientific productions. The world’s military forces initiate, fund and steer a large part of humanity’s scientific research and technological development.
When World War One bogged down into interminable trench warfare, both sides called in the scientists to break the deadlock and save the nation. The men in white answered the call, and out of the laboratories rolled a constant stream of new wonder-weapons: combat aircraft, poison gas, tanks, submarines and ever more efficient machine guns, artillery pieces, rifles and bombs.

33. German V-2 rocket ready to launch. It didn’t defeat the Allies, but it kept the Germans hoping for a technological miracle until the very last days of the war.

Science played an even larger role in World War Two. By late 1944 Germany was losing the war and defeat was imminent. A year earlier, the Germans’ allies, the Italians, had toppled Mussolini and surrendered to the Allies. But Germany kept fighting on, even though the British, American and Soviet armies were closing in. One reason German soldiers and civilians thought not all was lost was that they believed German scientists were about to turn the tide with so-called miracle weapons such as the V-2 rocket and jet-powered aircraft.
While the Germans were working on rockets and jets, the American Manhattan Project successfully developed atomic bombs. By the time the bomb was ready, in early August 1945, Germany had already surrendered, but Japan was fighting on. American forces were poised to invade its home islands. The Japanese vowed to resist the invasion and fight to the death, and there was every reason to believe that it was no idle threat. American generals told President Harry S. Truman that an invasion of Japan would cost the lives of a million American soldiers and would extend the war well into 1946. Truman decided to use the new bomb. Two weeks and two atom bombs later, Japan surrendered unconditionally and the war was over.
But science is not just about offensive weapons. It plays a major role in our defences as well. Today many Americans believe that the solution to terrorism is technological rather than political. Just give millions more to the nanotechnology industry, they believe, and the United States could send bionic spy-flies into every Afghan cave, Yemenite redoubt and North African encampment. Once that’s done, Osama Bin Laden’s heirs will not be able to make a cup of coffee without a CIA spy-fly passing this vital information back to headquarters in Langley. Allocate millions more to brain research, and every airport could be equipped with ultra-sophisticated FMRI scanners that could immediately recognise angry and hateful thoughts in people’s brains. Will it really work? Who knows. Is it wise to develop bionic flies and thought-reading scanners? Not necessarily. Be that as it may, as you read these lines, the US Department of Defense is transferring millions of dollars to nanotechnology and brain laboratories for work on these and other such ideas.
This obsession with military technology – from tanks to atom bombs to spy-flies – is a surprisingly recent phenomenon. Up until the nineteenth century, the vast majority of military revolutions were the product of organisational rather than technological changes. When alien civilisations met for the first time, technological gaps sometimes played an important role. But even in such cases, few thought of deliberately creating or enlarging such gaps. Most empires did not rise thanks to technological wizardry, and their rulers did not give much thought to technological improvement. The Arabs did not defeat the Sassanid Empire thanks to superior bows or swords, the Seljuks had no technological advantage over the Byzantines, and the Mongols did not conquer China with the help of some ingenious new weapon. In fact, in all these cases the vanquished enjoyed superior military and civilian technology.
The Roman army is a particularly good example. It was the best army of its day, yet technologically speaking, Rome had no edge over Carthage, Macedonia or the Seleucid Empire. Its advantage rested on efficient organisation, iron discipline and huge manpower reserves. The Roman army never set up a research and development department, and its weapons remained more or less the same for centuries on end. If the legions of Scipio Aemilianus – the general who levelled Carthage and defeated the Numantians in the second century BC – had suddenly popped up 500 years later in the age of Constantine the Great, Scipio would have had a fair chance of beating Constantine. Now imagine what would happen to a general from a few centuries back – say Napoleon – if he led his troops against a modern armoured brigade. Napoleon was a brilliant tactician, and his men were crack professionals, but their skills would be useless in the face of modern weaponry.
As in Rome, so also in ancient China: most generals and philosophers did not think it their duty to develop new weapons. The most important military invention in the history of China was gunpowder. Yet to the best of our knowledge, gunpowder was invented accidentally, by Daoist alchemists searching for the elixir of life. Gunpowder’s subsequent career is even more telling. One might have thought that the Daoist alchemists would have made China master of the world. In fact, the Chinese used the new compound mainly for firecrackers. Even as the Song Empire collapsed in the face of a Mongol invasion, no emperor set up a medieval Manhattan Project to save the empire by inventing a doomsday weapon. Only in the fifteenth century – about 600 years after the invention of gunpowder – did cannons become a decisive factor on Afro-Asian battlefields. Why did it take so long for the deadly potential of this substance to be put to military use? Because it appeared at a time when neither kings, scholars, nor merchants thought that new military technology could save them or make them rich.
The situation began to change in the fifteenth and sixteenth centuries, but another 200 years went by before most rulers evinced any interest in financing the research and development of new weapons. Logistics and strategy continued to have far greater impact on the outcome of wars than technology. The Napoleonic military machine that crushed the armies of the European powers at Austerlitz (1805) was armed with more or less the same weaponry that the army of Louis XVI had used. Napoleon himself, despite being an artilleryman, had little interest in new weapons, even though scientists and inventors tried to persuade him to fund the development of flying machines, submarines and rockets.
Science, industry and military technology intertwined only with the advent of the capitalist system and the Industrial Revolution. Once this relationship was established, however, it quickly transformed the world.
The Ideal of Progress

Until the Scientific Revolution most human cultures did not believe in progress. They thought the golden age was in the past, and that the world was stagnant, if not deteriorating. Strict adherence to the wisdom of the ages might perhaps bring back the good old times, and human ingenuity might conceivably improve this or that facet of daily life. However, it was considered impossible for human know-how to overcome the world’s fundamental problems. If even Muhammad, Jesus, Buddha and Confucius – who knew everything there is to know – were unable to abolish famine, disease, poverty and war from the world, how could we expect to do so?
Many faiths believed that some day a messiah would appear and end all wars, famines and even death itself. But the notion that humankind could do so by discovering new knowledge and inventing new tools was worse than ludicrous – it was hubris. The story of the Tower of Babel, the story of Icarus, the story of the Golem and countless other myths taught people that any attempt to go beyond human limitations would inevitably lead to disappointment and disaster.
When modern culture admitted that there were many important things that it still did not know, and when that admission of ignorance was married to the idea that scientific discoveries could give us new powers, people began suspecting that real progress might be possible after all. As science began to solve one unsolvable problem after another, many became convinced that humankind could overcome any and every problem by acquiring and applying new knowledge. Poverty, sickness, wars, famines, old age and death itself were not the inevitable fate of humankind. They were simply the fruits of our ignorance.

34. Benjamin Franklin disarming the gods.

A famous example is lightning. Many cultures believed that lightning was the hammer of an angry god, used to punish sinners. In the middle of the eighteenth century, in one of the most celebrated experiments in scientific history, Benjamin Franklin flew a kite during a lightning storm to test the hypothesis that lightning is simply an electric current. Franklins empirical observations, coupled with his knowledge about the qualities of electrical energy, enabled him to invent the lightning rod and disarm the gods.
Poverty is another case in point. Many cultures have viewed poverty as an inescapable part of this imperfect world. According to the New Testament, shortly before the crucifixion a woman anointed Christ with precious oil worth 300 denarii. Jesus’ disciples scolded the woman for wasting such a huge sum of money instead of giving it to the poor, but Jesus defended her, saying that ‘The poor you will always have with you, and you can help them any time you want. But you will not always have me’ (Mark 14:7). Today, fewer and fewer people, including fewer and fewer Christians, agree with Jesus on this matter. Poverty is increasingly seen as a technical problem amenable to intervention. It’s common wisdom that policies based on the latest findings in agronomy, economics, medicine and sociology can eliminate poverty.
And indeed, many parts of the world have already been freed from the worst forms of deprivation. Throughout history, societies have suffered from two kinds of poverty: social poverty, which withholds from some people the opportunities available to others; and biological poverty, which puts the very lives of individuals at risk due to lack of food and shelter. Perhaps social poverty can never be eradicated, but in many countries around the world biological poverty is a thing of the past.
Until recently, most people hovered very close to the biological poverty line, below which a person lacks enough calories to sustain life for long. Even small miscalculations or misfortunes could easily push people below that line, into starvation. Natural disasters and man-made calamities often plunged entire populations over the abyss, causing the death of millions. Today most of the world’s people have a safety net stretched below them. Individuals are protected from personal misfortune by insurance, state-sponsored social security and a plethora of local and international NGOs. When calamity strikes an entire region, worldwide relief efforts are usually successful in preventing the worst. People still suffer from numerous degradations, humiliations and poverty-related illnesses, but in most countries nobody is starving to death. In fact, in many societies more people are in danger of dying from obesity than from starvation.
The Gilgamesh Project

Of all mankind’s ostensibly insoluble problems, one has remained the most vexing, interesting and important: the problem of death itself. Before the late modern era, most religions and ideologies took it for granted that death was our inevitable fate. Moreover, most faiths turned death into the main source of meaning in life. Try to imagine Islam, Christianity or the ancient Egyptian religion in a world without death. These creeds taught people that they must come to terms with death and pin their hopes on the afterlife, rather than seek to overcome death and live for ever here on earth. The best minds were busy giving meaning to death, not trying to escape it.
That is the theme of the most ancient myth to come down to us – the Gilgamesh myth of ancient Sumer. Its hero is the strongest and most capable man in the world, King Gilgamesh of Uruk, who could defeat anyone in battle. One day, Gilgamesh’s best friend, Enkidu, died. Gilgamesh sat by the body and observed it for many days, until he saw a worm dropping out of his friend’s nostril. At that moment Gilgamesh was gripped by a terrible horror, and he resolved that he himself would never die. He would somehow find a way to defeat death. Gilgamesh then undertook a journey to the end of the universe, killing lions, battling scorpion-men and finding his way into the underworld. There he shattered the stone giants of Urshanabi and the ferryman of the river of the dead, and found Utnapishtim, the last survivor of the primordial flood. Yet Gilgamesh failed in his quest. He returned home empty-handed, as mortal as ever, but with one new piece of wisdom. When the gods created man, Gilgamesh had learned, they set death as man’s inevitable destiny, and man must learn to live with it.
Disciples of progress do not share this defeatist attitude. For men of science, death is not an inevitable destiny, but merely a technical problem. People die not because the gods decreed it, but due to various technical failures – a heart attack, cancer, an infection. And every technical problem has a technical solution. If the heart flutters, it can be stimulated by a pacemaker or replaced by a new heart. If cancer rampages, it can be killed with drugs or radiation. If bacteria proliferate, they can be subdued with antibiotics. True, at present we cannot solve all technical problems. But we are working on them. Our best minds are not wasting their time trying to give meaning to death. Instead, they are busy investigating the physiological, hormonal and genetic systems responsible for disease and old age. They are developing new medicines, revolutionary treatments and artificial organs that will lengthen our lives and might one day vanquish the Grim Reaper himself.
Until recently, you would not have heard scientists, or anyone else, speak so bluntly. ‘Defeat death?! What nonsense! We are only trying to cure cancer, tuberculosis and Alzheimer’s disease,’ they insisted. People avoided the issue of death because the goal seemed too elusive. Why create unreasonable expectations? We’re now at a point, however, where we can be frank about it. The leading project of the Scientific Revolution is to give humankind eternal life. Even if killing death seems a distant goal, we have already achieved things that were inconceivable a few centuries ago. In 1199, King Richard the Lionheart was struck by an arrow in his left shoulder. Today we’d say he incurred a minor injury. But in 1199, in the absence of antibiotics and effective sterilisation methods, this minor flesh wound turned infected and gangrene set in. The only way to stop the spread of gangrene in twelfth-century Europe was to cut off the infected limb, impossible when the infection was in a shoulder. The gangrene spread through the Lionheart’s body and no one could help the king. He died in great agony two weeks later.
As recently as the nineteenth century, the best doctors still did not know how to prevent infection and stop the putrefaction of tissues. In field hospitals doctors routinely cut off the hands and legs of soldiers who received even minor limb injuries, fearing gangrene. These amputations, as well as all other medical procedures (such as tooth extraction), were done without any anaesthetics. The first anaesthetics – ether, chloroform and morphine – entered regular usage in Western medicine only in the middle of the nineteenth century. Before the advent of chloroform, four soldiers had to hold down a wounded comrade while the doctor sawed off the injured limb. On the morning after the battle of Waterloo (1815), heaps of sawn-off hands and legs could be seen adjacent to the field hospitals. In those days, carpenters and butchers who enlisted to the army were often sent to serve in the medical corps, because surgery required little more than knowing your way with knives and saws.
In the two centuries since Waterloo, things have changed beyond recognition. Pills, injections and sophisticated operations save us from a spate of illnesses and injuries that once dealt an inescapable death sentence. They also protect us against countless daily aches and ailments, which premodern people simply accepted as part of life. The average life expectancy jumped from around twenty-five to forty years, to around sixty-seven in the entire world, and to around eighty years in the developed world.8
Death suffered its worst setbacks in the arena of child mortality. Until the twentieth century, between a quarter and a third of the children of agricultural societies never reached adulthood. Most succumbed to childhood diseases such as diphtheria, measles and smallpox. In seventeenth-century England, 150 out of every 1,000 newborns died during their first year, and a third of all children were dead before they reached fifteen.9 Today, only five out of 1,000 English babies die during their first year, and only seven out of 1,000 die before age fifteen.10
We can better grasp the full impact of these figures by setting aside statistics and telling some stories. A good example is the family of King Edward I of England (1237–1307) and his wife, Queen Eleanor (1241–90). Their children enjoyed the best conditions and the most nurturing surroundings that could be provided in medieval Europe. They lived in palaces, ate as much food as they liked, had plenty of warm clothing, well-stocked fireplaces, the cleanest water available, an army of servants and the best doctors. The sources mention sixteen children that Queen Eleanor bore between 1255 and 1284:
1. An anonymous daughter, born in 1255, died at birth.
2. A daughter, Catherine, died either at age one or age three.
3. A daughter, Joan, died at six months.
4. A son, John, died at age five.
5. A son, Henry, died at age six.
6. A daughter, Eleanor, died at age twenty-nine.
7. An anonymous daughter died at five months.
8. A daughter, Joan, died at age thirty-five.
9. A son, Alphonso, died at age ten.
10. A daughter, Margaret, died at age fifty-eight.
11. A daughter, Berengeria, died at age two.
12. An anonymous daughter died shortly after birth.
13. A daughter, Mary, died at age fifty-three.
14. An anonymous son died shortly after birth.
15. A daughter, Elizabeth, died at age thirty-four.
16. A son, Edward.
The youngest, Edward, was the first of the boys to survive the dangerous years of childhood, and at his fathers death he ascended the English throne as King Edward II. In other words, it took Eleanor sixteen tries to carry out the most fundamental mission of an English queen – to provide her husband with a male heir. Edward II’s mother must have been a woman of exceptional patience and fortitude. Not so the woman Edward chose for his wife, Isabella of France. She had him murdered when he was forty-three.11
To the best of our knowledge, Eleanor and Edward I were a healthy couple and passed no fatal hereditary illnesses on to their children. Nevertheless, ten out of the sixteen – 62 per cent – died during childhood. Only six managed to live beyond the age of eleven, and only three – just 18 per cent – lived beyond the age of forty. In addition to these births, Eleanor most likely had a number of pregnancies that ended in miscarriage. On average, Edward and Eleanor lost a child every three years, ten children one after another. It’s nearly impossible for a parent today to imagine such loss.
How long will the Gilgamesh Project – the quest for immortality – take to complete? A hundred years? Five hundred years? A thousand years? When we recall how little we knew about the human body in 1900, and how much knowledge we have gained in a single century, there is cause for optimism. Genetic engineers have recently managed to double the average life expectancy of Caenorhabditis elegans worms.12 Could they do the same for Homo sapiens? Nanotechnology experts are developing a bionic immune system composed of millions of nano-robots, who would inhabit our bodies, open blocked blood vessels, fight viruses and bacteria, eliminate cancerous cells and even reverse ageing processes.13 A few serious scholars suggest that by 2050, some humans will become a-mortal (not immortal, because they could still die of some accident, but a-mortal, meaning that in the absence of fatal trauma their lives could be extended indefinitely).
Whether or not Project Gilgamesh succeeds, from a historical perspective it is fascinating to see that most late-modern religions and ideologies have already taken death and the afterlife out of the equation. Until the eighteenth century, religions considered death and its aftermath central to the meaning of life. Beginning in the eighteenth century, religions and ideologies such as liberalism, socialism and feminism lost all interest in the afterlife. What, exactly, happens to a Communist after he or she dies? What happens to a capitalist? What happens to a feminist? It is pointless to look for the answer in the writings of Marx, Adam Smith or Simone de Beauvoir. The only modern ideology that still awards death a central role is nationalism. In its more poetic and desperate moments, nationalism promises that whoever dies for the nation will forever live in its collective memory. Yet this promise is so fuzzy that even most nationalists do not really know what to make of it.
The Sugar Daddy of Science

We are living in a technical age. Many are convinced that science and technology hold the answers to all our problems. We should just let the scientists and technicians go on with their work, and they will create heaven here on earth. But science is not an enterprise that takes place on some superior moral or spiritual plane above the rest of human activity. Like all other parts of our culture, it is shaped by economic, political and religious interests.
Science is a very expensive affair. A biologist seeking to understand the human immune system requires laboratories, test tubes, chemicals and electron microscopes, not to mention lab assistants, electricians, plumbers and cleaners. An economist seeking to model credit markets must buy computers, set up giant databanks and develop complicated data-processing programs. An archaeologist who wishes to understand the behaviour of archaic hunter-gatherers must travel to distant lands, excavate ancient ruins and date fossilised bones and artefacts. All of this costs money.
During the past 500 years modern science has achieved wonders thanks largely to the willingness of governments, businesses, foundations and private donors to channel billions of dollars into scientific research. These billions have done much more to chart the universe, map the planet and catalogue the animal kingdom than did Galileo Galilei, Christopher Columbus and Charles Darwin. If these particular geniuses had never been born, their insights would probably have occurred to others. But if the proper funding were unavailable, no intellectual brilliance could have compensated for that. If Darwin had never been born, for example, we’d today attribute the theory of evolution to Alfred Russel Wallace, who came up with the idea of evolution via natural selection independently of Darwin and just a few years later. But if the European powers had not financed geographical, zoological and botanical research around the world, neither Darwin nor Wallace would have had the necessary empirical data to develop the theory of evolution. It is likely that they would not even have tried.
Why did the billions start flowing from government and business coffers into labs and universities? In academic circles, many are naïve enough to believe in pure science. They believe that government and business altruistically give them money to pursue whatever research projects strike their fancy. But this hardly describes the realities of science funding.
Most scientific studies are funded because somebody believes they can help attain some political, economic or religious goal. For example, in the sixteenth century, kings and bankers channelled enormous resources to finance geographical expeditions around the world but not a penny for studying child psychology. This is because kings and bankers surmised that the discovery of new geographical knowledge would enable them to conquer new lands and set up trade empires, whereas they couldn’t see any profit in understanding child psychology.
In the 1940s the governments of America and the Soviet Union channelled enormous resources to the study of nuclear physics rather than underwater archaeology. They surmised that studying nuclear physics would enable them to develop nuclear weapons, whereas underwater archaeology was unlikely to help win wars. Scientists themselves are not always aware of the political, economic and religious interests that control the flow of money; many scientists do, in fact, act out of pure intellectual curiosity. However, only rarely do scientists dictate the scientific agenda.
Even if we wanted to finance pure science unaffected by political, economic or religious interests, it would probably be impossible. Our resources are limited, after all. Ask a congressman to allocate an additional million dollars to the National Science Foundation for basic research, and he’ll justifiably ask whether that money wouldn’t be better used to fund teacher training or to give a needed tax break to a troubled factory in his district. To channel limited resources we must answer questions such as ‘What is more important?’ and ‘What is good?’ And these are not scientific questions. Science can explain what exists in the world, how things work, and what might be in the future. By definition, it has no pretensions to knowing what should be in the future. Only religions and ideologies seek to answer such questions.
Consider the following quandary: two biologists from the same department, possessing the same professional skills, have both applied for a million-dollar grant to finance their current research projects. Professor Slughorn wants to study a disease that infects the udders of cows, causing a 10 per cent decrease in their milk production. Professor Sprout wants to study whether cows suffer mentally when they are separated from their calves. Assuming that the amount of money is limited, and that it is impossible to finance both research projects, which one should be funded?
There is no scientific answer to this question. There are only political, economic and religious answers. In today’s world, it is obvious that Slughorn has a better chance of getting the money. Not because udder diseases are scientifically more interesting than bovine mentality, but because the dairy industry, which stands to benefit from the research, has more political and economic clout than the animal-rights lobby.
Perhaps in a strict Hindu society, where cows are sacred, or in a society committed to animal rights, Professor Sprout would have a better shot. But as long as she lives in a society that values the commercial potential of milk and the health of its human citizens over the feelings of cows, she’d best write up her research proposal so as to appeal to those assumptions. For example, she might write that ‘Depression leads to a decrease in milk production. If we understand the mental world of dairy cows, we could develop psychiatric medication that will improve their mood, thus raising milk production by up to 10 per cent. I estimate that there is a global annual market of $250 million for bovine psychiatric medications.’
Science is unable to set its own priorities. It is also incapable of determining what to do with its discoveries. For example, from a purely scientific viewpoint it is unclear what we should do with our increasing understanding of genetics. Should we use this knowledge to cure cancer, to create a race of genetically engineered supermen, or to engineer dairy cows with super-sized udders? It is obvious that a liberal government, a Communist government, a Nazi government and a capitalist business corporation would use the very same scientific discovery for completely different purposes, and there is no scientific reason to prefer one usage over others.
In short, scientific research can flourish only in alliance with some religion or ideology. The ideology justifies the costs of the research. In exchange, the ideology influences the scientific agenda and determines what to do with the discoveries. Hence in order to comprehend how humankind has reached Alamogordo and the moon – rather than any number of alternative destinations – it is not enough to survey the achievements of physicists, biologists and sociologists. We have to take into account the ideological, political and economic forces that shaped physics, biology and sociology, pushing them in certain directions while neglecting others.
Two forces in particular deserve our attention: imperialism and capitalism. The feedback loop between science, empire and capital has arguably been history’s chief engine for the past 500 years. The following chapters analyse its workings. First we’ll look at how the twin turbines of science and empire were latched to one another, and then learn how both were hitched up to the money pump of capitalism.15

The Marriage of Science and Empire

HOW FAR IS THE SUN FROM THE EARTH? It’s a question that intrigued many early modern astronomers, particularly after Copernicus argued that the sun, rather than the earth, is located at the centre of the universe. A number of astronomers and mathematicians tried to calculate the distance, but their methods provided widely varying results. A reliable means of making the measurement was finally proposed in the middle of the eighteenth century. Every few years, the planet Venus passes directly between the sun and the earth. The duration of the transit differs when seen from distant points on the earths surface because of the tiny difference in the angle at which the observer sees it. If several observations of the same transit were made from different continents, simple trigonometry was all it would take to calculate our exact distance from the sun.
Astronomers predicted that the next Venus transits would occur in 1761 and 1769. So expeditions were sent from Europe to the four corners of the world in order to observe the transits from as many distant points as possible. In 1761 scientists observed the transit from Siberia, North America, Madagascar and South Africa. As the 1769 transit approached, the European scientific community mounted a supreme effort, and scientists were dispatched as far as northern Canada and California (which was then a wilderness). The Royal Society of London for the Improvement of Natural Knowledge concluded that this was not enough. To obtain the most accurate results it was imperative to send an astronomer all the way to the south-western Pacific Ocean.
The Royal Society resolved to send an eminent astronomer, Charles Green, to Tahiti, and spared neither effort nor money. But, since it was funding such an expensive expedition, it hardly made sense to use it to make just a single astronomical observation. Green was therefore accompanied by a team of eight other scientists from several disciplines, headed by botanists Joseph Banks and Daniel Solander. The team also included artists assigned to produce drawings of the new lands, plants, animals and peoples that the scientists would no doubt encounter. Equipped with the most advanced scientific instruments that Banks and the Royal Society could buy, the expedition was placed under the command of Captain James Cook, an experienced seaman as well as an accomplished geographer and ethnographer.
The expedition left England in 1768, observed the Venus transit from Tahiti in 1769, reconnoitred several Pacific islands, visited Australia and New Zealand, and returned to England in 1771. It brought back enormous quantities of astronomical, geographical, meteorological, botanical, zoological and anthropological data. Its findings made major contributions to a number of disciplines, sparked the imagination of Europeans with astonishing tales of the South Pacific, and inspired future generations of naturalists and astronomers.
One of the fields that benefited from the Cook expedition was medicine. At the time, ships that set sail to distant shores knew that more than half their crew members would die on the journey. The nemesis was not angry natives, enemy warships or homesickness. It was a mysterious ailment called scurvy. Men who came down with the disease grew lethargic and depressed, and their gums and other soft tissues bled. As the disease progressed, their teeth fell out, open sores appeared and they grew feverish, jaundiced, and lost control of their limbs. Between the sixteenth and eighteenth centuries, scurvy is estimated to have claimed the lives of about 2 million sailors. No one knew what caused it, and no matter what remedy was tried, sailors continued to die in droves. The turning point came in 1747, when a British physician, James Lind, conducted a controlled experiment on sailors who suffered from the disease. He separated them into several groups and gave each group a different treatment. One of the test groups was instructed to eat citrus fruits, a common folk remedy for scurvy. The patients in this group promptly recovered. Lind did not know what the citrus fruits had that the sailors’ bodies lacked, but we now know that it is vitamin C. A typical shipboard diet at that time was notably lacking in foods that are rich in this essential nutrient. On long-range voyages sailors usually subsisted on biscuits and beef jerky, and ate almost no fruits or vegetables.
The Royal Navy was not convinced by Lind’s experiments, but James Cook was. He resolved to prove the doctor right. He loaded his boat with a large quantity of sauerkraut and ordered his sailors to eat lots of fresh fruits and vegetables whenever the expedition made landfall. Cook did not lose a single sailor to scurvy. In the following decades, all the world’s navies adopted Cook’s nautical diet, and the lives of countless sailors and passengers were saved.1
However, the Cook expedition had another, far less benign result. Cook was not only an experienced seaman and geographer, but also a naval officer. The Royal Society financed a large part of the expedition’s expenses, but the ship itself was provided by the Royal Navy. The navy also seconded eighty-five well-armed sailors and marines, and equipped the ship with artillery, muskets, gunpowder and other weaponry. Much of the information collected by the expedition particularly the astronomical, geographical, meteorological and anthropological data – was of obvious political and military value. The discovery of an effective treatment for scurvy greatly contributed to British control of the world’s oceans and its ability to send armies to the other side of the world. Cook claimed for Britain many of the islands and lands he ‘discovered’, most notably Australia. The Cook expedition laid the foundation for the British occupation of the south-western Pacific Ocean; for the conquest of Australia, Tasmania and New Zealand; for the settlement of millions of Europeans in the new colonies; and for the extermination of their native cultures and most of their native populations.2
In the century following the Cook expedition, the most fertile lands of Australia and New Zealand were taken from their previous inhabitants by European settlers. The native population dropped by up to 90 per cent and the survivors were subjected to a harsh regime of racial oppression. For the Aborigines of Australia and the Maoris of New Zealand, the Cook expedition was the beginning of a catastrophe from which they have never recovered.
An even worse fate befell the natives of Tasmania. Having survived for 10,000 years in splendid isolation, they were completely wiped out, to the last man, woman and child, within a century of Cook’s arrival. European settlers first drove them off the richest parts of the island, and then, coveting even the remaining wilderness, hunted them down and killed them systematically. The few survivors were hounded into an evangelical concentration camp, where well-meaning but not particularly open-minded missionaries tried to indoctrinate them in the ways of the modern world. The Tasmanians were instructed in reading and writing, Christianity and various ‘productive skills’ such as sewing clothes and farming. But they refused to learn. They became ever more melancholic, stopped having children, lost all interest in life, and finally chose the only escape route from the modern world of science and progress – death.
Alas, science and progress pursued them even to the afterlife. The corpses of the last Tasmanians were seized in the name of science by anthropologists and curators. They were dissected, weighed and measured, and analysed in learned articles. The skulls and skeletons were then put on display in museums and anthropological collections. Only in 1976 did the Tasmanian Museum give up for burial the skeleton of Truganini, the last native Tasmanian, who had died a hundred years earlier. The English Royal College of Surgeons held on to samples of her skin and hair until 2002.
Was Cook’s ship a scientific expedition protected by a military force or a military expedition with a few scientists tagging along? That’s like asking whether your petrol tank is half empty or half full. It was both. The Scientific Revolution and modern imperialism were inseparable. People such as Captain James Cook and the botanist Joseph Banks could hardly distinguish science from empire. Nor could luckless Truganini.
Why Europe?

The fact that people from a large island in the northern Atlantic conquered a large island south of Australia is one of history’s more bizarre occurrences. Not long before Cook’s expedition, the British Isles and western Europe in general were but distant backwaters of the Mediterranean world. Little of importance ever happened there. Even the Roman Empire – the only important premodern European empire – derived most of its wealth from its North African, Balkan and Middle Eastern provinces. Rome’s western European provinces were a poor Wild West, which contributed little aside from minerals and slaves. Northern Europe was so desolate and barbarous that it wasn’t even worth conquering.

35. Truganini, the last native Tasmanian.

Only at the end of the fifteenth century did Europe become a hothouse of important military, political, economic and cultural developments. Between 1500 and 1750, western Europe gained momentum and became master of the ‘Outer World’, meaning the two American continents and the oceans. Yet even then Europe was no match for the great powers of Asia. Europeans managed to conquer America and gain supremacy at sea mainly because the Asiatic powers showed little interest in them. The early modern era was a golden age for the Ottoman Empire in the Mediterranean, the Safavid Empire in Persia, the Mughal Empire in India, and the Chinese Ming and Qing dynasties. They expanded their territories significantly and enjoyed unprecedented demographic and economic growth. In 1775 Asia accounted for 80 per cent of the world economy. The combined economies of India and China alone represented two-thirds of global production. In comparison, Europe was an economic dwarf.3
The global centre of power shifted to Europe only between 1750 and 1850, when Europeans humiliated the Asian powers in a series of wars and conquered large parts of Asia. By 1900 Europeans firmly controlled the worlds economy and most of its territory. In 1950 western Europe and the United States together accounted for more than half of global production, whereas Chinas portion had been reduced to 5 per cent.4 Under the European aegis a new global order and global culture emerged. Today all humans are, to a much greater extent than they usually want to admit, European in dress, thought and taste. They may be fiercely anti-European in their rhetoric, but almost everyone on the planet views politics, medicine, war and economics through European eyes, and listens to music written in European modes with words in European languages. Even today’s burgeoning Chinese economy, which may soon regain its global primacy, is built on a European model of production and finance.
How did the people of this frigid finger of Eurasia manage to break out of their remote corner of the globe and conquer the entire world? Europe’s scientists are often given much of the credit. It’s unquestionable that from 1850 onward European domination rested to a large extent on the military–industrial–scientific complex and technological wizardry. All successful late modern empires cultivated scientific research in the hope of harvesting technological innovations, and many scientists spent most of their time working on arms, medicines and machines for their imperial masters. A common saying among European soldiers facing African enemies was, ‘Come what may, we have machine guns, and they don’t.’ Civilian technologies were no less important. Canned food fed soldiers, railroads and steamships transported soldiers and their provisions, while a new arsenal of medicines cured soldiers, sailors and locomotive engineers. These logistical advances played a more significant role in the European conquest of Africa than did the machine gun.
But that wasn’t the case before 1850. The military-industrial-scientific complex was still in its infancy; the technological fruits of the Scientific Revolution were unripe; and the technological gap between European, Asiatic and African powers was small. In 1770, James Cook certainly had far better technology than the Australian Aborigines, but so did the Chinese and the Ottomans. Why then was Australia explored and colonised by Captain James Cook and not by Captain Wan Zhengse or Captain Hussein Pasha? More importantly, if in 1770 Europeans had no significant technological advantage over Muslims, Indians and Chinese, how did they manage in the following century to open such a gap between themselves and the rest of the world?
Why did the military-industrial-scientific complex blossom in Europe rather than India? When Britain leaped forward, why were France, Germany and the United States quick to follow, whereas China lagged behind? When the gap between industrial and non-industrial nations became an obvious economic and political factor, why did Russia, Italy and Austria succeed in closing it, whereas Persia, Egypt and the Ottoman Empire failed? After all, the technology of the first industrial wave was relatively simple. Was it so hard for Chinese or Ottomans to engineer steam engines, manufacture machine guns and lay down railroads?
The world’s first commercial railroad opened for business in 1830, in Britain. By 1850, Western nations were criss-crossed by almost 40,000 kilometres of railroads – but in the whole of Asia, Africa and Latin America there were only 4,000 kilometres of tracks. In 1880, the West boasted more than 350,000 kilometres of railroads, whereas in the rest of the world there were but 35,000 kilometres of train lines (and most of these were laid by the British in India).5 The first railroad in China opened only in 1876. It was twenty-five kilometres long and built by Europeans – the Chinese government destroyed it the following year. In 1880 the Chinese Empire did not operate a single railroad. The first railroad in Persia was built only in 1888, and it connected Tehran with a Muslim holy site about ten kilometres south of the capital. It was constructed and operated by a Belgian company. In 1950, the total railway network of Persia still amounted to a meagre 2,500 kilometres, in a country seven times the size of Britain.6
The Chinese and Persians did not lack technological inventions such as steam engines (which could be freely copied or bought). They lacked the values, myths, judicial apparatus and sociopolitical structures that took centuries to form and mature in the West and which could not be copied and internalised rapidly. France and the United States quickly followed in Britain’s footsteps because the French and Americans already shared the most important British myths and social structures. The Chinese and Persians could not catch up as quickly because they thought and organised their societies differently.
This explanation sheds new light on the period from 1500 to 1850. During this era Europe did not enjoy any obvious technological, political, military or economic advantage over the Asian powers, yet the continent built up a unique potential, whose importance suddenly became obvious around 1850. The apparent equality between Europe, China and the Muslim world in 1750 was a mirage. Imagine two builders, each busy constructing very tall towers. One builder uses wood and mud bricks, whereas the other uses steel and concrete. At first it seems that there is not much of a difference between the two methods, since both towers grow at a similar pace and reach a similar height. However, once a critical threshold is crossed, the wood and mud tower cannot stand the strain and collapses, whereas the steel and concrete tower grows storey by storey, as far as the eye can see.
What potential did Europe develop in the early modern period that enabled it to dominate the late modern world? There are two complementary answers to this question: modern science and capitalism. Europeans were used to thinking and behaving in a scientific and capitalist way even before they enjoyed any significant technological advantages. When the technological bonanza began, Europeans could harness it far better than anybody else. So it is hardly coincidental that science and capitalism form the most important legacy that European imperialism has bequeathed the post-European world of the twenty-first century. Europe and Europeans no longer rule the world, but science and capital are growing ever stronger. The victories of capitalism are examined in the following chapter. This chapter is dedicated to the love story between European imperialism and modern science.
The Mentality of Conquest

Modern science flourished in and thanks to European empires. The discipline obviously owes a huge debt to ancient scientific traditions, such as those of classical Greece, China, India and Islam, yet its unique character began to take shape only in the early modern period, hand in hand with the imperial expansion of Spain, Portugal, Britain, France, Russia and the Netherlands. During the early modern period, Chinese, Indians, Muslims, Native Americans and Polynesians continued to make important contributions to the Scientific Revolution. The insights of Muslim economists were studied by Adam Smith and Karl Marx, treatments pioneered by Native American doctors found their way into English medical texts and data extracted from Polynesian informants revolutionised Western anthropology. But until the mid-twentieth century, the people who collated these myriad scientific discoveries, creating scientific disciplines in the process, were the ruling and intellectual elites of the global European empires. The Far East and the Islamic world produced minds as intelligent and curious as those of Europe. However, between 1500 and 1950 they did not produce anything that comes even close to Newtonian physics or Darwinian biology.
This does not mean that Europeans have a unique gene for science, or that they will forever dominate the study of physics and biology. Just as Islam began as an Arab monopoly but was subsequently taken over by Turks and Persians, so modern science began as a European speciality, but is today becoming a multi-ethnic enterprise.
What forged the historical bond between modern science and European imperialism? Technology was an important factor in the nineteenth and twentieth centuries, but in the early modern era it was of limited importance. The key factor was that the plant-seeking botanist and the colony-seeking naval officer shared a similar mindset. Both scientist and conqueror began by admitting ignorance – they both said, ‘I don’t know what’s out there.’ They both felt compelled to go out and make new discoveries. And they both hoped the new knowledge thus acquired would make them masters of the world.
European imperialism was entirely unlike all other imperial projects in history. Previous seekers of empire tended to assume that they already understood the world. Conquest merely utilised and spread their view of the world. The Arabs, to name one example, did not conquer Egypt, Spain or India in order to discover something they did not know. The Romans, Mongols and Aztecs voraciously conquered new lands in search of power and wealth – not of knowledge. In contrast, European imperialists set out to distant shores in the hope of obtaining new knowledge along with new territories.
James Cook was not the first explorer to think this way. The Portuguese and Spanish voyagers of the fifteenth and sixteenth centuries already did. Prince Henry the Navigator and Vasco da Gama explored the coasts of Africa and, while doing so, seized control of islands and harbours. Christopher Columbus ‘discovered’ America and immediately claimed sovereignty over the new lands for the kings of Spain. Ferdinand Magellan found a way around the world, and simultaneously laid the foundation for the Spanish conquest of the Philippines.
As time went by, the conquest of knowledge and the conquest of territory became ever more tightly intertwined. In the eighteenth and nineteenth centuries, almost every important military expedition that left Europe for distant lands had on board scientists who set out not to fight but to make scientific discoveries. When Napoleon invaded Egypt in 1798, he took 165 scholars with him. Among other things, they founded an entirely new discipline, Egyptology, and made important contributions to the study of religion, linguistics and botany.
In 1831, the Royal Navy sent the ship HMS Beagle to map the coasts of South America, the Falklands Islands and the Galapagos Islands. The navy needed this knowledge in order to be better prepared in the event of war. The ship’s captain, who was an amateur scientist, decided to add a geologist to the expedition to study geological formations they might encounter on the way. After several professional geologists refused his invitation, the captain offered the job to a twenty-two-year-old Cambridge graduate, Charles Darwin. Darwin had studied to become an Anglican parson but was far more interested in geology and natural sciences than in the Bible. He jumped at the opportunity, and the rest is history. The captain spent his time on the voyage drawing military maps while Darwin collected the empirical data and formulated the insights that would eventually become the theory of evolution.
On 20 July 1969, Neil Armstrong and Buzz Aldrin landed on the surface of the moon. In the months leading up to their expedition, the Apollo 11 astronauts trained in a remote moon-like desert in the western United States. The area is home to several Native American communities, and there is a story – or legend – describing an encounter between the astronauts and one of the locals.
One day as they were training, the astronauts came across an old Native American. The man asked them what they were doing there. They replied that they were part of a research expedition that would shortly travel to explore the moon. When the old man heard that, he fell silent for a few moments, and then asked the astronauts if they could do him a favour.
‘What do you want?’ they asked.
‘Well,’ said the old man, ‘the people of my tribe believe that holy spirits live on the moon. I was wondering if you could pass an important message to them from my people.’
‘What’s the message?’ asked the astronauts.
The man uttered something in his tribal language, and then asked the astronauts to repeat it again and again until they had memorised it correctly.
‘What does it mean?’ asked the astronauts.
‘Oh, I cannot tell you. It’s a secret that only our tribe and the moon spirits are allowed to know.’
When they returned to their base, the astronauts searched and searched until they found someone who could speak the tribal language, and asked him to translate the secret message. When they repeated what they had memorised, the translator started to laugh uproariously. When he calmed down, the astronauts asked him what it meant. The man explained that the sentence they had memorised so carefully said, ‘Don’t believe a single word these people are telling you. They have come to steal your lands.’
Empty Maps

The modern ‘explore and conquer’ mentality is nicely illustrated by the development of world maps. Many cultures drew world maps long before the modern age. Obviously, none of them really knew the whole of the world. No Afro-Asian culture knew about America, and no American culture knew about Afro-Asia. But unfamiliar areas were simply left out, or filled with imaginary monsters and wonders. These maps had no empty spaces. They gave the impression of a familiarity with the entire world.
During the fifteenth and sixteenth centuries, Europeans began to draw world maps with lots of empty spaces – one indication of the development of the scientific mindset, as well as of the European imperial drive. The empty maps were a psychological and ideological breakthrough, a clear admission that Europeans were ignorant of large parts of the world.
The crucial turning point came in 1492, when Christopher Columbus sailed westward from Spain, seeking a new route to East Asia. Columbus still believed in the old ‘complete’ world maps. Using them, Columbus calculated that Japan should have been located about 7,000 kilometres west of Spain. In fact, more than 20,000 kilometres and an entire unknown continent separate East Asia from Spain. On 12 October 1492, at about 2:00 a.m., Columbus’ expedition collided with the unknown continent. Juan Rodriguez Bermejo, watching from the mast of the ship Pinta, spotted an island in what we now call the Bahamas, and shouted ‘Land! Land!’
Columbus believed he had reached a small island off the East Asian coast. He called the people he found there ‘Indians’ because he thought he had landed in the Indies – what we now call the East Indies or the Indonesian archipelago. Columbus stuck to this error for the rest of his life. The idea that he had discovered a completely unknown continent was inconceivable for him and for many of his generation. For thousands of years, not only the greatest thinkers and scholars but also the infallible Scriptures had known only Europe, Africa and Asia. Could they all have been wrong? Could the Bible have missed half the world? It would be as if in 1969, on its way to the moon, Apollo 11 had crashed into a hitherto unknown moon circling the earth, which all previous observations had somehow failed to spot. In his refusal to admit ignorance, Columbus was still a medieval man. He was convinced he knew the whole world, and even his momentous discovery failed to convince him otherwise.

36. A European world map from 1459 (Europe is in the top left corner). The map is filled with details, even when depicting areas that were completely unfamiliar to Europeans, such as southern Africa.

The first modern man was Amerigo Vespucci, an Italian sailor who took part in several expeditions to America in the years 1499–1504. Between 1502 and 1504, two texts describing these expeditions were published in Europe. They were attributed to Vespucci. These texts argued that the new lands discovered by Columbus were not islands off the East Asian coast, but rather an entire continent unknown to the Scriptures, classical geographers and contemporary Europeans. In 1507, convinced by these arguments, a respected mapmaker named Martin Waldseemüller published an updated world map, the first to show the place where Europe’s westward-sailing fleets had landed as a separate continent. Having drawn it, Waldseemüller had to give it a name. Erroneously believing that Amerigo Vespucci had been the person who discovered it, Waldseemüller named the continent in his honour – America. The Waldseemüller map became very popular and was copied by many other cartographers, spreading the name he had given the new land. There is poetic justice in the fact that a quarter of the world, and two of its seven continents, are named after a little-known Italian whose sole claim to fame is that he had the courage to say, ‘We don’t know.’
The discovery of America was the foundational event of the Scientific Revolution. It not only taught Europeans to favour present observations over past traditions, but the desire to conquer America also obliged Europeans to search for new knowledge at breakneck speed. If they really wanted to control the vast new territories, they had to gather enormous amounts of new data about the geography, climate, flora, fauna, languages, cultures and history of the new continent. Christian Scriptures, old geography books and ancient oral traditions were of little help.
Henceforth not only European geographers, but European scholars in almost all other fields of knowledge began to draw maps with spaces left to fill in. They began to admit that their theories were not perfect and that there were important things that they did not know.
The Europeans were drawn to the blank spots on the map as if they were magnets, and promptly started filling them in. During the fifteenth and sixteenth centuries, European expeditions circumnavigated Africa, explored America, crossed the Pacific and Indian Oceans, and created a network of bases and colonies all over the world. They established the first truly global empires and knitted together the first global trade network. The European imperial expeditions transformed the history of the world: from being a series of histories of isolated peoples and cultures, it became the history of a single integrated human society.

37. The Salviati World Map, 1525. While the 1459 world map is full of continents, islands and detailed explanations, the Salviati map is mostly empty. The eye wanders south along the American coastline, until it peters into emptiness. Anyone looking at the map and possessing even minimal curiosity is tempted to ask, ‘What’s beyond this point?’ The map gives no answers. It invites the observer to set sail and find out.

These European explore-and-conquer expeditions are so familiar to us that we tend to overlook just how extraordinary they were. Nothing like them had ever happened before. Long-distance campaigns of conquest are not a natural undertaking. Throughout history most human societies were so busy with local conflicts and neighbourhood quarrels that they never considered exploring and conquering distant lands. Most great empires extended their control only over their immediate neighbourhood – they reached far-flung lands simply because their neighbourhood kept expanding. Thus the Romans conquered Etruria in order to defend Rome (c.350–300 BC). They then conquered the Po Valley in order to defend Etruria (c.200 BC). They subsequently conquered Provence to defend the Po Valley (c.120 BC), Gaul to defend Provence (c.50 BC), and Britain in order to defend Gaul (c. AD 50). It took them 400 years to get from Rome to London. In 350 BC, no Roman would have conceived of sailing directly to Britain and conquering it.
Occasionally an ambitious ruler or adventurer would embark on a long-range campaign of conquest, but such campaigns usually followed well-beaten imperial or commercial paths. The campaigns of Alexander the Great, for example, did not result in the establishment of a new empire, but rather in the usurpation of an existing empire – that of the Persians. The closest precedents to the modern European empires were the ancient naval empires of Athens and Carthage, and the medieval naval empire of Majapahit, which held sway over much of Indonesia in the fourteenth century. Yet even these empires rarely ventured into unknown seas – their naval exploits were local undertakings when compared to the global ventures of the modern Europeans.
Many scholars argue that the voyages of Admiral Zheng He of the Chinese Ming dynasty heralded and eclipsed the European voyages of discovery. Between 1405 and 1433, Zheng led seven huge armadas from China to the far reaches of the Indian Ocean. The largest of these comprised almost 300 ships and carried close to 30,000 people.7 They visited Indonesia, Sri Lanka, India, the Persian Gulf, the Red Sea and East Africa. Chinese ships anchored in Jedda, the main harbour of the Hejaz, and in Malindi, on the Kenyan coast. Columbus’ fleet of 1492 – which consisted of three small ships manned by 120 sailors – was like a trio of mosquitoes compared to Zheng He’s drove of dragons.8
Yet there was a crucial difference. Zheng He explored the oceans, and assisted pro-Chinese rulers, but he did not try to conquer or colonise the countries he visited. Moreover, the expeditions of Zheng He were not deeply rooted in Chinese politics and culture. When the ruling faction in Beijing changed during the 1430s, the new overlords abruptly terminated the operation. The great fleet was dismantled, crucial technical and geographical knowledge was lost, and no explorer of such stature and means ever set out again from a Chinese port. Chinese rulers in the coming centuries, like most Chinese rulers in previous centuries, restricted their interests and ambitions to the Middle Kingdom’s immediate environs.
The Zheng He expeditions prove that Europe did not enjoy an outstanding technological edge. What made Europeans exceptional was their unparalleled and insatiable ambition to explore and conquer. Although they might have had the ability, the Romans never attempted to conquer India or Scandinavia, the Persians never attempted to conquer Madagascar or Spain, and the Chinese never attempted to conquer Indonesia or Africa. Most Chinese rulers left even nearby Japan to its own devices. There was nothing peculiar about that. The oddity is that early modern Europeans caught a fever that drove them to sail to distant and completely unknown lands full of alien cultures, take one step on to their beaches, and immediately declare, ‘I claim all these territories for my king!’

38. Zheng He’s flagship next to that of Columbus.

Invasion from Outer Space

Around 1517, Spanish colonists in the Caribbean islands began to hear vague rumours about a powerful empire somewhere in the centre of the Mexican mainland. A mere four years later, the Aztec capital was a smouldering ruin, the Aztec Empire was a thing of the past, and Hernán Cortés lorded over a vast new Spanish Empire in Mexico.
The Spaniards did not stop to congratulate themselves or even to catch their breath. They immediately commenced explore-and-conquer operations in all directions. The previous rulers of Central America – the Aztecs, the Toltecs, the Maya – barely knew South America existed, and never made any attempt to subjugate it, over the course of 2,000 years. Yet within little more than ten years of the Spanish conquest of Mexico, Francisco Pizarro had discovered the Inca Empire in South America, vanquishing it in 1532.
Had the Aztecs and Incas shown a bit more interest in the world surrounding them – and had they known what the Spaniards had done to their neighbours – they might have resisted the Spanish conquest more keenly and successfully. In the years separating Columbus’ first journey to America (1492) from the landing of Cortés in Mexico (1519), the Spaniards conquered most of the Caribbean islands, setting up a chain of new colonies. For the subjugated natives, these colonies were hell on earth. They were ruled with an iron fist by greedy and unscrupulous colonists who enslaved them and set them to work in mines and plantations, killing anyone who offered the slightest resistance. Most of the native population soon died, either because of the harsh working conditions or the virulence of the diseases that hitch-hiked to America on the conquerors’ sailing ships. Within twenty years, almost the entire native Caribbean population was wiped out. The Spanish colonists began importing African slaves to fill the vacuum.
This genocide took place on the very doorstep of the Aztec Empire, yet when Cortés landed on the empire’s eastern coast, the Aztecs knew nothing about it. The coming of the Spaniards was the equivalent of an alien invasion from outer space. The Aztecs were convinced that they knew the entire world and that they ruled most of it. To them it was unimaginable that outside their domain could exist anything like these Spaniards. When Cortés and his men landed on the sunny beaches of today’s Vera Cruz, it was the first time the Aztecs encountered a completely unknown people.
The Aztecs did not know how to react. They had trouble deciding what these strangers were. Unlike all known humans, the aliens had white skins. They also had lots of facial hair. Some had hair the colour of the sun. They stank horribly. (Native hygiene was far better than Spanish hygiene. When the Spaniards first arrived in Mexico, natives bearing incense burners were assigned to accompany them wherever they went. The Spaniards thought it was a mark of divine honour. We know from native sources that they found the newcomers’ smell unbearable.)

Map 7. The Aztec and Inca empires at the time of the Spanish conquest.

The aliens’ material culture was even more bewildering. They came in giant ships, the like of which the Aztecs had never imagined, let alone seen. They rode on the back of huge and terrifying animals, swift as the wind. They could produce lightning and thunder out of shiny metal sticks. They had flashing long swords and impenetrable armour, against which the natives’ wooden swords and flint spears were useless.
Some Aztecs thought these must be gods. Others argued that they were demons, or the ghosts of the dead, or powerful sorcerers. Instead of concentrating all available forces and wiping out the Spaniards, the Aztecs deliberated, dawdled and negotiated. They saw no reason to rush. After all, Cortés had no more than 550 Spaniards with him. What could 550 men do to an empire of millions?
Cortés was equally ignorant about the Aztecs, but he and his men held significant advantages over their adversaries. While the Aztecs had no experience to prepare them for the arrival of these strange-looking and foul-smelling aliens, the Spaniards knew that the earth was full of unknown human realms, and no one had greater expertise in invading alien lands and dealing with situations about which they were utterly ignorant. For the modern European conqueror, like the modern European scientist, plunging into the unknown was exhilarating.
So when Cortés anchored off that sunny beach in July 1519, he did not hesitate to act. Like a science-fiction alien emerging from his spaceship, he declared to the awestruck locals: ‘We come in peace. Take us to your leader.’ Cortés explained that he was a peaceful emissary from the great king of Spain, and asked for a diplomatic interview with the Aztec ruler, Montezuma II. (This was a shameless lie. Cortés led an independent expedition of greedy adventurers. The king of Spain had never heard of Cortés, nor of the Aztecs.) Cortés was given guides, food and some military assistance by local enemies of the Aztecs. He then marched towards the Aztec capital, the great metropolis of Tenochtitlan.
The Aztecs allowed the aliens to march all the way to the capital, then respectfully led the aliens’ leader to meet Emperor Montezuma. In the middle of the interview, Cortés gave a signal, and steel-armed Spaniards butchered Montezuma’s bodyguards (who were armed only with wooden clubs, and stone blades). The honoured guest took his host prisoner.
Cortés was now in a very delicate situation. He had captured the emperor, but was surrounded by tens of thousands of furious enemy warriors, millions of hostile civilians, and an entire continent about which he knew practically nothing. He had at his disposal only a few hundred Spaniards, and the closest Spanish reinforcements were in Cuba, more than 1,500 kilometres away.
Cortés kept Montezuma captive in the palace, making it look as if the king remained free and in charge and as if the ‘Spanish ambassador’ were no more than a guest. The Aztec Empire was an extremely centralised polity, and this unprecedented situation paralysed it. Montezuma continued to behave as if he ruled the empire, and the Aztec elite continued to obey him, which meant they obeyed Cortés. This situation lasted for several months, during which time Cortés interrogated Montezuma and his attendants, trained translators in a variety of local languages, and sent small Spanish expeditions in all directions to become familiar with the Aztec Empire and the various tribes, peoples and cities that it ruled.
The Aztec elite eventually revolted against Cortés and Montezuma, elected a new emperor, and drove the Spaniards from Tenochtitlan. However, by now numerous cracks had appeared in the imperial edifice. Cortés used the knowledge he had gained to prise the cracks open wider and split the empire from within. He convinced many of the empire’s subject peoples to join him against the ruling Aztec elite. The subject peoples miscalculated badly. They hated the Aztecs, but knew nothing of Spain or the Caribbean genocide. They assumed that with Spanish help they could shake off the Aztec yoke. The idea that the Spanish would take over never occurred to them. They were sure that if Cortés and his few hundred henchmen caused any trouble, they could easily be overwhelmed. The rebellious peoples provided Cortés with an army of tens of thousands of local troops, and with its help Cortés besieged Tenochtitlan and conquered the city.
At this stage more and more Spanish soldiers and settlers began arriving in Mexico, some from Cuba, others all the way from Spain. When the local peoples realised what was happening, it was too late. Within a century of the landing at Vera Cruz, the native population of the Americas had shrunk by about 90 per cent, due mainly to unfamiliar diseases that reached America with the invaders. The survivors found themselves under the thumb of a greedy and racist regime that was far worse than that of the Aztecs.
Ten years after Cortés landed in Mexico, Pizarro arrived on the shore of the Inca Empire. He had far fewer soldiers than Cortés – his expedition numbered just 168 men! Yet Pizarro benefited from all the knowledge and experience gained in previous invasions. The Inca, in contrast, knew nothing about the fate of the Aztecs. Pizarro plagiarised Cortés. He declared himself a peaceful emissary from the king of Spain, invited the Inca ruler, Atahualpa, to a diplomatic interview, and then kidnapped him. Pizarro proceeded to conquer the paralysed empire with the help of local allies. If the subject peoples of the Inca Empire had known the fate of the inhabitants of Mexico, they would not have thrown in their lot with the invaders. But they did not know.
The native peoples of America were not the only ones to pay a heavy price for their parochial outlook. The great empires of Asia – the Ottoman, the Safavid, the Mughal and the Chinese – very quickly heard that the Europeans had discovered something big. Yet they displayed little interest in these discoveries. They continued to believe that the world revolved around Asia, and made no attempt to compete with the Europeans for control of America or of the new ocean lanes in the Atlantic and the Pacific. Even puny European kingdoms such as Scotland and Denmark sent a few explore-and-conquer expeditions to America, but not one expedition of either exploration or conquest was ever sent to America from the Islamic world, India or China. The first non-European power that tried to send a military expedition to America was Japan. That happened in June 1942, when a Japanese expedition conquered Kiska and Attu, two small islands off the Alaskan coast, capturing in the process ten US soldiers and a dog. The Japanese never got any closer to the mainland.
It is hard to argue that the Ottomans or Chinese were too far away, or that they lacked the technological, economic or military wherewithal. The resources that sent Zheng He from China to East Africa in the 1420S should have been enough to reach America. The Chinese just weren’t interested. The first Chinese world map to show America was not issued until 1602 – and then by a European missionary!
For 300 years, Europeans enjoyed undisputed mastery in America and Oceania, in the Atlantic and the Pacific. The only significant struggles in those regions were between different European powers. The wealth and resources accumulated by the Europeans eventually enabled them to invade Asia too, defeat its empires, and divide it among themselves. When the Ottomans, Persians, Indians and Chinese woke up and began paying attention, it was too late.
Only in the twentieth century did non-European cultures adopt a truly global vision. This was one of the crucial factors that led to the collapse of European hegemony. Thus in the Algerian War of Independence (1954–62), Algerian guerrillas defeated a French army with an overwhelming numerical, technological and economic advantage. The Algerians prevailed because they were supported by a global anti-colonial network, and because they worked out how to harness the world’s media to their cause – as well as public opinion in France itself. The defeat that little North Vietnam inflicted on the American colossus was based on a similar strategy. These guerrilla forces showed that even superpowers could be defeated if a local struggle became a global cause. It is interesting to contemplate what might have happened had Montezuma been able to manipulate public opinion in Spain and gain assistance from one of Spain’s rivals – Portugal, France or the Ottoman Empire.
Rare Spiders and Forgotten Scripts

Modern science and modern empires were motivated by the restless feeling that perhaps something important awaited beyond the horizon – something they had better explore and master. Yet the connection between science and empire went much deeper. Not just the motivation, but also the practices of empire-builders were entangled with those of scientists. For modern Europeans, building an empire was a scientific project, while setting up a scientific discipline was an imperial project.
When the Muslims conquered India, they did not bring along archaeologists to systematically study Indian history, anthropologists to study Indian cultures, geologists to study Indian soils, or zoologists to study Indian fauna. When the British conquered India, they did all of these things. On 10 April 1802 the Great Survey of India was launched. It lasted sixty years. With the help of tens of thousands of native labourers, scholars and guides, the British carefully mapped the whole of India, marking borders, measuring distances, and even calculating for the first time the exact height of Mount Everest and the other Himalayan peaks. The British explored the military resources of Indian provinces and the location of their gold mines, but they also took the trouble to collect information about rare Indian spiders, to catalogue colourful butterflies, to trace the ancient origins of extinct Indian languages, and to dig up forgotten ruins.
Mohenjo-daro was one of the chief cities of the Indus Valley civilisation, which flourished in the third millennium BC and was destroyed around 1900 BC. None of India’s pre-British rulers – neither the Mauryas, nor the Guptas, nor the Delhi sultans, nor the great Mughals – had given the ruins a second glance. But a British archaeological survey took notice of the site in 1922. A British team then excavated it, and discovered the first great civilisation of India, which no Indian had been aware of.
Another telling example of British scientific curiosity was the deciphering of cuneiform script. This was the main script used throughout the Middle East for close to 3,000 years, but the last person able to read it probably died sometime in the early first millennium AD. Since then, inhabitants of the region frequently encountered cuneiform inscriptions on monuments, steles, ancient ruins and broken pots. But they had no idea how to read the weird, angular scratches and, as far as we know, they never tried. Cuneiform came to the attention of Europeans in 1618, when the Spanish ambassador in Persia went sightseeing in the ruins of ancient Persepolis, where he saw inscriptions that nobody could explain to him. News of the unknown script spread among European savants and piqued their curiosity. In 1657 European scholars published the first transcription of a cuneiform text from Persepolis. More and more transcriptions followed, and for close to two centuries scholars in the West tried to decipher them. None succeeded.
In the 1830s, a British officer named Henry Rawlinson was sent to Persia to help the shah train his army in the European style. In his spare time Rawlinson travelled around Persia and one day he was led by local guides to a cliff in the Zagros Mountains and shown the huge Behistun Inscription. About fifteen metres high and twenty-five metres wide, it had been etched high up on the cliff face on the command of King Darius I sometime around 500 BC. It was written in cuneiform script in three languages: Old Persian, Elamite and Babylonian. The inscription was well known to the local population, but nobody could read it. Rawlinson became convinced that if he could decipher the writing it would enable him and other scholars to read the numerous inscriptions and texts that were at the time being discovered all over the Middle East, opening a door into an ancient and forgotten world.
The first step in deciphering the lettering was to produce an accurate transcription that could be sent back to Europe. Rawlinson defied death to do so, scaling the steep cliff to copy the strange letters. He hired several locals to help him, most notably a Kurdish boy who climbed to the most inaccessible parts of the cliff in order to copy the upper portion of the inscription. In 1847 the project was completed, and a full and accurate copy was sent to Europe.
Rawlinson did not rest on his laurels. As an army officer, he had military and political missions to carry out, but whenever he had a spare moment he puzzled over the secret script. He tried one method after another and finally managed to decipher the Old Persian part of the inscription. This was easiest, since Old Persian was not that different from modern Persian, which Rawlinson knew well. An understanding of the Old Persian section gave him the key he needed to unlock the secrets of the Elamite and Babylonian sections. The great door swung open, and out came a rush of ancient but lively voices – the bustle of Sumerian bazaars, the proclamations of Assyrian kings, the arguments of Babylonian bureaucrats. Without the efforts of modern European imperialists such as Rawlinson, we would not have known much about the fate of the ancient Middle Eastern empires.
Another notable imperialist scholar was William Jones. Jones arrived in India in September 1783 to serve as a judge in the Supreme Court of Bengal. He was so captivated by the wonders of India that within less than six months of his arrival he had founded the Asiatic Society. This academic organisation was devoted to studying the cultures, histories and societies of Asia, and in particular those of India. Within two years Jones published his observations on the Sanskrit language, which pioneered the science of comparative linguistics.
In his publications Jones pointed out surprising similarities between Sanskrit, an ancient Indian language that became the sacred tongue of Hindu ritual, and the Greek and Latin languages, as well as similarities between all these languages and Gothic, Celtic, Old Persian, German, French and English. Thus in Sanskrit, ‘mother’ is ‘matar’, in Latin it is ‘mater’, and in Old Celtic it is ‘mathir’. Jones surmised that all these languages must share a common origin, developing from a now-forgotten ancient ancestor. He was thus the first to identify what later came to be called the Indo-European family of languages.
Jones’ study was an important milestone not merely due to his bold (and accurate) hypotheses, but also because of the orderly methodology that he developed to compare languages. It was adopted by other scholars, enabling them systematically to study the development of all the world’s languages.
Linguistics received enthusiastic imperial support. The European empires believed that in order to govern effectively they must know the languages and cultures of their subjects. British officers arriving in India were supposed to spend up to three years in a Calcutta college, where they studied Hindu and Muslim law alongside English law; Sanskrit, Urdu and Persian alongside Greek and Latin; and Tamil, Bengali and Hindustani culture alongside mathematics, economics and geography. The study of linguistics provided invaluable help in understanding the structure and grammar of local languages.
Thanks to the work of people like William Jones and Henry Rawlinson, the European conquerors knew their empires very well. Far better, indeed, than any previous conquerors, or even than the native population itself. Their superior knowledge had obvious practical advantages. Without such knowledge, it is unlikely that a ridiculously small number of Britons could have succeeded in governing, oppressing and exploiting so many hundreds of millions of Indians for two centuries. Throughout the nineteenth and early twentieth centuries, fewer than 5,000 British officials, about 40,000–70,000 British soldiers, and perhaps another 100,000 British business people, hangers-on, wives and children were sufficient to conquer and rule up to 300 million Indians.9
Yet these practical advantages were not the only reason why empires financed the study of linguistics, botany, geography and history. No less important was the fact that science gave the empires ideological justification. Modern Europeans came to believe that acquiring new knowledge was always good. The fact that the empires produced a constant stream of new knowledge branded them as progressive and positive enterprises. Even today, histories of sciences such as geography, archaeology and botany cannot avoid crediting the European empires, at least indirectly. Histories of botany have little to say about the suffering of the Aboriginal Australians, but they usually find some kind words for James Cook and Joseph Banks.
Furthermore, the new knowledge accumulated by the empires made it possible, at least in theory, to benefit the conquered populations and bring them the benefits of ‘progress’ – to provide them with medicine and education, to build railroads and canals, to ensure justice and prosperity. Imperialists claimed that their empires were not vast enterprises of exploitation but rather altruistic projects conducted for the sake of the non-European races – in Rudyard Kipling’s words, ‘the White Man’s burden’:
Take up the White Man’s burden –
Send forth the best ye breed –
Go bind your sons to exile
To serve your captives’ need;
To wait in heavy harness,
On fluttered folk and wild –
Your new-caught, sullen peoples,
Half-devil and half-child.
Of course, the facts often belied this myth. The British conquered Bengal, the richest province of India, in 1764. The new rulers were interested in little except enriching themselves. They adopted a disastrous economic policy that a few years later led to the outbreak of the Great Bengal Famine. It began in 1769, reached catastrophic levels in 1770, and lasted until 1773. About 10 million Bengalis, a third of the province’s population, died in the calamity.10
In truth, neither the narrative of oppression and exploitation nor that of ‘The White Man’s Burden’ completely matches the facts. The European empires did so many different things on such a large scale, that you can find plenty of examples to support whatever you want to say about them. You think that these empires were evil monstrosities that spread death, oppression and injustice around the world? You could easily fill an encyclopedia with their crimes. You want to argue that they in fact improved the conditions of their subjects with new medicines, better economic conditions and greater security? You could fill another encyclopedia with their achievements. Due to their close cooperation with science, these empires wielded so much power and changed the world to such an extent that perhaps they cannot be simply labelled as good or evil. They created the world as we know it, including the ideologies we use in order to judge them.
But science was also used by imperialists to more sinister ends. Biologists, anthropologists and even linguists provided scientific proof that Europeans are superior to all other races, and consequently have the right (if not perhaps the duty) to rule over them. After William Jones argued that all Indo-European languages descend from a single ancient language many scholars were eager to discover who the speakers of that language had been. They noticed that the earliest Sanskrit speakers, who had invaded India from Central Asia more than 3,000 years ago, had called themselves Arya. The speakers of the earliest Persian language called themselves Airiia. European scholars consequently surmised that the people who spoke the primordial language that gave birth to both Sanskrit and Persian (as well as to Greek, Latin, Gothic and Celtic) must have called themselves Aryans. Could it be a coincidence that those who founded the magnificent Indian, Persian, Greek and Roman civilisations were all Aryans?
Next, British, French and German scholars wedded the linguistic theory about the industrious Aryans to Darwin’s theory of natural selection and posited that the Aryans were not just a linguistic group but a biological entity – a race. And not just any race, but a master race of tall, light-haired, blue-eyed, hard-working, and super-rational humans who emerged from the mists of the north to lay the foundations of culture throughout the world. Regrettably, the Aryans who invaded India and Persia intermarried with the local natives they found in these lands, losing their light complexions and blond hair, and with them their rationality and diligence. The civilisations of India and Persia consequently declined. In Europe, on the other hand, the Aryans preserved their racial purity. This is why Europeans had managed to conquer the world, and why they were fit to rule it – provided they took precautions not to mix with inferior races.
Such racist theories, prominent and respectable for many decades, have become anathema among scientists and politicians alike. People continue to conduct a heroic struggle against racism without noticing that the battlefront has shifted, and that the place of racism in imperial ideology has now been replaced by ‘culturism’. There is no such word, but it’s about time we coined it. Among today’s elites, assertions about the contrasting merits of diverse human groups are almost always couched in terms of historical differences between cultures rather than biological differences between races. We no longer say, ‘It’s in their blood.’ We say, ‘It’s in their culture.’
Thus European right-wing parties which oppose Muslim immigration usually take care to avoid racial terminology. Marine le Pen’s speechwriters would have been shown the door on the spot had they suggested that the leader of the Front National go on television to declare that, ‘We don’t want those inferior Semites to dilute our Aryan blood and spoil our Aryan civilisation.’ Instead, the French Front National, the Dutch Party for Freedom, the Alliance for the Future of Austria and their like tend to argue that Western culture, as it has evolved in Europe, is characterised by democratic values, tolerance and gender equality, whereas Muslim culture, which evolved in the Middle East, is characterised by hierarchical politics, fanaticism and misogyny. Since the two cultures are so different, and since many Muslim immigrants are unwilling (and perhaps unable) to adopt Western values, they should not be allowed to enter, lest they foment internal conflicts and corrode European democracy and liberalism.
Such culturist arguments are fed by scientific studies in the humanities and social sciences that highlight the so-called clash of civilisations and the fundamental differences between different cultures. Not all historians and anthropologists accept these theories or support their political usages. But whereas biologists today have an easy time disavowing racism, simply explaining that the biological differences between present-day human populations are trivial, it is harder for historians and anthropologists to disavow culturism. After all, if the differences between human cultures are trivial, why should we pay historians and anthropologists to study them?
Scientists have provided the imperial project with practical knowledge, ideological justification and technological gadgets. Without this contribution it is highly questionable whether Europeans could have conquered the world. The conquerors returned the favour by providing scientists with information and protection, supporting all kinds of strange and fascinating projects and spreading the scientific way of thinking to the far corners of the earth. Without imperial support, it is doubtful whether modern science would have progressed very far. There are very few scientific disciplines that did not begin their lives as servants to imperial growth and that do not owe a large proportion of their discoveries, collections, buildings and scholarships to the generous help of army officers, navy captains and imperial governors.
This is obviously not the whole story. Science was supported by other institutions, not just by empires. And the European empires rose and flourished thanks also to factors other than science. Behind the meteoric rise of both science and empire lurks one particularly important force: capitalism. Were it not for businessmen seeking to make money, Columbus would not have reached America, James Cook would not have reached Australia, and Neil Armstrong would never have taken that small step on the surface of the moon.16

The Capitalist Creed

MONEY HAS BEEN ESSENTIAL BOTH FOR building empires and for promoting science. But is money the ultimate goal of these undertakings, or perhaps just a dangerous necessity?
It is not easy to grasp the true role of economics in modern history. Whole volumes have been written about how money founded states and ruined them, opened new horizons and enslaved millions, moved the wheels of industry and drove hundreds of species into extinction. Yet to understand modern economic history, you really need to understand just a single word. The word is growth. For better or worse, in sickness and in health, the modern economy has been growing like a hormone-soused teenager. It eats up everything it can find and puts on inches faster than you can count.
For most of history the economy stayed much the same size. Yes, global production increased, but this was due mostly to demographic expansion and the settlement of new lands. Per capita production remained static. But all that changed in the modern age. In 1500, global production of goods and services was equal to about $250 billion; today it hovers around $60 trillion. More importantly, in 1500, annual per capita production averaged $550, while today every man, woman and child produces, on the average, $8,800 a year.1 What accounts for this stupendous growth?
Economics is a notoriously complicated subject. To make things easier, let’s imagine a simple example.
Samuel Greedy, a shrewd financier, founds a bank in El Dorado, California.
A. A. Stone, an up-and-coming contractor in El Dorado, finishes his first big job, receiving payment in cash to the tune of $1 million. He deposits this sum in Mr Greedy’s bank. The bank now has $1 million in capital.
In the meantime, Jane McDoughnut, an experienced but impecunious El Dorado chef, thinks she sees a business opportunity – there’s no really good bakery in her part of town. But she doesn’t have enough money of her own to buy a proper facility complete with industrial ovens, sinks, knives and pots. She goes to the bank, presents her business plan to Greedy, and persuades him that it’s a worthwhile investment. He issues her a $1 million loan, by crediting her account in the bank with that sum.
McDoughnut now hires Stone, the contractor, to build and furnish her bakery. His price is $1,000,000.
When she pays him, with a cheque drawn on her account, Stone deposits it in his account in the Greedy bank.
So how much money does Stone have in his bank account? Right, $2 million.
How much money, cash, is actually located in the bank’s safe? Yes, $1 million.
It doesn’t stop there. As contractors are wont to do, two months into the job Stone informs McDoughnut that, due to unforeseen problems and expenses, the bill for constructing the bakery will actually be $2 million. Mrs McDoughnut is not pleased, but she can hardly stop the job in the middle. So she pays another visit to the bank, convinces Mr Greedy to give her an additional loan, and he puts another $1 million in her account. She transfers the money to the contractor’s account.
How much money does Stone have in his account now? He’s got $3 million.
But how much money is actually sitting in the bank? Still just $1 million. In fact, the same $1 million that’s been in the bank all along.
Current US banking law permits the bank to repeat this exercise seven more times. The contractor would eventually have $10 million in his account, even though the bank still has but $1 million in its vaults. Banks are allowed to loan $10 for every dollar they actually possess, which means that 90 per cent of all the money in our bank accounts is not covered by actual coins and notes.2 If all of the account holders at Barclays Bank suddenly demand their money, Barclays will promptly collapse (unless the government steps in to save it). The same is true of Lloyds, Deutsche Bank, Citibank, and all other banks in the world.
It sounds like a giant Ponzi scheme, doesn’t it? But if it’s a fraud, then the entire modern economy is a fraud. The fact is, it’s not a deception, but rather a tribute to the amazing abilities of the human imagination. What enables banks – and the entire economy – to survive and flourish is our trust in the future. This trust is the sole backing for most of the money in the world.
In the bakery example, the discrepancy between the contractor’s account statement and the amount of money actually in the bank is Mrs McDoughnut’s bakery. Mr Greedy has put the bank’s money into the asset, trusting that one day it would be profitable. The bakery hasn’t baked a loaf of bread yet, but McDoughnut and Greedy anticipate that a year hence it will be selling thousands of loaves, rolls, cakes and cookies each day, at a handsome profit. Mrs McDoughnut will then be able to repay her loan, with interest. If at that point Mr Stone decides to withdraw his savings, Greedy will be able to come up with the cash. The entire enterprise is thus founded on trust in an imaginary future – the trust that the entrepreneur and the banker have in the bakery of their dreams, along with the contractor’s trust in the future solvency of the bank.
We’ve already seen that money is an astounding thing because it can represent myriad different objects and convert anything into almost anything else. However, before the modern era this ability was limited. In most cases, money could represent and convert only things that actually existed in the present. This imposed a severe limitation on growth, since it made it very hard to finance new enterprises.
Consider our bakery again. Could McDoughnut get it built if money could represent only tangible objects? No. In the present, she has a lot of dreams, but no tangible resources. The only way she could get her bakery built would be to find a contractor willing to work today and receive payment in a few years’ time, if and when the bakery starts making money. Alas, such contractors are rare breeds. So our entrepreneur is in a bind. Without a bakery, she can’t bake cakes. Without cakes, she can’t make money. Without money, she can’t hire a contractor. Without a contractor, she has no bakery.
Humankind was trapped in this predicament for thousands of years. As a result, economies remained frozen. The way out of the trap was discovered only in the modern era, with the appearance of a new system based on trust in the future. In it, people agreed to represent imaginary goods – goods that do not exist in the present – with a special kind of money they called ‘credit’. Credit enables us to build the present at the expense of the future. It’s founded on the assumption that our future resources are sure to be far more abundant than our present resources. A host of new and wonderful opportunities open up if we can build things in the present using future income.
If credit is such a wonderful thing, why did nobody think of it earlier? Of course they did. Credit arrangements of one kind or another have existed in all known human cultures, going back at least to ancient Sumer. The problem in previous eras was not that no one had the idea or knew how to use it. It was that people seldom wanted to extend much credit because they didn’t trust that the future would be better than the present. They generally believed that times past had been better than their own times and that the future would be worse, or at best much the same. To put that in economic terms, they believed that the total amount of wealth was limited, if not dwindling. People therefore considered it a bad bet to assume that they personally, or their kingdom, or the entire world, would be producing more wealth ten years down the line. Business looked like a zero-sum game. Of course, the profits of one particular bakery might rise, but only at the expense of the bakery next door. Venice might flourish, but only by impoverishing Genoa. The king of England might enrich himself, but only by robbing the king of France. You could cut the pie in many different ways, but it never got any bigger.
That’s why many cultures concluded that making bundles of money was sinful. As Jesus said, ‘It is easier for a camel to pass through the eye of a needle than for a rich man to enter into the kingdom of God’ (Matthew 19:24). If the pie is static, and I have a big part of it, then I must have taken somebody else’s slice. The rich were obliged to do penance for their evil deeds by giving some of their surplus wealth to charity.

The Entrepreneur’s Dilemma

If the global pie stayed the same size, there was no margin for credit. Credit is the difference between today’s pie and tomorrows pie. If the pie stays the same, why extend credit? It would be an unacceptable risk unless you believed that the baker or king asking for your money might be able to steal a slice from a competitor. So it was hard to get a loan in the premodern world, and when you got one it was usually small, short-term, and subject to high interest rates. Upstart entrepreneurs thus found it difficult to open new bakeries and great kings who wanted to build palaces or wage wars had no choice but to raise the necessary funds through high taxes and tariffs.

The Magic Circle of the Modern Economy

That was fine for kings (as long as their subjects remained docile), but a scullery maid who had a great idea for a bakery and wanted to move up in the world generally could only dream of wealth while scrubbing down the royal kitchens floors.
It was lose-lose. Because credit was limited, people had trouble financing new businesses. Because there were few new businesses, the economy did not grow. Because it did not grow, people assumed it never would, and those who had capital were wary of extending credit. The expectation of stagnation fulfilled itself.
A Growing Pie

Then came the Scientific Revolution and the idea of progress. The idea of progress is built on the notion that if we admit our ignorance and invest resources in research, things can improve. This idea was soon translated into economic terms. Whoever believes in progress believes that geographical discoveries, technological inventions and organisational developments can increase the sum total of human production, trade and wealth. New trade routes in the Atlantic could flourish without ruining old routes in the Indian Ocean. New goods could be produced without reducing the production of old ones. For instance, one could open a new bakery specialising in chocolate cakes and croissants without causing bakeries specialising in bread to go bust. Everybody would simply develop new tastes and eat more. I can be wealthy without your becoming poor; I can be obese without your dying of hunger. The entire global pie can grow.
Over the last 500 years the idea of progress convinced people to put more and more trust in the future. This trust created credit; credit brought real economic growth; and growth strengthened the trust in the future and opened the way for even more credit. It didn’t happen overnight – the economy behaved more like a roller coaster than a balloon. But over the long run, with the bumps evened out, the general direction was unmistakable. Today, there is so much credit in the world that governments, business corporations and private individuals easily obtain large, long-term and low-interest loans that far exceed current income.

The Economic History of the World in a Nutshell

The belief in the growing global pie eventually turned revolutionary. In 1776 the Scottish economist Adam Smith published The Wealth of Nations, probably the most important economics manifesto of all time. In the eighth chapter of its first volume, Smith made the following novel argument: when a landlord, a weaver, or a shoemaker has greater profits than he needs to maintain his own family, he uses the surplus to employ more assistants, in order to further increase his profits. The more profits he has, the more assistants he can employ. It follows that an increase in the profits of private entrepreneurs is the basis for the increase in collective wealth and prosperity.
This may not strike you as very original, because we all live in a capitalist world that takes Smith’s argument for granted. We hear variations on this theme every day in the news. Yet Smith’s claim that the selfish human urge to increase private profits is the basis for collective wealth is one of the most revolutionary ideas in human history – revolutionary not just from an economic perspective, but even more so from a moral and political perspective. What Smith says is, in fact, that greed is good, and that by becoming richer I benefit everybody, not just myself. Egoism is altruism.
Smith taught people to think about the economy as a ‘win-win situation’, in which my profits are also your profits. Not only can we both enjoy a bigger slice of pie at the same time, but the increase in your slice depends upon the increase in my slice. If I am poor, you too will be poor since I cannot buy your products or services. If I am rich, you too will be enriched since you can now sell me something. Smith denied the traditional contradiction between wealth and morality, and threw open the gates of heaven for the rich. Being rich meant being moral. In Smiths story, people become rich not by despoiling their neighbours, but by increasing the overall size of the pie. And when the pie grows, everyone benefits. The rich are accordingly the most useful and benevolent people in society, because they turn the wheels of growth for everyone’s advantage.
All this depends, however, on the rich using their profits to open new factories and hire new employees, rather than wasting them on non-productive activities. Smith therefore repeated like a mantra the maxim that ‘When profits increase, the landlord or weaver will employ more assistants’ and not ‘When profits increase, Scrooge will hoard his money in a chest and take it out only to count his coins.’ A crucial part of the modern capitalist economy was the emergence of a new ethic, according to which profits ought to be reinvested in production. This brings about more profits, which are again reinvested in production, which brings more profits, et cetera ad infinitum. Investments can be made in many ways: enlarging the factory, conducting scientific research, developing new products. Yet all these investments must somehow increase production and translate into larger profits. In the new capitalist creed, the first and most sacred commandment is: ‘The profits of production must be reinvested in increasing production.’
That’s why capitalism is called ‘capitalism’. Capitalism distinguishes ‘capital’ from mere ‘wealth’. Capital consists of money, goods and resources that are invested in production. Wealth, on the other hand, is buried in the ground or wasted on unproductive activities. A pharaoh who pours resources into a non-productive pyramid is not a capitalist. A pirate who loots a Spanish treasure fleet and buries a chest full of glittering coins on the beach of some Caribbean island is not a capitalist. But a hard-working factory hand who reinvests part of his income in the stock market is.
The idea that ‘The profits of production must be reinvested in increasing production’ sounds trivial. Yet it was alien to most people throughout history. In premodern times, people believed that production was more or less constant. So why reinvest your profits if production won’t increase by much, no matter what you do? Thus medieval noblemen espoused an ethic of generosity and conspicuous consumption. They spent their revenues on tournaments, banquets, palaces and wars, and on charity and monumental cathedrals. Few tried to reinvest profits in increasing their manors’ output, developing better kinds of wheat, or looking for new markets.
 
In the modern era, the nobility has been overtaken by a new elite whose members are true believers in the capitalist creed. The new capitalist elite is made up not of dukes and marquises, but of board chairmen, stock traders and industrialists. These magnates are far richer than the medieval nobility, but they are far less interested in extravagant consumption, and they spend a much smaller part of their profits on non-productive activities.
Medieval noblemen wore colourful robes of gold and silk, and devoted much of their time to attending banquets, carnivals and glamorous tournaments. In comparison, modern CEOs don dreary uniforms called suits that afford them all the panache of a flock of crows, and they have little time for festivities. The typical venture capitalist rushes from one business meeting to another, trying to figure out where to invest his capital and following the ups and downs of the stocks and bonds he owns. True, his suits might be Versace and he might get to travel in a private jet, but these expenses are nothing compared to what he invests in increasing human production.
It’s not just Versace-clad business moguls who invest to increase productivity. Ordinary folk and government agencies think along similar lines. How many dinner conversations in modest neighbourhoods sooner or later bog down in interminable debate about whether it is better to invest one’s savings in the stock market, bonds or property? Governments too strive to invest their tax revenues in productive enterprises that will increase future income – for example, building a new port could make it easier for factories to export their products, enabling them to make more taxable income, thereby increasing the government’s future revenues. Another government might prefer to invest in education, on the grounds that educated people form the basis for the lucrative high-tech industries, which pay lots of taxes without needing extensive port facilities.
Capitalism began as a theory about how the economy functions. It was both descriptive and prescriptive – it offered an account of how money worked and promoted the idea that reinvesting profits in production leads to fast economic growth. But capitalism gradually became far more than just an economic doctrine. It now encompasses an ethic – a set of teachings about how people should behave, educate their children and even think. Its principal tenet is that economic growth is the supreme good, or at least a proxy for the supreme good, because justice, freedom and even happiness all depend on economic growth. Ask a capitalist how to bring justice and political freedom to a place like Zimbabwe or Afghanistan, and you are likely to get a lecture on how economic affluence and a thriving middle class are essential for stable democratic institutions, and about the need therefore to inculcate Afghan tribesmen in the values of free enterprise, thrift and self-reliance.
This new religion has had a decisive influence on the development of modern science, too. Scientific research is usually funded by either governments or private businesses. When capitalist governments and businesses consider investing in a particular scientific project, the first questions are usually, ‘Will this project enable us to increase production and profits? Will it produce economic growth?’ A project that can’t clear these hurdles has little chance of finding a sponsor. No history of modern science can leave capitalism out of the picture.
Conversely, the history of capitalism is unintelligible without taking science into account. Capitalisms belief in perpetual economic growth flies in the face of almost everything we know about the universe. A society of wolves would be extremely foolish to believe that the supply of sheep would keep on growing indefinitely. The human economy has nevertheless managed to grow exponentially throughout the modern era, thanks only to the fact that scientists come up with another discovery or gadget every few years – such as the continent of America, the internal combustion engine, or genetically engineered sheep. Banks and governments print money, but ultimately, it is the scientists who foot the bill.
Over the last few years, banks and governments have been frenziedly printing money. Everybody is terrified that the current economic crisis may stop the growth of the economy. So they are creating trillions of dollars, euros and yen out of thin air, pumping cheap credit into the system, and hoping that the scientists, technicians and engineers will manage to come up with something really big, before the bubble bursts. Everything depends on the people in the labs. New discoveries in fields such as biotechnology and nanotechnology could create entire new industries, whose profits could back the trillions of make-believe money that the banks and governments have created since 2008. If the labs do not fulfil these expectations before the bubble bursts, we are heading towards very rough times.
Columbus Searches for an Investor

Capitalism played a decisive role not only in the rise of modern science, but also in the emergence of European imperialism. And it was European imperialism that created the capitalist credit system in the first place. Of course, credit was not invented in modern Europe. It existed in almost all agricultural societies, and in the early modern period the emergence of European capitalism was closely linked to economic developments in Asia. Remember, too, that until the late eighteenth century, Asia was the world’s economic powerhouse, meaning that Europeans had far less capital at their disposal than the Chinese, Muslims or Indians.
However, in the sociopolitical systems of China, India and the Muslim world, credit played only a secondary role. Merchants and bankers in the markets of Istanbul, Isfahan, Delhi and Beijing may have thought along capitalist lines, but the kings and generals in the palaces and forts tended to despise merchants and mercantile thinking. Most non-European empires of the early modern era were established by great conquerors such as Nurhaci and Nader Shah, or by bureaucratic and military elites as in the Qing and Ottoman empires. Financing wars through taxes and plunder (without making fine distinctions between the two), they owed little to credit systems, and they cared even less about the interests of bankers and investors.
In Europe, on the other hand, kings and generals gradually adopted the mercantile way of thinking, until merchants and bankers became the ruling elite. The European conquest of the world was increasingly financed through credit rather than taxes, and was increasingly directed by capitalists whose main ambition was to receive maximum returns on their investments. The empires built by bankers and merchants in frock coats and top hats defeated the empires built by kings and noblemen in gold clothes and shining armour. The mercantile empires were simply much shrewder in financing their conquests. Nobody wants to pay taxes, but everyone is happy to invest.
In 1484 Christopher Columbus approached the king of Portugal with the proposal that he finance a fleet that would sail westward to find a new trade route to East Asia. Such explorations were a very risky and costly business. A lot of money was needed in order to build ships, buy supplies, and pay sailors and soldiers – and there was no guarantee that the investment would yield a return. The king of Portugal declined.
Like a present-day start-up entrepreneur, Columbus did not give up. He pitched his idea to other potential investors in Italy, France, England, and again in Portugal. Each time he was rejected. He then tried his luck with Ferdinand and Isabella, rulers of newly united Spain. He took on some experienced lobbyists, and with their help he managed to convince Queen Isabella to invest. As every school-child knows, Isabella hit the jackpot. Columbus’ discoveries enabled the Spaniards to conquer America, where they established gold and silver mines as well as sugar and tobacco plantations that enriched the Spanish kings, bankers and merchants beyond their wildest dreams.
A hundred years later, princes and bankers were willing to extend far more credit to Columbus’ successors, and they had more capital at their disposal, thanks to the treasures reaped from America. Equally important, princes and bankers had far more trust in the potential of exploration, and were more willing to part with their money. This was the magic circle of imperial capitalism: credit financed new discoveries; discoveries led to colonies; colonies provided profits; profits built trust; and trust translated into more credit. Nurhaci and Nader Shah ran out of fuel after a few thousand kilometres. Capitalist entrepreneurs only increased their financial momentum from conquest to conquest.
But these expeditions remained chancy affairs, so credit markets nevertheless remained quite cautious. Many expeditions returned to Europe empty-handed, having discovered nothing of value. The English, for instance, wasted a lot of capital in fruitless attempts to discover a north-western passage to Asia through the Arctic. Many other expeditions didn’t return at all. Ships hit icebergs, foundered in tropical storms, or fell victim to pirates. In order to increase the number of potential investors and reduce the risk they incurred, Europeans turned to limited liability joint-stock companies. Instead of a single investor betting all his money on a single rickety ship, the joint-stock company collected money from a large number of investors, each risking only a small portion of his capital. The risks were thereby curtailed, but no cap was placed on the profits. Even a small investment in the right ship could turn you into a millionaire.
Decade by decade, western Europe witnessed the development of a sophisticated financial system that could raise large amounts of credit on short notice and put it at the disposal of private entrepreneurs and governments. This system could finance explorations and conquests far more efficiently than any kingdom or empire. The new-found power of credit can be seen in the bitter struggle between Spain and the Netherlands. In the sixteenth century, Spain was the most powerful state in Europe, holding sway over a vast global empire. It ruled much of Europe, huge chunks of North and South America, the Philippine Islands, and a string of bases along the coasts of Africa and Asia. Every year, fleets heavy with American and Asian treasures returned to the ports of Seville and Cadiz. The Netherlands was a small and windy swamp, devoid of natural resources, a small corner of the king of Spain’s dominions.
In 1568 the Dutch, who were mainly Protestant, revolted against their Catholic Spanish overlord. At first the rebels seemed to play the role of Don Quixote, courageously tilting at invincible windmills. Yet within eighty years the Dutch had not only secured their independence from Spain, but had managed to replace the Spaniards and their Portuguese allies as masters of the ocean highways, build a global Dutch empire, and become the richest state in Europe.
The secret of Dutch success was credit. The Dutch burghers, who had little taste for combat on land, hired mercenary armies to fight the Spanish for them. The Dutch themselves meanwhile took to the sea in ever-larger fleets. Mercenary armies and cannon-brandishing fleets cost a fortune, but the Dutch were able to finance their military expeditions more easily than the mighty Spanish Empire because they secured the trust of the burgeoning European financial system at a time when the Spanish king was carelessly eroding its trust in him. Financiers extended the Dutch enough credit to set up armies and fleets, and these armies and fleets gave the Dutch control of world trade routes, which in turn yielded handsome profits. The profits allowed the Dutch to repay the loans, which strengthened the trust of the financiers. Amsterdam was fast becoming not only one of the most important ports of Europe, but also the continent’s financial Mecca.
How exactly did the Dutch win the trust of the financial system? Firstly, they were sticklers about repaying their loans on time and in full, making the extension of credit less risky for lenders. Secondly, their country’s judicial system enjoyed independence and protected private rights – in particular private property rights. Capital trickles away from dictatorial states that fail to defend private individuals and their property. Instead, it flows into states upholding the rule of law and private property.
Imagine that you are the son of a solid family of German financiers. Your father sees an opportunity to expand the business by opening branches in major European cities. He sends you to Amsterdam and your younger brother to Madrid, giving you each 10,000 gold coins to invest. Your brother lends his start-up capital at interest to the king of Spain, who needs it to raise an army to fight the king of France. You decide to lend yours to a Dutch merchant, who wants to invest in scrubland on the southern end of a desolate island called Manhattan, certain that property values there will skyrocket as the Hudson River turns into a major trade artery. Both loans are to be repaid within a year.
The year passes. The Dutch merchant sells the land he’s bought at a handsome markup and repays your money with the interest he promised. Your father is pleased. But your little brother in Madrid is getting nervous. The war with France ended well for the king of Spain, but he has now embroiled himself in a conflict with the Turks. He needs every penny to finance the new war, and thinks this is far more important than repaying old debts. Your brother sends letters to the palace and asks friends with connections at court to intercede, but to no avail. Not only has your brother not earned the promised interest – he’s lost the principal. Your father is not pleased.
Now, to make matters worse, the king sends a treasury official to your brother to tell him, in no uncertain terms, that he expects to receive another loan of the same size, forthwith. Your brother has no money to lend. He writes home to Dad, trying to persuade him that this time the king will come through. The paterfamilias has a soft spot for his youngest, and agrees with a heavy heart. Another 10,000 gold coins disappear into the Spanish treasury, never to be seen again. Meanwhile in Amsterdam, things are looking bright. You make more and more loans to enterprising Dutch merchants, who repay them promptly and in full. But your luck does not hold indefinitely. One of your usual clients has a hunch that wooden clogs are going to be the next fashion craze in Paris, and asks you for a loan to set up a footwear emporium in the French capital. You lend him the money, but unfortunately the clogs don’t catch on with the French ladies, and the disgruntled merchant refuses to repay the loan.
Your father is furious, and tells both of you it is time to unleash the lawyers. Your brother files suit in Madrid against the Spanish monarch, while you file suit in Amsterdam against the erstwhile wooden-shoe wizard. In Spain, the law courts are subservient to the king – the judges serve at his pleasure and fear punishment if they do not do his will. In the Netherlands, the courts are a separate branch of government, not dependent on the country’s burghers and princes. The court in Madrid throws out your brother’s suit, while the court in Amsterdam finds in your favour and puts a lien on the clog-merchant’s assets to force him to pay up. Your father has learned his lesson. Better to do business with merchants than with kings, and better to do it in Holland than in Madrid.
And your brother’s travails are not over. The king of Spain desperately needs more money to pay his army. He’s sure that your father has cash to spare. So he brings trumped-up treason charges against your brother. If he doesn’t come up with 20,000 gold coins forthwith, he’ll get cast into a dungeon and rot there until he dies.
Your father has had enough. He pays the ransom for his beloved son, but swears never to do business in Spain again. He closes his Madrid branch and relocates your brother to Rotterdam. Two branches in Holland now look like a really good idea. He hears that even Spanish capitalists are smuggling their fortunes out of their country. They, too, realise that if they want to keep their money and use it to gain more wealth, they are better off investing it where the rule of law prevails and where private property is respected – in the Netherlands, for example.
In such ways did the king of Spain squander the trust of investors at the same time that Dutch merchants gained their confidence. And it was the Dutch merchants – not the Dutch state – who built the Dutch Empire. The king of Spain kept on trying to finance and maintain his conquests by raising unpopular taxes from a disgruntled populace. The Dutch merchants financed conquest by getting loans, and increasingly also by selling shares in their companies that entitled their holders to receive a portion of the company’s profits. Cautious investors who would never have given their money to the king of Spain, and who would have thought twice before extending credit to the Dutch government, happily invested fortunes in the Dutch joint-stock companies that were the mainstay of the new empire.
If you thought a company was going to make a big profit but it had already sold all its shares, you could buy some from people who owned them, probably for a higher price than they originally paid. If you bought shares and later discovered that the company was in dire straits, you could try to unload your stock for a lower price. The resulting trade in company shares led to the establishment in most major European cities of stock exchanges, places where the shares of companies were traded.
The most famous Dutch joint-stock company, the Vereenigde Oostindische Compagnie, or VOC for short, was chartered in 1602, just as the Dutch were throwing off Spanish rule and the boom of Spanish artillery could still be heard not far from Amsterdam’s ramparts. VOC used the money it raised from selling shares to build ships, send them to Asia, and bring back Chinese, Indian and Indonesian goods. It also financed military actions taken by company ships against competitors and pirates. Eventually VOC money financed the conquest of Indonesia.
Indonesia is the world’s biggest archipelago. Its thousands upon thousands of islands were ruled in the early seventeenth century by hundreds of kingdoms, principalities, sultanates and tribes. When VOC merchants first arrived in Indonesia in 1603, their aims were strictly commercial. However, in order to secure their commercial interests and maximise the profits of the shareholders, VOC merchants began to fight against local potentates who charged inflated tariffs, as well as against European competitors. VOC armed its merchant ships with cannons; it recruited European, Japanese, Indian and Indonesian mercenaries; and it built forts and conducted full-scale battles and sieges. This enterprise may sound a little strange to us, but in the early modern age it was common for private companies to hire not only soldiers, but also generals and admirals, cannons and ships, and even entire off-the-shelf armies. The international community took this for granted and didn’t raise an eyebrow when a private company established an empire.
Island after island fell to VOC mercenaries and a large part of Indonesia became a VOC colony. VOC ruled Indonesia for close to 200 years. Only in 1800 did the Dutch state assume control of Indonesia, making it a Dutch national colony for the following 150 years. Today some people warn that twenty-first-century corporations are accumulating too much power. Early modern history shows just how far that can go if businesses are allowed to pursue their self-interest unchecked.
While VOC operated in the Indian Ocean, the Dutch West Indies Company, or WIC, plied the Atlantic. In order to control trade on the important Hudson River, WIC built a settlement called New Amsterdam on an island at the river’s mouth. The colony was threatened by Indians and repeatedly attacked by the British, who eventually captured it in 1664. The British changed its name to New York. The remains of the wall built by WIC to defend its colony against Indians and British are today paved over by the world’s most famous street – Wall Street.
As the seventeenth century wound to an end, complacency and costly continental wars caused the Dutch to lose not only New York, but also their place as Europe’s financial and imperial engine. The vacancy was hotly contested by France and Britain. At first France seemed to be in a far stronger position. It was bigger than Britain, richer, more populous, and it possessed a larger and more experienced army. Yet Britain managed to win the trust of the financial system whereas France proved itself unworthy. The behaviour of the French crown was particularly notorious during what was called the Mississippi Bubble, the largest financial crisis of eighteenth-century Europe. That story also begins with an empire-building joint-stock company.
In 1717 the Mississippi Company, chartered in France, set out to colonise the lower Mississippi valley, establishing the city of New Orleans in the process. To finance its ambitious plans, the company, which had good connections at the court of King Louis XV, sold shares on the Paris stock exchange. John Law, the company’s director, was also the governor of the central bank of France. Furthermore, the king had appointed him controller-general of finances, an office roughly equivalent to that of a modern finance minister. In 1717 the lower Mississippi valley offered few attractions besides swamps and alligators, yet the Mississippi Company spread tales of fabulous riches and boundless opportunities. French aristocrats, businessmen and the stolid members of the urban bourgeoisie fell for these fantasies, and Mississippi share prices skyrocketed. Initially, shares were offered at 500 livres apiece. On 1 August 1719, shares traded at 2,750 livres. By 30 August, they were worth 4,100 livres, and on 4 September, they reached 5,000 livres. On 2 December the price of a Mississippi share crossed the threshold of 10,000 livres. Euphoria swept the streets of Paris. People sold all their possessions and took huge loans in order to buy Mississippi shares. Everybody believed they’d discovered the easy way to riches.

39. New Amsterdam in 1660, at the tip of Manhattan Island. The settlement’s protective wall is today paved over by Wall Street.

A few days later, the panic began. Some speculators realised that the share prices were totally unrealistic and unsustainable. They figured that they had better sell while stock prices were at their peak. As the supply of shares available rose, their price declined. When other investors saw the price going down, they also wanted to get out quick. The stock price plummeted further, setting off an avalanche. In order to stabilise prices, the central bank of France – at the direction of its governor, John Law – bought up Mississippi shares, but it could not do so for ever. Eventually it ran out of money. When this happened, the controller-general of finances, the same John Law, authorised the printing of more money in order to buy additional shares. This placed the entire French financial system inside the bubble. And not even this financial wizardry could save the day. The price of Mississippi shares dropped from 10,000 livres back to 1,000 livres, and then collapsed completely, and the shares lost every sou of their worth. By now, the central bank and the royal treasury owned a huge amount of worthless stock and had no money. The big speculators emerged largely unscathed – they had sold in time. Small investors lost everything, and many committed suicide.
The Mississippi Bubble was one of history’s most spectacular financial crashes. The royal French financial system never recuperated fully from the blow. The way in which the Mississippi Company used its political clout to manipulate share prices and fuel the buying frenzy caused the public to lose faith in the French banking system and in the financial wisdom of the French king. Louis XV found it more and more difficult to raise credit. This became one of the chief reasons that the overseas French Empire fell into British hands. While the British could borrow money easily and at low interest rates, France had difficulties securing loans, and had to pay high interest on them. In order to finance his growing debts, the king of France borrowed more and more money at higher and higher interest rates. Eventually, in the 1780s, Louis XVI, who had ascended to the throne on his grandfather’s death, realised that half his annual budget was tied to servicing the interest on his loans, and that he was heading towards bankruptcy. Reluctantly, in 1789, Louis XVI convened the Estates General, the French parliament that had not met for a century and a half, in order to find a solution to the crisis. Thus began the French Revolution.
While the French overseas empire was crumbling, the British Empire was expanding rapidly. Like the Dutch Empire before it, the British Empire was established and run largely by private joint-stock companies based in the London stock exchange. The first English settlements in North America were established in the early seventeenth century by joint-stock companies such as the London Company, the Plymouth Company, the Dorchester Company and the Massachusetts Company.
The Indian subcontinent too was conquered not by the British state, but by the mercenary army of the British East India Company. This company outperformed even the VOC. From its headquarters in Leadenhall Street, London, it ruled a mighty Indian empire for about a century, maintaining a huge military force of up to 350,000 soldiers, considerably outnumbering the armed forces of the British monarchy. Only in 1858 did the British crown nationalise India along with the company’s private army. Napoleon made fun of the British, calling them a nation of shopkeepers. Yet these shopkeepers defeated Napoleon himself, and their empire was the largest the world has ever seen.
In the Name of Capital

The nationalisation of Indonesia by the Dutch crown (1800) and of India by the British crown (1858) hardly ended the embrace of capitalism and empire. On the contrary, the connection only grew stronger during the nineteenth century. Joint-stock companies no longer needed to establish and govern private colonies – their managers and large shareholders now pulled the strings of power in London, Amsterdam and Paris, and they could count on the state to look after their interests. As Marx and other social critics quipped, Western governments were becoming a capitalist trade union.
The most notorious example of how governments did the bidding of big money was the First Opium War, fought between Britain and China (1840–42). In the first half of the nineteenth century, the British East India Company and sundry British business people made fortunes by exporting drugs, particularly opium, to China. Millions of Chinese became addicts, debilitating the country both economically and socially. In the late 1830s the Chinese government issued a ban on drug trafficking, but British drug merchants simply ignored the law. Chinese authorities began to confiscate and destroy drug cargos. The drug cartels had close connections in Westminster and Downing Street – many MPs and Cabinet ministers in fact held stock in the drug companies – so they pressured the government to take action.
In 1840 Britain duly declared war on China in the name of ‘free trade’. It was a walkover. The overconfident Chinese were no match for Britain’s new wonder weapons – steamboats, heavy artillery, rockets and rapid-fire rifles. Under the subsequent peace treaty, China agreed not to constrain the activities of British drug merchants and to compensate them for damages inflicted by the Chinese police. Furthermore, the British demanded and received control of Hong Kong, which they proceeded to use as a secure base for drug trafficking (Hong Kong remained in British hands until 1997). In the late nineteenth century, about 40 million Chinese, a tenth of the country’s population, were opium addicts.3
Egypt, too, learned to respect the long arm of British capitalism. During the nineteenth century, French and British investors lent huge sums to the rulers of Egypt, first in order to finance the Suez Canal project, and later to fund far less successful enterprises. Egyptian debt swelled, and European creditors increasingly meddled in Egyptian affairs. In 1881 Egyptian nationalists had had enough and rebelled. They declared a unilateral abrogation of all foreign debt. Queen Victoria was not amused. A year later she dispatched her army and navy to the Nile and Egypt remained a British protectorate until after World War Two.
These were hardly the only wars fought in the interests of investors. In fact, war itself could become a commodity, just like opium. In 1821 the Greeks rebelled against the Ottoman Empire. The uprising aroused great sympathy in liberal and romantic circles in Britain – Lord Byron, the poet, even went to Greece to fight alongside the insurgents. But London financiers saw an opportunity as well. They proposed to the rebel leaders the issue of tradable Greek Rebellion Bonds on the London stock exchange. The Greeks would promise to repay the bonds, plus interest, if and when they won their independence. Private investors bought bonds to make a profit, or out of sympathy for the Greek cause, or both. The value of Greek Rebellion Bonds rose and fell on the London stock exchange in tempo with military successes and failures on the battlefields of Hellas. The Turks gradually gained the upper hand. With a rebel defeat imminent, the bondholders faced the prospect of losing their trousers. The bondholders’ interest was the national interest, so the British organised an international fleet that, in 1827, sank the main Ottoman flotilla in the Battle of Navarino. After centuries of subjugation, Greece was finally free. But freedom came with a huge debt that the new country had no way of repaying. The Greek economy was mortgaged to British creditors for decades to come.
The bear hug between capital and politics has had far-reaching implications for the credit market. The amount of credit in an economy is determined not only by purely economic factors such as the discovery of a new oil field or the invention of a new machine, but also by political events such as regime changes or more ambitious foreign policies. After the Battle of Navarino, British capitalists were more willing to invest their money in risky overseas deals. They had seen that if a foreign debtor refused to repay loans, Her Majesty’s army would get their money back.
This is why today a country’s credit rating is far more important to its economic well-being than are its natural resources. Credit ratings indicate the probability that a country will pay its debts. In addition to purely economic data, they take into account political, social and even cultural factors. An oil-rich country cursed with a despotic government, endemic warfare and a corrupt judicial system will usually receive a low credit rating. As a result, it is likely to remain relatively poor since it will not be able to raise the necessary capital to make the most of its oil bounty. A country devoid of natural resources, but which enjoys peace, a fair judicial system and a free government is likely to receive a high credit rating. As such, it may be able to raise enough cheap capital to support a good education system and foster a flourishing high-tech industry.
The Cult of the Free Market

Capital and politics influence each other to such an extent that their relations are hotly debated by economists, politicians and the general public alike. Ardent capitalists tend to argue that capital should be free to influence politics, but politics should not be allowed to influence capital. They argue that when governments interfere in the markets, political interests cause them to make unwise investments that result in slower growth. For example, a government may impose heavy taxation on industrialists and use the money to give lavish unemployment benefits, which are popular with voters. In the view of many business people, it would be far better if the government left the money with them. They would use it, they claim, to open new factories and hire the unemployed.
In this view, the wisest economic policy is to keep politics out of the economy, reduce taxation and government regulation to a minimum, and allow market forces free rein to take their course. Private investors, unencumbered by political considerations, will invest their money where they can get the most profit, so the way to ensure the most economic growth – which will benefit everyone, industrialists and workers – is for the government to do as little as possible. This free-market doctrine is today the most common and influential variant of the capitalist creed. The most enthusiastic advocates of the free market criticise military adventures abroad with as much zeal as welfare programmes at home. They offer governments the same advice that Zen masters offer initiates: just do nothing.
But in its extreme form, belief in the free market is as naïve as belief in Santa Claus. There simply is no such thing as a market free of all political bias. The most important economic resource is trust in the future, and this resource is constantly threatened by thieves and charlatans. Markets by themselves offer no protection against fraud, theft and violence. It is the job of political systems to ensure trust by legislating sanctions against cheats and to establish and support police forces, courts and jails which will enforce the law. When kings fail to do their jobs and regulate the markets properly, it leads to loss of trust, dwindling credit and economic depression. That was the lesson taught by the Mississippi Bubble of 1719, and anyone who forgot it was reminded by the US housing bubble of 2007, and the ensuing credit crunch and recession.
The Capitalist Hell

There is an even more fundamental reason why it’s dangerous to give markets a completely free rein. Adam Smith taught that the shoemaker would use his surplus to employ more assistants. This implies that egoistic greed is beneficial for all, since profits are utilised to expand production and hire more employees.
Yet what happens if the greedy shoemaker increases his profits by paying employees less and increasing their work hours? The standard answer is that the free market would protect the employees. If our shoemaker pays too little and demands too much, the best employees would naturally abandon him and go to work for his competitors. The tyrant shoemaker would find himself left with the worst labourers, or with no labourers at all. He would have to mend his ways or go out of business. His own greed would compel him to treat his employees well.
This sounds bulletproof in theory, but in practice the bullets get through all too easily. In a completely free market, unsupervised by kings and priests, avaricious capitalists can establish monopolies or collude against their workforces. If there is a single corporation controlling all shoe factories in a country, or if all factory owners conspire to reduce wages simultaneously, then the labourers are no longer able to protect themselves by switching jobs.
Even worse, greedy bosses might curtail the workers’ freedom of movement through debt peonage or slavery. At the end of the Middle Ages, slavery was almost unknown in Christian Europe. During the early modern period, the rise of European capitalism went hand in hand with the rise of the Atlantic slave trade. Unrestrained market forces, rather than tyrannical kings or racist ideologues, were responsible for this calamity.
When the Europeans conquered America, they opened gold and silver mines and established sugar, tobacco and cotton plantations. These mines and plantations became the mainstay of American production and export. The sugar plantations were particularly important. In the Middle Ages, sugar was a rare luxury in Europe. It was imported from the Middle East at prohibitive prices and used sparingly as a secret ingredient in delicacies and snake-oil medicines. After large sugar plantations were established in America, ever-increasing amounts of sugar began to reach Europe. The price of sugar dropped and Europe developed an insatiable sweet tooth. Entrepreneurs met this need by producing huge quantities of sweets: cakes, cookies, chocolate, candy, and sweetened beverages such as cocoa, coffee and tea. The annual sugar intake of the average Englishman rose from near zero in the early seventeenth century to around eight kilograms in the early nineteenth century.
However, growing cane and extracting its sugar was a labour-intensive business. Few people wanted to work long hours in malaria-infested sugar fields under a tropical sun. Contract labourers would have produced a commodity too expensive for mass consumption. Sensitive to market forces, and greedy for profits and economic growth, European plantation owners switched to slaves.
From the sixteenth to the nineteenth centuries, about 10 million African slaves were imported to America. About 70 per cent of them worked on the sugar plantations. Labour conditions were abominable. Most slaves lived a short and miserable life, and millions more died during wars waged to capture slaves or during the long voyage from inner Africa to the shores of America. All this so that Europeans could enjoy their sweet tea and candy – and sugar barons could enjoy huge profits.
The slave trade was not controlled by any state or government. It was a purely economic enterprise, organised and financed by the free market according to the laws of supply and demand. Private slave-trading companies sold shares on the Amsterdam, London and Paris stock exchanges. Middle-class Europeans looking for a good investment bought these shares. Relying on this money, the companies bought ships, hired sailors and soldiers, purchased slaves in Africa, and transported them to America. There they sold the slaves to the plantation owners, using the proceeds to purchase plantation products such as sugar, cocoa, coffee, tobacco, cotton and rum. They returned to Europe, sold the sugar and cotton for a good price, and then sailed to Africa to begin another round. The shareholders were very pleased with this arrangement. Throughout the eighteenth century the yield on slave-trade investments was about 6 per cent a year – they were extremely profitable, as any modern consultant would be quick to admit.
This is the fly in the ointment of free-market capitalism. It cannot ensure that profits are gained in a fair way, or distributed in a fair manner. On the contrary, the craving to increase profits and production blinds people to anything that might stand in the way. When growth becomes a supreme good, unrestricted by any other ethical considerations, it can easily lead to catastrophe. Some religions, such as Christianity and Nazism, have killed millions out of burning hatred. Capitalism has killed millions out of cold indifference coupled with greed. The Atlantic slave trade did not stem from racist hatred towards Africans. The individuals who bought the shares, the brokers who sold them, and the managers of the slave-trade companies rarely thought about the Africans. Nor did the owners of the sugar plantations. Many owners lived far from their plantations, and the only information they demanded were neat ledgers of profits and losses.
It is important to remember that the Atlantic slave trade was not a single aberration in an otherwise spotless record. The Great Bengal Famine, discussed in the previous chapter, was caused by a similar dynamic – the British East India Company cared more about its profits than about the lives of 10 million Bengalis. VOC’s military campaigns in Indonesia were financed by upstanding Dutch burghers who loved their children, gave to charity, and enjoyed good music and fine art, but had no regard for the suffering of the inhabitants of Java, Sumatra and Malacca. Countless other crimes and misdemeanours accompanied the growth of the modern economy in other parts of the planet.
The nineteenth century brought no improvement in the ethics of capitalism. The Industrial Revolution that swept through Europe enriched the bankers and capital-owners, but condemned millions of workers to a life of abject poverty. In the European colonies things were even worse. In 1876, King Leopold II of Belgium set up a nongovernmental humanitarian organisation with the declared aim of exploring Central Africa and fighting the slave trade along the Congo River. It was also charged with improving conditions for the inhabitants of the region by building roads, schools and hospitals. In 1885 the European powers agreed to give this organisation control of 2.3 million square kilometres in the Congo basin. This territory, seventy-five times the size of Belgium, was henceforth known as the Congo Free State. Nobody asked the opinion of the territory’s 20–30 million inhabitants.
Within a short time the humanitarian organisation became a business enterprise whose real aim was growth and profit. The schools and hospitals were forgotten, and the Congo basin was instead filled with mines and plantations, run by mostly Belgian officials who ruthlessly exploited the local population. The rubber industry was particularly notorious. Rubber was fast becoming an industrial staple, and rubber export was the Congo’s most important source of income. The African villagers who collected the rubber were required to provide higher and higher quotas. Those who failed to deliver their quota were punished brutally for their ‘laziness’. Their arms were chopped off and occasionally entire villages were massacred. According to the most moderate estimates, between 1885 and 1908 the pursuit of growth and profits cost the lives of 6 million individuals (at least 20 per cent of the Congo’s population). Some estimates reach up to 10 million deaths.4
After 1908, and especially after 1945, capitalist greed was somewhat reined in, not least due to the fear of Communism. Yet inequities are still rampant. The economic pie of 2014 is far larger than the pie of 1500, but it is distributed so unevenly that many African peasants and Indonesian labourers return home after a hard day’s work with less food than did their ancestors 500 years ago. Much like the Agricultural Revolution, so too the growth of the modern economy might turn out to be a colossal fraud. The human species and the global economy may well keep growing, but many more individuals may live in hunger and want.
Capitalism has two answers to this criticism. First, capitalism has created a world that nobody but a capitalist is capable of running. The only serious attempt to manage the world differently – Communism – was so much worse in almost every conceivable way that nobody has the stomach to try again. In 8500 BC one could cry bitter tears over the Agricultural Revolution, but it was too late to give up agriculture. Similarly, we may not like capitalism, but we cannot live without it.
The second answer is that we just need more patience – paradise, the capitalists promise, is right around the corner. True, mistakes have been made, such as the Atlantic slave trade and the exploitation of the European working class. But we have learned our lesson, and if we just wait a little longer and allow the pie to grow a little bigger, everybody will receive a fatter slice. The division of spoils will never be equitable, but there will be enough to satisfy every man, woman and child – even in the Congo.
There are, indeed, some positive signs. At least when we use purely material criteria – such as life expectancy, child mortality and calorie intake – the standard of living of the average human in 2014 is significantly higher than it was in 1914, despite the exponential growth in the number of humans.
Yet can the economic pie grow indefinitely? Every pie requires raw materials and energy. Prophets of doom warn that sooner or later Homo sapiens will exhaust the raw materials and energy of planet Earth. And what will happen then?17

The Wheels of Industry

THE MODERN ECONOMY GROWS THANKS to our trust in the future and to the willingness of capitalists to reinvest their profits in production. Yet that does not suffice. Economic growth also requires energy and raw materials, and these are finite. When and if they run out, the entire system will collapse.
But the evidence provided by the past is that they are finite only in theory. Counter-intuitively, while humankind’s use of energy and raw materials has mushroomed in the last few centuries, the amounts available for our exploitation have actually increased. Whenever a shortage of either has threatened to slow economic growth, investments have flowed into scientific and technological research. These have invariably produced not only more efficient ways of exploiting existing resources, but also completely new types of energy and materials.
Consider the vehicle industry. Over the last 300 years, humankind has manufactured billions of vehicles – from carts and wheelbarrows, to trains, cars, supersonic jets and space shuttles. One might have expected that such a prodigious effort would have exhausted the energy sources and raw materials available for vehicle production, and that today we would be scraping the bottom of the barrel. Yet the opposite is the case. Whereas in 1700 the global vehicle industry relied overwhelmingly on wood and iron, today it has at its disposal a cornucopia of new-found materials such as plastic, rubber, aluminium and titanium, none of which our ancestors even knew about. Whereas in 1700 carts were built mainly by the muscle power of carpenters and smiths, today the machines in Toyota and Boeing factories are powered by petroleum combustion engines and nuclear power stations. A similar revolution has swept almost all other fields of industry. We call it the Industrial Revolution.
For millennia prior to the Industrial Revolution, humans already knew how to make use of a large variety of energy sources. They burned wood in order to smelt iron, heat houses and bake cakes. Sailing ships harnessed wind power to move around, and watermills captured the flow of rivers to grind grain. Yet all these had clear limits and problems. Trees were not available everywhere, the wind didn’t always blow when you needed it, and water power was only useful if you lived near a river.
An even bigger problem was that people didn’t know how to convert one type of energy into another. They could harness the movement of wind and water to sail ships and push millstones, but not to heat water or smelt iron. Conversely, they could not use the heat energy produced by burning wood to make a millstone move. Humans had only one machine capable of performing such energy conversion tricks: the body. In the natural process of metabolism, the bodies of humans and other animals burn organic fuels known as food and convert the released energy into the movement of muscles. Men, women and beasts could consume grain and meat, burn up their carbohydrates and fats, and use the energy to haul a rickshaw or pull a plough.
Since human and animal bodies were the only energy conversion device available, muscle power was the key to almost all human activities. Human muscles built carts and houses, ox muscles ploughed fields, and horse muscles transported goods. The energy that fuelled these organic muscle-machines came ultimately from a single source – plants. Plants in their turn obtained their energy from the sun. By the process of photosynthesis, they captured solar energy and packed it into organic compounds. Almost everything people did throughout history was fuelled by solar energy that was captured by plants and converted into muscle power.
Human history was consequently dominated by two main cycles: the growth cycles of plants and the changing cycles of solar energy (day and night, summer and winter). When sunlight was scarce and when wheat fields were still green, humans had little energy. Granaries were empty, tax collectors were idle, soldiers found it difficult to move and fight, and kings tended to keep the peace. When the sun shone brightly and the wheat ripened, peasants harvested the crops and filled the granaries. Tax collectors hurried to take their share. Soldiers flexed their muscles and sharpened their swords. Kings convened councils and planned their next campaigns. Everyone was fuelled by solar energy – captured and packaged in wheat, rice and potatoes.
The Secret in the Kitchen

Throughout these long millennia, day in and day out, people stood face to face with the most important invention in the history of energy production – and failed to notice it. It stared them in the eye every time a housewife or servant put up a kettle to boil water for tea or put a pot full of potatoes on the stove. The minute the water boiled, the lid of the kettle or the pot jumped. Heat was being converted to movement. But jumping pot lids were an annoyance, especially if you forgot the pot on the stove and the water boiled over. Nobody saw their real potential.
A partial breakthrough in converting heat into movement followed the invention of gunpowder in ninth-century China. At first, the idea of using gunpowder to propel projectiles was so counter-intuitive that for centuries gunpowder was used primarily to produce fire bombs. But eventually – perhaps after some bomb expert ground gunpowder in a mortar only to have the pestle shoot out with force – guns made their appearance. About 600 years passed between the invention of gunpowder and the development of effective artillery.
Even then, the idea of converting heat into motion remained so counter-intuitive that another three centuries went by before people invented the next machine that used heat to move things around. The new technology was born in British coal mines. As the British population swelled, forests were cut down to fuel the growing economy and make way for houses and fields. Britain suffered from an increasing shortage of firewood. It began burning coal as a substitute. Many coal seams were located in waterlogged areas, and flooding prevented miners from accessing the lower strata of the mines. It was a problem looking for a solution. Around 1700, a strange noise began reverberating around British mineshafts. That noise – harbinger of the Industrial Revolution – was subtle at first, but it grew louder and louder with each passing decade until it enveloped the entire world in a deafening cacophony. It emanated from a steam engine.
There are many types of steam engines, but they all share one common principle. You burn some kind of fuel, such as coal, and use the resulting heat to boil water, producing steam. As the steam expands it pushes a piston. The piston moves, and anything that is connected to the piston moves with it. You have converted heat into movement! In eighteenth-century British coal mines, the piston was connected to a pump that extracted water from the bottom of the mineshafts. The earliest engines were incredibly inefficient. You needed to burn a huge load of coal in order to pump out even a tiny amount of water. But in the mines coal was plentiful and close at hand, so nobody cared.
In the decades that followed, British entrepreneurs improved the efficiency of the steam engine, brought it out of the mineshafts, and connected it to looms and gins. This revolutionised textile production, making it possible to produce ever-larger quantities of cheap textiles. In the blink of an eye, Britain became the workshop of the world. But even more importantly, getting the steam engine out of the mines broke an important psychological barrier. If you could burn coal in order to move textile looms, why not use the same method to move other things, such as vehicles?
In 1825, a British engineer connected a steam engine to a train of mine wagons full of coal. The engine drew the wagons along an iron rail some twenty kilometres long from the mine to the nearest harbour. This was the first steam-powered locomotive in history. Clearly, if steam could be used to transport coal, why not other goods? And why not even people? On 15 September 1830, the first commercial railway line was opened, connecting Liverpool with Manchester. The trains moved under the same steam power that had previously pumped water and moved textile looms. A mere twenty years later, Britain had tens of thousands of kilometres of railway tracks.1
Henceforth, people became obsessed with the idea that machines and engines could be used to convert one type of energy into another. Any type of energy, anywhere in the world, might be harnessed to whatever need we had, if we could just invent the right machine. For example, when physicists realised that an immense amount of energy is stored within atoms, they immediately started thinking about how this energy could be released and used to make electricity, power submarines and annihilate cities. Six hundred years passed between the moment Chinese alchemists discovered gunpowder and the moment Turkish cannon pulverised the walls of Constantinople. Only forty years passed between the moment Einstein determined that any kind of mass could be converted into energy – that’s what E = mc2 means – and the moment atom bombs obliterated Hiroshima and Nagasaki and nuclear power stations mushroomed all over the globe.
Another crucial discovery was the internal combustion engine, which took little more than a generation to revolutionise human transportation and turn petroleum into liquid political power. Petroleum had been known for thousands of years, and was used to waterproof roofs and lubricate axles. Yet until just a century ago nobody thought it was useful for much more than that. The idea of spilling blood for the sake of oil would have seemed ludicrous. You might fight a war over land, gold, pepper or slaves, but not oil.
The career of electricity was more startling yet. Two centuries ago electricity played no role in the economy, and was used at most for arcane scientific experiments and cheap magic tricks. A series of inventions turned it into our universal genie in a lamp. We flick our fingers and it prints books and sews clothes, keeps our vegetables fresh and our ice cream frozen, cooks our dinners and executes our criminals, registers our thoughts and records our smiles, lights up our nights and entertains us with countless television shows. Few of us understand how electricity does all these things, but even fewer can imagine life without it.
An Ocean of Energy

At heart, the Industrial Revolution has been a revolution in energy conversion. It has demonstrated again and again that there is no limit to the amount of energy at our disposal. Or, more precisely, that the only limit is set by our ignorance. Every few decades we discover a new energy source, so that the sum total of energy at our disposal just keeps growing.
Why are so many people afraid that we are running out of energy? Why do they warn of disaster if we exhaust all available fossil fuels? Clearly the world does not lack energy. All we lack is the knowledge necessary to harness and convert it to our needs. The amount of energy stored in all the fossil fuel on earth is negligible compared to the amount that the sun dispenses every day, free of charge. Only a tiny proportion of the sun’s energy reaches us, yet it amounts to 3,766,800 exajoules of energy each year (a joule is a unit of energy in the metric system, about the amount you expend to lift a small apple one yard straight up; an exajoule is a billion billion joules – that’s a lot of apples).2 All the world’s plants capture only about 3,000 of those solar exajoules through the process of photosynthesis.3 All human activities and industries put together consume about 500 exajoules annually, equivalent to the amount of energy earth receives from the sun in just ninety minutes.4 And that’s only solar energy. In addition, we are surrounded by other enormous sources of energy, such as nuclear energy and gravitational energy, the latter most evident in the power of the ocean tides caused by the moon’s pull on the earth.
Prior to the Industrial Revolution, the human energy market was almost completely dependent on plants. People lived alongside a green energy reservoir carrying 3,000 exajoules a year, and tried to pump as much of its energy as they could. Yet there was a clear limit to how much they could extract. During the Industrial Revolution, we came to realise that we are actually living alongside an enormous ocean of energy, one holding billions upon billions of exajoules of potential power. All we need to do is invent better pumps.
*

Learning how to harness and convert energy effectively solved the other problem that slows economic growth – the scarcity of raw materials. As humans worked out how to harness large quantities of cheap energy, they could begin exploiting previously inaccessible deposits of raw materials (for example, mining iron in the Siberian wastelands), or transporting raw materials from ever more distant locations (for example, supplying a British textile mill with Australian wool). Simultaneously, scientific breakthroughs enabled humankind to invent completely new raw materials, such as plastic, and discover previously unknown natural materials, such as silicon and aluminium.
Chemists discovered aluminium only in the 1820s, but separating the metal from its ore was extremely difficult and costly. For decades, aluminium was much more expensive than gold. In the 1860S, Emperor Napoleon III of France commissioned aluminium cutlery to be laid out for his most distinguished guests. Less important visitors had to make do with the gold knives and forks.5 But at the end of the nineteenth century chemists discovered a way to extract immense amounts of cheap aluminium, and current global production stands at 30 million tons per year. Napoleon III would be surprised to hear that his subjects’ descendants use cheap disposable aluminium foil to wrap their sandwiches and put away their leftovers.
Two thousand years ago, when people in the Mediterranean basin suffered from dry skin they smeared olive oil on their hands. Today, they open a tube of hand cream. Below is the list of ingredients of a simple modern hand cream that I bought at a local store:
deionised water, stearic acid, glycerin, caprylic/caprictiglyceride, propylene glycol, isopropyl myristate, panax ginseng root extract, fragrance, cetyl alcohol, triethanolamine, dimeticone, arctostaphylos uva-ursi leaf extract, magnesium ascorbyl phosphate, imidazolidinyl urea, methyl paraben, camphor, propyl paraben, hydroxyisohexyl 3-cyclohexene carboxaldehyde, hydroxyl-citronellal, linalool, butylphenyl methylproplonal, citronnellol, limonene, geraniol.
Almost all of these ingredients were invented or discovered in the last two centuries.
During World War One, Germany was placed under blockade and suffered severe shortages of raw materials, in particular saltpetre, an essential ingredient in gunpowder and other explosives. The most important saltpetre deposits were in Chile and India; there were none at all in Germany. True, saltpetre could be replaced by ammonia, but that was expensive to produce as well. Luckily for the Germans, one of their fellow citizens, a Jewish chemist named Fritz Haber, had discovered in 1908 a process for producing ammonia literally out of thin air. When war broke out, the Germans used Haber’s discovery to commence industrial production of explosives using air as a raw material. Some scholars believe that if it hadn’t been for Haber’s discovery, Germany would have been forced to surrender long before November 1918.6 The discovery won Haber (who during the war also pioneered the use of poison gas in battle) a Nobel Prize in 1918. In chemistry, not in peace.
Life on the Conveyor Belt

The Industrial Revolution yielded an unprecedented combination of cheap and abundant energy and cheap and abundant raw materials. The result was an explosion in human productivity. The explosion was felt first and foremost in agriculture. Usually, when we think of the Industrial Revolution, we think of an urban landscape of smoking chimneys, or the plight of exploited coal miners sweating in the bowels of the earth. Yet the Industrial Revolution was above all else the Second Agricultural Revolution.
During the last 200 years, industrial production methods became the mainstay of agriculture. Machines such as tractors began to undertake tasks that were previously performed by muscle power, or not performed at all. Fields and animals became vastly more productive thanks to artificial fertilisers, industrial insecticides and an entire arsenal of hormones and medications. Refrigerators, ships and aeroplanes have made it possible to store produce for months, and transport it quickly and cheaply to the other side of the world. Europeans began to dine on fresh Argentinian beef and Japanese sushi.
Even plants and animals were mechanised. Around the time that Homo sapiens was elevated to divine status by humanist religions, farm animals stopped being viewed as living creatures that could feel pain and distress, and instead came to be treated as machines. Today these animals are often mass-produced in factory-like facilities, their bodies shaped in accordance with industrial needs. They pass their entire lives as cogs in a giant production line, and the length and quality of their existence is determined by the profits and losses of business corporations. Even when the industry takes care to keep them alive, reasonably healthy and well fed, it has no intrinsic interest in the animals’ social and psychological needs (except when these have a direct impact on production).
Egg-laying hens, for example, have a complex world of behavioural needs and drives. They feel strong urges to scout their environment, forage and peck around, determine social hierarchies, build nests and groom themselves. But the egg industry often locks the hens inside tiny coops, and it is not uncommon for it to squeeze four hens to a cage, each given a floor space of about twenty-five by twenty-two centimetres. The hens receive sufficient food, but they are unable to claim a territory, build a nest or engage in other natural activities. Indeed, the cage is so small that hens are often unable even to flap their wings or stand fully erect.
Pigs are among the most intelligent and inquisitive of mammals, second perhaps only to the great apes. Yet industrialised pig farms routinely confine nursing sows inside such small crates that they are literally unable to turn around (not to mention walk or forage). The sows are kept in these crates day and night for four weeks after giving birth. Their offspring are then taken away to be fattened up and the sows are impregnated with the next litter of piglets.
Many dairy cows live almost all their allotted years inside a small enclosure; standing, sitting and sleeping in their own urine and excrement. They receive their measure of food, hormones and medications from one set of machines, and get milked every few hours by another set of machines. The cow in the middle is treated as little more than a mouth that takes in raw materials and an udder that produces a commodity. Treating living creatures possessing complex emotional worlds as if they were machines is likely to cause them not only physical discomfort, but also much social stress and psychological frustration.7

40. Chicks on a conveyor belt in a commercial hatchery. Male chicks and imperfect female chicks are picked off the conveyor belt and are then asphyxiated in gas chambers, dropped into automatic shredders, or simply thrown into the rubbish, where they are crushed to death. Hundreds of millions of chicks die each year in such hatcheries.

Just as the Atlantic slave trade did not stem from hatred towards Africans, so the modern animal industry is not motivated by animosity. Again, it is fuelled by indifference. Most people who produce and consume eggs, milk and meat rarely stop to think about the fate of the chickens, cows or pigs whose flesh and emissions they are eating. Those who do think often argue that such animals are really little different from machines, devoid of sensations and emotions, incapable of suffering. Ironically, the same scientific disciplines which shape our milk machines and egg machines have lately demonstrated beyond reasonable doubt that mammals and birds have a complex sensory and emotional make-up. They not only feel physical pain, but can also suffer from emotional distress.
Evolutionary psychology maintains that the emotional and social needs of farm animals evolved in the wild, when they were essential for survival and reproduction. For example, a wild cow had to know how to form close relations with other cows and bulls, or else she could not survive and reproduce. In order to learn the necessary skills, evolution implanted in calves – as in the young of all other social mammals – a strong desire to play (playing is the mammalian way of learning social behaviour). And it implanted in them an even stronger desire to bond with their mothers, whose milk and care were essential for survival.
What happens if farmers now take a young calf, separate her from her mother, put her in a closed cage, give her food, water and inoculations against diseases, and then, when she is old enough, inseminate her with bull sperm? From an objective perspective, this calf no longer needs either maternal bonding or playmates in order to survive and reproduce. But from a subjective perspective, the calf still feels a very strong urge to bond with her mother and to play with other calves. If these urges are not fulfilled, the calf suffers greatly. This is the basic lesson of evolutionary psychology: a need shaped in the wild continues to be felt subjectively even if it is no longer really necessary for survival and reproduction. The tragedy of industrial agriculture is that it takes great care of the objective needs of animals, while neglecting their subjective needs.
The truth of this theory has been known at least since the 1950s, when the American psychologist Harry Harlow studied the development of monkeys. Harlow separated infant monkeys from their mothers several hours after birth. The monkeys were isolated inside cages, and then raised by dummy mothers. In each cage, Harlow placed two dummy mothers. One was made of metal wires, and was fitted with a milk bottle from which the infant monkey could suck. The other was made of wood covered with cloth, which made it resemble a real monkey mother, but it provided the infant monkey with no material sustenance whatsoever. It was assumed that the infants would cling to the nourishing metal mother rather than to the barren cloth one.
To Harlow’s surprise, the infant monkeys showed a marked preference for the cloth mother, spending most of their time with her. When the two mothers were placed in close proximity, the infants held on to the cloth mother even while they reached over to suck milk from the metal mother. Harlow suspected that perhaps the infants did so because they were cold. So he fitted an electric bulb inside the wire mother, which now radiated heat. Most of the monkeys, except for the very young ones, continued to prefer the cloth mother.

41. One of Harlow’s orphaned monkeys clings to the cloth mother even while sucking milk from the metal mother.

Follow-up research showed that Harlow’s orphaned monkeys grew up to be emotionally disturbed even though they had received all the nourishment they required. They never fitted into monkey society, had difficulties communicating with other monkeys, and suffered from high levels of anxiety and aggression. The conclusion was inescapable: monkeys must have psychological needs and desires that go beyond their material requirements, and if these are not fulfilled, they will suffer greatly. Harlow’s infant monkeys preferred to spend their time in the hands of the barren cloth mother because they were looking for an emotional bond and not only for milk. In the following decades, numerous studies showed that this conclusion applies not only to monkeys, but to other mammals, as well as birds. At present, millions of farm animals are subjected to the same conditions as Harlow’s monkeys, as farmers routinely separate calves, kids and other youngsters from their mothers, to be raised in isolation.8
Altogether, tens of billions of farm animals live today as part of a mechanised assembly line, and about 50 billion of them are slaughtered annually. These industrial livestock methods have led to a sharp increase in agricultural production and in human food reserves. Together with the mechanisation of plant cultivation, industrial animal husbandry is the basis for the entire modern socio-economic order. Before the industrialisation of agriculture, most of the food produced in fields and farms was ‘wasted’ feeding peasants and farmyard animals. Only a small percentage was available to feed artisans, teachers, priests and bureaucrats. Consequently, in almost all societies peasants comprised more than 90 per cent of the population. Following the industrialisation of agriculture, a shrinking number of farmers was enough to feed a growing number of clerks and factory hands. Today in the United States, only 2 per cent of the population makes a living from agriculture, yet this 2 per cent produces enough not only to feed the entire US population, but also to export surpluses to the rest of the world.9 Without the industrialisation of agriculture the urban Industrial Revolution could never have taken place – there would not have been enough hands and brains to staff factories and offices.
As those factories and offices absorbed the billions of hands and brains that were released from fieldwork, they began pouring out an unprecedented avalanche of products. Humans now produce far more steel, manufacture much more clothing, and build many more structures than ever before. In addition, they produce a mind-boggling array of previously unimaginable goods, such as light bulbs, mobile phones, cameras and dishwashers. For the first time in human history, supply began to outstrip demand. And an entirely new problem was born: who is going to buy all this stuff?
The Age of Shopping

The modern capitalist economy must constantly increase production if it is to survive, like a shark that must swim or suffocate. Yet it’s not enough just to produce. Somebody must also buy the products, or industrialists and investors alike will go bust. To prevent this catastrophe and to make sure that people will always buy whatever new stuff industry produces, a new kind of ethic appeared: consumerism.
Most people throughout history lived under conditions of scarcity. Frugality was thus their watchword. The austere ethics of the Puritans and Spartans are but two famous examples. A good person avoided luxuries, never threw food away, and patched up torn trousers instead of buying a new pair. Only kings and nobles allowed themselves to renounce such values publicly and conspicuously flaunt their riches.
Consumerism sees the consumption of ever more products and services as a positive thing. It encourages people to treat themselves, spoil themselves, and even kill themselves slowly by overconsumption. Frugality is a disease to be cured. You don’t have to look far to see the consumer ethic in action – just read the back of a cereal box. Here’s a quote from a box of one of my favourite breakfast cereals, produced by an Israeli firm, Telma:
Sometimes you need a treat. Sometimes you need a little extra energy. There are times to watch your weight and times when you’ve just got to have something … right now! Telma offers a variety of tasty cereals just for you – treats without remorse.
The same package sports an ad for another brand of cereal called Health Treats:
Health Treats offers lots of grains, fruits and nuts for an experience that combines taste, pleasure and health. For an enjoyable treat in the middle of the day, suitable for a healthy lifestyle. A real treat with the wonderful taste of more [emphasis in the original].
Throughout most of history, people were likely to be have been repelled rather than attracted by such a text. They would have branded it as selfish, decadent and morally corrupt. Consumerism has worked very hard, with the help of popular psychology (‘Just do it!’) to convince people that indulgence is good for you, whereas frugality is self-oppression.
It has succeeded. We are all good consumers. We buy countless products that we don’t really need, and that until yesterday we didn’t know existed. Manufacturers deliberately design short-term goods and invent new and unnecessary models of perfectly satisfactory products that we must purchase in order to stay ‘in’. Shopping has become a favourite pastime, and consumer goods have become essential mediators in relationships between family members, spouses and friends. Religious holidays such as Christmas have become shopping festivals. In the United States, even Memorial Day – originally a solemn day for remembering fallen soldiers – is now an occasion for special sales. Most people mark this day by going shopping, perhaps to prove that the defenders of freedom did not die in vain.
The flowering of the consumerist ethic is manifested most clearly in the food market. Traditional agricultural societies lived in the awful shade of starvation. In the affluent world of today one of the leading health problems is obesity, which strikes the poor (who stuff themselves with hamburgers and pizzas) even more severely than the rich (who eat organic salads and fruit smoothies). Each year the US population spends more money on diets than the amount needed to feed all the hungry people in the rest of the world. Obesity is a double victory for consumerism. Instead of eating little, which will lead to economic contraction, people eat too much and then buy diet products – contributing to economic growth twice over.
How can we square the consumerist ethic with the capitalist ethic of the business person, according to which profits should not be wasted, and should instead be reinvested in production? It’s simple. As in previous eras, there is today a division of labour between the elite and the masses. In medieval Europe, aristocrats spent their money carelessly on extravagant luxuries, whereas peasants lived frugally, minding every penny. Today, the tables have turned. The rich take great care managing their assets and investments, while the less well heeled go into debt buying cars and televisions they don’t really need.
The capitalist and consumerist ethics are two sides of the same coin, a merger of two commandments. The supreme commandment of the rich is ‘Invest!’ The supreme commandment of the rest of us is ‘Buy!’
The capitalist-consumerist ethic is revolutionary in another respect. Most previous ethical systems presented people with a pretty tough deal. They were promised paradise, but only if they cultivated compassion and tolerance, overcame craving and anger, and restrained their selfish interests. This was too tough for most. The history of ethics is a sad tale of wonderful ideals that nobody can live up to. Most Christians did not imitate Christ, most Buddhists failed to follow Buddha, and most Confucians would have caused Confucius a temper tantrum.
In contrast, most people today successfully live up to the capitalist-consumerist ideal. The new ethic promises paradise on condition that the rich remain greedy and spend their time making more money, and that the masses give free rein to their cravings and passions – and buy more and more. This is the first religion in history whose followers actually do what they are asked to do. How, though, do we know that we’ll really get paradise in return? We’ve seen it on television.18

A Permanent Revolution

THE INDUSTRIAL REVOLUTION OPENED up new ways to convert energy and to produce goods, largely liberating humankind from its dependence on the surrounding ecosystem. Humans cut down forests, drained swamps, dammed rivers, flooded plains, laid down tens of thousands of kilometres of railroad tracks, and built skyscraping metropolises. As the world was moulded to fit the needs of Homo sapiens, habitats were destroyed and species went extinct. Our once green and blue planet is becoming a concrete and plastic shopping centre.
Today, the earths continents are home to almost 7 billion Sapiens. If you took all these people and put them on a large set of scales, their combined mass would be about 300 million tons. If you then took all our domesticated farmyard animals – cows, pigs, sheep and chickens – and placed them on an even larger set of scales, their mass would amount to about 700 million tons. In contrast, the combined mass of all surviving large wild animals – from porcupines and penguins to elephants and whales – is less than 100 million tons. Our children’s books, our iconography and our TV screens are still full of giraffes, wolves and chimpanzees, but the real world has very few of them left. There are about 80,000 giraffes in the world, compared to 1.5 billion cattle; only 200,000 wolves, compared to 400 million domesticated dogs; only 250,000 chimpanzees – in contrast to billions of humans. Humankind really has taken over the world.1
Ecological degradation is not the same as resource scarcity. As we saw in the previous chapter, the resources available to humankind are constantly increasing, and are likely to continue to do so. That’s why doomsday prophesies of resource scarcity are probably misplaced. In contrast, the fear of ecological degradation is only too well founded. The future may see Sapiens gaining control of a cornucopia of new materials and energy sources, while simultaneously destroying what remains of the natural habitat and driving most other species to extinction.
In fact, ecological turmoil might endanger the survival of Homo sapiens itself. Global warming, rising oceans and widespread pollution could make the earth less hospitable to our kind, and the future might consequently see a spiralling race between human power and human-induced natural disasters. As humans use their power to counter the forces of nature and subjugate the ecosystem to their needs and whims, they might cause more and more unanticipated and dangerous side effects. These are likely to be controllable only by even more drastic manipulations of the ecosystem, which would result in even worse chaos.
Many call this process ‘the destruction of nature’. But it’s not really destruction, it’s change. Nature cannot be destroyed. Sixty-five million years ago, an asteroid wiped out the dinosaurs, but in so doing opened the way forward for mammals. Today, humankind is driving many species into extinction and might even annihilate itself. But other organisms are doing quite well. Rats and cockroaches, for example, are in their heyday. These tenacious creatures would probably creep out from beneath the smoking rubble of a nuclear Armageddon, ready and able to spread their DNA. Perhaps 65 million years from now, intelligent rats will look back gratefully on the decimation wrought by humankind, just as we today can thank that dinosaur-busting asteroid.
Still, the rumours of our own extinction are premature. Since the Industrial Revolution, the world’s human population has burgeoned as never before. In 1700 the world was home to some 700 million humans. In 1800 there were 950 million of us. By 1900 we almost doubled our numbers to 1.6 billion. And by 2000 that quadrupled to 6 billion. Today there are just shy of 7 billion Sapiens.
Modern Time

While all these Sapiens have grown increasingly impervious to the whims of nature, they have become ever more subject to the dictates of modern industry and government. The Industrial Revolution opened the way to a long line of experiments in social engineering and an even longer series of unpremeditated changes in daily life and human mentality. One example among many is the replacement of the rhythms of traditional agriculture with the uniform and precise schedule of industry.
Traditional agriculture depended on cycles of natural time and organic growth. Most societies were unable to make precise time measurements, nor were they terribly interested in doing so. The world went about its business without clocks and timetables, subject only to the movements of the sun and the growth cycles of plants. There was no uniform working day, and all routines changed drastically from season to season. People knew where the sun was, and watched anxiously for portents of the rainy season and harvest time, but they did not know the hour and hardly cared about the year. If a lost time traveller popped up in a medieval village and asked a passerby, ‘What year is this?’ the villager would be as bewildered by the question as by the strangers ridiculous clothing.
In contrast to medieval peasants and shoemakers, modern industry cares little about the sun or the season. It sanctifies precision and uniformity. For example, in a medieval workshop each shoemaker made an entire shoe, from sole to buckle. If one shoemaker was late for work, it did not stall the others. However, in a modern footwear-factory assembly line, every worker mans a machine that produces just a small part of a shoe, which is then passed on to the next machine. If the worker who operates machine no. 5 has overslept, it stalls all the other machines. In order to prevent such calamities, everybody must adhere to a precise timetable. Each worker arrives at work at exactly the same time. Everybody takes their lunch break together, whether they are hungry or not. Everybody goes home when a whistle announces that the shift is over – not when they have finished their project.

42. Charlie Chaplin as a simple worker caught in the wheels of the industrial assembly line, from the film Modern Times (1936).

The Industrial Revolution turned the timetable and the assembly line into a template for almost all human activities. Shortly after factories imposed their time frames on human behaviour, schools too adopted precise timetables, followed by hospitals, government offices and grocery stores. Even in places devoid of assembly lines and machines, the timetable became king. If the shift at the factory ends at 5 p.m., the local pub had better be open for business by 5:02.
A crucial link in the spreading timetable system was public transportation. If workers needed to start their shift by 08:00, the train or bus had to reach the factory gate by 07:55. A few minutes’ delay would lower production and perhaps even lead to the lay-offs of the unfortunate latecomers. In 1784 a carriage service with a published schedule began operating in Britain. Its timetable specified only the hour of departure, not arrival. Back then, each British city and town had its own local time, which could differ from London time by up to half an hour. When it was 12:00 in London, it was perhaps 12:20 in Liverpool and 11:50 in Canterbury. Since there were no telephones, no radio or television, and no fast trains – who could know, and who cared?2
The first commercial train service began operating between Liverpool and Manchester in 1830. Ten years later, the first train timetable was issued. The trains were much faster than the old carriages, so the quirky differences in local hours became a severe nuisance. In 1847, British train companies put their heads together and agreed that henceforth all train timetables would be calibrated to Greenwich Observatory time, rather than the local times of Liverpool, Manchester or Glasgow. More and more institutions followed the lead of the train companies. Finally, in 1880, the British government took the unprecedented step of legislating that all timetables in Britain must follow Greenwich. For the first time in history, a country adopted a national time and obliged its population to live according to an artificial clock rather than local ones or sunrise-to-sunset cycles.
This modest beginning spawned a global network of timetables, synchronised down to the tiniest fractions of a second. When the broadcast media – first radio, then television – made their debut, they entered a world of timetables and became its main enforcers and evangelists. Among the first things radio stations broadcast were time signals, beeps that enabled far-flung settlements and ships at sea to set their clocks. Later, radio stations adopted the custom of broadcasting the news every hour. Nowadays, the first item of every news broadcast – more important even than the outbreak of war – is the time. During World War Two, BBC News was broadcast to Nazi-occupied Europe. Each news programme opened with a live broadcast of Big Ben tolling the hour – the magical sound of freedom. Ingenious German physicists found a way to determine the weather conditions in London based on tiny differences in the tone of the broadcast ding-dongs. This information offered invaluable help to the Luftwaffe. When the British Secret Service discovered this, they replaced the live broadcast with a set recording of the famous clock.
In order to run the timetable network, cheap but precise portable clocks became ubiquitous. In Assyrian, Sassanid or Inca cities there might have been at most a few sundials. In European medieval cities there was usually a single clock – a giant machine mounted on top of a high tower in the town square. These tower clocks were notoriously inaccurate, but since there were no other clocks in town to contradict them, it hardly made any difference. Today, a single affluent family generally has more timepieces at home than an entire medieval country. You can tell the time by looking at your wristwatch, glancing at your Android, peering at the alarm clock by your bed, gazing at the clock on the kitchen wall, staring at the microwave, catching a glimpse of the TV or DVD, or taking in the taskbar on your computer out of the corner of your eye. You need to make a conscious effort not to know what time it is.
The typical person consults these clocks several dozen times a day, because almost everything we do has to be done on time. An alarm clock wakes us up at 7 a.m., we heat our frozen bagel for exactly fifty seconds in the microwave, brush our teeth for three minutes until the electric toothbrush beeps, catch the 07:40 train to work, run on the treadmill at the gym until the beeper announces that half an hour is over, sit down in front of the TV at 7 p.m. to watch our favourite show, get interrupted at preordained moments by commercials that cost $1,000 per second, and eventually unload all our angst on a therapist who restricts our prattle to the now standard fifty-minute therapy hour.
The Industrial Revolution brought about dozens of major upheavals in human society. Adapting to industrial time is just one of them. Other notable examples include urbanisation, the disappearance of the peasantry, the rise of the industrial proletariat, the empowerment of the common person, democratisation, youth culture and the disintegration of patriarchy.
Yet all of these upheavals are dwarfed by the most momentous social revolution that ever befell humankind: the collapse of the family and the local community and their replacement by the state and the market. As best we can tell, from the earliest times, more than a million years ago, humans lived in small, intimate communities, most of whose members were kin. The Cognitive Revolution and the Agricultural Revolution did not change that. They glued together families and communities to create tribes, cities, kingdoms and empires, but families and communities remained the basic building blocks of all human societies. The Industrial Revolution, on the other hand, managed within little more than two centuries to break these building blocks into atoms. Most of the traditional functions of families and communities were handed over to states and markets.
The Collapse of the Family and the Community

Prior to the Industrial Revolution, the daily life of most humans ran its course within three ancient frames: the nuclear family, the extended family and the local intimate community.* Most people worked in the family business – the family farm or the family workshop, for example – or they worked in their neighbours’ family businesses. The family was also the welfare system, the health system, the education system, the construction industry, the trade union, the pension fund, the insurance company, the radio, the television, the newspapers, the bank and even the police.
When a person fell sick, the family took care of her. When a person grew old, the family supported her, and her children were her pension fund. When a person died, the family took care of the orphans. If a person wanted to build a hut, the family lent a hand. If a person wanted to open a business, the family raised the necessary money. If a person wanted to marry, the family chose, or at least vetted, the prospective spouse. If conflict arose with a neighbour, the family muscled in. But if a person’s illness was too grave for the family to manage, or a new business demanded too large an investment, or the neighbourhood quarrel escalated to the point of violence, the local community came to the rescue.
The community offered help on the basis of local traditions and an economy of favours, which often differed greatly from the supply and demand laws of the free market. In an old-fashioned medieval community, when my neighbour was in need, I helped build his hut and guard his sheep, without expecting any payment in return. When I was in need, my neighbour returned the favour. At the same time, the local potentate might have drafted all of us villagers to construct his castle without paying us a penny. In exchange, we counted on him to defend us against brigands and barbarians. Village life involved many transactions but few payments. There were some markets, of course, but their roles were limited. You could buy rare spices, cloth and tools, and hire the services of lawyers and doctors. Yet less than 10 per cent of commonly used products and services were bought in the market. Most human needs were taken care of by the family and the community.
There were also kingdoms and empires that performed important tasks such as waging wars, building roads and constructing palaces. For these purposes kings raised taxes and occasionally enlisted soldiers and labourers. Yet, with few exceptions, they tended to stay out of the daily affairs of families and communities. Even if they wanted to intervene, most kings could do so only with difficulty. Traditional agricultural economies had few surpluses with which to feed crowds of government officials, policemen, social workers, teachers and doctors. Consequently, most rulers did not develop mass welfare systems, health-care systems or educational systems. They left such matters in the hands of families and communities. Even on rare occasions when rulers tried to intervene more intensively in the daily lives of the peasantry (as happened, for example, in the Qin Empire in China), they did so by converting family heads and community elders into government agents.
Often enough, transportation and communication difficulties made it so difficult to intervene in the affairs of remote communities that many kingdoms preferred to cede even the most basic royal prerogatives – such as taxation and violence – to communities. The Ottoman Empire, for instance, allowed family vendettas to mete out justice, rather than supporting a large imperial police force. If my cousin killed somebody, the victim’s brother might kill me in sanctioned revenge. The sultan in Istanbul or even the provincial pasha did not intervene in such clashes, as long as violence remained within acceptable limits.
In the Chinese Ming Empire (1368–1644), the population was organised into the baojia system. Ten families were grouped to form a jia, and ten jia constituted a bao. When a member of a bao commited a crime, other bao members could be punished for it, in particular the bao elders. Taxes too were levied on the bao, and it was the responsibility of the bao elders rather than of the state officials to assess the situation of each family and determine the amount of tax it should pay. From the empire’s perspective, this system had a huge advantage. Instead of maintaining thousands of revenue officials and tax collectors, who would have to monitor the earnings and expenses of every family, these tasks were left to the community elders. The elders knew how much each villager was worth and they could usually enforce tax payments without involving the imperial army.
Many kingdoms and empires were in truth little more than large protection rackets. The king was the capo di tutti capi who collected protection money, and in return made sure that neighbouring crime syndicates and local small fry did not harm those under his protection. He did little else.
Life in the bosom of family and community was far from ideal. Families and communities could oppress their members no less brutally than do modern states and markets, and their internal dynamics were often fraught with tension and violence – yet people had little choice. A person who lost her family and community around 1750 was as good as dead. She had no job, no education and no support in times of sickness and distress. Nobody would loan her money or defend her if she got into trouble. There were no policemen, no social workers and no compulsory education. In order to survive, such a person quickly had to find an alternative family or community. Boys and girls who ran away from home could expect, at best, to become servants in some new family. At worst, there was the army or the brothel.
All this changed dramatically over the last two centuries. The Industrial Revolution gave the market immense new powers, provided the state with new means of communication and transportation, and placed at the government’s disposal an army of clerks, teachers, policemen and social workers. At first the market and the state discovered their path blocked by traditional families and communities who had little love for outside intervention. Parents and community elders were reluctant to let the younger generation be indoctrinated by nationalist education systems, conscripted into armies or turned into a rootless urban proletariat.
Over time, states and markets used their growing power to weaken the traditional bonds of family and community. The state sent its policemen to stop family vendettas and replace them with court decisions. The market sent its hawkers to wipe out longstanding local traditions and replace them with ever-changing commercial fashions. Yet this was not enough. In order really to break the power of family and community, they needed the help of a fifth column.
The state and the market approached people with an offer that could not be refused. ‘Become individuals,’ they said. ‘Marry whomever you desire, without asking permission from your parents. Take up whatever job suits you, even if community elders frown. Live wherever you wish, even if you cannot make it every week to the family dinner. You are no longer dependent on your family or your community. We, the state and the market, will take care of you instead. We will provide food, shelter, education, health, welfare and employment. We will provide pensions, insurance and protection.’
Romantic literature often presents the individual as somebody caught in a struggle against the state and the market. Nothing could be further from the truth. The state and the market are the mother and father of the individual, and the individual can survive only thanks to them. The market provides us with work, insurance and a pension. If we want to study a profession, the government’s schools are there to teach us. If we want to open a business, the bank loans us money. If we want to build a house, a construction company builds it and the bank gives us a mortgage, in some cases subsidised or insured by the state. If violence flares up, the police protect us. If we are sick for a few days, our health insurance takes care of us. If we are debilitated for months, social security steps in. If we need around-the-clock assistance, we can go to the market and hire a nurse – usually some stranger from the other side of the world who takes care of us with the kind of devotion that we no longer expect from our own children. If we have the means, we can spend our golden years at a senior citizens’ home. The tax authorities treat us as individuals, and do not expect us to pay the neighbours’ taxes. The courts, too, see us as individuals, and never punish us for the crimes of our cousins.
Not only adult men, but also women and children, are recognised as individuals. Throughout most of history, women were often seen as the property of family or community. Modern states, on the other hand, see women as individuals, enjoying economic and legal rights independently of their family and community. They may hold their own bank accounts, decide whom to marry, and even choose to divorce or live on their own.
But the liberation of the individual comes at a cost. Many of us now bewail the loss of strong families and communities and feel alienated and threatened by the power the impersonal state and market wield over our lives. States and markets composed of alienated individuals can intervene in the lives of their members much more easily than states and markets composed of strong families and communities. When neighbours in a high-rise apartment building cannot even agree on how much to pay their janitor, how can we expect them to resist the state?
The deal between states, markets and individuals is an uneasy one. The state and the market disagree about their mutual rights and obligations, and individuals complain that both demand too much and provide too little. In many cases individuals are exploited by markets, and states employ their armies, police forces and bureaucracies to persecute individuals instead of defending them. Yet it is amazing that this deal works at all – however imperfectly. For it breaches countless generations of human social arrangements. Millions of years of evolution have designed us to live and think as community members. Within a mere two centuries we have become alienated individuals. Nothing testifies better to the awesome power of culture.
The nuclear family did not disappear completely from the modern landscape. When states and markets took from the family most of its economic and political roles, they left it some important emotional functions. The modern family is still supposed to provide for intimate needs, which state and market are (so far) incapable of providing. Yet even here the family is subject to increasing interventions. The market shapes to an ever-greater degree the way people conduct their romantic and sexual lives. Whereas traditionally the family was the main matchmaker, today it’s the market that tailors our romantic and sexual preferences, and then lends a hand in providing for them – for a fat fee. Previously bride and groom met in the family living room, and money passed from the hands of one father to another. Today courting is done at bars and cafés, and money passes from the hands of lovers to waitresses. Even more money is transferred to the bank accounts of fashion designers, gym managers, dieticians, cosmeticians and plastic surgeons, who help us arrive at the café looking as similar as possible to the markets ideal of beauty.

Family and community vs. state and market

The state, too, keeps a sharper eye on family relations, especially between parents and children. Parents are obliged to send their children to be educated by the state. Parents who are especially abusive or violent with their children may be restrained by the state. If need be, the state may even imprison the parents or transfer their children to foster families. Until not long ago, the suggestion that the state ought to prevent parents from beating or humiliating their children would have been rejected out of hand as ludicrous and unworkable. In most societies parental authority was sacred. Respect of and obedience to one’s parents were among the most hallowed values, and parents could do almost anything they wanted, including killing newborn babies, selling children into slavery and marrying off daughters to men more than twice their age. Today, parental authority is in full retreat. Youngsters are increasingly excused from obeying their elders, whereas parents are blamed for anything that goes wrong in the life of their child. Mum and Dad are about as likely to get off in the Freudian courtroom as were defendants in a Stalinist show trial.
Imagined Communities

Like the nuclear family, the community could not completely disappear from our world without any emotional replacement. Markets and states today provide most of the material needs once provided by communities, but they must also supply tribal bonds.
Markets and states do so by fostering ‘imagined communities’ that contain millions of strangers, and which are tailored to national and commercial needs. An imagined community is a community of people who don’t really know each other, but imagine that they do. Such communities are not a novel invention. Kingdoms, empires and churches functioned for millennia as imagined communities. In ancient China, tens of millions of people saw themselves as members of a single family, with the emperor as its father. In the Middle Ages, millions of devout Muslims imagined that they were all brothers and sisters in the great community of Islam. Yet throughout history, such imagined communities played second fiddle to intimate communities of several dozen people who knew each other well. The intimate communities fulfilled the emotional needs of their members and were essential for everyone’s survival and welfare. In the last two centuries, the intimate communities have withered, leaving imagined communities to fill in the emotional vacuum.
The two most important examples for the rise of such imagined communities are the nation and the consumer tribe. The nation is the imagined community of the state. The consumer tribe is the imagined community of the market. Both are imagined communities because it is impossible for all customers in a market or for all members of a nation really to know one another the way villagers knew one another in the past. No German can intimately know the other 80 million members of the German nation, or the other 500 million customers inhabiting the European Common Market (which evolved first into the European Community and finally became the European Union).
Consumerism and nationalism work extra hours to make us imagine that millions of strangers belong to the same community as ourselves, that we all have a common past, common interests and a common future. This isn’t a lie. It’s imagination. Like money, limited liability companies and human rights, nations and consumer tribes are inter-subjective realities. They exist only in our collective imagination, yet their power is immense. As long as millions of Germans believe in the existence of a German nation, get excited at the sight of German national symbols, retell German national myths, and are willing to sacrifice money, time and limbs for the German nation, Germany will remain one of the strongest powers in the world.
The nation does its best to hide its imagined character. Most nations argue that they are a natural and eternal entity, created in some primordial epoch by mixing the soil of the motherland with the blood of the people. Yet such claims are usually exaggerated. Nations existed in the distant past, but their importance was much smaller than today because the importance of the state was much smaller. A resident of medieval Nuremberg might have felt some loyalty towards the German nation, but she felt far more loyalty towards her family and local community, which took care of most of her needs. Moreover, whatever importance ancient nations may have had, few of them survived. Most existing nations evolved only after the Industrial Revolution.
The Middle East provides ample examples. The Syrian, Lebanese, Jordanian and Iraqi nations are the product of haphazard borders drawn in the sand by French and British diplomats who ignored local history, geography and economy. These diplomats determined in 1918 that the people of Kurdistan, Baghdad and Basra would henceforth be ‘Iraqis’. It was primarily the French who decided who would be Syrian and who Lebanese. Saddam Hussein and Hafez el-Asad tried their best to promote and reinforce their Anglo-French-manufactured national consciousnesses, but their bombastic speeches about the allegedly eternal Iraqi and Syrian nations had a hollow ring.
It goes without saying that nations cannot be created from thin air. Those who worked hard to construct Iraq or Syria made use of real historical, geographical and cultural raw materials – some of which are centuries and millennia old. Saddam Hussein co-opted the heritage of the Abbasid caliphate and the Babylonian Empire, even calling one of his crack armoured units the Hammurabi Division. Yet that does not turn the Iraqi nation into an ancient entity. If I bake a cake from flour, oil and sugar, all of which have been sitting in my pantry for the past two months, it does not mean that the cake itself is two months old.
In recent decades, national communities have been increasingly eclipsed by tribes of customers who do not know one another intimately but share the same consumption habits and interests, and therefore feel part of the same consumer tribe – and define themselves as such. This sounds very strange, but we are surrounded by examples. Madonna fans, for example, constitute a consumer tribe. They define themselves largely by shopping. They buy Madonna concert tickets, CDs, posters, shirts and ring tones, and thereby define who they are. Manchester United fans, vegetarians and environmentalists are other examples. They, too, are defined above all by what they consume. It is the keystone of their identity. A German vegetarian might well prefer to marry a French vegetarian than a German carnivore.
Perpetuum Mobile

The revolutions of the last two centuries have been so swift and radical that they have changed the most fundamental characteristic of the social order. Traditionally, the social order was hard and rigid. ‘Order’ implied stability and continuity. Swift social revolutions were exceptional, and most social transformations resulted from the accumulation of numerous small steps. Humans tended to assume that the social structure was inflexible and eternal. Families and communities might struggle to change their place within the order, but the idea that you could change the fundamental structure of the order was alien. People tended to reconcile themselves to the status quo, declaring that ‘this is how it always was, and this is how it always will be’.
Over the last two centuries, the pace of change became so quick that the social order acquired a dynamic and malleable nature. It now exists in a state of permanent flux. When we speak of modern revolutions we tend to think of 1789 (the French Revolution), 1848 (the liberal revolutions) or 1917 (the Russian Revolution). But the fact is that, these days, every year is revolutionary. Today, even a thirty-year-old can honestly tell disbelieving teenagers, ‘When I was young, the world was completely different.’ The Internet, for example, came into wide usage only in the early 1990s, hardly twenty years ago. Today we cannot imagine the world without it.
Hence any attempt to define the characteristics of modern society is akin to defining the colour of a chameleon. The only characteristic of which we can be certain is the incessant change. People have become used to this, and most of us think about the social order as something flexible, which we can engineer and improve at will. The main promise of premodern rulers was to safeguard the traditional order or even to go back to some lost golden age. In the last two centuries, the currency of politics is that it promises to destroy the old world and build a better one in its place. Not even the most conservative of political parties vows merely to keep things as they are. Everybody promises social reform, educational reform, economic reform – and they often fulfil those promises.
Just as geologists expect that tectonic movements will result in earthquakes and volcanic eruptions, so might we expect that drastic social movements will result in bloody outbursts of violence. The political history of the nineteenth and twentieth centuries is often told as a series of deadly wars, holocausts and revolutions. Like a child in new boots leaping from puddle to puddle, this view sees history as leapfrogging from one bloodbath to the next, from World War One to World War Two to the Cold War, from the Armenian genocide to the Jewish genocide to the Rwandan genocide, from Robespierre to Lenin to Hitler.
There is truth here, but this all too familiar list of calamities is somewhat misleading. We focus too much on the puddles and forget about the dry land separating them. The late modern era has seen unprecedented levels not only of violence and horror, but also of peace and tranquillity. Charles Dickens wrote of the French Revolution that ‘It was the best of times, it was the worst of times.’ This may be true not only of the French Revolution, but of the entire era it heralded.
It is especially true of the seven decades that have elapsed since the end of World War Two. During this period humankind has for the first time faced the possibility of complete self-annihilation and has experienced a fair number of actual wars and genocides. Yet these decades were also the most peaceful era in human history – and by a wide margin. This is surprising because these very same decades experienced more economic, social and political change than any previous era. The tectonic plates of history are moving at a frantic pace, but the volcanoes are mostly silent. The new elastic order seems to be able to contain and even initiate radical structural changes without collapsing into violent conflict.3
Peace in Our Time

Most people don’t appreciate just how peaceful an era we live in. None of us was alive a thousand years ago, so we easily forget how much more violent the world used to be. And as wars become more rare they attract more attention. Many more people think about the wars raging today in Afghanistan and Iraq than about the peace in which most Brazilians and Indians live.
Even more importantly, it’s easier to relate to the suffering of individuals than of entire populations. However, in order to understand macro-historical processes, we need to examine mass statistics rather than individual stories. In the year 2000, wars caused the deaths of 310,000 individuals, and violent crime killed another 520,000. Each and every victim is a world destroyed, a family ruined, friends and relatives scarred for life. Yet from a macro perspective these 830,000 victims comprised only 1.5 per cent of the 56 million people who died in 2000. That year 1.26 million people died in car accidents (2.25 per cent of total mortality) and 815,000 people committed suicide (1.45 per cent).4
The figures for 2002 are even more surprising. Out of 57 million dead, only 172,000 people died in war and 569,000 died of violent crime (a total of 741,000 victims of human violence). In contrast, 873,000 people committed suicide.5 It turns out that in the year following the 9/11 attacks, despite all the talk of terrorism and war, the average person was more likely to kill himself than to be killed by a terrorist, a soldier or a drug dealer.
In most parts of the world, people go to sleep without fearing that in the middle of the night a neighbouring tribe might surround their village and slaughter everyone. Well-off British subjects travel daily from Nottingham to London through Sherwood Forest without fear that a gang of merry green-clad brigands will ambush them and take their money to give to the poor (or, more likely, murder them and take the money for themselves). Students brook no canings from their teachers, children need not fear that they will be sold into slavery when their parents can’t pay their bills, and women know that the law forbids their husbands from beating them and forcing them to stay at home. Increasingly, around the world, these expectations are fulfilled.
The decline of violence is due largely to the rise of the state. Throughout history, most violence resulted from local feuds between families and communities. (Even today, as the above figures indicate, local crime is a far deadlier threat than international wars.) As we have seen, early farmers, who knew no political organisations larger than the local community, suffered rampant violence.6 As kingdoms and empires became stronger, they reined in communities and the level of violence decreased. In the decentralised kingdoms of medieval Europe, about twenty to forty people were murdered each year for every 100,000 inhabitants. In recent decades, when states and markets have become all-powerful and communities have vanished, violence rates have dropped even further. Today the global average is only nine murders a year per 100,000 people, and most of these murders take place in weak states such as Somalia and Colombia. In the centralised states of Europe, the average is one murder a year per 100,000 people.7
There are certainly cases where states use their power to kill their own citizens, and these often loom large in our memories and fears. During the twentieth century, tens of millions if not hundreds of millions of people were killed by the security forces of their own states. Still, from a macro perspective, state-run courts and police forces have probably increased the level of security worldwide. Even in oppressive dictatorships, the average modern person is far less likely to die at the hands of another person than in premodern societies. In 1964 a military dictatorship was established in Brazil. It ruled the country until 1985. During these twenty years, several thousand Brazilians were murdered by the regime. Thousands more were imprisoned and tortured. Yet even in the worst years, the average Brazilian in Rio de Janeiro was far less likely to die at human hands than the average Waorani, Arawete or Yanomamo. The Waorani, Arawete and Yanomamo are indigenous people who live in the depths of the Amazon forest, without army, police or prisons. Anthropological studies have indicated that between a quarter and a half of their menfolk die sooner or later in violent conflicts over property, women or prestige.8
Imperial Retirement

It is perhaps debatable whether violence within states has decreased or increased since 1945. What nobody can deny is that international violence has dropped to an all-time low. Perhaps the most obvious example is the collapse of the European empires. Throughout history empires have crushed rebellions with an iron fist, and when its day came, a sinking empire used all its might to save itself, usually collapsing into a bloodbath. Its final demise generally led to anarchy and wars of succession. Since 1945 most empires have opted for peaceful early retirement. Their process of collapse became relatively swift, calm and orderly.
In 1945 Britain ruled a quarter of the globe. Thirty years later it ruled just a few small islands. In the intervening decades it retreated from most of its colonies in a peaceful and orderly manner. Though in some places such as Malaya and Kenya the British tried to hang on by force of arms, in most places they accepted the end of empire with a sigh rather than with a temper tantrum. They focused their efforts not on retaining power, but on transferring it as smoothly as possible. At least some of the praise usually heaped on Mahatma Gandhi for his non-violent creed is actually owed to the British Empire. Despite many years of bitter and often violent struggle, when the end of the Raj came, the Indians did not have to fight the British in the streets of Delhi and Calcutta. The empire’s place was taken by a slew of independent states, most of which have since enjoyed stable borders and have for the most part lived peacefully alongside their neighbours. True, tens of thousands of people perished at the hands of the threatened British Empire, and in several hot spots its retreat led to the eruption of ethnic conflicts that claimed hundreds of thousands of lives (particularly in India). Yet when compared to the long-term historical average, the British withdrawal was an exemplar of peace and order. The French Empire was more stubborn. Its collapse involved bloody rearguard actions in Vietnam and Algeria that cost hundreds of thousands of lives. Yet the French, too, retreated from the rest of their dominions quickly and peacefully, leaving behind orderly states rather than a chaotic free-for-all.
The Soviet collapse in 1989 was even more peaceful, despite the eruption of ethnic conflict in the Balkans, the Caucasus and Central Asia. Never before has such a mighty empire disappeared so swiftly and so quietly. The Soviet Empire of 1989 had suffered no military defeat except in Afghanistan, no external invasions, no rebellions, nor even large-scale Martin Luther King-style campaigns of civil disobedience. The Soviets still had millions of soldiers, tens of thousands of tanks and aeroplanes, and enough nuclear weapons to wipe out the whole of humankind several times over. The Red Army and the other Warsaw Pact armies remained loyal. Had the last Soviet ruler, Mikhail Gorbachev, given the order, the Red Army would have opened fire on the subjugated masses.
Yet the Soviet elite, and the Communist regimes through most of eastern Europe (Romania and Serbia were the exceptions), chose not to use even a tiny fraction of this military power. When its members realised that Communism was bankrupt, they renounced force, admitted their failure, packed their suitcases and went home. Gorbachev and his colleagues gave up without a struggle not only the Soviet conquests of World War Two, but also the much older tsarist conquests in the Baltic, the Ukraine, the Caucasus and Central Asia. It is chilling to contemplate what might have happened if Gorbachev had behaved like the Serbian leadership – or like the French in Algeria.
Pax Atomica

The independent states that came after these empires were remarkably uninterested in war. With very few exceptions, since 1945 states no longer invade other states in order to conquer and swallow them up. Such conquests had been the bread and butter of political history since time immemorial. It was how most great empires were established, and how most rulers and populations expected things to stay. But campaigns of conquest like those of the Romans, Mongols and Ottomans cannot take place today anywhere in the world. Since 1945, no independent country recognised by the UN has been conquered and wiped off the map. Limited international wars still occur from time to time, and millions still die in wars, but wars are no longer the norm.
Many people believe that the disappearance of international war is unique to the rich democracies of western Europe. In fact, peace reached Europe after it prevailed in other parts of the world. Thus the last serious international wars between South American countries were the Peru-Ecuador War of 1941 and the Bolivia-Paraguay War of 1932–5. And before that there hadn’t been a serious war between South American countries since 1879–84, with Chile on one side and Bolivia and Peru on the other.
We seldom think of the Arab world as particularly peaceful. Yet only once since the Arab countries won their independence has one of them mounted a full-scale invasion of another (the Iraqi invasion of Kuwait in 1990). There have been quite a few border clashes (e.g. Syria vs Jordan in 1970), many armed interventions of one in the affairs of another (e.g. Syria in Lebanon), numerous civil wars (Algeria, Yemen, Libya) and an abundance of coups and revolts. Yet there have been no full-scale international wars among the Arab states except the Gulf War. Even widening the scope to include the entire Muslim world adds only one more example, the Iran-Iraq War. There was no Turkey—Iran War, Pakistan-Afghanistan War, or Indonesia-Malaysia War.
In Africa things are far less rosy. But even there, most conflicts are civil wars and coups. Since African states won their independence in the 1960s and 1970s, very few countries have invaded one another in the hope of conquest.
There have been periods of relative calm before, as, for example, in Europe between 1871 and 1914, and they always ended badly. But this time it is different. For real peace is not the mere absence of war. Real peace is the implausibility of war. There has never been real peace in the world. Between 1871 and 1914, a European war remained a plausible eventuality, and the expectation of war dominated the thinking of armies, politicians and ordinary citizens alike. This foreboding was true for all other peaceful periods in history. An iron law of international politics decreed, ‘For every two nearby polities, there is a plausible scenario that will cause them to go to war against one another within one year.’ This law of the jungle was in force in late nineteenth-century Europe, in medieval Europe, in ancient China and in classical Greece. If Sparta and Athens were at peace in 450 BC, there was a plausible scenario that they would be at war by 449 BC.
Today humankind has broken the law of the jungle. There is at last real peace, and not just absence of war. For most polities, there is no plausible scenario leading to full-scale conflict within one year. What could lead to war between Germany and France next year? Or between China and Japan? Or between Brazil and Argentina? Some minor border clash might occur, but only a truly apocalyptic scenario could result in an old-fashioned full-scale war between Brazil and Argentina in 2014, with Argentinian armoured divisions sweeping to the gates of Rio, and Brazilian carpet-bombers pulverising the neighbourhoods of Buenos Aires. Such wars might still erupt between several pairs of states, e.g. between Israel and Syria, Ethiopia and Eritrea, or the USA and Iran, but these are only the exceptions that prove the rule.
This situation might of course change in the future and, with hindsight, the world of today might seem incredibly naïve. Yet from a historical perspective, our very naïvety is fascinating. Never before has peace been so prevalent that people could not even imagine war.
 
Scholars have sought to explain this happy development in more books and articles than you would ever want to read yourself, and they have identified several contributing factors. First and foremost, the price of war has gone up dramatically. The Nobel Peace Prize to end all peace prizes should have been given to Robert Oppenheimer and his fellow architects of the atomic bomb. Nuclear weapons have turned war between superpowers into collective suicide, and made it impossible to seek world domination by force of arms.
Secondly, while the price of war soared, its profits declined. For most of history, polities could enrich themselves by looting or annexing enemy territories. Most wealth consisted of fields, cattle, slaves and gold, so it was easy to loot it or occupy it. Today, wealth consists mainly of human capital, technical know-how and complex socio-economic structures such as banks. Consequently it is difficult to carry it off or incorporate it into one’s territory.
Consider California. Its wealth was initially built on gold mines. But today it is built on silicon and celluloid – Silicon Valley and the celluloid hills of Hollywood. What would happen if the Chinese were to mount an armed invasion of California, land a million soldiers on the beaches of San Francisco and storm inland? They would gain little. There are no silicon mines in Silicon Valley. The wealth resides in the minds of Google engineers and Hollywood script doctors, directors and special-effects wizards, who would be on the first plane to Bangalore or Mumbai long before the Chinese tanks rolled into Sunset Boulevard. It is not coincidental that the few full-scale international wars that still take place in the world, such as the Iraqi invasion of Kuwait, occur in places were wealth is old-fashioned material wealth. The Kuwaiti sheikhs could flee abroad, but the oil fields stayed put and were occupied.

43. and 44. Gold miners in California during the Gold Rush, and Facebook’s headquarters near San Francisco. In 1849 California built its fortunes on gold. Today, California builds its fortunes on silicon. But whereas in 1849 the gold actually lay there in the Californian soil, the real treasures of Silicon Valley are locked inside the heads of high-tech employees.

While war became less profitable, peace became more lucrative than ever. In traditional agricultural economies long-distance trade and foreign investment were sideshows. Consequently, peace brought little profit, aside from avoiding the costs of war. If, say, in 1400 England and France were at peace, the French did not have to pay heavy war taxes and to suffer destructive English invasions, but otherwise it did not benefit their wallets. In modern capitalist economies, foreign trade and investments have become all-important. Peace therefore brings unique dividends. As long as China and the USA are at peace, the Chinese can prosper by selling products to the USA, trading in Wall Street and receiving US investments.
Last but not least, a tectonic shift has taken place in global political culture. Many elites in history – Hun chieftains, Viking noblemen and Aztec priests, for example – viewed war as a positive good. Others viewed it as evil, but an inevitable one, which we had better turn to our own advantage. Ours is the first time in history that the world is dominated by a peace-loving elite – politicians, business people, intellectuals and artists who genuinely see war as both evil and avoidable. (There were pacifists in the past, such as the early Christians, but in the rare cases that they gained power, they tended to forget about their requirement to ‘turn the other cheek’.)
There is a positive feedback loop between all these four factors. The threat of nuclear holocaust fosters pacifism; when pacifism spreads, war recedes and trade flourishes; and trade increases both the profits of peace and the costs of war. Over time, this feedback loop creates another obstacle to war, which may ultimately prove the most important of all. The tightening web of international connections erodes the independence of most countries, lessening the chance that any one of them might single-handedly let slip the dogs of war. Most countries no longer engage in full-scale war for the simple reason that they are no longer independent. Though citizens in Israel, Italy, Mexico or Thailand may harbour illusions of independence, the fact is that their governments cannot conduct independent economic or foreign policies, and they are certainly incapable of initiating and conducting full-scale war on their own. As explained in Chapter 11, we are witnessing the formation of a global empire. Like previous empires, this one, too, enforces peace within its borders. And since its borders cover the entire globe, the World Empire effectively enforces world peace.
So, is the modern era one of mindless slaughter, war and oppression, typified by the trenches of World War One, the nuclear mushroom cloud over Hiroshima and the gory manias of Hitler and Stalin? Or is it an era of peace, epitomised by the trenches never dug in South America, the mushroom clouds that never appeared over Moscow and New York, and the serene visages of Mahatma Gandhi and Martin Luther King?
The answer is a matter of timing. It is sobering to realise how often our view of the past is distorted by events of the last few years. If this chapter had been written in 1945 or 1962, it would probably have been much more glum. Since it was written in 2014, it takes a relatively buoyant approach to modern history.
To satisfy both optimists and pessimists, we may conclude by saying that we are on the threshold of both heaven and hell, moving nervously between the gateway of the one and the anteroom of the other. History has still not decided where we will end up, and a string of coincidences might yet send us rolling in either direction.
* An ‘intimate community’ is a group of people who know one another well and depend on each other for survival.19

And They Lived Happily Ever After

THE LAST 500 YEARS HAVE WITNESSED A breathtaking series of revolutions. The earth has been united into a single ecological and historical sphere. The economy has grown exponentially, and humankind today enjoys the kind of wealth that used to be the stuff of fairy tales. Science and the Industrial Revolution have given humankind superhuman powers and practically limitless energy. The social order has been completely transformed, as have politics, daily life and human psychology.
But are we happier? Did the wealth humankind accumulated over the last five centuries translate into a new-found contentment? Did the discovery of inexhaustible energy resources open before us inexhaustible stores of bliss? Going further back, have the seventy or so turbulent millennia since the Cognitive Revolution made the world a better place to live? Was the late Neil Armstrong, whose footprint remains intact on the windless moon, happier than the nameless hunter-gatherer who 30,000 years ago left her handprint on a wall in Chauvet Cave? If not, what was the point of developing agriculture, cities, writing, coinage, empires, science and industry?
Historians seldom ask such questions. They do not ask whether the citizens of Uruk and Babylon were happier than their foraging ancestors, whether the rise of Islam made Egyptians more pleased with their lives, or how the collapse of the European empires in Africa have influenced the happiness of countless millions. Yet these are the most important questions one can ask of history. Most current ideologies and political programmes are based on rather flimsy ideas concerning the real source of human happiness. Nationalists believe that political self-determination is essential for our happiness. Communists postulate that everyone would be blissful under the dictatorship of the proletariat. Capitalists maintain that only the free market can ensure the greatest happiness of the greatest number, by creating economic growth and material abundance and by teaching people to be self-reliant and enterprising.
What would happen if serious research were to disprove these hypotheses? If economic growth and self-reliance do not make people happier, what’s the benefit of Capitalism? What if it turns out that the subjects of large empires are generally happier than the citizens of independent states and that, for example, Algerians were happier under French rule than under their own? What would that say about the process of decolonisation and the value of national self-determination?
These are all hypothetical possibilities, because so far historians have avoided raising these questions – not to mention answering them. They have researched the history of just about everything politics, society, economics, gender, diseases, sexuality, food, clothing – yet they have seldom stopped to ask how these influence human happiness.
Though few have studied the long-term history of happiness, almost every scholar and layperson has some vague preconception about it. In one common view, human capabilities have increased throughout history. Since humans generally use their capabilities to alleviate miseries and fulfil aspirations, it follows that we must be happier than our medieval ancestors, and they must have been happier than Stone Age hunter-gatherers.
But this progressive account is unconvincing. As we have seen, new aptitudes, behaviours and skills do not necessarily make for a better life. When humans learned to farm in the Agricultural Revolution, their collective power to shape their environment increased, but the lot of many individual humans grew harsher. Peasants had to work harder than foragers to eke out less varied and nutritious food, and they were far more exposed to disease and exploitation. Similarly, the spread of European empires greatly increased the collective power of humankind, by circulating ideas, technologies and crops, and opening new avenues of commerce. Yet this was hardly good news for millions of Africans, Native Americans and Aboriginal Australians. Given the proven human propensity for misusing power, it seems naïve to believe that the more clout people have, the happier they will be.
Some challengers of this view take a diametrically opposed position. They argue for a reverse correlation between human capabilities and happiness. Power corrupts, they say, and as humankind gained more and more power, it created a cold mechanistic world ill-suited to our real needs. Evolution moulded our minds and bodies to the life of hunter-gatherers. The transition first to agriculture and then to industry has condemned us to living unnatural lives that cannot give full expression to our inherent inclinations and instincts, and therefore cannot satisfy our deepest yearnings. Nothing in the comfortable lives of the urban middle class can approach the wild excitement and sheer joy experienced by a forager band on a successful mammoth hunt. Every new invention just puts another mile between us and the Garden of Eden.
Yet this romantic insistence on seeing a dark shadow behind each invention is as dogmatic as the belief in the inevitability of progress. Perhaps we are out of touch with our inner hunter-gatherer, but it’s not all bad. For instance, over the last two centuries modern medicine has decreased child mortality from 33 per cent to less than 5 per cent. Can anyone doubt that this made a huge contribution to the happiness not only of those children who would otherwise have died, but also of their families and friends?
A more nuanced position takes the middle road. Until the Scientific Revolution there was no clear correlation between power and happiness. Medieval peasants may indeed have been more miserable than their hunter-gatherer forebears. But in the last few centuries humans have learned to use their capacities more wisely. The triumphs of modern medicine are just one example. Other unprecedented achievements include the steep drop in violence, the virtual disappearance of international wars, and the near elimination of large-scale famines.
Yet this, too, is an oversimplification. Firstly, it bases its optimistic assessment on a very small sample of years. The majority of humans began to enjoy the fruits of modern medicine no earlier than 1850, and the drastic drop in child mortality is a twentieth-century phenomenon. Mass famines continued to blight much of humanity up to the middle of the twentieth century. During Communist Chinas Great Leap Forward of 1958–61, somewhere between 10 and 50 million human beings starved to death. International wars became rare only after 1945, largely thanks to the new threat of nuclear annihilation. Hence, though the last few decades have been an unprecedented golden age for humanity, it is too early to know whether this represents a fundamental shift in the currents of history or an ephemeral eddy of good fortune. When judging modernity, it is all too tempting to take the viewpoint of a twenty-first-century middle-class Westerner. We must not forget the viewpoints of a nineteenth-century Welsh coal miner, Chinese opium addict or Tasmanian Aborigine. Truganini is no less important than Homer Simpson.
Secondly, even the brief golden age of the last half-century may turn out to have sown the seeds of future catastrophe. Over the last few decades, we have been disturbing the ecological equilibrium of our planet in myriad new ways, with what seem likely to be dire consequences. A lot of evidence indicates that we are destroying the foundations of human prosperity in an orgy of reckless consumption.
Finally, we can congratulate ourselves on the unprecedented accomplishments of modern Sapiens only if we completely ignore the fate of all other animals. Much of the vaunted material wealth that shields us from disease and famine was accumulated at the expense of laboratory monkeys, dairy cows and conveyor-belt chickens. Over the last two centuries tens of billions of them have been subjected to a regime of industrial exploitation whose cruelty has no precedent in the annals of planet Earth. If we accept a mere tenth of what animal-rights activists are claiming, then modern industrial agriculture might well be the greatest crime in history. When evaluating global happiness, it is wrong to count the happiness only of the upper classes, of Europeans or of men. Perhaps it is also wrong to consider only the happiness of humans.
Counting Happiness

So far we have discussed happiness as if it were largely a product of material factors, such as health, diet and wealth. If people are richer and healthier, then they must also be happier. But is that really so obvious? Philosophers, priests and poets have brooded over the nature of happiness for millennia, and many have concluded that social, ethical and spiritual factors have as great an impact on our happiness as material conditions. Perhaps people in modern affluent societies suffer greatly from alienation and meaninglessness despite their prosperity. And perhaps our less well-to-do ancestors found much contentment in community, religion and a bond with nature.
In recent decades, psychologists and biologists have taken up the challenge of studying scientifically what really makes people happy. Is it money, family, genetics or perhaps virtue? The first step is to define what is to be measured. The generally accepted definition of happiness is ‘subjective well-being’. Happiness, according to this view, is something I feel inside myself, a sense of either immediate pleasure or long-term contentment with the way my life is going. If it’s something felt inside, how can it be measured from outside? Presumably, we can do so by asking people to tell us how they feel. So psychologists or biologists who want to assess how happy people feel give them questionnaires to fill out and tally the results.
A typical subjective well-being questionnaire asks interviewees to grade on a scale of zero to ten their agreement with statements such as ‘I feel pleased with the way I am’, ‘I feel that life is very rewarding’, ‘I am optimistic about the future’ and ‘Life is good’. The researcher then adds up all the answers and calculates the interviewee’s general level of subjective well-being.
Such questionnaires are used in order to correlate happiness with various objective factors. One study might compare a thousand people who earn $100,000 a year with a thousand people who earn $50,000. If the study discovers that the first group has an average subjective well-being level of 8.7, while the latter has an average of only 7.3, the researcher may reasonably conclude that there is a positive correlation between wealth and subjective well-being. To put it in simple English, money brings happiness. The same method can be used to examine whether people living in democracies are happier than people living in dictatorships, and whether married people are happier than singles, divorcees or widowers.
This provides a grounding for historians, who can examine wealth, political freedom and divorce rates in the past. If people are happier in democracies and married people are happier than divorcees, a historian has a basis for arguing that the democratisation process of the last few decades contributed to the happiness of humankind, whereas the growing rates of divorce indicate an opposite trend.
This way of thinking is not flawless, but before pointing out some of the holes, it is worth considering the findings.
One interesting conclusion is that money does indeed bring happiness. But only up to a point, and beyond that point it has little significance. For people stuck at the bottom of the economic ladder, more money means greater happiness. If you are an American single mother earning $12,000 a year cleaning houses and you suddenly win $500,000 in the lottery, you will probably experience a significant and long-term surge in your subjective well-being. You’ll be able to feed and clothe your children without sinking further into debt. However, if you’re a top executive earning $250,000 a year and you win $1 million in the lottery, or your company board suddenly decides to double your salary, your surge is likely to last only a few weeks. According to the empirical findings, it’s almost certainly not going to make a big difference to the way you feel over the long run. You’ll buy a snazzier car, move into a palatial home, get used to drinking Chateau Pétrus instead of California Cabernet, but it’ll soon all seem routine and unexceptional.
Another interesting finding is that illness decreases happiness in the short term, but is a source of long-term distress only if a person’s condition is constantly deteriorating or if the disease involves ongoing and debilitating pain. People who are diagnosed with chronic illness such as diabetes are usually depressed for a while, but if the illness does not get worse they adjust to their new condition and rate their happiness as highly as healthy people do. Imagine that Lucy and Luke are middle-class twins, who agree to take part in a subjective well-being study. On the way back from the psychology laboratory, Lucy’s car is hit by a bus, leaving Lucy with a number of broken bones and a permanently lame leg. Just as the rescue crew is cutting her out of the wreckage, the phone rings and Luke shouts that he has won the lottery’s $10,000,000 jackpot. Two years later she’ll be limping and he’ll be a lot richer, but when the psychologist comes around for a follow-up study, they are both likely to give the same answers they did on the morning of that fateful day.
Family and community seem to have more impact on our happiness than money and health. People with strong families who live in tight-knit and supportive communities are significantly happier than people whose families are dysfunctional and who have never found (or never sought) a community to be part of. Marriage is particularly important. Repeated studies have found that there is a very close correlation between good marriages and high subjective well-being, and between bad marriages and misery. This holds true irrespective of economic or even physical conditions. An impecunious invalid surrounded by a loving spouse, a devoted family and a warm community may well feel better than an alienated billionaire, provided that the invalid’s poverty is not too severe and that his illness is not degenerative or painful.
This raises the possibility that the immense improvement in material conditions over the last two centuries was offset by the collapse of the family and the community. If so, the average person might well be no happier today than in 1800. Even the freedom we value so highly may be working against us. We can choose our spouses, friends and neighbours, but they can choose to leave us. With the individual wielding unprecedented power to decide her own path in life, we find it ever harder to make commitments. We thus live in an increasingly lonely world of unravelling communities and families.
But the most important finding of all is that happiness does not really depend on objective conditions of either wealth, health or even community. Rather, it depends on the correlation between objective conditions and subjective expectations. If you want a bullock-cart and get a bullock-cart, you are content. If you want a brand-new Ferrari and get only a second-hand Fiat you feel deprived. This is why winning the lottery has, over time, the same impact on people’s happiness as a debilitating car accident. When things improve, expectations balloon, and consequently even dramatic improvements in objective conditions can leave us dissatisfied. When things deteriorate, expectations shrink, and consequently even a severe illness might leave you pretty much as happy as you were before.
You might say that we didn’t need a bunch of psychologists and their questionnaires to discover this. Prophets, poets and philosophers realised thousands of years ago that being satisfied with what you already have is far more important than getting more of what you want. Still, it’s nice when modern research – bolstered by lots of numbers and charts – reaches the same conclusions the ancients did.
The crucial importance of human expectations has far-reaching implications for understanding the history of happiness. If happiness depended only on objective conditions such as wealth, health and social relations, it would have been relatively easy to investigate its history. The finding that it depends on subjective expectations makes the task of historians far harder. We moderns have an arsenal of tranquillisers and painkillers at our disposal, but our expectations of ease and pleasure, and our intolerance of inconvenience and discomfort, have increased to such an extent that we may well suffer from pain more than our ancestors ever did.
It’s hard to accept this line of thinking. The problem is a fallacy of reasoning embedded deep in our psyches. When we try to guess or imagine how happy other people are now, or how people in the past were, we inevitably imagine ourselves in their shoes. But that won’t work because it pastes our expectations on to the material conditions of others. In modern affluent societies it is customary to take a shower and change your clothes every day. Medieval peasants went without washing for months on end, and hardly ever changed their clothes. The very thought of living like that, filthy and reeking to the bone, is abhorrent to us. Yet medieval peasants seem not to have minded. They were used to the feel and smell of a long-unlaundered shirt. It’s not that they wanted a change of clothes but couldn’t get it – they had what they wanted. So, at least as far as clothing goes, they were content.
That’s not so surprising, when you think of it. After all, our chimpanzee cousins seldom wash and never change their clothes. Nor are we disgusted by the fact that our pet dogs and cats don’t shower or change their coats daily. We pat, hug and kiss them all the same. Small children in affluent societies often dislike showering, and it takes them years of education and parental discipline to adopt this supposedly attractive custom. It is all a matter of expectations.
If happiness is determined by expectations, then two pillars of our society – mass media and the advertising industry – may unwittingly be depleting the globe’s reservoirs of contentment. If you were an eighteen-year-old youth in a small village 5,000 years ago you’d probably think you were good-looking because there were only fifty other men in your village and most of them were either old, scarred and wrinkled, or still little kids. But if you are a teenager today you are a lot more likely to feel inadequate. Even if the other guys at school are an ugly lot, you don’t measure yourself against them but against the movie stars, athletes and supermodels you see all day on television, Facebook and giant billboards.
So maybe Third World discontent is fomented not merely by poverty, disease, corruption and political oppression but also by mere exposure to First World standards. The average Egyptian was far less likely to die from starvation, plague or violence under Hosni Mubarak than under Ramses II or Cleopatra. Never had the material condition of most Egyptians been so good. You’d think they would have been dancing in the streets in 2011, thanking Allah for their good fortune. Instead they rose up furiously to overthrow Mubarak. They weren’t comparing themselves to their ancestors under the pharaohs, but rather to their contemporaries in Obama’s America.
If that’s the case, even immortality might lead to discontent. Suppose science comes up with cures for all diseases, effective anti-ageing therapies and regenerative treatments that keep people indefinitely young. In all likelihood, the immediate result will be an unprecedented epidemic of anger and anxiety.
Those unable to afford the new miracle treatments – the vast majority of people – will be beside themselves with rage. Throughout history, the poor and oppressed comforted themselves with the thought that at least death is even-handed – that the rich and powerful will also die. The poor will not be comfortable with the thought that they have to die, while the rich will remain young and beautiful for ever.

45. In previous eras the standard of beauty was set by the handful of people who lived next door to you. Today the media and the fashion industry expose us to a totally unrealistic standard of beauty. They search out the most gorgeous people on the planet, and then parade them constantly before our eyes. No wonder we are far less happy with the way we look.

But the tiny minority able to afford the new treatments will not be euphoric either. They will have much to be anxious about. Although the new therapies could extend life and youth, they cannot revive corpses. How dreadful to think that I and my loved ones can live for ever, but only if we don’t get hit by a truck or blown to smithereens by a terrorist! Potentially a-mortal people are likely to grow averse to taking even the slightest risk, and the agony of losing a spouse, child or close friend will be unbearable.
Chemical Happiness

Social scientists distribute subjective well-being questionnaires and correlate the results with socio-economic factors such as wealth and political freedom. Biologists use the same questionnaires, but correlate the answers people give them with biochemical and genetic factors. Their findings are shocking.
Biologists hold that our mental and emotional world is governed by biochemical mechanisms shaped by millions of years of evolution. Like all other mental states, our subjective well-being is not determined by external parameters such as salary, social relations or political rights. Rather, it is determined by a complex system of nerves, neurons, synapses and various biochemical substances such as serotonin, dopamine and oxytocin.
Nobody is ever made happy by winning the lottery, buying a house, getting a promotion or even finding true love. People are made happy by one thing and one thing only – pleasant sensations in their bodies. A person who just won the lottery or found new love and jumps from joy is not really reacting to the money or the lover. She is reacting to various hormones coursing through her bloodstream, and to the storm of electric signals flashing between different parts of her brain.
Unfortunately for all hopes of creating heaven on earth, our internal biochemical system seems to be programmed to keep happiness levels relatively constant. There’s no natural selection for happiness as such – a happy hermit’s genetic line will go extinct as the genes of a pair of anxious parents get carried on to the next generation. Happiness and misery play a role in evolution only to the extent that they encourage or discourage survival and reproduction. Perhaps it’s not surprising, then, that evolution has moulded us to be neither too miserable nor too happy. It enables us to enjoy a momentary rush of pleasant sensations, but these never last for ever. Sooner or later they subside and give place to unpleasant sensations.
For example, evolution provided pleasant feelings as rewards to males who spread their genes by having sex with fertile females. If sex were not accompanied by such pleasure, few males would bother. At the same time, evolution made sure that these pleasant feelings quickly subsided. If orgasms were to last for ever, the very happy males would die of hunger for lack of interest in food, and would not take the trouble to look for additional fertile females.
Some scholars compare human biochemistry to an air-conditioning system that keeps the temperature constant, come heatwave or snowstorm. Events might momentarily change the temperature, but the air-conditioning system always returns the temperature to the same set point.
Some air-conditioning systems are set at twenty-five degrees Celsius. Others are set at twenty degrees. Human happiness conditioning systems also differ from person to person. On a scale from one to ten, some people are born with a cheerful biochemical system that allows their mood to swing between levels six and ten, stabilising with time at eight. Such a person is quite happy even if she lives in an alienating big city, loses all her money in a stock-exchange crash and is diagnosed with diabetes. Other people are cursed with a gloomy biochemistry that swings between three and seven and stabilises at five. Such an unhappy person remains depressed even if she enjoys the support of a tight-knit community, wins millions in the lottery and is as healthy as an Olympic athlete. Indeed, even if our gloomy friend wins $50,000,000 in the morning, discovers the cure for both AIDS and cancer by noon, makes peace between Israelis and Palestinians that afternoon, and then in the evening reunites with her long-lost child who disappeared years ago – she would still be incapable of experiencing anything beyond level seven happiness. Her brain is simply not built for exhilaration, come what may.
Think for a moment of your family and friends. You know some people who remain relatively joyful, no matter what befalls them. And then there are those who are always disgruntled, no matter what gifts the world lays at their feet. We tend to believe that if we could just change our workplace, get married, finish writing that novel, buy a new car or repay the mortgage, we would be on top of the world. Yet when we get what we desire we don’t seem to be any happier. Buying cars and writing novels do not change our biochemistry. They can startle it for a fleeting moment, but it is soon back to its set point.
How can this be squared with the above-mentioned psychological and sociological findings that, for example, married people are happier on average than singles? First, these findings are correlations – the direction of causation may be the opposite of what some researchers have assumed. It is true that married people are happier than singles and divorcees, but that does not necessarily mean that marriage produces happiness. It could be that happiness causes marriage. Or more correctly, that serotonin, dopamine and oxytocin bring about and maintain a marriage. People who are born with a cheerful biochemistry are generally happy and content. Such people are more attractive spouses, and consequently they have a greater chance of getting married. They are also less likely to divorce, because it is far easier to live with a happy and content spouse than with a depressed and dissatisfied one. Consequently, it’s true that married people are happier on average than singles, but a single woman prone to gloom because of her biochemistry would not necessarily become happier if she were to hook up with a husband.
In addition, most biologists are not fanatics. They maintain that happiness is determined mainly by biochemistry, but they agree that psychological and sociological factors also have their place. Our mental air-conditioning system has some freedom of movement within predetermined borders. It is almost impossible to exceed the upper and lower emotional boundaries, but marriage and divorce can have an impact in the area between the two. Somebody born with an average of level five happiness would never dance wildly in the streets. But a good marriage should enable her to enjoy level seven from time to time, and to avoid the despondency of level three.
If we accept the biological approach to happiness, then history turns out to be of minor importance, since most historical events have had no impact on our biochemistry. History can change the external stimuli that cause serotonin to be secreted, yet it does not change the resulting serotonin levels, and hence it cannot make people happier.
Compare a medieval French peasant to a modern Parisian banker. The peasant lived in an unheated mud hut overlooking the local pigsty, while the banker goes home to a splendid penthouse with all the latest technological gadgets and a view to the Champs-Elysées. Intuitively, we would expect the banker to be much happier than the peasant. However, mud huts, penthouses and the Champs-Elysées don’t really determine our mood. Serotonin does. When the medieval peasant completed the construction of his mud hut, his brain neurons secreted serotonin, bringing it up to level X. When in 2014 the banker made the last payment on his wonderful penthouse, brain neurons secreted a similar amount of serotonin, bringing it up to a similar level X. It makes no difference to the brain that the penthouse is far more comfortable than the mud hut. The only thing that matters is that at present the level of serotonin is X. Consequently the banker would not be one iota happier than his great-great-great-grandfather, the poor medieval peasant.
This is true not only of private lives, but also of great collective events. Take, for example, the French Revolution. The revolutionaries were busy: they executed the king, gave lands to the peasants, declared the rights of man, abolished noble privileges and waged war against the whole of Europe. Yet none of that changed French biochemistry. Consequently, despite all the political, social, ideological and economic upheavals brought about by the revolution, its impact on French happiness was small. Those who won a cheerful biochemistry in the genetic lottery were just as happy before the revolution as after. Those with a gloomy biochemistry complained about Robespierre and Napoleon with the same bitterness with which they earlier complained about Louis XVI and Marie Antoinette.
If so, what good was the French Revolution? If people did not become any happier, then what was the point of all that chaos, fear, blood and war? Biologists would never have stormed the Bastille. People think that this political revolution or that social reform will make them happy, but their biochemistry tricks them time and again.
There is only one historical development that has real significance. Today, when we finally realise that the keys to happiness are in the hands of our biochemical system, we can stop wasting our time on politics and social reforms, putsches and ideologies, and focus instead on the only thing that can make us truly happy: manipulating our biochemistry. If we invest billions in understanding our brain chemistry and developing appropriate treatments, we can make people far happier than ever before, without any need of revolutions. Prozac, for example, does not change regimes, but by raising serotonin levels it lifts people out of their depression.
Nothing captures the biological argument better than the famous New Age slogan: ‘Happiness Begins Within.’ Money, social status, plastic surgery, beautiful houses, powerful positions – none of these will bring you happiness. Lasting happiness comes only from serotonin, dopamine and oxytocin.1
In Aldous Huxley’s dystopian novel Brave New World, published in 1932 at the height of the Great Depression, happiness is the supreme value and psychiatric drugs replace the police and the ballot as the foundation of politics. Each day, each person takes a dose of ‘soma’, a synthetic drug which makes people happy without harming their productivity and efficiency. The World State that governs the entire globe is never threatened by wars, revolutions, strikes or demonstrations, because all people are supremely content with their current conditions, whatever they may be. Huxley’s vision of the future is far more troubling than George Orwell’s Nineteen Eighty-Four. Huxley’s world seems monstrous to most readers, but it is hard to explain why. Everybody is happy all the time – what could be wrong with that?
The Meaning of Life

Huxley’s disconcerting world is based on the biological assumption that happiness equals pleasure. To be happy is no more and no less than experiencing pleasant bodily sensations. Since our biochemistry limits the volume and duration of these sensations, the only way to make people experience a high level of happiness over an extended period of time is to manipulate their biochemical system.
But that definition of happiness is contested by some scholars. In a famous study, Daniel Kahneman, winner of the Nobel Prize in economics, asked people to recount a typical work day, going through it episode by episode and evaluating how much they enjoyed or disliked each moment. He discovered what seems to be a paradox in most people’s view of their lives. Take the work involved in raising a child. Kahneman found that when counting moments of joy and moments of drudgery, bringing up a child turns out to be a rather unpleasant affair. It consists largely of changing nappies, washing dishes and dealing with temper tantrums, which nobody likes to do. Yet most parents declare that their children are their chief source of happiness. Does it mean that people don’t really know what’s good for them?
That’s one option. Another is that the findings demonstrate that happiness is not the surplus of pleasant over unpleasant moments. Rather, happiness consists in seeing one’s life in its entirety as meaningful and worthwhile. There is an important cognitive and ethical component to happiness. Our values make all the difference to whether we see ourselves as ‘miserable slaves to a baby dictator’ or as ‘lovingly nurturing a new life’.2 As Nietzsche put it, if you have a why to live, you can bear almost any how. A meaningful life can be extremely satisfying even in the midst of hardship, whereas a meaningless life is a terrible ordeal no matter how comfortable it is.
Though people in all cultures and eras have felt the same type of pleasures and pains, the meaning they have ascribed to their experiences has probably varied widely. If so, the history of happiness might have been far more turbulent than biologists imagine. It’s a conclusion that does not necessarily favour modernity. Assessing life minute by minute, medieval people certainly had it rough. However, if they believed the promise of everlasting bliss in the afterlife, they may well have viewed their lives as far more meaningful and worthwhile than modern secular people, who in the long term can expect nothing but complete and meaningless oblivion. Asked ‘Are you satisfied with your life as a whole?’, people in the Middle Ages might have scored quite highly in a subjective well-being questionnaire.
So our medieval ancestors were happy because they found meaning to life in collective delusions about the afterlife? Yes. As long as nobody punctured their fantasies, why shouldn’t they? As far as we can tell, from a purely scientific viewpoint, human life has absolutely no meaning. Humans are the outcome of blind evolutionary processes that operate without goal or purpose. Our actions are not part of some divine cosmic plan, and if planet Earth were to blow up tomorrow morning, the universe would probably keep going about its business as usual. As far as we can tell at this point, human subjectivity would not be missed. Hence any meaning that people ascribe to their lives is just a delusion. The other-worldly meanings medieval people found in their lives were no more deluded than the modern humanist, nationalist and capitalist meanings modern people find. The scientist who says her life is meaningful because she increases the store of human knowledge, the soldier who declares that his life is meaningful because he fights to defend his homeland, and the entrepreneur who finds meaning in building a new company are no less delusional than their medieval counterparts who found meaning in reading scriptures, going on a crusade or building a new cathedral.
So perhaps happiness is synchronising one’s personal delusions of meaning with the prevailing collective delusions. As long as my personal narrative is in line with the narratives of the people around me, I can convince myself that my life is meaningful, and find happiness in that conviction.
This is quite a depressing conclusion. Does happiness really depend on self-delusion?
Know Thyself

If happiness is based on feeling pleasant sensations, then in order to be happier we need to re-engineer our biochemical system. If happiness is based on feeling that life is meaningful, then in order to be happier we need to delude ourselves more effectively. Is there a third alternative?
Both the above views share the assumption that happiness is some sort of subjective feeling (of either pleasure or meaning), and that in order to judge people’s happiness, all we need to do is ask them how they feel. To many of us, that seems logical because the dominant religion of our age is liberalism. Liberalism sanctifies the subjective feelings of individuals. It views these feelings as the supreme source of authority. What is good and what is bad, what is beautiful and what is ugly, what ought to be and what ought not to be, are all determined by what each one of us feels.
Liberal politics is based on the idea that the voters know best, and there is no need for Big Brother to tell us what is good for us. Liberal economics is based on the idea that the customer is always right. Liberal art declares that beauty is in the eye of the beholder. Students in liberal schools and universities are taught to think for themselves. Commercials urge us to ‘Just do it!’ Action films, stage dramas, soap operas, novels and catchy pop songs indoctrinate us constantly: ‘Be true to yourself’, ‘Listen to yourself’, ‘Follow your heart’. Jean-Jacques Rousseau stated this view most classically: ‘What I feel to be good – is good. What I feel to be bad – is bad.’
People who have been raised from infancy on a diet of such slogans are prone to believe that happiness is a subjective feeling and that each individual best knows whether she is happy or miserable. Yet this view is unique to liberalism. Most religions and ideologies throughout history stated that there are objective yardsticks for goodness and beauty, and for how things ought to be. They were suspicious of the feelings and preferences of the ordinary person. At the entrance of the temple of Apollo at Delphi, pilgrims were greeted by the inscription: ‘Know thyself!’ The implication was that the average person is ignorant of his true self, and is therefore likely to be ignorant of true happiness. Freud would probably concur.*
And so would Christian theologians. St Paul and St Augustine knew perfectly well that if you asked people about it, most of them would prefer to have sex than pray to God. Does that prove that having sex is the key to happiness? Not according to Paul and Augustine. It proves only that humankind is sinful by nature, and that people are easily seduced by Satan. From a Christian viewpoint, the vast majority of people are in more or less the same situation as heroin addicts. Imagine that a psychologist embarks on a study of happiness among drug users. He polls them and finds that they declare, every single one of them, that they are only happy when they shoot up. Would the psychologist publish a paper declaring that heroin is the key to happiness?
The idea that feelings are not to be trusted is not restricted to Christianity. At least when it comes to the value of feelings, even Darwin and Dawkins might find common ground with St Paul and St Augustine. According to the selfish gene theory, natural selection makes people, like other organisms, choose what is good for the reproduction of their genes, even if it is bad for them as individuals. Most males spend their lives toiling, worrying, competing and fighting, instead of enjoying peaceful bliss, because their DNA manipulates them for its own selfish aims. Like Satan, DNA uses fleeting pleasures to tempt people and place them in its power.
Most religions and philosophies have consequently taken a very different approach to happiness than liberalism does.3 The Buddhist position is particularly interesting. Buddhism has assigned the question of happiness more importance than perhaps any other human creed. For 2,500 years, Buddhists have systematically studied the essence and causes of happiness, which is why there is a growing interest among the scientific community both in their philosophy and their meditation practices.
Buddhism shares the basic insight of the biological approach to happiness, namely that happiness results from processes occurring within one’s body, and not from events in the outside world. However, starting from the same insight, Buddhism reaches very different conclusions.
According to Buddhism, most people identify happiness with pleasant feelings, while identifying suffering with unpleasant feelings. People consequently ascribe immense importance to what they feel, craving to experience more and more pleasures, while avoiding pain. Whatever we do throughout our lives, whether scratching our leg, fidgeting slightly in the chair, or fighting world wars, we are just trying to get pleasant feelings.
The problem, according to Buddhism, is that our feelings are no more than fleeting vibrations, changing every moment, like the ocean waves. If five minutes ago I felt joyful and purposeful, now these feelings are gone, and I might well feel sad and dejected. So if I want to experience pleasant feelings, I have to constantly chase them, while driving away the unpleasant feelings. Even if I succeed, I immediately have to start all over again, without ever getting any lasting reward for my troubles.
What is so important about obtaining such ephemeral prizes? Why struggle so hard to achieve something that disappears almost as soon as it arises? According to Buddhism, the root of suffering is neither the feeling of pain nor of sadness nor even of meaninglessness. Rather, the real root of suffering is this never-ending and pointless pursuit of ephemeral feelings, which causes us to be in a constant state of tension, restlessness and dissatisfaction. Due to this pursuit, the mind is never satisfied. Even when experiencing pleasure, it is not content, because it fears this feeling might soon disappear, and craves that this feeling should stay and intensify.
People are liberated from suffering not when they experience this or that fleeting pleasure, but rather when they understand the impermanent nature of all their feelings, and stop craving them. This is the aim of Buddhist meditation practices. In meditation, you are supposed to closely observe your mind and body, witness the ceaseless arising and passing of all your feelings, and realise how pointless it is to pursue them. When the pursuit stops, the mind becomes very relaxed, clear and satisfied. All kinds of feelings go on arising and passing – joy, anger, boredom, lust – but once you stop craving particular feelings, you can just accept them for what they are. You live in the present moment instead of fantasising about what might have been.
The resulting serenity is so profound that those who spend their lives in the frenzied pursuit of pleasant feelings can hardly imagine it. It is like a man standing for decades on the seashore, embracing certain ‘good’ waves and trying to prevent them from disintegrating, while simultaneously pushing back ‘bad’ waves to prevent them from getting near him. Day in, day out, the man stands on the beach, driving himself crazy with this fruitless exercise. Eventually, he sits down on the sand and just allows the waves to come and go as they please. How peaceful!
This idea is so alien to modern liberal culture that when Western New Age movements encountered Buddhist insights, they translated them into liberal terms, thereby turning them on their head. New Age cults frequently argue: ‘Happiness does not depend on external conditions. It depends only on what we feel inside. People should stop pursuing external achievements such as wealth and status, and connect instead with their inner feelings.’ Or more succinctly, ‘Happiness Begins Within.’ This is exactly what biologists argue, but more or less the opposite of what Buddha said.
Buddha agreed with modern biology and New Age movements that happiness is independent of external conditions. Yet his more important and far more profound insight was that true happiness is also independent of our inner feelings. Indeed, the more significance we give our feelings, the more we crave them, and the more we suffer. Buddha’s recommendation was to stop not only the pursuit of external achievements, but also the pursuit of inner feelings.
To sum up, subjective well-being questionnaires identify our well-being with our subjective feelings, and identify the pursuit of happiness with the pursuit of particular emotional states. In contrast, for many traditional philosophies and religions, such as Buddhism, the key to happiness is to know the truth about yourself – to understand who, or what, you really are. Most people wrongly identify themselves with their feelings, thoughts, likes and dislikes. When they feel anger, they think, ‘I am angry. This is my anger.’ They consequently spend their life avoiding some kinds of feelings and pursuing others. They never realise that they are not their feelings, and that the relentless pursuit of particular feelings just traps them in misery.
If this is so, then our entire understanding of the history of happiness might be misguided. Maybe it isn’t so important whether people’s expectations are fulfilled and whether they enjoy pleasant feelings. The main question is whether people know the truth about themselves. What evidence do we have that people today understand this truth any better than ancient foragers or medieval peasants?
Scholars began to study the history of happiness only a few years ago, and we are still formulating initial hypotheses and searching for appropriate research methods. It’s much too early to adopt rigid conclusions and end a debate that’s hardly yet begun. What is important is to get to know as many different approaches as possible and to ask the right questions.
Most history books focus on the ideas of great thinkers, the bravery of warriors, the charity of saints and the creativity of artists. They have much to tell about the weaving and unravelling of social structures, about the rise and fall of empires, about the discovery and spread of technologies. Yet they say nothing about how all this influenced the happiness and suffering of individuals. This is the biggest lacuna in our understanding of history. We had better start filling it.
* Paradoxically, while psychological studies of subjective well-being rely on people’s ability to diagnose their happiness correctly, the basic raison d’être of psychotherapy is that people don’t really know themselves and that they sometimes need professional help to free themselves of self-destructive behaviours.20

The End of Homo Sapiens

THIS BOOK BEGAN BY PRESENTING HISTORY as the next stage in the continuum of physics to chemistry to biology. Sapiens are subject to the same physical forces, chemical reactions and natural-selection processes that govern all living beings. Natural selection may have provided Homo sapiens with a much larger playing field than it has given to any other organism, but the field has still had its boundaries. The implication has been that, no matter what their efforts and achievements, Sapiens are incapable of breaking free of their biologically determined limits.
But at the dawn of the twenty-first century, this is no longer true: Homo sapiens is transcending those limits. It is now beginning to break the laws of natural selection, replacing them with the laws of intelligent design.
For close to 4 billion years, every single organism on the planet evolved subject to natural selection. Not even one was designed by an intelligent creator. The giraffe, for example, got its long neck thanks to competition between archaic giraffes rather than to the whims of a super-intelligent being. Proto-giraffes who had longer necks had access to more food and consequently produced more offspring than did those with shorter necks. Nobody, certainly not the giraffes, said, ‘A long neck would enable giraffes to munch leaves off the treetops. Let’s extend it.’ The beauty of Darwin’s theory is that it does not need to assume an intelligent designer to explain how giraffes ended up with long necks.
For billions of years, intelligent design was not even an option, because there was no intelligence which could design things. Microorganisms, which until quite recently were the only living things around, are capable of amazing feats. A microorganism belonging to one species can incorporate genetic codes from a completely different species into its cell and thereby gain new capabilities, such as resistance to antibiotics. Yet, as best we know, microorganisms have no consciousness, no aims in life, and no ability to plan ahead.
At some stage organisms such as giraffes, dolphins, chimpanzees and Neanderthals evolved consciousness and the ability to plan ahead. But even if a Neanderthal fantasised about fowls so fat and slow-moving that he could just scoop them up whenever he was hungry, he had no way of turning that fantasy into reality. He had to hunt the birds that had been naturally selected.
The first crack in the old regime appeared about 10,000 years ago, during the Agricultural Revolution. Sapiens who dreamed of fat, slow-moving chickens discovered that if they mated the fattest hen with the slowest cock, some of their offspring would be both fat and slow. If you mated those offspring with each other, you could produce a line of fat, slow birds. It was a race of chickens unknown to nature, produced by the intelligent design not of a god but of a human.
Still, compared to an all-powerful deity, Homo sapiens had limited design skills. Sapiens could use selective breeding to detour around and accelerate the natural-selection processes that normally affected chickens, but they could not introduce completely new characteristics that were absent from the genetic pool of wild chickens. In a way, the relationship between Homo sapiens and chickens was similar to many other symbiotic relationships that have so often arisen on their own in nature. Sapiens exerted peculiar selective pressures on chickens that caused the fat and slow ones to proliferate, just as pollinating bees select flowers, causing the bright colourful ones to proliferate.
Today, the 4-billion-year-old regime of natural selection is facing a completely different challenge. In laboratories throughout the world, scientists are engineering living beings. They break the laws of natural selection with impunity, unbridled even by an organisms original characteristics. Eduardo Kac, a Brazilian bio-artist, decided in 2000 to create a new work of art: a fluorescent green rabbit. Kac contacted a French laboratory and offered it a fee to engineer a radiant bunny according to his specifications. The French scientists took a run-of-the-mill white rabbit embryo, implanted in its DNA a gene taken from a green fluorescent jellyfish, and voilà! One green fluorescent rabbit for le monsieur. Kac named the rabbit Alba.
It is impossible to explain the existence of Alba through the laws of natural selection. She is the product of intelligent design. She is also a harbinger of things to come. If the potential Alba signifies is realised in full – and if humankind doesn’t annihilate itself meanwhile – the Scientific Revolution might prove itself far greater than a mere historical revolution. It may turn out to be the most important biological revolution since the appearance of life on earth. After 4 billion years of natural selection, Alba stands at the dawn of a new cosmic era, in which life will be ruled by intelligent design. If this happens, the whole of human history up to that point might, with hindsight, be reinterpreted as a process of experimentation and apprenticeship that revolutionised the game of life. Such a process should be understood from a cosmic perspective of billions of years, rather than from a human perspective of millennia.
Biologists the world over are locked in battle with the intelligent-design movement, which opposes the teaching of Darwinian evolution in schools and claims that biological complexity proves there must be a creator who thought out all biological details in advance. The biologists are right about the past, but the proponents of intelligent design might, ironically, be right about the future.
At the time of writing, the replacement of natural selection by intelligent design could happen in any of three ways: through biological engineering, cyborg engineering (cyborgs are beings that combine organic with non-organic parts) or the engineering of inorganic life.
Of Mice and Men

Biological engineering is deliberate human intervention on the biological level (e.g. implanting a gene) aimed at modifying an organisms shape, capabilities, needs or desires, in order to realize some preconceived cultural idea, such as the artistic predilections of Eduardo Kac.
There is nothing new about biological engineering, per se. People have been using it for millennia in order to reshape themselves and other organisms. A simple example is castration. Humans have been castrating bulls for perhaps 10,000 years in order to create oxen. Oxen are less aggressive, and are thus easier to train to pull ploughs. Humans also castrated their own young males to create soprano singers with enchanting voices and eunuchs who could safely be entrusted with overseeing the sultans harem.
But recent advances in our understanding of how organisms work, down to the cellular and nuclear levels, have opened up previously unimaginable possibilities. For instance, we can today not merely castrate a man, but also change his sex through surgical and hormonal treatments. But that’s not all. Consider the surprise, disgust and consternation that ensued when, in 1996, the following photograph appeared in newspapers and on television:

46. A mouse on whose back scientists grew an ‘ear’ made of cattle cartilage cells. It is an eerie echo of the lion-man statue from the Stadel Cave. Thirty thousand years ago, humans were already fantasising about combining different species. Today, they can actually produce such chimeras.

No, Photoshop was not involved. It’s an untouched photo of a real mouse on whose back scientists implanted cattle cartilage cells. The scientists were able to control the growth of the new tissue, shaping it in this case into something that looks like a human ear. The process may soon enable scientists to manufacture artificial ears, which could then be implanted in humans.1
Even more remarkable wonders can be performed with genetic engineering, which is why it raises a host of ethical, political and ideological issues. And it’s not just pious monotheists who object that man should not usurp God’s role. Many confirmed atheists are no less shocked by the idea that scientists are stepping into nature’s shoes. Animal-rights activists decry the suffering caused to lab animals in genetic engineering experiments, and to the farmyard animals that are engineered in complete disregard of their needs and desires. Human-rights activists are afraid that genetic engineering might be used to create supermen who will make serfs of the rest of us. Jeremiahs offer apocalyptic visions of bio-dictatorships that will clone fearless soldiers and obedient workers. The prevailing feeling is that too many opportunities are opening too quickly and that our ability to modify genes is outpacing our capacity for making wise and far-sighted use of the skill.
The result is that we’re at present using only a fraction of the potential of genetic engineering. Most of the organisms now being engineered are those with the weakest political lobbies – plants, fungi, bacteria and insects. For example, lines of E. coli, a bacterium that lives symbiotically in the human gut (and which makes headlines when it gets out of the gut and causes deadly infections), have been genetically engineered to produce biofuel.2 E. coli and several species of fungi have also been engineered to produce insulin, thereby lowering the cost of diabetes treatment.3 A gene extracted from an Arctic fish has been inserted into potatoes, making the plants more frost-resistant.4
A few mammals have also been subject to genetic engineering. Every year the dairy industry suffers billions of dollars in damages due to mastitis, a disease that strikes dairy-cow udders. Scientists are currently experimenting with genetically engineered cows whose milk contains lysostaphin, a biochemical that attacks the bacteria responsible for the disease.5 The pork industry, which has suffered from falling sales because consumers are wary of the unhealthy fats in ham and bacon, has hopes for a still-experimental line of pigs implanted with genetic material from a worm. The new genes cause the pigs to turn bad omega 6 fatty acid into its healthy cousin, omega 3.6
The next generation of genetic engineering will make pigs with good fat look like child’s play. Geneticists have managed not merely to extend sixfold the average life expectancy of worms, but also to engineer genius mice that display much-improved memory and learning skills.7 Voles are small, stout rodents resembling mice, and most varieties of voles are promiscuous. But there is one species in which boy and girl voles form lasting and monogamous relationships. Geneticists claim to have isolated the genes responsible for vole monogamy. If the addition of a gene can turn a vole Don Juan into a loyal and loving husband, are we far off from being able to genetically engineer not only the individual abilities of rodents (and humans), but also their social structures?8
The Return of the Neanderthals

But geneticists do not only want to transform living lineages. They aim to revive extinct creatures as well. And not just dinosaurs, as in Jurassic Park. A team of Russian, Japanese and Korean scientists has recently mapped the genome of ancient mammoths, found frozen in the Siberian ice. They now plan to take a fertilised egg-cell of a present-day elephant, replace the elephantine DNA with a reconstructed mammoth DNA, and implant the egg in the womb of an elephant. After about twenty-two months, they expect the first mammoth in 5,000 years to be born.9
But why stop at mammoths? Professor George Church of Harvard University recently suggested that, with the completion of the Neanderthal Genome Project, we can now implant reconstructed Neanderthal DNA into a Sapiens ovum, thus producing the first Neanderthal child in 30,000 years. Church claimed that he could do the job for a paltry $30 million. Several women have already volunteered to serve as surrogate mothers.10
What do we need Neanderthals for? Some argue that if we could study live Neanderthals, we could answer some of the most nagging questions about the origins and uniqueness of Homo sapiens. By comparing a Neanderthal to a Homo sapiens brain, and mapping out where their structures differ, perhaps we could identify what biological change produced consciousness as we experience it. There’s an ethical reason, too – some have argued that if Homo sapiens was responsible for the extinction of the Neanderthals, it has a moral duty to resurrect them. And having some Neanderthals around might be useful. Lots of industrialists would be glad to pay one Neanderthal to do the menial work of two Sapiens.
But why stop even at Neanderthals? Why not go back to God’s drawing board and design a better Sapiens? The abilities, needs and desires of Homo sapiens have a genetic basis, and the Sapiens genome is no more complex than that of voles and mice. (The mouse genome contains about 2.5 billion nucleobases, the Sapiens genome about 2.9 billion bases – meaning the latter is only 14 per cent larger.)11 In the medium range – perhaps in a few decades – genetic engineering and other forms of biological engineering might enable us to make far-reaching alterations not only to our physiology, immune system and life expectancy, but also to our intellectual and emotional capacities. If genetic engineering can create genius mice, why not genius humans? If it can create monogamous voles, why not humans hard-wired to remain faithful to their partners?
The Cognitive Revolution that turned Homo sapiens from an insignificant ape into the master of the world did not require any noticeable change in physiology or even in the size and external shape of the Sapiens brain. It apparently involved no more than a few small changes to internal brain structure. Perhaps another small change would be enough to ignite a Second Cognitive Revolution, create a completely new type of consciousness, and transform Homo sapiens into something altogether different.
True, we still don’t have the acumen to achieve this, but there seems to be no insurmountable technical barrier preventing us from producing superhumans. The main obstacles are the ethical and political objections that have slowed down research on humans. And no matter how convincing the ethical arguments may be, it is hard to see how they can hold back the next step for long, especially if what is at stake is the possibility of prolonging human life indefinitely, conquering incurable diseases, and upgrading our cognitive and emotional abilities.
What would happen, for example, if we developed a cure for Alzheimer’s disease that, as a side benefit, could dramatically improve the memories of healthy people? Would anyone be able to halt the relevant research? And when the cure is developed, could any law enforcement agency limit it to Alzheimer’s patients and prevent healthy people from using it to acquire super-memories?
It’s unclear whether bioengineering could really resurrect the Neanderthals, but it would very likely bring down the curtain on Homo sapiens. Tinkering with our genes won’t necessarily kill us. But we might fiddle with Homo sapiens to such an extent that we would no longer be Homo sapiens.
Bionic Life

There is another new technology which could change the laws of life: cyborg engineering. Cyborgs are beings which combine organic and inorganic parts, such as a human with bionic hands. In a sense, nearly all of us are bionic these days, since our natural senses and functions are supplemented by devices such as eyeglasses, pacemakers, orthotics, and even computers and mobile phones (which relieve our brains of some of their data storage and processing burdens). We stand poised on the brink of becoming true cyborgs, of having inorganic features that are inseparable from our bodies, features that modify our abilities, desires, personalities and identities.
The Defense Advanced Research Projects Agency (DARPA), a US military research agency, is developing cyborgs out of insects. The idea is to implant electronic chips, detectors and processors in the body of a fly or cockroach, which will enable either a human or an automatic operator to control the insect’s movements remotely and to absorb and transmit information. Such a fly could be sitting on the wall at enemy headquarters, eavesdrop on the most secret conversations, and if it isn’t caught first by a spider, could inform us exactly what the enemy is planning.12 In 2006 the US Naval Undersea Warfare Center reported its intention to develop cyborg sharks, declaring, ‘NUWC is developing a fish tag whose goal is behaviour control of host animals via neural implants.’ The developers hope to identify underwater electromagnetic fields made by submarines and mines, by exploiting the natural magnetic detecting capabilities of sharks, which are superior to those of any man-made detectors.13
Sapiens, too, are being turned into cyborgs. The newest generation of hearing aids are sometimes referred to as ‘bionic ears’. The device consists of an implant that absorbs sound through a microphone located in the outer part of the ear. The implant filters the sounds, identifies human voices, and translates them into electric signals that are sent directly to the central auditory nerve and from there to the brain.14
Retina Implant, a government-sponsored German company, is developing a retinal prosthesis that may allow blind people to gain partial vision. It involves implanting a small microchip inside the patient’s eye. Photocells absorb light falling on the eye and transform it into electrical energy, which stimulates the intact nerve cells in the retina. The nervous impulses from these cells stimulate the brain, where they are translated into sight. At present the technology allows patients to orientate themselves in space, identify letters, and even recognise faces.15
Jesse Sullivan, an American electrician, lost both arms up to the shoulder in a 2001 accident. Today he uses two bionic arms, courtesy of the Rehabilitation Institute of Chicago. The special feature of Jesse’s new arms is that they are operated by thought alone. Neural signals arriving from Jesse’s brain are translated by micro-computers into electrical commands, and the arms move. When Jesse wants to raise his arm, he does what any normal person unconsciously does – and the arm rises. These arms can perform a much more limited range of movements than organic arms, but they enable Jesse to carry out simple daily functions. A similar bionic arm has recently been outfitted for Claudia Mitchell, an American soldier who lost her arm in a motorcycle accident. Scientists believe that we will soon have bionic arms that will not only move when willed to move, but will also be able to transmit signals back to the brain, thereby enabling amputees to regain even the sensation of touch!16

47. Jesse Sullivan and Claudia Mitchell holding hands. The amazing thing about their bionic arms is that they are operated by thought.

At present these bionic arms are a poor replacement for our organic originals, but they have the potential for unlimited development. Bionic arms, for example, can be made far more powerful than their organic kin, making even a boxing champion feel like a weakling. Moreover, bionic arms have the advantage that they can be replaced every few years, or detached from the body and operated at a distance.
Scientists at Duke University in North Carolina have recently demonstrated this with rhesus monkeys whose brains have been implanted with electrodes. The electrodes gather signals from the brain and transmit them to external devices. The monkeys have been trained to control detached bionic arms and legs through thought alone. One monkey, named Aurora, learned to thought-control a detached bionic arm while simultaneously moving her two organic arms. Like some Hindu goddess, Aurora now has three arms, and her arms can be located in different rooms – or even cities. She can sit in her North Carolina lab, scratch her back with one hand, scratch her head with a second hand, and simultaneously steal a banana in New York (although the ability to eat a purloined fruit at a distance remains a dream). Another rhesus monkey, Idoya, won world fame in 2008 when she thought-controlled a pair of bionic legs in Kyoto, Japan, from her North Carolina chair. The legs were twenty times Idoya’s weight.17
Locked-in syndrome is a condition in which a person loses all or nearly all her ability to move any part of her body, while her cognitive abilities remain intact. Patients suffering from the syndrome have up till now been able to communicate with the outside world only through small eye movements. However, a few patients have had brain-signal-gathering electrodes implanted in their brains. Efforts are being made to translate such signals not merely into movements but also into words. If the experiments succeed, locked-in patients could finally speak directly with the outside world, and we might eventually be able to use the technology to read other peoples minds.18
Yet of all the projects currently under development, the most revolutionary is the attempt to devise a direct two-way brain-computer interface that will allow computers to read the electrical signals of a human brain, simultaneously transmitting signals that the brain can read in turn. What if such interfaces are used to directly link a brain to the Internet, or to directly link several brains to each other, thereby creating a sort of Inter-brain-net? What might happen to human memory, human consciousness and human identity if the brain has direct access to a collective memory bank? In such a situation, one cyborg could, for example, retrieve the memories of another – not hear about them, not read about them in an autobiography, not imagine them, but directly remember them as if they were his own. Or her own. What happens to concepts such as the self and gender identity when minds become collective? How could you know thyself or follow your dream if the dream is not in your mind but in some collective reservoir of aspirations?
Such a cyborg would no longer be human, or even organic. It would be something completely different. It would be so fundamentally another kind of being that we cannot even grasp the philosophical, psychological or political implications.
Another Life

The third way to change the laws of life is to engineer completely inorganic beings. The most obvious examples are computer programs and computer viruses that can undergo independent evolution.
The field of genetic programming is today one of the most interesting spots in the computer science world. It tries to emulate the methods of genetic evolution. Many programmers dream of creating a program that could learn and evolve completely independently of its creator. In this case, the programmer would be a primum mobile, a first mover, but his creation would be free to evolve in directions neither its maker nor any other human could ever have envisaged.
A prototype for such a program already exists – it’s called a computer virus. As it spreads through the Internet, the virus replicates itself millions upon millions of times, all the while being chased by predatory antivirus programs and competing with other viruses for a place in cyberspace. One day when the virus replicates itself a mistake occurs – a computerised mutation. Perhaps the mutation occurs because the human engineer programmed the virus to make occasional random replication mistakes. Perhaps the mutation was due to a random error. If, by chance, the modified virus is better at evading antivirus programs without losing its ability to invade other computers, it will spread through cyberspace. If so, the mutants will survive and reproduce. As time goes by, cyberspace would be full of new viruses that nobody engineered, and that undergo non-organic evolution.
Are these living creatures? It depends on what you mean by ‘living creatures’. They have certainly been produced by a new evolutionary process, completely independent of the laws and limitations of organic evolution.
Imagine another possibility – suppose you could back up your brain to a portable hard drive and then run it on your laptop. Would your laptop be able to think and feel just like a Sapiens? If so, would it be you or someone else? What if computer programmers could create an entirely new but digital mind, composed of computer code, complete with a sense of self, consciousness and memory? If you ran the program on your computer, would it be a person? If you deleted it could you be charged with murder?
We might soon have the answer to such questions. The Human Brain Project, founded in 2005, hopes to recreate a complete human brain inside a computer, with electronic circuits in the computer emulating neural networks in the brain. The projects director has claimed that, if funded properly, within a decade or two we could have an artificial human brain inside a computer that could talk and behave very much as a human does. If successful, that would mean that after 4 billion years of milling around inside the small world of organic compounds, life will suddenly break out into the vastness of the inorganic realm, ready to take up shapes beyond our wildest dreams. Not all scholars agree that the mind works in a manner analogous to today’s digital computers – and if it doesn’t, present-day computers would not be able to simulate it. Yet it would be foolish to categorically dismiss the possibility before giving it a try. In 2013 the project received a grant of €1 billion from the European Union.19
The Singularity

Presently, only a tiny fraction of these new opportunities have been realised. Yet the world of 2014 is already a world in which culture is releasing itself from the shackles of biology. Our ability to engineer not merely the world around us, but above all the world inside our bodies and minds, is developing at breakneck speed. More and more spheres of activity are being shaken out of their complacent ways. Lawyers need to rethink issues of privacy and identity; governments are faced with rethinking matters of health care and equality; sports associations and educational institutions need to redefine fair play and achievement; pension funds and labour markets should readjust to a world in which sixty might be the new thirty. They must all deal with the conundrums of bioengineering, cyborgs and inorganic life.
Mapping the first human genome required fifteen years and $3 billion. Today you can map a person’s DNA within a few weeks and at the cost of a few hundred dollars.20 The era of personalized medicine – medicine that matches treatment to DNA – has begun. The family doctor could soon tell you with greater certainty that you face high risks of liver cancer, whereas you needn’t worry too much about heart attacks. She could determine that a popular medication that helps 92 per cent of people is useless to you, and you should instead take another pill, fatal to many people but just right for you. The road to near-perfect medicine stands before us.
However, with improvements in medical knowledge will come new ethical conundrums. Ethicists and legal experts are already wrestling with the thorny issue of privacy as it relates to DNA. Would insurance companies be entitled to ask for our DNA scans and to raise premiums if they discover a genetic tendency to reckless behaviour? Would we be required to fax our DNA, rather than our CV, to potential employers? Could an employer favour a candidate because his DNA looks better? Or could we sue in such cases for ‘genetic discrimination’? Could a company that develops a new creature or a new organ register a patent on its DNA sequences? It is obvious that one can own a particular chicken, but can one own an entire species?
Such dilemmas are dwarfed by the ethical, social and political implications of the Gilgamesh Project and of our potential new abilities to create superhumans. The Universal Declaration of Human Rights, government medical programmes throughout the world, national health insurance programmes and national constitutions worldwide recognise that a humane society ought to give all its members fair medical treatment and keep them in relatively good health. That was all well and good as long as medicine was chiefly concerned with preventing illness and healing the sick. What might happen once medicine becomes preoccupied with enhancing human abilities? Would all humans be entitled to such enhanced abilities, or would there be a new superhuman elite?
Our late modern world prides itself on recognising, for the first time in history, the basic equality of all humans, yet it might be poised to create the most unequal of all societies. Throughout history, the upper classes always claimed to be smarter, stronger and generally better than the underclass. They were usually deluding themselves. A baby born to a poor peasant family was likely to be as intelligent as the crown prince. With the help of new medical capabilities, the pretensions of the upper classes might soon become an objective reality.
This is not science fiction. Most science-fiction plots describe a world in which Sapiens – identical to us – enjoy superior technology such as light-speed spaceships and laser guns. The ethical and political dilemmas central to these plots are taken from our own world, and they merely recreate our emotional and social tensions against a futuristic backdrop. Yet the real potential of future technologies is to change Homo sapiens itself, including our emotions and desires, and not merely our vehicles and weapons. What is a spaceship compared to an eternally young cyborg who does not breed and has no sexuality, who can share thoughts directly with other beings, whose abilities to focus and remember are a thousand times greater than our own, and who is never angry or sad, but has emotions and desires that we cannot begin to imagine?
Science fiction rarely describes such a future, because an accurate description is by definition incomprehensible. Producing a film about the life of some super-cyborg is akin to producing Hamlet for an audience of Neanderthals. Indeed, the future masters of the world will probably be more different from us than we are from Neanderthals. Whereas we and the Neanderthals are at least human, our inheritors will be godlike.
Physicists define the Big Bang as a singularity. It is a point at which all the known laws of nature did not exist. Time too did not exist. It is thus meaningless to say that anything existed ‘before’ the Big Bang. We may be fast approaching a new singularity, when all the concepts that give meaning to our world – me, you, men, women, love and hate – will become irrelevant. Anything happening beyond that point is meaningless to us.
The Frankenstein Prophecy

In 1818 Mary Shelley published Frankenstein, the story of a scientist who creates an artificial being that goes out of control and wreaks havoc. In the last two centuries, the same story has been told over and over again in countless versions. It has become a central pillar of our new scientific mythology. At first sight, the Frankenstein story appears to warn us that if we try to play God and engineer life we will be punished severely. Yet the story has a deeper meaning.
The Frankenstein myth confronts Homo sapiens with the fact that the last days are fast approaching. Unless some nuclear or ecological catastrophe intervenes, so goes the story, the pace of technological development will soon lead to the replacement of Homo sapiens by completely different beings who possess not only different physiques, but also very different cognitive and emotional worlds. This is something most Sapiens find extremely disconcerting. We like to believe that in the future people just like us will travel from planet to planet in fast spaceships. We don’t like to contemplate the possibility that in the future, beings with emotions and identities like ours will no longer exist, and our place will be taken by alien life forms whose abilities dwarf our own.
We somehow find comfort in the idea that Dr Frankenstein created a terrible monster, whom we had to destroy in order to save ourselves. We like to tell the story that way because it implies that we are the best of all beings, that there never was and never will be something better than us. Any attempt to improve us will inevitably fail, because even if our bodies might be improved, you cannot touch the human spirit.
We would have a hard time swallowing the fact that scientists could engineer spirits as well as bodies, and that future Dr Frankensteins could therefore create something truly superior to us, something that will look at us as condescendingly as we look at the Neanderthals.
We cannot be certain whether today’s Frankensteins will indeed fulfil this prophecy. The future is unknown, and it would be surprising if the forecasts of the last few pages were realised in full. History teaches us that what seems to be just around the corner may never materialise due to unforeseen barriers, and that other unimagined scenarios will in fact come to pass. When the nuclear age erupted in the 1940S, many forecasts were made about the future nuclear world of the year 2000. When sputnik and Apollo 11 fired the imagination of the world, everyone began predicting that by the end of the century, people would be living in space colonies on Mars and Pluto. Few of these forecasts came true. On the other hand, nobody foresaw the Internet.
So don’t go out just yet to buy liability insurance to indemnify you against lawsuits filed by digital beings. The above fantasies – or nightmares – are just stimulants for your imagination. What we should take seriously is the idea that the next stage of history will include not only technological and organisational transformations, but also fundamental transformations in human consciousness and identity. And these could be transformations so fundamental that they will call the very term ‘human’ into question. How long do we have? No one really knows. As already mentioned, some say that by 2050 a few humans will already be a-mortal. Less radical forecasts speak of the next century, or the next millennium. Yet from the perspective of 70,000 years of Sapiens history, what are a few millennia?
If the curtain is indeed about to drop on Sapiens history, we members of one of its final generations should devote some time to answering one last question: what do we want to become? This question, sometimes known as the Human Enhancement question, dwarfs the debates that currently preoccupy politicians, philosophers, scholars and ordinary people. After all, today’s debate between today’s religions, ideologies, nations and classes will in all likelihood disappear along with Homo sapiens. If our successors indeed function on a different level of consciousness (or perhaps possess something beyond consciousness that we cannot even conceive), it seems doubtful that Christianity or Islam will be of interest to them, that their social organisation could be Communist or capitalist, or that their genders could be male or female.
And yet the great debates of history are important because at least the first generation of these gods would be shaped by the cultural ideas of their human designers. Would they be created in the image of capitalism, of Islam, or of feminism? The answer to this question might send them careening in entirely different directions.
Most people prefer not to think about it. Even the field of bioethics prefers to address another question, ‘What is it forbidden to do?’ Is it acceptable to carry out genetic experiments on living human beings? On aborted fetuses? On stem cells? Is it ethical to clone sheep? And chimpanzees? And what about humans? All of these are important questions, but it is naïve to imagine that we might simply hit the brakes and stop the scientific projects that are upgrading Homo sapiens into a different kind of being. For these projects are inextricably meshed together with the Gilgamesh Project. Ask scientists why they study the genome, or try to connect a brain to a computer, or try to create a mind inside a computer. Nine out of ten times you’ll get the same standard answer: we are doing it to cure diseases and save human lives. Even though the implications of creating a mind inside a computer are far more dramatic than curing psychiatric illnesses, this is the standard justification given, because nobody can argue with it. This is why the Gilgamesh Project is the flagship of science. It serves to justify everything science does. Dr Frankenstein piggybacks on the shoulders of Gilgamesh. Since it is impossible to stop Gilgamesh, it is also impossible to stop Dr Frankenstein.
The only thing we can try to do is to influence the direction scientists are taking. Since we might soon be able to engineer our desires too, perhaps the real question facing us is not ‘What do we want to become?’, but ‘What do we want to want?’ Those who are not spooked by this question probably haven’t given it enough thought.Afterword:
The Animal that Became a God

SEVENTY THOUSAND YEARS AGO, HOMO sapiens was still an insignificant animal minding its own business in a corner of Africa. In the following millennia it transformed itself into the master of the entire planet and the terror of the ecosystem. Today it stands on the verge of becoming a god, poised to acquire not only eternal youth, but also the divine abilities of creation and destruction.
Unfortunately, the Sapiens regime on earth has so far produced little that we can be proud of. We have mastered our surroundings, increased food production, built cities, established empires and created far-flung trade networks. But did we decrease the amount of suffering in the world? Time and again, massive increases in human power did not necessarily improve the well-being of individual Sapiens, and usually caused immense misery to other animals.
In the last few decades we have at last made some real progress as far as the human condition is concerned, with the reduction of famine, plague and war. Yet the situation of other animals is deteriorating more rapidly than ever before, and the improvement in the lot of humanity is too recent and fragile to be certain of.
Moreover, despite the astonishing things that humans are capable of doing, we remain unsure of our goals and we seem to be as discontented as ever. We have advanced from canoes to galleys to steamships to space shuttles – but nobody knows where we’re going. We are more powerful than ever before, but have very little idea what to do with all that power. Worse still, humans seem to be more irresponsible than ever. Self-made gods with only the laws of physics to keep us company, we are accountable to no one. We are consequently wreaking havoc on our fellow animals and on the surrounding ecosystem, seeking little more than our own comfort and amusement, yet never finding satisfaction.
Is there anything more dangerous than dissatisfied and irresponsible gods who don’t know what they want?
