{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen eines einfachen Chatbots from Scratch in Python (mit NLTK)\n",
    "\n",
    "Die Geschichte der Chatbots geht auf das Jahr 1966 zurück, als Weizenbaum ein Computerprogramm namens ELIZA erfand. Es imitierte die Sprache eines Psychotherapeuten aus nur 200 Zeilen Code. Siehe: [Eliza](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm?utm_source=ubisend.com&utm_medium=blog-link&utm_campaign=ubisend). \n",
    "\n",
    "Ähnlich wie Weizenbaum erstellen wir einen sehr einfachen Chatbot, der jedoch die NLTK-Bibliothek von Python verwendet. Einen sehr einfachen Bot mit kaum kognitiven Fähigkeiten, aber ein kleiner Einstieg in NLP einzusteigen und um Chatbots kennenzulernen...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction import text \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK importieren\n",
    "NLTK(Natural Language Toolkit) ist eine führende Plattform für die Erstellung von Python-Programmen zur Natural Language Programming.. Es bietet benutzerfreundliche Schnittstellen zu über 50 Korpora- und lexikalischen Ressourcen wie WordNet sowie eine Reihe von Textverarbeitungsbibliotheken für Klassifizierung, Tokenisierung, Stemming, Tagging, Parsing und semantisches Denken, auch Wrapper für industrietaugliche NLP-Bibliotheken.\n",
    "\n",
    "[Natural Language Processing with Python](http://www.nltk.org/book/) bietet eine praktische Einführung in die Programmierung der Sprachverarbeitung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import GermanStemmer #from nltk.stem import german stemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text # um stopwortliste zu erweitern\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenen Text einlesen, z.B.:\n",
    "\n",
    "Die Reden von Christian Lindner von 2017-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('./data/lindner-talks_17-19.txt','r',errors = 'ignore', encoding='utf-8')\n",
    "raw = f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorverarbeitung des Textdatensatzes in Schritten:\n",
    "\n",
    "* Umwandlung des gesamten Textes in **Großbuchstaben** oder **Kleinbuchstaben**, so dass der Algorithmus nicht in verschiedenen Fällen die gleichen Wörter als unterschiedlich behandelt.\n",
    "\n",
    "* * **Tokenisierung**: Tokenisierung ist nur der Begriff, der verwendet wird, um den Prozess der Umwandlung der normalen Textzeichenketten in eine Liste von Token zu beschreiben.\n",
    "\n",
    "Das NLTK-Datenpaket enthält einen pretrained Punkt-Tokenizer für Englisch\n",
    "\n",
    "* Entfernen von **Rauschen**, d.h. alles, was nicht in einer Standardzahl oder einem Standardbrief enthalten ist.\n",
    "\n",
    "* Entfernen der **Stop-Wörter**. Manchmal werden einige sehr gebräuchliche Wörter, vollständig aus dem Vokabular ausgeschlossen. Diese Wörter werden als Stoppwörter bezeichnet.\n",
    "\n",
    "* * **Stemming**: Stemming ist der Prozess der Reduzierung von gebeugten (oder manchmal abgeleiteten) Wörtern auf ihre Stamm-, Basis- oder Wurzelform. (wir benutzen den nltk_german-stemmer) \n",
    "\n",
    "* * **Lemmatisierung**: Eine leichte Variante des Stemmens ist die Lemmatisierung. Der Hauptunterschied zwischen diesen besteht darin, dass das Stemmen oft nicht existierende Wörter hervorbringt, während Lemmata tatsächliche Wörter sind. \n",
    "Ein Beispiel für Lemmatisierung ist, dass das Wort \"better\" und \"good\" in derselben Lemma stehen, so dass sie als gleich angesehen werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Wir werden nun unseren individuellen Tokenizer als eine `Funktion namens my_tokenizer definieren`, die die Token als Eingabe übernimmt und normalisierte Token zurückgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deutsche stopwörter in die stopwords-datenbank hinzufügen\n",
    "\n",
    "#1. stopwörter finden:\n",
    "\n",
    "stop_words=set(stopwords.words(\"german\"))\n",
    "#print(stop_words)\n",
    "\n",
    "#2. stopwörter hinzufügen\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"diesen\", \"es\", \"für\", \"hin\", \"sehr\", \"deinem\", \"ihrem\", \"ob\", \"anderes\", \"eurer\", \"jede\", \"dass\", \"der\", \"ihn\", \"ist\", \"zu\", \"bis\", \"einen\", \"hinter\", \"sind\", \"unseres\", \"würden\", \"euch\", \"deinen\", \"meines\", \"warst\", \"anderen\", \"mich\", \"muss\", \"am\", \"von\", \"wo\", \"sollte\", \"ich\", \"keine\", \"deine\", \"meinen\", \"nichts\", \"nicht\", \"diesem\", \"hat\", \"jenem\", \"wollen\", \"mir\", \"habe\", \"dort\", \"solchen\", \"seine\", \"die\", \"etwas\", \"dein\", \"könnte\", \"solche\", \"können\", \"keinem\", \"dieselben\", \"jenes\", \"auch\", \"einiger\", \"diese\", \"weil\", \"jener\", \"gewesen\", \"kein\", \"nun\", \"anderer\", \"welcher\", \"anderem\", \"haben\", \"dieses\", \"zur\", \"was\", \"durch\", \"ihnen\", \"mein\", \"weiter\", \"im\", \"wenn\", \"wieder\", \"werde\", \"selbst\", \"keines\", \"ihrer\", \"nur\", \"eure\", \"andern\", \"ander\", \"zwar\", \"bei\", \"ein\", \"würde\", \"man\", \"aller\", \"werden\", \"anders\", \"anderm\", \"machen\", \"aus\", \"seinem\", \"auf\", \"in\", \"sondern\", \"meinem\", \"sein\", \"waren\", \"während\", \"welchen\", \"desselben\", \"unseren\", \"alles\", \"also\", \"denselben\", \"euren\", \"manches\", \"jedes\", \"unter\", \"dasselbe\", \"eurem\", \"zwischen\", \"musste\", \"eures\", \"euer\", \"will\", \"da\", \"dann\", \"soll\", \"über\", \"sich\", \"seines\", \"jetzt\", \"dazu\", \"manche\", \"hier\", \"dies\", \"allen\", \"damit\", \"deiner\", \"doch\", \"meine\", \"derselben\", \"jenen\", \"einigem\", \"seiner\", \"einmal\", \"einer\", \"manchem\", \"demselben\", \"unsere\", \"wir\", \"derselbe\", \"welche\", \"jedem\", \"und\", \"einiges\", \"hatten\", \"war\", \"keiner\", \"einig\", \"zum\", \"wie\", \"denn\", \"du\", \"einige\", \"einigen\", \"welchem\", \"ihm\", \"des\", \"einem\", \"das\", \"andere\", \"eines\", \"dich\", \"alle\", \"vor\", \"weg\", \"uns\", \"meiner\", \"dessen\", \"jene\", \"sie\", \"derer\", \"ihr\", \"solcher\", \"hab\", \"er\", \"noch\", \"bist\", \"anderr\", \"manchen\", \"deines\", \"dieser\", \"mit\", \"als\", \"an\", \"dem\", \"keinen\", \"den\", \"ihres\", \"eine\", \"bin\", \"unserem\", \"solchem\", \"wollte\", \"so\", \"jeder\", \"hatte\", \"jeden\", \"wirst\", \"ohne\", \"dir\", \"allem\", \"ins\", \"seinen\", \"ihren\", \"oder\", \"viel\", \"welches\", \"solches\", \"unser\", \"wird\", \"nach\", \"um\", \"aber\", \"daß\", \"vom\", \"sonst\", \"ihre\", \"dieselbe\", \"kann\", \"gegen\", \"indem\", \"mancher\"])\n",
    "#print(my_stop_words)\n",
    "\n",
    "#3. deutschen tokenizer als Funktion definieren\n",
    "\n",
    "def my_tokenizer(doc):\n",
    "   stemmer = GermanStemmer()\n",
    "   return(' '.join ([stemmer.stem(t.lower()) \n",
    "                     for t in nltk.word_tokenize(doc) \n",
    "                     if t.lower() \n",
    "                     not in my_stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword-Matching\n",
    "\n",
    "Als nächstes definieren wir eine Funktion für eine Begrüßung durch den Bot, d.h. wenn die Eingabe eines Benutzers eine Begrüßung ist, soll der Bot eine Begrüßungsantwort zurückgeben. ELIZA verwendet ein einfaches Keyword-Matching für Begrüßungen. Wir werden hier das gleiche Konzept anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hallo\", \"guten tag\", \"servus\", \"wie geht es ihnen\", \"wie geht es dir\",\"hi\",)\n",
    "GREETING_RESPONSES = [\"lieber parteifreund\", \"lieber mensch\", \"*nods*\", \"hallo\", \"lieber freund\", \"ich freue mich, dass sie sich mit mir unterhalten möchten\"]\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generierung von Antworten\n",
    "\n",
    "### Bag of Words\n",
    "Nach der ersten Vorverarbeitungsphase wandeln wir den Text in einen sinnvollen Vektor (oder ein Array) von Zahlen um. Der BoW ist eine Darstellung von Text, der das Auftreten von Wörtern in einem Dokument beschreibt. Es geht um zwei Dinge:\n",
    "\n",
    "* Ein Vokabular bekannter Wörter.\n",
    "\n",
    "* Ein Maß für das Vorhandensein bekannter Wörter.\n",
    "\n",
    "Warum wird es als \"Bag\" mit Wörtern bezeichnet? Das liegt daran, dass alle Informationen über die Reihenfolge oder Struktur der Wörter im Dokument verworfen werden und das Modell sich nur damit beschäftigt, **ob die bekannten Wörter im Dokument vorkommen, nicht aber, wo sie im Dokument vorkommen.**\n",
    "\n",
    "Die Intuition hinter dem Bag of Words ist, dass Dokumente ähnlich sind, wenn sie einen ähnlichen Inhalt haben. Auch können wir etwas über die Bedeutung des Dokuments allein aus seinem Inhalt lernen.\n",
    "\n",
    "Wenn unser Wörterbuch zum Beispiel die Wörter {Learning, is, the, not, great} enthält und wir den Text \"Learning is great\" vektorisieren wollen, hätten wir den folgenden Vektor: (1, 1, 0, 0, 1).\n",
    "\n",
    "\n",
    "### TF-IDF-Ansatz\n",
    "Ein Problem mit dem Bag of Words-Ansatz ist, dass sehr häufige Wörter beginnen, im Dokument zu dominieren (z.B. größere Punktzahl), aber möglicherweise nicht so viel \"Informationsgehalt\" enthalten. Außerdem wird es längeren Dokumenten mehr Gewicht verleihen als kürzeren Dokumenten.\n",
    "\n",
    "Ein Ansatz besteht darin, die Häufigkeit der Wörter danach zu skalieren, wie oft sie in allen Dokumenten vorkommen, so dass die Ergebnisse für häufige Wörter wie \"die\", die auch in allen Dokumenten häufig vorkommen, bestraft werden. Dieser Ansatz zur Bewertung wird als Term Frequency-Inverse Document Frequency, kurz TF-IDF, bezeichnet:\n",
    "\n",
    "**Term Frequency: ist eine Bewertung der Häufigkeit des Wortes im aktuellen Dokument.**\n",
    "\n",
    "```\n",
    "TF = (Anzahl der Erscheinungen von Begriff t in einem Dokument)/(Anzahl der Begriffe im Dokument)\n",
    "```\n",
    "\n",
    "**Inverse Document Frequency: ist eine Bewertung, wie selten das Wort in allen Dokumenten ist.**\n",
    "\n",
    "```\n",
    "IDF = 1+log(N/n), wobei N die Anzahl der Dokumente und n die Anzahl der Dokumente ist, in denen ein Begriff t erschienen ist.\n",
    "```\n",
    "### Cosine Similarity\n",
    "\n",
    "Tf-idf-Gewicht ist ein Gewicht, das häufig bei der Informationsbeschaffung und beim Text-Mining verwendet wird. Diese Gewichtung ist ein statistisches Maß, um zu beurteilen, wie wichtig ein Wort für ein Dokument in einer Sammlung oder einem Korpus ist.\n",
    "\n",
    "```\n",
    "Kosinusähnlichkeit (d1, d2) = Punktprodukt (d1, d2) / |||d1|||||| * |||d2|||||\n",
    "```\n",
    "wobei d1,d2 zwei Nicht-Null-Vektoren sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Um eine Antwort von unserem Bot für Eingabefragen zu generieren, wird das Konzept der Dokumentenähnlichkeit (document similarity) verwendet. Wir definieren eine Funktionsantwort, die die Äußerung des Benutzers nach einem oder mehreren bekannten Schlüsselwörtern durchsucht und eine von mehreren möglichen Antworten zurückgibt. Wenn es die Eingabe, die einem der Schlüsselwörter entspricht, nicht findet, gibt es eine Antwort zurück: \"Entschuldigen Sie bitte, ich verstehe Sie nicht!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    #using my own stopwordlist\n",
    "#    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='my_stop_words')\n",
    "    TfidfVec = TfidfVectorizer(preprocessor=my_tokenizer)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"Entschuldigen Sie bitte, ich verstehe Sie nicht!\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schließlich werden wir die Zeilen eingeben, die unser Bot sagen soll, während er ein Gespräch beginnt und beendet, je nach Eingabe des Benutzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDBOT: Mein Name ist Christian Lindner. Ich beantworte Ihre Fragen zu unseren Werten. Wenn Sie genug haben, tippen Sie Bye!\n",
      "hallo\n",
      "FDBOT: ich freue mich, dass sie sich mit mir unterhalten möchten\n",
      "wie stehts mit ihren werten?\n",
      "FDBOT: wie muss man das dann werten?\n",
      "für welche Werte stehen sie ein?\n",
      "FDBOT: wie muss man das dann werten?\n",
      "Für welche Werte steht die FDP?\n",
      "FDBOT: wie muss man das dann werten?\n",
      "Erzählen Sie mir von Ihrem Parteiprogramm\n",
      "FDBOT: die\n",
      "wettbewerbsfähigkeit deutschlands in der zukunft und die fairness unserer gesellschaft\n",
      "werden nicht zuerst vom steuersystem sondern vom bildungssystem entschieden und\n",
      "deshalb beginnt unsere politische erzählung an dieser stelle.\n",
      "welche Rolle wpielt künstliche intelligenz für sie?\n",
      "FDBOT: bis 2020 sollen\n",
      "50 forschungseinrichtungen für künstliche intelligenz entstehen.\n",
      "Wo?\n",
      "FDBOT: Entschuldigen Sie bitte, ich verstehe Sie nicht!\n",
      "wo entstehen diese forschungseinrichtungen?\n",
      "FDBOT: bis 2020 sollen\n",
      "50 forschungseinrichtungen für künstliche intelligenz entstehen.\n",
      "mit entwicklern?\n",
      "FDBOT: welche\n",
      "entwicklung geht die eurozone.\n",
      "entwickler künstlicher intelligenz\n",
      "FDBOT: bis 2020 sollen\n",
      "50 forschungseinrichtungen für künstliche intelligenz entstehen.\n",
      "in Bäckereien?\n",
      "FDBOT: wer mit der jetzigen lage zufrieden ist und sie für so\n",
      "selbstverständlich hält wie frische brötchen in der bäckerei, der gibt sich einer\n",
      "wohlstandshalluzination hin.\n",
      "und ausländer?\n",
      "FDBOT: an ausländer.\n",
      "illegal?\n",
      "FDBOT: man kann beim bäcker in der schlange nicht unterscheiden, wenn einer\n",
      "mit gebrochenen deutsch ein brötchen bestellt, ob das der hochqualifizierte\n",
      "entwickler künstlicher intelligenz aus indien ist oder eigentlich ein sich bei uns\n",
      "illegal au altender, höchstens geduldeter ausländer.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"FDBOT: Mein Name ist Christian Lindner. Ich beantworte Ihre Fragen zu unseren Werten. Wenn Sie genug haben, tippen Sie Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"FDBOT: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"FDBOT: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"FDBOT: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"FDBOT: Auf Wiedersehn!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
