{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Net in Tensorflow & Keras\n",
    "https://keras.io/\n",
    "![Image](./data/02_network_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Keras Python Machine Learning API importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# importieren der gesamten MNIST Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe der Shape (Form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe der Dimension des Arrays mit `.nidm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_images.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# CNN erstellen\n",
    "\n",
    "...im Prinzip wieder genauso wie bei einem `fully connected` Netz benötigen wir:\n",
    "\n",
    "1. `Layer` \n",
    "\n",
    "2. `Eingabedaten` \n",
    "\n",
    "3. `Verlustfunktion`\n",
    "\n",
    "4. `Optimierer`\n",
    "<!--![Image](./data/nn.png)-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module (layers & models) importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisieren eines leeren Netzes\n",
    "#### mit dem Modul `models` bestimmen wir im folgenden einen linearen Stapel von Schichten `(Sequential)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mit dem Argument `layers` definieren wir eine Liste von Layern, die dem Modell hinzugefügt werden sollen und legen die jewewiligen Attribute fest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. wir definieren einen Stapel von Conv2D- und MaxPooling-Layern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.\n",
    "`conv2d_1` Output > 32 Filter, bzw. Kanäle und jeder dieser Filter enthält ein 26x26 Raster mit Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer (type)                 Output Shape              Param #   \n",
    "#=================================================================\n",
    "#conv2d_1 (Conv2D)            (None, 26, 26, 32)        320 \n",
    "cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input_shape` = 28 * 28 * Graustufenwert (image_height, image_with, image_channels)\n",
    "![Image](./data/mnist-input.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Beispiel für eine Filteranwendung > `Filter` 3x3\n",
    "![Image](./data/conv2d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2.\n",
    "`max_pooling2d_1` halbiert von 26x26 auf 13x13 und nimmt / extrahiert kleine Fenster daraus aus der Eingabe-Feature-Map, ausgehend von `conv2d_1` und liefert als Ausgabe dann die Maximalwerte weiter\n",
    "<img src=\"./data/maxpooling.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer (type)                 Output Shape              Param #   \n",
    "#=================================================================\n",
    "#max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0   \n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3.\n",
    "`conv2d_2` Output > 64 Filter, bzw. Kanäle und jeder dieser Filter enthält ein 11x11 Raster mit Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer (type)                 Output Shape              Param #   \n",
    "#=================================================================\n",
    "#conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.4.\n",
    "`max_pooling2D`  halbiert von 11x11 auf 5x5 (durch padding kleiner) und nimmt / extrahiert kleine Fenster daraus aus der Eingabe-Feature-Map, ausgehend von `conv2d_2` und liefert als Ausgabe dann die Maximalwerte weiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer (type)                 Output Shape              Param #   \n",
    "#=================================================================\n",
    "#max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.5.\n",
    "`conv2d_3` Output > 64 Filter, bzw. Kanäle und jeder dieser Filter enthält ein 3x3 Raster mit Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer (type)                 Output Shape              Param #   \n",
    "#=================================================================\n",
    "#conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darstellung der Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## 2. nun definieren wir einen Stapel von Dense Layern als Klassifizierer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network.add(layers.Dense(10, activation='softmax'))\n",
    "cnn_model.add(layers.Flatten())\n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `Flatten` Vektorisiert die Shape auf 1-Dim, da Fully-Connected `Dense`-Layer nur eindimensionale Vektoren verarbeiten können\n",
    "+ `64` bzw `10` = Anzahl der (fully-connected) Knoten\n",
    "+ ...auf die letzten Endes die `softmax` funktion angewandt wird:\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "# Darstellung des gesamten Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "import graphviz\n",
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(cnn_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainings- bzw. Testdaten vorbereiten, bzw. skalieren:\n",
    "<img src=\"./data/scale.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "#print(\"Datentyp before: \", test_images.dtype)\n",
    "#print(test_images[1])\n",
    "test_images = test_images.astype('float32') / 255\n",
    "#print(\"Datentyp after: \", test_images.dtype)\n",
    "#print(test_images[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Kodieren der Klassen (Ziffern) von 0 bis 9 - mit One-Hot-Kodierung .:\n",
    "<img src=\"./data/one-hot.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel Label vor der Encodierung:  0\n",
      "Beilspiel Label nach der Encodierung:  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "print(\"Beispiel Label vor der Encodierung: \", train_labels[1])\n",
    "train_labels = to_categorical(train_labels)\n",
    "print(\"Beilspiel Label nach der Encodierung: \", train_labels[1])\n",
    "\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# kompilieren des Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `optimizer` Gewichtsanpassung https://keras.io/optimizers/ https://keras.io/optimizers/#rmsprop ...hier findet die Backpropagation (basierend auf Output der Verlustfunktion) statt\n",
    "\n",
    "Verschiedene Optimierer können eines oder mehrere Konzepte (Algorithmen) nutzen, um die Effizienz des Gradientenabfalls bei einem bestimmten Trainingssatz zu verbessern - hier eine Liste von Gradientenabstiegsalgorithmen: http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms\n",
    "\n",
    "Ein Optimizer ist eine spezifische Implementierung des Gradientenabstiegs- Algorithmus. \n",
    "\n",
    "\n",
    "    Gewichtsaktualisierungsformel mathematisch ausgedrückt:\n",
    "        delta_wjk = alpha * Ek * OK (1-OK) * OJT\n",
    "        \n",
    "            delta_wjk = Gewichtsänderung der Gewichte zwischen versteckter (j) und Ausgabeschicht (k)\n",
    "            alpha     = Lernrate \n",
    "            EK        = Fehlerwerte der Ausgabeschicht << output_errors\n",
    "            OK        = Ausgabewerte der Ausgabeschicht << final_outputs\n",
    "            OjT       = Ausgabewerte der versteckten Schicht << hidden_outputs #was heißt T????\n",
    "        \n",
    "    Gewichtsaktualisierungsformel in Python:\n",
    "        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), \n",
    "                                        numpy.transpose(hidden_outputs))\n",
    "\n",
    "            self.lr    = Lernrate\n",
    "            numpy.dot()= Matrizenmultiplikation\n",
    "            +=         = Addiere zum Wert self.who das Ergebnis des Ausdrucks auf der rechten Seite\n",
    "            hidden_outputs = Ausgabewerte der versteckten Layer\n",
    "            final_outputs  = Ausgabewerte des Outputlayers\n",
    "            ouput_errors  = Fehlerwerte des Outputlayers \n",
    "\n",
    "+ `loss` Verlustfunktion E = S-K https://keras.io/losses/ https://keras.io/backend/#categorical_crossentropy\n",
    "Verlustfunktion basierend auf dem absoluten Wert der Differenz zwischen den von einem Modell vorhergesagten Werten und den tatsächlichen Werten der Beschriftungen\n",
    "\n",
    "  + Kreuzvalidierungsverfahren sind Testverfahren der Statistik bzw. der Datenanalyse\n",
    "\n",
    "\n",
    "     Fehlerberechnung mathematisch ausgedrückt:\n",
    "        E = S - K \n",
    "\n",
    "            Fehler = Sollwert - aktuellem Istwert\n",
    "\n",
    "                       (  w11          w12  )\n",
    "                       |-------     ------- |\n",
    "                       |w11+w21     w12+w22 |   ( e1 )          ( w11   w12 )   ( e1 )\n",
    "        error_hidden = |                    | * |    |     =    |           | * |    |\n",
    "                       |  w21          w22  |   ( e2 )          (w21    w22 )   ( e2 )\n",
    "                       |-------     ------- |\n",
    "                       (w21+w11     w22+w12 )\n",
    "        \n",
    "        error_hidden = w_hidden-output         * error_output          \n",
    "\n",
    "            hidden_errors = Gewichte zwischen Eingabe- und versteckter Schicht\n",
    "            output_errors = Gewichte zwischen versteckter- und Ausgabeschicht   \n",
    "                    output_errors = targets - final_outputs\n",
    "                    targets = Optimalwerte aus der Testphase (Sollwert)\n",
    "\n",
    "    Fehlerberechnung in Python:\n",
    "        hidden_errors = numpy.dot(self.who.T, output_errors)      \n",
    "\n",
    "+ `metrics`, definiert den Anteil der Bilder, die korrekt klassifiziert wurden https://keras.io/metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# trainieren des Netzes\n",
    "+ `fit()` übergibt dem Modell (NN) dee Numpy-Arrays der Eingabedaten `train_images` + Zielwerte `train_labels` \n",
    "+ `epochs` wie oft wir den gesamten Trainingssatz  durchlaufen möchten \n",
    "+ `batch_size`, Stapelgröße - wie viele Samples wir für eine Aktualisierung der Modellgewichte verwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.1718 - accuracy: 0.9462\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0465 - accuracy: 0.9855\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0325 - accuracy: 0.9900\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.0252 - accuracy: 0.9924\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0203 - accuracy: 0.9938\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0156 - accuracy: 0.9954\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0126 - accuracy: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc7c7713a90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_images, train_labels, epochs=7, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 82us/step\n",
      "test_loss (Verlustrate): 0.028279399598229883\n",
      "test_acc (Korrektklassifzierungsrate): 0.9922999739646912\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = cnn_model.evaluate(test_images, test_labels)\n",
    "print('test_loss (Verlustrate):', test_loss)\n",
    "print('test_acc (Korrektklassifzierungsrate):', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzeige:\n",
    "+ `loss` Verlustfunktion\n",
    "+ `acc` accuracy, Genauigkeit der Trainningsdaten, Korrektklassifizierungsrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# visuelle Evaluation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorhersage:\n",
      "[1.9838503e-15 4.9550327e-13 1.5146971e-13 1.2228893e-10 2.7463810e-12\n",
      " 1.3398654e-13 3.2413297e-22 1.0000000e+00 1.5337268e-14 1.6315378e-11]\n",
      "7\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] \n",
      "\n",
      "Originalbild aus der Datenbank:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'imshow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-8d63606b4ca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Originalbild aus der Datenbank:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_im\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'imshow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = cnn_model.predict(test_images)\n",
    "print(\"Vorhersage:\")\n",
    "print(predictions[17])\n",
    "print(np.argmax(predictions[17]))\n",
    "print(test_labels[17], \"\\n\")\n",
    "\n",
    "print(\"Originalbild aus der Datenbank:\")\n",
    "test_im = test_images[17]\n",
    "plt.imshow(test_im.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Modell speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Save model\n",
    "cnn_model.save('mnist_trained_cnn_model.h5')  # creates a HDF5 file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
